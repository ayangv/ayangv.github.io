[{"title":"Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门","date":"2018-09-17T16:00:00.000Z","path":"2018/09/18/flink-install/","text":"准备工作1、安装查看 Java 的版本号，推荐使用 Java 8。 安装 Flink2、在 Mac OS X 上安装 Flink 是非常方便的。推荐通过 homebrew 来安装。 1brew install apache-flink 3、检查安装： 1flink --version 结果： 1Version: 1.6.0, Commit ID: ff472b4 4、启动 flink 1234567891011121314进入安装目录：cd /usr/local/Cellar/apache-flink/1.6.0/libexec/bin执行下面命令./start-cluster.sh启动日志：Starting cluster.Starting standalonesession daemon on host zhisheng.Starting taskexecutor daemon on host zhisheng. 接着就可以进入 web 页面(http://localhost:8081/) 查看 demo1、新建一个 maven 项目 创建一个 SocketTextStreamWordCount 文件，加入以下代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com.zhisheng.flink;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.util.Collector;/** * Created by zhisheng_tian on 2018/9/18 */public class SocketTextStreamWordCount &#123; public static void main(String[] args) throws Exception &#123; //参数检查 if (args.length != 2) &#123; System.err.println(\"USAGE:\\nSocketTextStreamWordCount &lt;hostname&gt; &lt;port&gt;\"); return; &#125; String hostname = args[0]; Integer port = Integer.parseInt(args[1]); // set up the streaming execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //获取数据 DataStreamSource&lt;String&gt; stream = env.socketTextStream(hostname, port); //计数 SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = stream.flatMap(new LineSplitter()) .keyBy(0) .sum(1); sum.print(); env.execute(\"Java WordCount from SocketTextStream Example\"); &#125; public static final class LineSplitter implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; @Override public void flatMap(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector) &#123; String[] tokens = s.toLowerCase().split(\"\\\\W+\"); for (String token: tokens) &#123; if (token.length() &gt; 0) &#123; collector.collect(new Tuple2&lt;String, Integer&gt;(token, 1)); &#125; &#125; &#125; &#125;&#125; 接着进入工程目录，使用以下命令打包。 1mvn clean package -Dmaven.test.skip=true 然后我们开启监听 9000 端口: 1nc -l 9000 最后进入 flink 安装目录 bin 下执行以下命令跑程序： 1flink run -c com.zhisheng.flink.SocketTextStreamWordCount /Users/zhisheng/IdeaProjects/flink/word-count/target/original-word-count-1.0-SNAPSHOT.jar 127.0.0.1 9000 注意换成你自己项目的路径。 执行完上述命令后，我们可以在 webUI 中看到正在运行的程序： 我们可以在 nc 监听端口中输入 text，比如： 然后我们通过 tail 命令看一下输出的 log 文件，来观察统计结果。进入目录 apache-flink/1.6.0/libexec/log，执行以下命令: 1tail -f flink-zhisheng-taskexecutor-0-zhisheng.out 注意：切换成你自己的路径和查看自己的目录。 总结本文描述了如何在 Mac 电脑上安装 Flink，及运行它。接着通过一个简单的 Flink 程序来介绍如何构建及运行Flink 程序。 关注我 转载请注明地址：http://www.54tianzhisheng.cn/2018/09/18/flink-install","tags":[{"name":"Flink","slug":"Flink","permalink":"http://yoursite.com/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/tags/大数据/"}]},{"title":"Go 并发——实现协程同步的几种方式","date":"2018-08-29T16:00:00.000Z","path":"2018/08/30/go-sync/","text":"前言Java 中有一系列的线程同步的方法，go 里面有 goroutine（协程），先看下下面的代码执行的结果是什么呢？ 123456789101112131415package mainimport ( \"fmt\")func main() &#123; go func() &#123; fmt.Println(\"Goroutine 1\") &#125;() go func() &#123; fmt.Println(\"Goroutine 2\") &#125;()&#125; 执行以上代码很可能看不到输出。 因为有可能这两个协程还没得到执行，主协程就已经结束了，而主协程结束时会结束所有其他协程，所以导致代码运行的结果什么都没有。 估计不少新接触 go 的童鞋都会对此郁闷😒，可能会问那么该如何等待主协程中创建的协程执行完毕之后再结束主协程呢？ 下面说几种可以解决的方法： Sleep 一段时间在 main 方法退出之前 sleep 一段时间就可能会出现结果了，如下代码： 123456789101112131415161718package mainimport ( \"fmt\" \"time\")func main() &#123; go func() &#123; fmt.Println(\"Goroutine 1\") &#125;() go func() &#123; fmt.Println(\"Goroutine 2\") &#125;() time.Sleep(time.Second * 1) // 睡眠1秒，等待上面两个协程结束&#125; 这两个简单的协程执行消耗的时间很短的，所以你会发现现在就有结果出现了。 12Goroutine 1Goroutine 2 为什么上面我要说 “可能会出现” ？ 因为 sleep 这个时间目前是设置的 1s，如果我这两个协程里面执行了很复杂的逻辑操作（时间大于 1s），那么就会发现依旧也是无结果打印出来的。 那么就可以发现这种方式得到问题所在了：我们无法确定需要睡眠多久 上面那种方式有问题，go 里面其实也可以用管道来实现同步的。 管道实现同步那么用管道怎么实现同步呢？show code： 123456789101112131415161718192021222324252627282930package mainimport ( \"fmt\")func main() &#123; ch := make(chan struct&#123;&#125;) count := 2 // count 表示活动的协程个数 go func() &#123; fmt.Println(\"Goroutine 1\") ch &lt;- struct&#123;&#125;&#123;&#125; // 协程结束，发出信号 &#125;() go func() &#123; fmt.Println(\"Goroutine 2\") ch &lt;- struct&#123;&#125;&#123;&#125; // 协程结束，发出信号 &#125;() for range ch &#123; // 每次从ch中接收数据，表明一个活动的协程结束 count-- // 当所有活动的协程都结束时，关闭管道 if count == 0 &#123; close(ch) &#125; &#125;&#125; 这种方式是一种比较完美的解决方案， goroutine / channel 它们也是在 go 里面经常搭配在一起的一对。 sync.WaitGroup其实 go 里面也提供了更简单的方式 —— 使用 sync.WaitGroup。 WaitGroup 顾名思义，就是用来等待一组操作完成的。WaitGroup 内部实现了一个计数器，用来记录未完成的操作个数，它提供了三个方法： Add() 用来添加计数 Done() 用来在操作结束时调用，使计数减一 Wait() 用来等待所有的操作结束，即计数变为 0，该函数会在计数不为 0 时等待，在计数为 0 时立即返回 继续 show code： 123456789101112131415161718192021222324package mainimport ( \"fmt\" \"sync\")func main() &#123; var wg sync.WaitGroup wg.Add(2) // 因为有两个动作，所以增加2个计数 go func() &#123; fmt.Println(\"Goroutine 1\") wg.Done() // 操作完成，减少一个计数 &#125;() go func() &#123; fmt.Println(\"Goroutine 2\") wg.Done() // 操作完成，减少一个计数 &#125;() wg.Wait() // 等待，直到计数为0&#125; 你会发现也是可以看到运行结果的，是不是发现这种方式是很简单的。 总结多看别人写的代码；多想想为啥要这样写；多查自己不理解的地方；多写 demo 测试；多写文章总结。 关注我 本文地址为：http://www.54tianzhisheng.cn/2018/08/30/go-sync/ ，转载请注明原文出处！","tags":[{"name":"GO","slug":"GO","permalink":"http://yoursite.com/tags/GO/"}]},{"title":"教你如何在 IDEA 远程 Debug ElasticSearch","date":"2018-08-13T16:00:00.000Z","path":"2018/08/14/idea-remote-debug-elasticsearch/","text":"前提之前在源码阅读环境搭建文章中写过我遇到的一个问题迟迟没有解决，也一直困扰着我。问题如下，在启动的时候解决掉其他异常和报错后，最后剩下这个错误一直解决不了： 12345678910111213141516171819202122[2018-08-01T09:44:27,370][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [] fatal error in thread [main], exitingjava.lang.NoClassDefFoundError: org/elasticsearch/plugins/ExtendedPluginsClassLoader at org.elasticsearch.plugins.PluginsService.loadBundle(PluginsService.java:632) ~[main/:?] at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:557) ~[main/:?] at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:162) ~[main/:?] at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:311) ~[main/:?] at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:252) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:213) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:213) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:326) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:136) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) ~[main/:?] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[main/:?] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[main/:?] at org.elasticsearch.cli.Command.main(Command.java:90) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) ~[main/:?]Caused by: java.lang.ClassNotFoundException: org.elasticsearch.plugins.ExtendedPluginsClassLoader at jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:582) ~[?:?] at jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:190) ~[?:?] at java.lang.ClassLoader.loadClass(ClassLoader.java:499) ~[?:?] ... 15 more 网上的解决办法也试了很多遍，包括自己也在 GitHub issue 提问了，也没能解决。然后后面自己分享文章在掘金也发现有人和我有同样的问题。 下面讲讲另一种可以让你继续看源码的方法。 远程 Debug前提条件是你之前已经把项目导入进 IDEA 了，如果你还没了解，请看之前的文章，这里不重复了。 启动一个实例在你 git 拉取下的代码，切换你要阅读的分支代码后，执行下面这条命令启动一个 debug 的实例： 1./gradlew run --debug-jvm 启动等会后，就可以看到启动好后的端口号为 8000 了。 配置 IDEA新建一个远程的 debug： 配置如下图： 接下来点击 OK 就好了。 然后点击下面的 debug 图标： 启动后如下： 这时就可以发现是可以把整个流程全启动了，也不会报什么错误！ 流程全启动后，你会发现终端的日志都打印出来了（注意：这时不是打印在你的 IDEA 控制台） 总结遇到问题，多思考，多搜索，多想办法解决！这样才能够不断的提升你解决问题的能力！ 关注我最后转载请务必注明文章出处为： http://www.54tianzhisheng.cn/2018/08/14/idea-remote-debug-elasticsearch/ 相关文章1、渣渣菜鸡为什么要看 ElasticSearch 源码？ 2、渣渣菜鸡的 ElasticSearch 源码解析 —— 环境搭建 3、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(上) 4、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(下) 5、Elasticsearch 系列文章（一）：Elasticsearch 默认分词器和中分分词器之间的比较及使用方法 6、Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程 7、Elasticsearch 系列文章（三）：ElasticSearch 集群监控 8、Elasticsearch 系列文章（四）：ElasticSearch 单个节点监控 9、Elasticsearch 系列文章（五）：ELK 实时日志分析平台环境搭建 10、教你如何在 IDEA 远程 Debug ElasticSearch","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://yoursite.com/tags/ElasticSearch/"}]},{"title":"渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程（下）","date":"2018-08-11T16:00:00.000Z","path":"2018/08/12/es-code03/","text":"关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/08/12/es-code03/ 前提上篇文章写完了 ES 流程启动的一部分，main 方法都入口，以及创建 Elasticsearch 运行的必须环境以及相关配置，接着就是创建该环境的节点了。 Node 的创建看下新建节点的代码：(代码比较多，这里是比较关键的地方，我就把注释直接写在代码上面了，实在不好拆开这段代码，300 多行代码) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278public Node(Environment environment) &#123; this(environment, Collections.emptyList()); //执行下面的代码 &#125;protected Node(final Environment environment, Collection&lt;Class&lt;? extends Plugin&gt;&gt; classpathPlugins) &#123; final List&lt;Closeable&gt; resourcesToClose = new ArrayList&lt;&gt;(); // register everything we need to release in the case of an error boolean success = false; &#123;// use temp logger just to say we are starting. we can't use it later on because the node name might not be set Logger logger = Loggers.getLogger(Node.class, NODE_NAME_SETTING.get(environment.settings())); logger.info(\"initializing ...\"); &#125; try &#123; originalSettings = environment.settings(); Settings tmpSettings = Settings.builder().put(environment.settings()) .put(Client.CLIENT_TYPE_SETTING_S.getKey(), CLIENT_TYPE).build();// create the node environment as soon as possible, to recover the node id and enable logging try &#123; nodeEnvironment = new NodeEnvironment(tmpSettings, environment); //1、创建节点环境,比如节点名称,节点ID,分片信息,存储元,以及分配内存准备给节点使用 resourcesToClose.add(nodeEnvironment); &#125; catch (IOException ex) &#123; throw new IllegalStateException(\"Failed to create node environment\", ex); &#125; final boolean hadPredefinedNodeName = NODE_NAME_SETTING.exists(tmpSettings); final String nodeId = nodeEnvironment.nodeId(); tmpSettings = addNodeNameIfNeeded(tmpSettings, nodeId); final Logger logger = Loggers.getLogger(Node.class, tmpSettings);// this must be captured after the node name is possibly added to the settings final String nodeName = NODE_NAME_SETTING.get(tmpSettings); if (hadPredefinedNodeName == false) &#123; logger.info(\"node name derived from node ID [&#123;&#125;]; set [&#123;&#125;] to override\", nodeId, NODE_NAME_SETTING.getKey()); &#125; else &#123; logger.info(\"node name [&#123;&#125;], node ID [&#123;&#125;]\", nodeName, nodeId); &#125; //2、打印出JVM相关信息 final JvmInfo jvmInfo = JvmInfo.jvmInfo(); logger.info(\"version[&#123;&#125;], pid[&#123;&#125;], build[&#123;&#125;/&#123;&#125;/&#123;&#125;/&#123;&#125;], OS[&#123;&#125;/&#123;&#125;/&#123;&#125;], JVM[&#123;&#125;/&#123;&#125;/&#123;&#125;/&#123;&#125;]\", Version.displayVersion(Version.CURRENT, Build.CURRENT.isSnapshot()), jvmInfo.pid(), Build.CURRENT.flavor().displayName(), Build.CURRENT.type().displayName(), Build.CURRENT.shortHash(), Build.CURRENT.date(), Constants.OS_NAME, Constants.OS_VERSION, Constants.OS_ARCH,Constants.JVM_VENDOR,Constants.JVM_NAME, Constants.JAVA_VERSION,Constants.JVM_VERSION); logger.info(\"JVM arguments &#123;&#125;\", Arrays.toString(jvmInfo.getInputArguments())); //检查当前版本是不是 pre-release 版本（Snapshot）， warnIfPreRelease(Version.CURRENT, Build.CURRENT.isSnapshot(), logger); 。。。 this.pluginsService = new PluginsService(tmpSettings, environment.configFile(), environment.modulesFile(), environment.pluginsFile(), classpathPlugins); //3、利用PluginsService加载相应的模块和插件 this.settings = pluginsService.updatedSettings(); localNodeFactory = new LocalNodeFactory(settings, nodeEnvironment.nodeId());// create the environment based on the finalized (processed) view of the settings// this is just to makes sure that people get the same settings, no matter where they ask them from this.environment = new Environment(this.settings, environment.configFile()); Environment.assertEquivalent(environment, this.environment); final List&lt;ExecutorBuilder&lt;?&gt;&gt; executorBuilders = pluginsService.getExecutorBuilders(settings); //线程池 final ThreadPool threadPool = new ThreadPool(settings, executorBuilders.toArray(new ExecutorBuilder[0])); resourcesToClose.add(() -&gt; ThreadPool.terminate(threadPool, 10, TimeUnit.SECONDS)); // adds the context to the DeprecationLogger so that it does not need to be injected everywhere DeprecationLogger.setThreadContext(threadPool.getThreadContext()); resourcesToClose.add(() -&gt; DeprecationLogger.removeThreadContext(threadPool.getThreadContext())); final List&lt;Setting&lt;?&gt;&gt; additionalSettings = new ArrayList&lt;&gt;(pluginsService.getPluginSettings()); //额外配置 final List&lt;String&gt; additionalSettingsFilter = new ArrayList&lt;&gt;(pluginsService.getPluginSettingsFilter()); for (final ExecutorBuilder&lt;?&gt; builder : threadPool.builders()) &#123; //4、加载一些额外配置 additionalSettings.addAll(builder.getRegisteredSettings()); &#125; client = new NodeClient(settings, threadPool);//5、创建一个节点客户端 //6、缓存一系列模块,如NodeModule,ClusterModule,IndicesModule,ActionModule,GatewayModule,SettingsModule,RepositioriesModule，scriptModule，analysisModule final ResourceWatcherService resourceWatcherService = new ResourceWatcherService(settings, threadPool); final ScriptModule scriptModule = new ScriptModule(settings, pluginsService.filterPlugins(ScriptPlugin.class)); AnalysisModule analysisModule = new AnalysisModule(this.environment, pluginsService.filterPlugins(AnalysisPlugin.class)); // this is as early as we can validate settings at this point. we already pass them to ScriptModule as well as ThreadPool so we might be late here already final SettingsModule settingsModule = new SettingsModule(this.settings, additionalSettings, additionalSettingsFilter);scriptModule.registerClusterSettingsListeners(settingsModule.getClusterSettings()); resourcesToClose.add(resourceWatcherService); final NetworkService networkService = new NetworkService( getCustomNameResolvers(pluginsService.filterPlugins(DiscoveryPlugin.class))); List&lt;ClusterPlugin&gt; clusterPlugins = pluginsService.filterPlugins(ClusterPlugin.class); final ClusterService clusterService = new ClusterService(settings, settingsModule.getClusterSettings(), threadPool, ClusterModule.getClusterStateCustomSuppliers(clusterPlugins)); clusterService.addStateApplier(scriptModule.getScriptService()); resourcesToClose.add(clusterService); final IngestService ingestService = new IngestService(settings, threadPool, this.environment, scriptModule.getScriptService(), analysisModule.getAnalysisRegistry(), pluginsService.filterPlugins(IngestPlugin.class)); final DiskThresholdMonitor listener = new DiskThresholdMonitor(settings, clusterService::state, clusterService.getClusterSettings(), client); final ClusterInfoService clusterInfoService = newClusterInfoService(settings, clusterService, threadPool, client,listener::onNewInfo); final UsageService usageService = new UsageService(settings); ModulesBuilder modules = new ModulesBuilder();// plugin modules must be added here, before others or we can get crazy injection errors... for (Module pluginModule : pluginsService.createGuiceModules()) &#123; modules.add(pluginModule); &#125; final MonitorService monitorService = new MonitorService(settings, nodeEnvironment, threadPool, clusterInfoService); ClusterModule clusterModule = new ClusterModule(settings, clusterService, clusterPlugins, clusterInfoService); modules.add(clusterModule); IndicesModule indicesModule = new IndicesModule(pluginsService.filterPlugins(MapperPlugin.class)); modules.add(indicesModule); SearchModule searchModule = new SearchModule(settings, false, pluginsService.filterPlugins(SearchPlugin.class)); CircuitBreakerService circuitBreakerService = createCircuitBreakerService(settingsModule.getSettings(), settingsModule.getClusterSettings()); resourcesToClose.add(circuitBreakerService); modules.add(new GatewayModule()); PageCacheRecycler pageCacheRecycler = createPageCacheRecycler(settings); BigArrays bigArrays = createBigArrays(pageCacheRecycler, circuitBreakerService); resourcesToClose.add(bigArrays); modules.add(settingsModule); List&lt;NamedWriteableRegistry.Entry&gt; namedWriteables = Stream.of( NetworkModule.getNamedWriteables().stream(), indicesModule.getNamedWriteables().stream(), searchModule.getNamedWriteables().stream(), pluginsService.filterPlugins(Plugin.class).stream() .flatMap(p -&gt; p.getNamedWriteables().stream()), ClusterModule.getNamedWriteables().stream()) .flatMap(Function.identity()).collect(Collectors.toList()); final NamedWriteableRegistry namedWriteableRegistry = new NamedWriteableRegistry(namedWriteables); NamedXContentRegistry xContentRegistry = new NamedXContentRegistry(Stream.of( NetworkModule.getNamedXContents().stream(), searchModule.getNamedXContents().stream(), pluginsService.filterPlugins(Plugin.class).stream() .flatMap(p -&gt; p.getNamedXContent().stream()), ClusterModule.getNamedXWriteables().stream()).flatMap(Function.identity()).collect(toList())); modules.add(new RepositoriesModule(this.environment, pluginsService.filterPlugins(RepositoryPlugin.class), xContentRegistry)); final MetaStateService metaStateService = new MetaStateService(settings, nodeEnvironment, xContentRegistry); final IndicesService indicesService = new IndicesService(settings, pluginsService, nodeEnvironment, xContentRegistry,analysisModule.getAnalysisRegistry(), clusterModule.getIndexNameExpressionResolver(), indicesModule.getMapperRegistry(), namedWriteableRegistry,threadPool, settingsModule.getIndexScopedSettings(), circuitBreakerService, bigArrays, scriptModule.getScriptService(),client, metaStateService); Collection&lt;Object&gt; pluginComponents = pluginsService.filterPlugins(Plugin.class).stream() .flatMap(p -&gt; p.createComponents(client, clusterService, threadPool, resourceWatcherService,scriptModule.getScriptService(), xContentRegistry, environment, nodeEnvironment,namedWriteableRegistry).stream()).collect(Collectors.toList()); ActionModule actionModule = new ActionModule(false, settings, clusterModule.getIndexNameExpressionResolver(), settingsModule.getIndexScopedSettings(), settingsModule.getClusterSettings(), settingsModule.getSettingsFilter(),threadPool, pluginsService.filterPlugins(ActionPlugin.class), client, circuitBreakerService, usageService); modules.add(actionModule); //7、获取RestController,用于处理各种Elasticsearch的rest命令,如_cat,_all,_cat/health,_clusters等rest命令(Elasticsearch称之为action) final RestController restController = actionModule.getRestController(); final NetworkModule networkModule = new NetworkModule(settings, false, pluginsService.filterPlugins(NetworkPlugin.class),threadPool, bigArrays, pageCacheRecycler, circuitBreakerService, namedWriteableRegistry, xContentRegistry,networkService, restController); Collection&lt;UnaryOperator&lt;Map&lt;String, MetaData.Custom&gt;&gt;&gt; customMetaDataUpgraders = pluginsService.filterPlugins(Plugin.class).stream() .map(Plugin::getCustomMetaDataUpgrader) .collect(Collectors.toList()); Collection&lt;UnaryOperator&lt;Map&lt;String, IndexTemplateMetaData&gt;&gt;&gt; indexTemplateMetaDataUpgraders = pluginsService.filterPlugins(Plugin.class).stream() .map(Plugin::getIndexTemplateMetaDataUpgrader) .collect(Collectors.toList()); Collection&lt;UnaryOperator&lt;IndexMetaData&gt;&gt; indexMetaDataUpgraders = pluginsService.filterPlugins(Plugin.class).stream() .map(Plugin::getIndexMetaDataUpgrader).collect(Collectors.toList()); final MetaDataUpgrader metaDataUpgrader = new MetaDataUpgrader(customMetaDataUpgraders, indexTemplateMetaDataUpgraders); final MetaDataIndexUpgradeService metaDataIndexUpgradeService = new MetaDataIndexUpgradeService(settings, xContentRegistry, indicesModule.getMapperRegistry(), settingsModule.getIndexScopedSettings(), indexMetaDataUpgraders); final GatewayMetaState gatewayMetaState = new GatewayMetaState(settings, nodeEnvironment, metaStateService, metaDataIndexUpgradeService, metaDataUpgrader); new TemplateUpgradeService(settings, client, clusterService, threadPool, indexTemplateMetaDataUpgraders); final Transport transport = networkModule.getTransportSupplier().get(); Set&lt;String&gt; taskHeaders = Stream.concat( pluginsService.filterPlugins(ActionPlugin.class).stream().flatMap(p -&gt; p.getTaskHeaders().stream()), Stream.of(\"X-Opaque-Id\") ).collect(Collectors.toSet()); final TransportService transportService = newTransportService(settings, transport, threadPool, networkModule.getTransportInterceptor(), localNodeFactory, settingsModule.getClusterSettings(), taskHeaders); final ResponseCollectorService responseCollectorService = new ResponseCollectorService(this.settings, clusterService); final SearchTransportService searchTransportService = new SearchTransportService(settings, transportService, SearchExecutionStatsCollector.makeWrapper(responseCollectorService)); final Consumer&lt;Binder&gt; httpBind; final HttpServerTransport httpServerTransport; if (networkModule.isHttpEnabled()) &#123; httpServerTransport = networkModule.getHttpServerTransportSupplier().get(); httpBind = b -&gt; &#123;b.bind(HttpServerTransport.class).toInstance(httpServerTransport); &#125;; &#125; else &#123; httpBind = b -&gt; &#123; b.bind(HttpServerTransport.class).toProvider(Providers.of(null)); &#125;; httpServerTransport = null; &#125; final DiscoveryModule discoveryModule = new DiscoveryModule(this.settings, threadPool, transportService, namedWriteableRegistry,networkService, clusterService.getMasterService(), clusterService.getClusterApplierService(),clusterService.getClusterSettings(), pluginsService.filterPlugins(DiscoveryPlugin.class),clusterModule.getAllocationService()); this.nodeService = new NodeService(settings, threadPool, monitorService, discoveryModule.getDiscovery(),transportService, indicesService, pluginsService, circuitBreakerService, scriptModule.getScriptService(),httpServerTransport, ingestService, clusterService, settingsModule.getSettingsFilter(), responseCollectorService,searchTransportService); final SearchService searchService = newSearchService(clusterService, indicesService, threadPool, scriptModule.getScriptService(), bigArrays, searchModule.getFetchPhase(),responseCollectorService); final List&lt;PersistentTasksExecutor&lt;?&gt;&gt; tasksExecutors = pluginsService .filterPlugins(PersistentTaskPlugin.class).stream() .map(p -&gt; p.getPersistentTasksExecutor(clusterService, threadPool, client)) .flatMap(List::stream) .collect(toList()); final PersistentTasksExecutorRegistry registry = new PersistentTasksExecutorRegistry(settings, tasksExecutors); final PersistentTasksClusterService persistentTasksClusterService = new PersistentTasksClusterService(settings, registry, clusterService); final PersistentTasksService persistentTasksService = new PersistentTasksService(settings, clusterService, threadPool, client);//8、绑定处理各种服务的实例,这里是最核心的地方,也是Elasticsearch能处理各种服务的核心. modules.add(b -&gt; &#123; b.bind(Node.class).toInstance(this); b.bind(NodeService.class).toInstance(nodeService); b.bind(NamedXContentRegistry.class).toInstance(xContentRegistry); b.bind(PluginsService.class).toInstance(pluginsService); b.bind(Client.class).toInstance(client); b.bind(NodeClient.class).toInstance(client); b.bind(Environment.class).toInstance(this.environment); b.bind(ThreadPool.class).toInstance(threadPool); b.bind(NodeEnvironment.class).toInstance(nodeEnvironment); b.bind(ResourceWatcherService.class).toInstance(resourceWatcherService);b.bind(CircuitBreakerService.class).toInstance(circuitBreakerService); b.bind(BigArrays.class).toInstance(bigArrays); b.bind(ScriptService.class).toInstance(scriptModule.getScriptService()); b.bind(AnalysisRegistry.class).toInstance(analysisModule.getAnalysisRegistry()); b.bind(IngestService.class).toInstance(ingestService); b.bind(UsageService.class).toInstance(usageService); b.bind(NamedWriteableRegistry.class).toInstance(namedWriteableRegistry); b.bind(MetaDataUpgrader.class).toInstance(metaDataUpgrader); b.bind(MetaStateService.class).toInstance(metaStateService); b.bind(IndicesService.class).toInstance(indicesService); b.bind(SearchService.class).toInstance(searchService); b.bind(SearchTransportService.class).toInstance(searchTransportService);b.bind(SearchPhaseController.class).toInstance(new SearchPhaseController(settings, searchService::createReduceContext)); b.bind(Transport.class).toInstance(transport); b.bind(TransportService.class).toInstance(transportService); b.bind(NetworkService.class).toInstance(networkService); b.bind(UpdateHelper.class).toInstance(new UpdateHelper(settings, scriptModule.getScriptService()));b.bind(MetaDataIndexUpgradeService.class).toInstance(metaDataIndexUpgradeService); b.bind(ClusterInfoService.class).toInstance(clusterInfoService); b.bind(GatewayMetaState.class).toInstance(gatewayMetaState); b.bind(Discovery.class).toInstance(discoveryModule.getDiscovery()); &#123; RecoverySettings recoverySettings = new RecoverySettings(settings, settingsModule.getClusterSettings()); processRecoverySettings(settingsModule.getClusterSettings(), recoverySettings); b.bind(PeerRecoverySourceService.class).toInstance(new PeerRecoverySourceService(settings, transportService,indicesService, recoverySettings)); b.bind(PeerRecoveryTargetService.class).toInstance(new PeerRecoveryTargetService(settings, threadPool,transportService, recoverySettings, clusterService)); &#125; httpBind.accept(b); pluginComponents.stream().forEach(p -&gt; b.bind((Class) p.getClass()).toInstance(p));b.bind(PersistentTasksService.class).toInstance(persistentTasksService); b.bind(PersistentTasksClusterService.class).toInstance(persistentTasksClusterService);b.bind(PersistentTasksExecutorRegistry.class).toInstance(registry); &#125;); injector = modules.createInjector(); // TODO hack around circular dependencies problems in AllocationServiceclusterModule.getAllocationService().setGatewayAllocator(injector.getInstance(GatewayAllocator.class)); List&lt;LifecycleComponent&gt; pluginLifecycleComponents = pluginComponents.stream() .filter(p -&gt; p instanceof LifecycleComponent) .map(p -&gt; (LifecycleComponent) p).collect(Collectors.toList()); //9、利用Guice将各种模块以及服务(xxxService)注入到Elasticsearch环境中pluginLifecycleComponents.addAll(pluginsService.getGuiceServiceClasses().stream() .map(injector::getInstance).collect(Collectors.toList())); resourcesToClose.addAll(pluginLifecycleComponents); this.pluginLifecycleComponents = Collections.unmodifiableList(pluginLifecycleComponents); client.initialize(injector.getInstance(new Key&lt;Map&lt;GenericAction, TransportAction&gt;&gt;() &#123;&#125;), () -&gt; clusterService.localNode().getId(), transportService.getRemoteClusterService()); if (NetworkModule.HTTP_ENABLED.get(settings)) &#123; //如果elasticsearch.yml文件中配置了http.enabled参数(默认为true),则会初始化RestHandlers logger.debug(\"initializing HTTP handlers ...\"); actionModule.initRestHandlers(() -&gt; clusterService.state().nodes()); //初始化RestHandlers, 解析集群命令,如_cat/,_cat/health &#125; //10、初始化工作完成 logger.info(\"initialized\"); success = true; &#125; catch (IOException ex) &#123; throw new ElasticsearchException(\"failed to bind service\", ex); &#125; finally &#123; if (!success) &#123; IOUtils.closeWhileHandlingException(resourcesToClose); &#125; &#125;&#125; 上面代码真的很多，这里再说下上面这么多代码主要干了什么吧：（具体是哪行代码执行的如下流程，上面代码中也标记了） 1、创建节点环境,比如节点名称,节点 ID,分片信息,存储元,以及分配内存准备给节点使用 2、打印出 JVM 相关信息 3、利用 PluginsService 加载相应的模块和插件，具体哪些模块可以去 modules 目录下查看 4、加载一些额外的配置参数 5、创建一个节点客户端 6、缓存一系列模块,如NodeModule,ClusterModule,IndicesModule,ActionModule,GatewayModule,SettingsModule,RepositioriesModule，scriptModule，analysisModule 7、获取 RestController，用于处理各种 Elasticsearch 的 rest 命令,如 _cat, _all, _cat/health, _clusters 等 rest命令 8、绑定处理各种服务的实例 9、利用 Guice 将各种模块以及服务(xxxService)注入到 Elasticsearch 环境中 10、初始化工作完成（打印日志） JarHell 报错解释前一篇阅读源码环境搭建的文章写过用 JDK 1.8 编译 ES 源码是会遇到如下异常： 1org.elasticsearch.bootstrap.StartupException: java.lang.IllegalStateException: jar hell! 这里说下就是 setup 方法中的如下代码导致的 1234567try &#123; // look for jar hell final Logger logger = ESLoggerFactory.getLogger(JarHell.class); JarHell.checkJarHell(logger::debug);&#125; catch (IOException | URISyntaxException e) &#123; throw new BootstrapException(e);&#125; 所以你如果是用 JDK 1.8 编译的，那么就需要把所有的有这块的代码给注释掉就可以编译成功的。 我自己试过用 JDK 10 编译是没有出现这里报错的。 正式启动 ES 节点回到上面 Bootstrap 中的静态 init 方法中，接下来就是正式启动 elasticsearch 节点了： 123456INSTANCE.start(); //调用下面的 start 方法private void start() throws NodeValidationException &#123; node.start(); //正式启动 Elasticsearch 节点 keepAliveThread.start();&#125; 接下来看看这个 start 方法里面的 node.start() 方法源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120public Node start() throws NodeValidationException &#123; if (!lifecycle.moveToStarted()) &#123; return this; &#125; Logger logger = Loggers.getLogger(Node.class, NODE_NAME_SETTING.get(settings)); logger.info(\"starting ...\"); pluginLifecycleComponents.forEach(LifecycleComponent::start); //1、利用Guice获取上述注册的各种模块以及服务 //Node 的启动其实就是 node 里每个组件的启动，同样的，分别调用不同的实例的 start 方法来启动这个组件, 如下： injector.getInstance(MappingUpdatedAction.class).setClient(client); injector.getInstance(IndicesService.class).start(); injector.getInstance(IndicesClusterStateService.class).start(); injector.getInstance(SnapshotsService.class).start(); injector.getInstance(SnapshotShardsService.class).start(); injector.getInstance(RoutingService.class).start(); injector.getInstance(SearchService.class).start(); nodeService.getMonitorService().start(); final ClusterService clusterService = injector.getInstance(ClusterService.class); final NodeConnectionsService nodeConnectionsService = injector.getInstance(NodeConnectionsService.class); nodeConnectionsService.start(); clusterService.setNodeConnectionsService(nodeConnectionsService); injector.getInstance(ResourceWatcherService.class).start(); injector.getInstance(GatewayService.class).start(); Discovery discovery = injector.getInstance(Discovery.class); clusterService.getMasterService().setClusterStatePublisher(discovery::publish); // Start the transport service now so the publish address will be added to the local disco node in ClusterService TransportService transportService = injector.getInstance(TransportService.class); transportService.getTaskManager().setTaskResultsService(injector.getInstance(TaskResultsService.class)); transportService.start(); assert localNodeFactory.getNode() != null; assert transportService.getLocalNode().equals(localNodeFactory.getNode()) : \"transportService has a different local node than the factory provided\"; final MetaData onDiskMetadata; try &#123; // we load the global state here (the persistent part of the cluster state stored on disk) to // pass it to the bootstrap checks to allow plugins to enforce certain preconditions based on the recovered state. if (DiscoveryNode.isMasterNode(settings) || DiscoveryNode.isDataNode(settings)) &#123;//根据配置文件看当前节点是master还是data节点 onDiskMetadata = injector.getInstance(GatewayMetaState.class).loadMetaState(); &#125; else &#123; onDiskMetadata = MetaData.EMPTY_META_DATA; &#125; assert onDiskMetadata != null : \"metadata is null but shouldn't\"; // this is never null &#125; catch (IOException e) &#123; throw new UncheckedIOException(e); &#125; validateNodeBeforeAcceptingRequests(new BootstrapContext(settings, onDiskMetadata), transportService.boundAddress(), pluginsService .filterPlugins(Plugin .class) .stream() .flatMap(p -&gt; p.getBootstrapChecks().stream()).collect(Collectors.toList())); //2、将当前节点加入到一个集群簇中去,并启动当前节点 clusterService.addStateApplier(transportService.getTaskManager()); // start after transport service so the local disco is known discovery.start(); // start before cluster service so that it can set initial state on ClusterApplierService clusterService.start(); assert clusterService.localNode().equals(localNodeFactory.getNode()) : \"clusterService has a different local node than the factory provided\"; transportService.acceptIncomingRequests(); discovery.startInitialJoin(); // tribe nodes don't have a master so we shouldn't register an observer s final TimeValue initialStateTimeout = DiscoverySettings.INITIAL_STATE_TIMEOUT_SETTING.get(settings); if (initialStateTimeout.millis() &gt; 0) &#123; final ThreadPool thread = injector.getInstance(ThreadPool.class); ClusterState clusterState = clusterService.state(); ClusterStateObserver observer = new ClusterStateObserver(clusterState, clusterService, null, logger, thread.getThreadContext()); if (clusterState.nodes().getMasterNodeId() == null) &#123; logger.debug(\"waiting to join the cluster. timeout [&#123;&#125;]\", initialStateTimeout); final CountDownLatch latch = new CountDownLatch(1); observer.waitForNextChange(new ClusterStateObserver.Listener() &#123; @Override public void onNewClusterState(ClusterState state) &#123; latch.countDown(); &#125; @Override public void onClusterServiceClose() &#123; latch.countDown(); &#125; @Override public void onTimeout(TimeValue timeout) &#123; logger.warn(\"timed out while waiting for initial discovery state - timeout: &#123;&#125;\", initialStateTimeout); latch.countDown(); &#125; &#125;, state -&gt; state.nodes().getMasterNodeId() != null, initialStateTimeout); try &#123; latch.await(); &#125; catch (InterruptedException e) &#123; throw new ElasticsearchTimeoutException(\"Interrupted while waiting for initial discovery state\"); &#125; &#125; &#125; if (NetworkModule.HTTP_ENABLED.get(settings)) &#123; injector.getInstance(HttpServerTransport.class).start(); &#125; if (WRITE_PORTS_FILE_SETTING.get(settings)) &#123; if (NetworkModule.HTTP_ENABLED.get(settings)) &#123; HttpServerTransport http = injector.getInstance(HttpServerTransport.class); writePortsFile(\"http\", http.boundAddress()); &#125; TransportService transport = injector.getInstance(TransportService.class); writePortsFile(\"transport\", transport.boundAddress()); &#125; logger.info(\"started\"); pluginsService.filterPlugins(ClusterPlugin.class).forEach(ClusterPlugin::onNodeStarted); return this;&#125; 上面代码主要是： 1、利用 Guice 获取上述注册的各种模块以及服务，然后启动 node 里每个组件（分别调用不同的实例的 start 方法来启动） 2、打印日志（启动节点完成） 总结这篇文章主要把大概启动流程串通了，讲了下 node 节点的创建和正式启动 ES 节点了。因为篇幅较多所以拆开成两篇，先不扣细节了，后面流程启动文章写完后我们再单一的扣细节。 相关文章1、渣渣菜鸡为什么要看 ElasticSearch 源码？ 2、渣渣菜鸡的 ElasticSearch 源码解析 —— 环境搭建 3、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(上) 4、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(下) 5、Elasticsearch 系列文章（一）：Elasticsearch 默认分词器和中分分词器之间的比较及使用方法 6、Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程 7、Elasticsearch 系列文章（三）：ElasticSearch 集群监控 8、Elasticsearch 系列文章（四）：ElasticSearch 单个节点监控 9、Elasticsearch 系列文章（五）：ELK 实时日志分析平台环境搭建 10、教你如何在 IDEA 远程 Debug ElasticSearch","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://yoursite.com/tags/ElasticSearch/"}]},{"title":"渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程（上）","date":"2018-08-10T16:00:00.000Z","path":"2018/08/11/es-code02/","text":"关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/08/11/es-code02/ 前提上篇文章写了 ElasticSearch 源码解析 —— 环境搭建 ，其中里面说了启动 打开 server 模块下的 Elasticsearch 类：org.elasticsearch.bootstrap.Elasticsearch，运行里面的 main 函数就可以启动 ElasticSearch 了，这篇文章讲讲启动流程，因为篇幅会很多，所以分了两篇来写。 启动流程main 方法入口可以看到入口其实是一个 main 方法，方法里面先是检查权限，然后是一个错误日志监听器（确保在日志配置之前状态日志没有出现 error），然后是 Elasticsearch 对象的创建，然后调用了静态方法 main 方法（18 行），并把创建的对象和参数以及 Terminal 默认值传进去。静态的 main 方法里面调用 elasticsearch.main 方法。 1234567891011121314151617181920public static void main(final String[] args) throws Exception &#123; //1、入口 // we want the JVM to think there is a security manager installed so that if internal policy decisions that would be based on the // presence of a security manager or lack thereof act as if there is a security manager present (e.g., DNS cache policy) System.setSecurityManager(new SecurityManager() &#123; @Override public void checkPermission(Permission perm) &#123; // grant all permissions so that we can later set the security manager to the one that we want &#125; &#125;); LogConfigurator.registerErrorListener(); // final Elasticsearch elasticsearch = new Elasticsearch(); int status = main(args, elasticsearch, Terminal.DEFAULT); //2、调用Elasticsearch.main方法 if (status != ExitCodes.OK) &#123; exit(status); &#125;&#125;static int main(final String[] args, final Elasticsearch elasticsearch, final Terminal terminal) throws Exception &#123; return elasticsearch.main(args, terminal); //3、command main&#125; 因为 Elasticsearch 类是继承了 EnvironmentAwareCommand 类，EnvironmentAwareCommand 类继承了 Command 类，但是 Elasticsearch 类并没有重写 main 方法，所以上面调用的 elasticsearch.main 其实是调用了 Command 的 main 方法，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839/** Parses options for this command from args and executes it. */public final int main(String[] args, Terminal terminal) throws Exception &#123; if (addShutdownHook()) &#123; //利用Runtime.getRuntime().addShutdownHook方法加入一个Hook，在程序退出时触发该Hook shutdownHookThread = new Thread(() -&gt; &#123; try &#123; this.close(); &#125; catch (final IOException e) &#123; try ( StringWriter sw = new StringWriter(); PrintWriter pw = new PrintWriter(sw)) &#123; e.printStackTrace(pw); terminal.println(sw.toString()); &#125; catch (final IOException impossible) &#123; // StringWriter#close declares a checked IOException from the Closeable interface but the Javadocs for StringWriter // say that an exception here is impossible throw new AssertionError(impossible); &#125; &#125; &#125;); Runtime.getRuntime().addShutdownHook(shutdownHookThread); &#125; beforeMain.run(); try &#123; mainWithoutErrorHandling(args, terminal);//4、mainWithoutErrorHandling &#125; catch (OptionException e) &#123; printHelp(terminal); terminal.println(Terminal.Verbosity.SILENT, \"ERROR: \" + e.getMessage()); return ExitCodes.USAGE; &#125; catch (UserException e) &#123; if (e.exitCode == ExitCodes.USAGE) &#123; printHelp(terminal); &#125; terminal.println(Terminal.Verbosity.SILENT, \"ERROR: \" + e.getMessage()); return e.exitCode; &#125; return ExitCodes.OK;&#125; 上面代码一开始利用一个勾子函数，在程序退出时触发该 Hook，该方法主要代码是 mainWithoutErrorHandling() 方法，然后下面的是 catch 住方法抛出的异常，方法代码如下： 12345678910111213141516/*** Executes the command, but all errors are thrown. */void mainWithoutErrorHandling(String[] args, Terminal terminal) throws Exception &#123; final OptionSet options = parser.parse(args); if (options.has(helpOption)) &#123; printHelp(terminal); return; &#125; if (options.has(silentOption)) &#123; terminal.setVerbosity(Terminal.Verbosity.SILENT); &#125; else if (options.has(verboseOption)) &#123; terminal.setVerbosity(Terminal.Verbosity.VERBOSE); &#125; else &#123; terminal.setVerbosity(Terminal.Verbosity.NORMAL); &#125; execute(terminal, options);//5、执行 EnvironmentAwareCommand 中的 execute()，（重写了command里面抽象的execute方法）&#125; 上面的代码从 3 ～ 14 行是解析传进来的参数并配置 terminal，重要的 execute() 方法，执行的是 EnvironmentAwareCommand 中的 execute() （重写了 Command 类里面的抽象 execute 方法），从上面那个继承图可以看到 EnvironmentAwareCommand 继承了 Command，重写的 execute 方法代码如下： 1234567891011121314151617181920212223@Overrideprotected void execute(Terminal terminal, OptionSet options) throws Exception &#123; final Map&lt;String, String&gt; settings = new HashMap&lt;&gt;(); for (final KeyValuePair kvp : settingOption.values(options)) &#123; if (kvp.value.isEmpty()) &#123; throw new UserException(ExitCodes.USAGE, \"setting [\" + kvp.key + \"] must not be empty\"); &#125; if (settings.containsKey(kvp.key)) &#123; final String message = String.format( Locale.ROOT, \"setting [%s] already set, saw [%s] and [%s]\", kvp.key, settings.get(kvp.key), kvp.value); throw new UserException(ExitCodes.USAGE, message); &#125; settings.put(kvp.key, kvp.value); &#125; //6、根据我们ide配置的 vm options 进行设置path.data、path.home、path.logs putSystemPropertyIfSettingIsMissing(settings, \"path.data\", \"es.path.data\"); putSystemPropertyIfSettingIsMissing(settings, \"path.home\", \"es.path.home\"); putSystemPropertyIfSettingIsMissing(settings, \"path.logs\", \"es.path.logs\"); execute(terminal, options, createEnv(terminal, settings));//7、先调用 createEnv 创建环境 //9、执行elasticsearch的execute方法，elasticsearch中重写了EnvironmentAwareCommand中的抽象execute方法&#125; 方法前面是根据传参去判断配置的，如果配置为空，就会直接跳到执行 putSystemPropertyIfSettingIsMissing 方法，这里会配置三个属性：path.data、path.home、path.logs 设置 es 的 data、home、logs 目录，它这里是根据我们 ide 配置的 vm options 进行设置的，这也是为什么我们上篇文章说的配置信息，如果不配置的话就会直接报错。下面看看 putSystemPropertyIfSettingIsMissing 方法代码里面怎么做到的： 12345678910111213141516/** Ensure the given setting exists, reading it from system properties if not already set. */private static void putSystemPropertyIfSettingIsMissing(final Map&lt;String, String&gt; settings, final String setting, final String key) &#123; final String value = System.getProperty(key);//获取key（es.path.data）找系统设置 if (value != null) &#123; if (settings.containsKey(setting)) &#123; final String message = String.format( Locale.ROOT, \"duplicate setting [%s] found via command-line [%s] and system property [%s]\", setting, settings.get(setting), value); throw new IllegalArgumentException(message); &#125; else &#123; settings.put(setting, value); &#125; &#125;&#125; 执行这三个方法后： 跳出此方法，继续看会发现 execute 方法调用了方法， 1execute(terminal, options, createEnv(terminal, settings)); 这里我们先看看 createEnv(terminal, settings) 方法： 1234567protected Environment createEnv(final Terminal terminal, final Map&lt;String, String&gt; settings) throws UserException &#123; final String esPathConf = System.getProperty(\"es.path.conf\");//8、读取我们 vm options 中配置的 es.path.conf if (esPathConf == null) &#123; throw new UserException(ExitCodes.CONFIG, \"the system property [es.path.conf] must be set\"); &#125; return InternalSettingsPreparer.prepareEnvironment(Settings.EMPTY, terminal, settings, getConfigPath(esPathConf)); //8、准备环境 prepareEnvironment&#125; 读取我们 ide vm options 中配置的 es.path.conf，同上篇文章也讲了这个一定要配置的，因为 es 启动的时候会加载我们的配置和一些插件。这里继续看下上面代码第 6 行的 prepareEnvironment 方法： 1234567891011121314151617181920212223242526272829303132333435public static Environment prepareEnvironment(Settings input, Terminal terminal, Map&lt;String, String&gt; properties, Path configPath) &#123; // just create enough settings to build the environment, to get the config dir Settings.Builder output = Settings.builder(); initializeSettings(output, input, properties); Environment environment = new Environment(output.build(), configPath); //查看 es.path.conf 目录下的配置文件是不是 yml 格式的，如果不是则抛出一个异常 if (Files.exists(environment.configFile().resolve(\"elasticsearch.yaml\"))) &#123; throw new SettingsException(\"elasticsearch.yaml was deprecated in 5.5.0 and must be renamed to elasticsearch.yml\"); &#125; if (Files.exists(environment.configFile().resolve(\"elasticsearch.json\"))) &#123; throw new SettingsException(\"elasticsearch.json was deprecated in 5.5.0 and must be converted to elasticsearch.yml\"); &#125; output = Settings.builder(); // start with a fresh output Path path = environment.configFile().resolve(\"elasticsearch.yml\"); if (Files.exists(path)) &#123; try &#123; output.loadFromPath(path); //加载文件并读取配置文件内容 &#125; catch (IOException e) &#123; throw new SettingsException(\"Failed to load settings from \" + path.toString(), e); &#125; &#125; // re-initialize settings now that the config file has been loaded initializeSettings(output, input, properties); //再一次初始化设置 finalizeSettings(output, terminal); environment = new Environment(output.build(), configPath); // we put back the path.logs so we can use it in the logging configuration file output.put(Environment.PATH_LOGS_SETTING.getKey(), environment.logsFile().toAbsolutePath().normalize().toString()); return new Environment(output.build(), configPath);&#125; 准备的环境如上图，通过构建的环境查看配置文件 elasticsearch.yml 是不是以 yml 结尾，如果是 yaml 或者 json 结尾的则抛出异常（在 5.5.0 版本其他两种格式过期了，只能使用 yml 格式），然后加载该配置文件并读取里面的内容（KV结构）。 跳出 createEnv 方法，我们继续看 execute 方法吧。 EnvironmentAwareCommand 类的 execute 方法代码如下： 1protected abstract void execute(Terminal terminal, OptionSet options, Environment env) throws Exception; 这是个抽象方法，那么它的实现方法在 Elasticsearch 类中，代码如下： 1234567891011121314151617181920212223242526272829303132333435@Overrideprotected void execute(Terminal terminal, OptionSet options, Environment env) throws UserException &#123; if (options.nonOptionArguments().isEmpty() == false) &#123; throw new UserException(ExitCodes.USAGE, \"Positional arguments not allowed, found \" + options.nonOptionArguments()); &#125; if (options.has(versionOption)) &#123; final String versionOutput = String.format( Locale.ROOT, \"Version: %s, Build: %s/%s/%s/%s, JVM: %s\", Version.displayVersion(Version.CURRENT, Build.CURRENT.isSnapshot()), Build.CURRENT.flavor().displayName(), Build.CURRENT.type().displayName(), Build.CURRENT.shortHash(), Build.CURRENT.date(), JvmInfo.jvmInfo().version()); terminal.println(versionOutput); return; &#125; final boolean daemonize = options.has(daemonizeOption); final Path pidFile = pidfileOption.value(options); final boolean quiet = options.has(quietOption); // a misconfigured java.io.tmpdir can cause hard-to-diagnose problems later, so reject it immediately try &#123; env.validateTmpFile(); &#125; catch (IOException e) &#123; throw new UserException(ExitCodes.CONFIG, e.getMessage()); &#125; try &#123; init(daemonize, pidFile, quiet, env); //10、初始化 &#125; catch (NodeValidationException e) &#123; throw new UserException(ExitCodes.CONFIG, e.getMessage()); &#125;&#125; 上面代码里主要还是看看 init(daemonize, pidFile, quiet, env); 初始化方法吧。 12345678910void init(final boolean daemonize, final Path pidFile, final boolean quiet, Environment initialEnv) throws NodeValidationException, UserException &#123; try &#123; Bootstrap.init(!daemonize, pidFile, quiet, initialEnv); //11、执行 Bootstrap 中的 init 方法 &#125; catch (BootstrapException | RuntimeException e) &#123; // format exceptions to the console in a special way // to avoid 2MB stacktraces from guice, etc. throw new StartupException(e); &#125;&#125; init 方法Bootstrap 中的静态 init 方法如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101static void init( final boolean foreground, final Path pidFile, final boolean quiet, final Environment initialEnv) throws BootstrapException, NodeValidationException, UserException &#123; // force the class initializer for BootstrapInfo to run before // the security manager is installed BootstrapInfo.init(); INSTANCE = new Bootstrap(); //12、创建一个 Bootstrap 实例 final SecureSettings keystore = loadSecureSettings(initialEnv);//如果注册了安全模块则将相关配置加载进来 final Environment environment = createEnvironment(foreground, pidFile, keystore, initialEnv.settings(), initialEnv.configFile()); //干之前干过的事情 try &#123; LogConfigurator.configure(environment); //13、log 配置环境 &#125; catch (IOException e) &#123; throw new BootstrapException(e); &#125; if (environment.pidFile() != null) &#123; try &#123; PidFile.create(environment.pidFile(), true); &#125; catch (IOException e) &#123; throw new BootstrapException(e); &#125; &#125; final boolean closeStandardStreams = (foreground == false) || quiet; try &#123; if (closeStandardStreams) &#123; final Logger rootLogger = ESLoggerFactory.getRootLogger(); final Appender maybeConsoleAppender = Loggers.findAppender(rootLogger, ConsoleAppender.class); if (maybeConsoleAppender != null) &#123; Loggers.removeAppender(rootLogger, maybeConsoleAppender); &#125; closeSystOut(); &#125; // fail if somebody replaced the lucene jars checkLucene(); //14、检查Lucene版本// install the default uncaught exception handler; must be done before security is initialized as we do not want to grant the runtime permission setDefaultUncaughtExceptionHandler Thread.setDefaultUncaughtExceptionHandler( new ElasticsearchUncaughtExceptionHandler(() -&gt; Node.NODE_NAME_SETTING.get(environment.settings()))); INSTANCE.setup(true, environment); //15、调用 setup 方法 try &#123; // any secure settings must be read during node construction IOUtils.close(keystore); &#125; catch (IOException e) &#123; throw new BootstrapException(e); &#125; INSTANCE.start(); //26、调用 start 方法 if (closeStandardStreams) &#123; closeSysError(); &#125; &#125; catch (NodeValidationException | RuntimeException e) &#123; // disable console logging, so user does not see the exception twice (jvm will show it already) final Logger rootLogger = ESLoggerFactory.getRootLogger(); final Appender maybeConsoleAppender = Loggers.findAppender(rootLogger, ConsoleAppender.class); if (foreground &amp;&amp; maybeConsoleAppender != null) &#123; Loggers.removeAppender(rootLogger, maybeConsoleAppender); &#125; Logger logger = Loggers.getLogger(Bootstrap.class); if (INSTANCE.node != null) &#123; logger = Loggers.getLogger(Bootstrap.class, Node.NODE_NAME_SETTING.get(INSTANCE.node.settings())); &#125; // HACK, it sucks to do this, but we will run users out of disk space otherwise if (e instanceof CreationException) &#123; // guice: log the shortened exc to the log file ByteArrayOutputStream os = new ByteArrayOutputStream(); PrintStream ps = null; try &#123; ps = new PrintStream(os, false, \"UTF-8\"); &#125; catch (UnsupportedEncodingException uee) &#123; assert false; e.addSuppressed(uee); &#125; new StartupException(e).printStackTrace(ps); ps.flush(); try &#123; logger.error(\"Guice Exception: &#123;&#125;\", os.toString(\"UTF-8\")); &#125; catch (UnsupportedEncodingException uee) &#123; assert false; e.addSuppressed(uee); &#125; &#125; else if (e instanceof NodeValidationException) &#123; logger.error(\"node validation exception\\n&#123;&#125;\", e.getMessage()); &#125; else &#123; // full exception logger.error(\"Exception\", e); &#125; // re-enable it if appropriate, so they can see any logging during the shutdown process if (foreground &amp;&amp; maybeConsoleAppender != null) &#123; Loggers.addAppender(rootLogger, maybeConsoleAppender); &#125; throw e; &#125;&#125; 该方法主要有： 1、创建 Bootstrap 实例 2、如果注册了安全模块则将相关配置加载进来 3、创建 Elasticsearch 运行的必须环境以及相关配置, 如将 config、scripts、plugins、modules、logs、lib、bin 等配置目录加载到运行环境中 4、log 配置环境，创建日志上下文 5、检查是否存在 PID 文件，如果不存在，创建 PID 文件 6、检查 Lucene 版本 7、调用 setup 方法（用当前环境来创建一个节点） setup 方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152private void setup(boolean addShutdownHook, Environment environment) throws BootstrapException &#123; Settings settings = environment.settings();//根据环境得到配置 try &#123; spawner.spawnNativeControllers(environment); &#125; catch (IOException e) &#123; throw new BootstrapException(e); &#125; initializeNatives( environment.tmpFile(), BootstrapSettings.MEMORY_LOCK_SETTING.get(settings), BootstrapSettings.SYSTEM_CALL_FILTER_SETTING.get(settings), BootstrapSettings.CTRLHANDLER_SETTING.get(settings)); // initialize probes before the security manager is installed initializeProbes(); if (addShutdownHook) &#123; Runtime.getRuntime().addShutdownHook(new Thread() &#123; @Override public void run() &#123; try &#123; IOUtils.close(node, spawner); LoggerContext context = (LoggerContext) LogManager.getContext(false); Configurator.shutdown(context); &#125; catch (IOException ex) &#123; throw new ElasticsearchException(\"failed to stop node\", ex); &#125; &#125; &#125;); &#125; try &#123; // look for jar hell final Logger logger = ESLoggerFactory.getLogger(JarHell.class); JarHell.checkJarHell(logger::debug); &#125; catch (IOException | URISyntaxException e) &#123; throw new BootstrapException(e); &#125; // Log ifconfig output before SecurityManager is installed IfConfig.logIfNecessary(); // install SM after natives, shutdown hooks, etc. try &#123; Security.configure(environment, BootstrapSettings.SECURITY_FILTER_BAD_DEFAULTS_SETTING.get(settings)); &#125; catch (IOException | NoSuchAlgorithmException e) &#123; throw new BootstrapException(e); &#125; node = new Node(environment) &#123; //16、新建节点 @Override protected void validateNodeBeforeAcceptingRequests( final BootstrapContext context, final BoundTransportAddress boundTransportAddress, List&lt;BootstrapCheck&gt; checks) throws NodeValidationException &#123; BootstrapChecks.check(context, boundTransportAddress, checks); &#125; &#125;;&#125; 上面代码最后就是 Node 节点的创建，这篇文章就不讲 Node 的创建了，下篇文章会好好讲一下 Node 节点的创建和正式启动 ES 节点的。 总结这篇文章主要先把大概启动流程串通，因为篇幅较多所以拆开成两篇，先不扣细节了，后面流程启动文章写完后我们再单一的扣细节。 相关文章1、渣渣菜鸡为什么要看 ElasticSearch 源码？ 2、渣渣菜鸡的 ElasticSearch 源码解析 —— 环境搭建 3、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(上) 4、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(下) 5、Elasticsearch 系列文章（一）：Elasticsearch 默认分词器和中分分词器之间的比较及使用方法 6、Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程 7、Elasticsearch 系列文章（三）：ElasticSearch 集群监控 8、Elasticsearch 系列文章（四）：ElasticSearch 单个节点监控 9、Elasticsearch 系列文章（五）：ELK 实时日志分析平台环境搭建 10、教你如何在 IDEA 远程 Debug ElasticSearch","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://yoursite.com/tags/ElasticSearch/"}]},{"title":"渣渣菜鸡的 ElasticSearch 源码解析 —— 环境搭建","date":"2018-08-04T16:00:00.000Z","path":"2018/08/05/es-code01/","text":"关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/08/05/es-code01/ 软件环境1、Intellij Idea:2018.2版本 2、Elasticsearch 源码版本: 6.3.2 3、JDK:10.0.2 4、Gradle : 建议 4.5 及以上版本 5、Macbook Pro 2017 安装 ElasticSearch 去 https://www.elastic.co/downloads/past-releases 这里找到 ElasticSearch 6.3.2 版本，下载后然后解压就好了。（注意：这个版本需要和下面的源码版本一致） 下载源码从 https://github.com/elastic/elasticsearch 上下载相应版本的源代码，这里建议用 git clone ，这样的话后面你可以随意切换到 ElasticSearch 的其他版本去。 1git clone git@github.com:elastic/elasticsearch.git 我们看下有哪些版本的： 1git tag 找到了目前源码版本最新的版本的稳定版为：v6.3.2 切换到该版本： 1git checkout v6.3.2 于是就可以切换到该稳定版本了。接下来不要直接导入到 IDEA/Eclipse 中。 编译GitHub 这里已经有描述如何导入 IDEA/Eclipse 中： 1234567891011121314151617181920JDK 10 is required to build Elasticsearch. You must have a JDK 10 installation with the environment variable JAVA_HOME referencing the path to Java home for your JDK 10 installation. By default, tests use the same runtime as JAVA_HOME. However, since Elasticsearch, supports JDK 8 the build supports compiling with JDK 10 and testing on a JDK 8 runtime; to do this, set RUNTIME_JAVA_HOME pointing to the Java home of a JDK 8 installation. Note that this mechanism can be used to test against other JDKs as well, this is not only limited to JDK 8.Note: It is also required to have JAVA7_HOME, JAVA8_HOME and JAVA10_HOME available so that the tests can pass.Warning: do not use sdkman for Java installations which do not have proper jrunscript for jdk distributions.Elasticsearch uses the Gradle wrapper for its build. You can execute Gradle using the wrapper via the gradlew script in the root of the repository.Configuring IDEs And Running TestsEclipse users can automatically configure their IDE: ./gradlew eclipse then File: Import: Existing Projects into Workspace. Select the option Search for nested projects. Additionally you will want to ensure that Eclipse is using 2048m of heap by modifying eclipse.ini accordingly to avoid GC overhead errors.IntelliJ users can automatically configure their IDE: ./gradlew idea then File-&gt;New Project From Existing Sources. Point to the root of the source directory, select Import project from external model-&gt;Gradle, enable Use auto-import. In order to run tests directly from IDEA 2017.2 and above, it is required to disable the IDEA run launcher in order to avoid idea_rt.jar causing &quot;jar hell&quot;. This can be achieved by adding the -Didea.no.launcher=true JVM option. Alternatively, idea.no.launcher=true can be set in the idea.properties file which can be accessed under Help &gt; Edit Custom Properties (this will require a restart of IDEA). For IDEA 2017.3 and above, in addition to the JVM option, you will need to go to Run-&gt;Edit Configurations-&gt;...-&gt;Defaults-&gt;JUnit and verify that the Shorten command line setting is set to user-local default: none. You may also need to remove ant-javafx.jar from your classpath if that is reported as a source of jar hell.To run an instance of elasticsearch from the source code run ./gradlew runThe Elasticsearch codebase makes heavy use of Java asserts and the test runner requires that assertions be enabled within the JVM. This can be accomplished by passing the flag -ea to the JVM on startup.For IntelliJ, go to Run-&gt;Edit Configurations...-&gt;Defaults-&gt;JUnit-&gt;VM options and input -ea.For Eclipse, go to Preferences-&gt;Java-&gt;Installed JREs and add -ea to VM Arguments. 上面说了下如何编译 Elasticsearch 和如何在 ide 中配置好环境。下面说下步骤吧：（这里我只是演示在 IDEA 中如何导入） 1、在我们下载的 Elasticsearch 根目录下执行命令：(执行已经写好的脚本 gradlew) 1./gradlew idea 请注意版本和我的一致，早的版本可能没有该执行脚本，需要执行 gradle idea 命令 最后结果如下： 2、导入 IDEA idea 中 File -&gt; New Project From Existing Sources 选择你下载的 Elasticsearch 根目录，然后点 open ，之后 Import project from external model -&gt; Gradle , 选中 Use auto-import, 然后就可以了。 导入进去后，gradle 又会编译一遍，需要等一会，好了之后如下： 运行打开 server 模块下的 Elasticsearch 类：org.elasticsearch.bootstrap.Elasticsearch，运行里面的 main 函数。 1、报错如下： 1ERROR: the system property [es.path.conf] must be set 我们在运行的配置 vm options 如下：（后面启动流程会写为什么会报这个错误） 1-Des.path.conf=&quot;/usr/local/elasticsearch-6.3.2/config&quot; 2、再次运行，报错如下： 12345678910Exception in thread &quot;main&quot; java.lang.IllegalStateException: path.home is not configured at org.elasticsearch.env.Environment.&lt;init&gt;(Environment.java:103) at org.elasticsearch.env.Environment.&lt;init&gt;(Environment.java:94) at org.elasticsearch.node.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:86) at org.elasticsearch.cli.EnvironmentAwareCommand.createEnv(EnvironmentAwareCommand.java:95) at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) at org.elasticsearch.cli.Command.main(Command.java:90) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) 我们在运行的配置 vm options 如下：（后面启动流程会写为什么会报这个错误） 1-Des.path.home=&quot;/usr/local/elasticsearch-6.3.2&quot; 3、再次运行，报错如下： 123456789101112131415161718192021222324252018-08-01 09:38:03,974 main ERROR Could not register mbeans java.security.AccessControlException: access denied (&quot;javax.management.MBeanTrustPermission&quot; &quot;register&quot;) at java.base/java.security.AccessControlContext.checkPermission(AccessControlContext.java:472) at java.base/java.lang.SecurityManager.checkPermission(SecurityManager.java:371) at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.checkMBeanTrustPermission(DefaultMBeanServerInterceptor.java:1805) at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:318) at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522) at org.apache.logging.log4j.core.jmx.Server.register(Server.java:389) at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:167) at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140) at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:556) at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:261) at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:206) at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:220) at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:197) at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:171) at org.elasticsearch.common.logging.LogConfigurator.configure(LogConfigurator.java:140) at org.elasticsearch.common.logging.LogConfigurator.configure(LogConfigurator.java:119) at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:294) at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:136) at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) at org.elasticsearch.cli.Command.main(Command.java:90) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) 我们在运行的配置 vm options 如下： 1-Dlog4j2.disable.jmx=true 4、如果你用的是 JDK 1.8 编译的应该还会报这个错误 123456789101112131415161718192021222324[2018-08-01T11:02:24,663][WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [] uncaught exception in thread [main]org.elasticsearch.bootstrap.StartupException: java.lang.IllegalStateException: jar hell!class: jdk.packager.services.UserJvmOptionsServicejar1: /Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home/lib/ant-javafx.jarjar2: /Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home/lib/packager.jar at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:140) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) ~[main/:?] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[main/:?] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[main/:?] at org.elasticsearch.cli.Command.main(Command.java:90) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) ~[main/:?]Caused by: java.lang.IllegalStateException: jar hell!class: jdk.packager.services.UserJvmOptionsServicejar1: /Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home/lib/ant-javafx.jarjar2: /Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home/lib/packager.jar at org.elasticsearch.bootstrap.JarHell.checkClass(JarHell.java:273) ~[main/:?] at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:190) ~[main/:?] at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:86) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:198) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:326) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:136) ~[main/:?] ... 6 more2018-08-01 11:02:24,713 Thread-2 ERROR No log4j2 configuration file found. Using default configuration: logging only errors to the console. Set system property &apos;log4j2.debug&apos; to show Log4j2 internal initialization logging. 有两个解决方法就是， （1）、把源码中有关使用了 JarHell.checkJarHell 代码的地方全部注释掉就好了 （2）、换成 JDK 10 编译 两种方法我都试了是可行的，建议直接换第二种方案吧！ 5、然后再启动的话，应该没问题了,出现下面日志：（网上很多人在这步就好了） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[elasticsearch] Java HotSpot(TM) 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.[elasticsearch] [2018-08-04T16:42:26,073][INFO ][o.e.n.Node ] [node-0] initializing ...[elasticsearch] [2018-08-04T16:42:26,185][INFO ][o.e.e.NodeEnvironment ] [node-0] using [1] data paths, mounts [[/ (/dev/disk1s1)]], net usable_space [109.3gb], net total_space [233.4gb], types [apfs][elasticsearch] [2018-08-04T16:42:26,187][INFO ][o.e.e.NodeEnvironment ] [node-0] heap size [494.9mb], compressed ordinary object pointers [true][elasticsearch] [2018-08-04T16:42:26,190][INFO ][o.e.n.Node ] [node-0] node name [node-0], node ID [o9SuMXP-R7uvJLtE3h37Rw][elasticsearch] [2018-08-04T16:42:26,191][INFO ][o.e.n.Node ] [node-0] version[6.3.2-SNAPSHOT], pid[61499], build[default/zip/053779d/2018-08-04T08:39:59.714654Z], OS[Mac OS X/10.13.5/x86_64], JVM[&quot;Oracle Corporation&quot;/Java HotSpot(TM) 64-Bit Server VM/10.0.2/10.0.2+13][elasticsearch] [2018-08-04T16:42:26,191][INFO ][o.e.n.Node ] [node-0] JVM arguments [-Xms1g, -Xmx1g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Djava.io.tmpdir=/var/folders/mb/3vpbvkkx13l2jmpt2kmmt0fr0000gn/T/elasticsearch.URRKTybG, -XX:+HeapDumpOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Djava.locale.providers=COMPAT, -XX:UseAVX=2, -ea, -esa, -Xms512m, -Xmx512m, -Des.path.home=/Users/zhisheng/IdeaProjects/github/elasticsearch/distribution/build/cluster/run node0/elasticsearch-6.3.2-SNAPSHOT, -Des.path.conf=/Users/zhisheng/IdeaProjects/github/elasticsearch/distribution/build/cluster/run node0/elasticsearch-6.3.2-SNAPSHOT/config, -Des.distribution.flavor=default, -Des.distribution.type=zip][elasticsearch] [2018-08-04T16:42:26,191][WARN ][o.e.n.Node ] [node-0] version [6.3.2-SNAPSHOT] is a pre-release version of Elasticsearch and is not suitable for production[elasticsearch] [2018-08-04T16:42:28,808][INFO ][o.e.p.PluginsService ] [node-0] loaded module [aggs-matrix-stats][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [analysis-common][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [ingest-common][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [lang-expression][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [lang-mustache][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [lang-painless][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [mapper-extras][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [parent-join][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [percolator][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [rank-eval][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [reindex][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [repository-url][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [transport-netty4][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [tribe][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-core][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-deprecation][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-graph][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-logstash][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-ml][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-monitoring][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-rollup][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-security][elasticsearch] [2018-08-04T16:42:28,811][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-sql][elasticsearch] [2018-08-04T16:42:28,811][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-upgrade][elasticsearch] [2018-08-04T16:42:28,811][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-watcher][elasticsearch] [2018-08-04T16:42:28,811][INFO ][o.e.p.PluginsService ] [node-0] no plugins loaded[elasticsearch] [2018-08-04T16:42:32,722][INFO ][o.e.x.s.a.s.FileRolesStore] [node-0] parsed [0] roles from file [/Users/zhisheng/IdeaProjects/github/elasticsearch/distribution/build/cluster/run node0/elasticsearch-6.3.2-SNAPSHOT/config/roles.yml][elasticsearch] [2018-08-04T16:42:33,358][INFO ][o.e.x.m.j.p.l.CppLogMessageHandler] [controller/61517] [Main.cc@109] controller (64 bit): Version 6.3.2-SNAPSHOT (Build 903094f295d249) Copyright (c) 2018 Elasticsearch BV[elasticsearch] [2018-08-04T16:42:33,783][DEBUG][o.e.a.ActionModule ] Using REST wrapper from plugin org.elasticsearch.xpack.security.Security[elasticsearch] [2018-08-04T16:42:34,110][INFO ][o.e.d.DiscoveryModule ] [node-0] using discovery type [zen][elasticsearch] [2018-08-04T16:42:34,971][INFO ][o.e.n.Node ] [node-0] initialized[elasticsearch] [2018-08-04T16:42:34,971][INFO ][o.e.n.Node ] [node-0] starting ...[elasticsearch] [2018-08-04T16:42:35,217][INFO ][o.e.t.TransportService ] [node-0] publish_address &#123;127.0.0.1:9300&#125;, bound_addresses &#123;[::1]:9300&#125;, &#123;127.0.0.1:9300&#125;[elasticsearch] [2018-08-04T16:42:38,291][INFO ][o.e.c.s.MasterService ] [node-0] zen-disco-elected-as-master ([0] nodes joined)[, ], reason: new_master &#123;node-0&#125;&#123;o9SuMXP-R7uvJLtE3h37Rw&#125;&#123;xjoT1zvpRsm1ZDGLCab1sA&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;&#123;ml.machine_memory=17179869184, xpack.installed=true, testattr=test, ml.max_open_jobs=20, ml.enabled=true&#125;[elasticsearch] [2018-08-04T16:42:38,295][INFO ][o.e.c.s.ClusterApplierService] [node-0] new_master &#123;node-0&#125;&#123;o9SuMXP-R7uvJLtE3h37Rw&#125;&#123;xjoT1zvpRsm1ZDGLCab1sA&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;&#123;ml.machine_memory=17179869184, xpack.installed=true, testattr=test, ml.max_open_jobs=20, ml.enabled=true&#125;, reason: apply cluster state (from master [master &#123;node-0&#125;&#123;o9SuMXP-R7uvJLtE3h37Rw&#125;&#123;xjoT1zvpRsm1ZDGLCab1sA&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;&#123;ml.machine_memory=17179869184, xpack.installed=true, testattr=test, ml.max_open_jobs=20, ml.enabled=true&#125; committed version [1] source [zen-disco-elected-as-master ([0] nodes joined)[, ]]])[elasticsearch] [2018-08-04T16:42:38,317][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [node-0] publish_address &#123;127.0.0.1:9200&#125;, bound_addresses &#123;[::1]:9200&#125;, &#123;127.0.0.1:9200&#125;[elasticsearch] [2018-08-04T16:42:38,319][INFO ][o.e.n.Node ] [node-0] started[elasticsearch] [2018-08-04T16:42:38,358][WARN ][o.e.x.s.a.s.m.NativeRoleMappingStore] [node-0] Failed to clear cache for realms [[]][elasticsearch] [2018-08-04T16:42:38,413][INFO ][o.e.g.GatewayService ] [node-0] recovered [0] indices into cluster_state[elasticsearch] [2018-08-04T16:42:38,597][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.watch-history-7] for index patterns [.watcher-history-7*][elasticsearch] [2018-08-04T16:42:38,660][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.watches] for index patterns [.watches*][elasticsearch] [2018-08-04T16:42:38,707][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.triggered_watches] for index patterns [.triggered_watches*][elasticsearch] [2018-08-04T16:42:38,771][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.monitoring-logstash] for index patterns [.monitoring-logstash-6-*][elasticsearch] [2018-08-04T16:42:38,836][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.monitoring-es] for index patterns [.monitoring-es-6-*][elasticsearch] [2018-08-04T16:42:38,878][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.monitoring-alerts] for index patterns [.monitoring-alerts-6][elasticsearch] [2018-08-04T16:42:38,926][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.monitoring-beats] for index patterns [.monitoring-beats-6-*][elasticsearch] [2018-08-04T16:42:38,970][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.monitoring-kibana] for index patterns [.monitoring-kibana-6-*][elasticsearch] [2018-08-04T16:42:39,055][INFO ][o.e.l.LicenseService ] [node-0] license [79704513-d3c4-4535-8276-beeb146765de] mode [basic] - valid 6、但是我出现了下面这个问题，一直困扰着我呢，我是直接跳过去的。 12345678910111213141516171819202122[2018-08-01T09:44:27,370][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [] fatal error in thread [main], exitingjava.lang.NoClassDefFoundError: org/elasticsearch/plugins/ExtendedPluginsClassLoader at org.elasticsearch.plugins.PluginsService.loadBundle(PluginsService.java:632) ~[main/:?] at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:557) ~[main/:?] at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:162) ~[main/:?] at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:311) ~[main/:?] at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:252) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:213) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:213) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:326) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:136) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) ~[main/:?] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[main/:?] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[main/:?] at org.elasticsearch.cli.Command.main(Command.java:90) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) ~[main/:?]Caused by: java.lang.ClassNotFoundException: org.elasticsearch.plugins.ExtendedPluginsClassLoader at jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:582) ~[?:?] at jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:190) ~[?:?] at java.lang.ClassLoader.loadClass(ClassLoader.java:499) ~[?:?] ... 15 more 遇到的这个问题，我在 GitHub 求助信息如下： https://github.com/elastic/elasticsearch/issues/30774 但是并没有解决我的问题，这里暂时先记录下来！我自己也跟了下源码，定位到错误信息是怎么产生的，但是没有解决方案！ 后面写了篇文章：教你如何在 IDEA 远程 Debug ElasticSearch 或许可以帮你解决上面问题带给你的困扰！ 更新后面有一个读者提醒了我一下，他自己也遇到这个问题，然后他的解决方案挺好的，完美解决我的问题。这里做个记录： 解决方法： 打开 IDEA Edit Configurations ，给 Include dependencies with Provided scope 打上勾即可解决，很简单吧！！ 继续RUN，又来一个 EXceptin： 1234567891011121314151617[2018-08-23T01:13:38,551][WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [] uncaught exception in thread [main]org.elasticsearch.bootstrap.StartupException: java.security.AccessControlException: access denied (&quot;java.lang.RuntimePermission&quot; &quot;createClassLoader&quot;) at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:140) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) ~[main/:?] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[main/:?] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[main/:?] at org.elasticsearch.cli.Command.main(Command.java:90) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) ~[main/:?]Caused by: java.security.AccessControlException: access denied (&quot;java.lang.RuntimePermission&quot; &quot;createClassLoader&quot;) at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472) ~[?:?] at java.security.AccessController.checkPermission(AccessController.java:895) ~[?:?] at java.lang.SecurityManager.checkPermission(SecurityManager.java:335) ~[?:?] at java.lang.SecurityManager.checkCreateClassLoader(SecurityManager.java:397) ~[?:?]...Exception: java.security.AccessControlException thrown from the UncaughtExceptionHandler in thread &quot;Thread-2&quot; 第一种： 在 config 目录下新建 java.policy 文件，填入下面内容: 123grant &#123; permission java.lang.RuntimePermission &quot;createClassLoader&quot;;&#125;; 然后在 VM options 加入 java.security.policy 的设置，指向该文件即可 1-Djava.security.policy=/usr/local/elasticsearch-6.3.2/config/java.policy 第二种： 就是在 %JAVA_HOME%/conf/security 目录下（JDK10是这个路径，之前的版本不确定），我的目录是 /Library/Java/JavaVirtualMachines/jdk-10.0.2.jdk/Contents/Home/conf/security，打开 java.policy 文件，在 grant 中加入下面这句，赋予权限。 12//for es 6.3.2permission java.lang.RuntimePermission &quot;createClassLoader&quot;; 再 RUN，这次可终于运行起来了！！！ 再次感谢下读者，他的文章地址是：http://laijianfeng.org/2018/08/%E6%95%99%E4%BD%A0%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95Elasticsearch-6-3-2%E6%BA%90%E7%A0%81/ 总结折腾的路上少不了各种错误烦扰你，学会解决问题！ 相关文章1、渣渣菜鸡为什么要看 ElasticSearch 源码？ 2、渣渣菜鸡的 ElasticSearch 源码解析 —— 环境搭建 3、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(上) 4、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(下) 5、Elasticsearch 系列文章（一）：Elasticsearch 默认分词器和中分分词器之间的比较及使用方法 6、Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程 7、Elasticsearch 系列文章（三）：ElasticSearch 集群监控 8、Elasticsearch 系列文章（四）：ElasticSearch 单个节点监控 9、Elasticsearch 系列文章（五）：ELK 实时日志分析平台环境搭建 10、教你如何在 IDEA 远程 Debug ElasticSearch","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://yoursite.com/tags/ElasticSearch/"}]},{"title":"渣渣菜鸡为什么要看 ElasticSearch 源码？","date":"2018-08-03T16:00:00.000Z","path":"2018/08/04/why-see-es-code/","text":"前提人工智能、大数据快速发展的今天，对于 TB 甚至 PB 级大数据的快速检索已然成为刚需，大型企业早已淹没在系统生成的浩瀚数据流当中。大数据技术业已集中在如何存储和处理这些海量的数据上。Elasticsearch 作为开源领域的后起之秀，从2010年至今得到飞跃式的发展。 Elasticsearch 以其开源、分布式、RESTFul API 三大优势，已经成为当下风口中“会飞的猪”。 在我的电脑本地写了几篇 ElasticSearch 的源码解析了，回过头来想想应该也写一篇为何我会去看它的源码？ 为什么呢？下面我讲讲自己从接触搜索到现在看源码的过程！ 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/08/24/why-see-es-code/ 第一次接触搜索搜索，我们首先想到的是搜索引擎：Google、百度，这个就算是接触的最早的了。 我自己项目里面接触搜索是大二暑假那时练习的一个项目，里面用了 Solr，然后当时自己也稍微了解了下，并用在了项目里面。 第二次接触搜索从第一次项目里面用到了搜索，后面自己对这方面就比较感兴趣。再一次接触搜索是实习的时候进公司。第一件事情就是被老大叫的去学习搭建 Elasticsearch 集群，于是乎，电脑就装了三个虚拟机，Elasticsearch 就一个个的装了起来了。也记录了博客下来：Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程，当时搭建的时候 ES 的版本才刚从 2.x 升级到 5.x 呢，截止本文时间 2018.08.04，现在 ES 版本已经是到 7.0 了，这版本升级是真的的快，这也说明了 ES 的活跃度很高，背后的开发工程师维护也快，侧面突出要去看它源码的重要性。 当时自己在本地测试搭建集群后，给分配了另外一个任务就是去了解 ES 中的自带分词、英文分词、中文分词的相同与差异、以及自己建立分词需要注意的点。于是乎：当时在公司 wiki 贡献了这篇文章：Elasticsearch 系列文章（一）：Elasticsearch 默认分词器和中分分词器之间的比较及使用方法。这篇文章几乎已经把市面上所有的分词都写进去了，包括他们的相同点、不同点、如何使用、如何自定义分词器。 然后还有就是我同组的一个同学，她的任务就是 2.x 升级到 5.x 中 mapping 的大改变有哪些？后面我也看了她总结的文档，很详细！ 在这次接触了 ES 后，因为我自己本地已经有环境了，所以自己测试了一些功能，给 ES 安装插件（IK、x-pack、支持 sql 的、），后面自己也去测试 ES 的索引、文档、REST API。 第三次接触搜索由于是自己对其感兴趣，所以后面就去找了些相关的视频，比如：中华石衫的《Elasticsearch 顶尖高手系列-高手进阶篇》几个系列视频教程个人感觉还是不错的，看完这几个系列估计入门肯定是没有问题的。版权原因，不提供下载链接。 另外就是《Elasticsearch 权威指南》翻译的版本，翻译还没有全，可以去看看，讲得很详细的，市面上应该还没有哪本书讲的有这么清楚，如果英文不错的可以直接啃英文吧。 还有就是官网的文档了，非常非常详细，还有 demo，2.x 版本的是有中文的官方文档，可以凑合着看。 学习新东西，要学会先看官方文档，何况 Elasticsearch 的官方文档这么详细呢！ 第四次接触搜索后面实习的时候，又分配了公司中间件监控的两个模块：Elasticsearch 和 HBase 组件的监控。于是乎，再次有机会接触 Elasticsearch 了，这次主要还是利用 Elasticsearch 自带的 REST API ：_cluster/health 、_cluster/stats、_nodes、_nodes/stats 去获取到集群的健康信息、节点信息（内存、CPU、网络、JVM等信息）。为了做这个项目自己当时也去找了网上很多类似的文章参考常用的监控指标和他们是怎么做监控的。我当时的任务主要还是采集信息，然后存到公司大项目的 influxdb 中，最后用 grafana 展示出来，后面我组的运维大佬给我看了监控大盘，界面挺酷炫的，哈哈哈，牛逼！ 当时写的两篇博客： 1、Elasticsearch 系列文章（三）：ElasticSearch 集群监控 2、Elasticsearch 系列文章（四）：ElasticSearch 单个节点监控 取之网络，还之网络，希望给后面做类似任务的小伙伴给点参考意见！ 再就是自己搭建 ELK（ElasticSearch, Logstash, Kibana）日志分析平台，然后玩了下！ 搭建环境博客：Elasticsearch 系列文章（五）：ELK 实时日志分析平台环境搭建 第五次接触搜索后面就没怎么接触 ElasticSearch 了，一直忙着其他的东西。 实习辞职后，毕业出来找工作的那段日子，自己又花了一星期稍微过了一遍 《Elasticsearch 权威指南》 这本书，话说还帮我面试挺过不少关呢，哈哈哈！因为我项目里写了 Elasticsearch 的监控，如果你对 Elasticsearch 其他的不熟悉，面试官稍微问些其他关于这方面的，那就不知道就有点尴尬😅了，所以还是准备了下。看完之后应付面试没多大的问题。 第六次接触看起来我接触了 Elasticsearch 很久了，其实真正项目里面是没有用到 Elasticsearch 做过项目的，没有用到 Elasticsearch 的搜索做什么项目，于是自己当时找工作其实也打算找到工作后看能不能自己做个项目或者公司项目里面用用 Elasticsearch 呢？ 结果在新公司新项目里，很快就用到了。只不过这次不是 Java 项目里面用了，而是和 GoLang 整合。不过 API 都差不多，多熟悉几次就很快上手了，关键还是要懂 Elasticsearch 如何构造 DSL 查询语句，这样再转换成 GO 里面的 API 就快了。 还有就是公司里刚好有个中科院研究生大佬，他就写过 Elasticsearch 这块的书籍《从 lucene 到 Elasticsearch 全文检索实战》，另外他的 CSDN 博客也很火，阅读量很高，感兴趣的可以买本书支持下。 中途自己遇到 Elasticsearch 实在不会的问题也会主动去找大佬咨询，然后大佬耐心教教我这个渣渣菜鸡，在文章这里感谢下大佬这段时间的关照。 萌生阅读源码的想法既然接触了这么久的 Elasticsearch ，项目里用过，书籍也看过，虽然还不是很熟，但是如果看看它的源码是不是会让我对它的印象更深呢？ 说干就干，晚上回家就从 GitHub clone 了源码在本地，那时刚好回家，就在火车上直接用 VS code 看了会源码，也没有在 ide 里 debug 起来。 写这篇文章的时候已经把 Elasticsearch 的整个启动流程（加载读取配置、加载插件等）、如何支持 REST API 看了下，后面会在下班后回家继续阅读源码，继续分享我的源码解析的。 有想法就去干，不尝试下，怎么知道适不适合你？ 总结其实阅读源码的主要原因还是自己感兴趣；另外就是这东西现在项目里确实也用到了，如果我对源码熟悉的话可能会对我的理解会更加透彻点；还有就是 Elasticsearch 确实火，公司几乎都用的，所以学习下还是有必要的。","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://yoursite.com/tags/ElasticSearch/"}]},{"title":"渣渣菜鸡的蚂蚁金服面试经历(二)","date":"2018-07-30T16:00:00.000Z","path":"2018/07/31/alipay02/","text":"蚂蚁金服电话二面（85 分钟）1、自我介绍、工作经历、技术栈 2、项目中你学到了什么技术？（把三项目具体描述了很久） 3、微服务划分的粒度 4、微服务的高可用怎么保证的？ 5、常用的负载均衡，该怎么用，你能说下吗？ 6、网关能够为后端服务带来哪些好处？ 7、Spring Bean 的生命周期 8、xml 中配置的 init、destroy 方法怎么可以做到调用具体的方法？ 9、反射的机制 10、Object 类中的方法 11、hashcode 和 equals 方法常用地方 12、对象比较是否相同 13、hashmap put 方法存放的时候怎么判断是否是重复的 14、Object toString 方法常用的地方，为什么要重写该方法 15、Set 和 List 区别？ 16、ArrayList 和 LinkedList 区别 17、如果存取相同的数据，ArrayList 和 LinkedList 谁占用空间更大？ 18、Set 存的顺序是有序的吗？ 19、常见 Set 的实现有哪些？ 20、TreeSet 对存入对数据有什么要求呢？ 21、HashSet 的底层实现呢 22、TreeSet 底层源码有看过吗？ 23、HashSet 是不是线程安全的？为什么不是线程安全的？ 24、Java 中有哪些线程安全的 Map？ 25、Concurrenthashmap 是怎么做到线程安全的？ 26、HashTable 你了解过吗？ 27、如何保证线程安全问题？ 28、synchronized、lock 29、volatile 的原子性问题？为什么 i++ 这种不支持原子性？从计算机原理的设计来讲下不能保证原子性的原因 30、happens before 原理 31、cas 操作 32、lock 和 synchronized 的区别？ 33、公平锁和非公平锁 34、Java 读写锁 35、读写锁设计主要解决什么问题？ 36、你项目除了写 Java 代码，还有前端代码，那你知道前端有哪些框架吗？ 37、MySQL 分页查询语句 38、MySQL 事务特性和隔离级别 39、不可重复读会出现在什么场景？ 40、sql having 的使用场景 41、前端浏览器地址的一个 http 请求到后端整个流程是怎么样？能够说下吗？ 42、http 默认端口，https 默认端口 43、DNS 你知道是干嘛的吗？ 44、你们开发用的 ide 是啥？你能说下 idea 的常用几个快捷键吧？ 45、代码版本管理你们用的是啥？ 46、git rebase 和 merge 有什么区别？ 47、你们公司加班多吗？ 48、后面一起聊 high 了，之间扯了些蛋，哈哈哈 相关文章：1、秋招第一站 —— 亚信科技 2、秋招第二站 —— 内推爱奇艺 3、秋招第三站 —— 内推阿里（一面） 4、那些年我看过的书 —— 致敬我的大学生活 —— Say Good Bye ！ 5、面试过阿里等互联网大公司，我知道了这些套路 6、渣渣菜鸡的有赞面试经历（一） 7、渣渣菜鸡的蚂蚁金服面试经历（一） 最后本地地址：http://www.54tianzhisheng.cn/2018/07/31/alipay02 ，转载请授权，否则禁止转载！ 本文首发在我的知识星球，最近自己一直在写前段时间的所有面试情况，已经分享在我的知识星球，如果感兴趣，可以加入我的知识星球！","tags":[{"name":"面经","slug":"面经","permalink":"http://yoursite.com/tags/面经/"}]},{"title":"渣渣菜鸡的蚂蚁金服面试经历(一)","date":"2018-07-29T16:00:00.000Z","path":"2018/07/30/alipay01/","text":"蚂蚁金服电话一面1、自我介绍、自己做的项目和技术领域 2、项目中的监控：那个监控指标常见的哪些？ 3、微服务涉及到的技术以及需要注意的问题有哪些？ 4、注册中心你了解了哪些？ 5、consul 的可靠性你了解吗？ 6、consul 的机制你有没有具体深入过？有没有和其他的注册中心对比过？ 7、项目用 Spring 比较多，有没有了解 Spring 的原理？AOP 和 IOC 的原理 8、Spring Boot除了自动配置，相比传统的 Spring 有什么其他的区别？ 9、Spring Cloud 有了解多少？ 10、Spring Bean 的生命周期 11、HashMap 和 hashTable 区别？ 12、Object 的 hashcode 方法重写了，equals 方法要不要改？ 13、Hashmap 线程不安全的出现场景 14、线上服务 CPU 很高该怎么做？有哪些措施可以找到问题 15、JDK 中有哪几个线程池？顺带把线程池讲了个遍 16、SQL 优化的常见方法有哪些 17、SQL 索引的顺序，字段的顺序 18、查看 SQL 是不是使用了索引？（有什么工具） 19、TCP 和 UDP 的区别？TCP 数据传输过程中怎么做到可靠的？ 20、说下你知道的排序算法吧 21、查找一个数组的中位数？ 22、你有什么问题想问我的吗？ 相关文章：1、秋招第一站 —— 亚信科技 2、秋招第二站 —— 内推爱奇艺 3、秋招第三站 —— 内推阿里（一面） 4、那些年我看过的书 —— 致敬我的大学生活 —— Say Good Bye ！ 5、面试过阿里等互联网大公司，我知道了这些套路 6、渣渣菜鸡的有赞面试经历（一） 最后本地地址：http://www.54tianzhisheng.cn/2018/07/30/alipay01 ，转载请授权，否则禁止转载！ 本文首发在我的知识星球，最近自己一直在写前段时间的所有面试情况，已经分享在我的知识星球，如果感兴趣，可以加入我的知识星球！","tags":[{"name":"面经","slug":"面经","permalink":"http://yoursite.com/tags/面经/"}]},{"title":"渣渣菜鸡的有赞面试经历（一）","date":"2018-07-11T16:00:00.000Z","path":"2018/07/12/youzan/","text":"出去面试的话还是得好好准备，不然会被虐的有点惨！ 有赞（框架组）四月份面试有赞的时候，自己还在实习，所以也没有复习，是在 Boss 直聘上投的，当时看到了有赞的 2018 届春招，就投了下，然后不知道怎么就被推到了框架组，结果后面就感觉自己被虐的可惨了。 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/07/12/youzan/ 电话一面好像是清明节还是五一劳动节来着，我还在睡觉，就接到一面面试官的电话，说现在有时间吗，能够接受下电话面试吗？我勒个去，今天过节、我被电话吵醒的，现在人都没清醒、这面试那肯定得一面就挂了，所以就老实得说现在还是不方便呢，约了周一上午 10 点面试。 周一 10 点面试官准时打电话过来了！ 以下是面试的问题： 1、自我介绍 2、Map 的底层结构？（HashMap） 3、线程安全的 Map （concurrentHashMap）简单的说了下这两 1。7 和 1.8 的区别，本想问下要不要深入的讲下（源码级别），结果面试官说不用了。 4、项目 MySQL 的数据量和并发量有多大？ 5、你对数据库了解多少？ 6、你说下数据库的索引实现和非主键的二级索引 7、项目用的是 SpringBoot ，你能说下 Spring Boot 与 Spring 的区别吗？ 8、SpringBoot 的自动配置是怎么做的？ 9、MyBatis 定义的接口，怎么找到实现的？ 10、Java 内存结构 11、对象是否可 GC？ 12、Minor GC 和 Full GC 13、垃圾回收算法 14、垃圾回收器 G1 15、项目里用过 ElasticSearch 和 Hbase，有深入了解他们的调优技巧吗？ 16、Spring RestTemplate 的具体实现 17、描述下网页一个 Http 请求，到后端的整个请求过程 18、多线程的常用方法和接口类及线程池的机制 19、总结我的 Java 基础还是不错，但是一些主流的框架源码还是处在使用的状态，需要继续去看源码 20、死锁 21、自己研究比较新的技术，说下成果！ 22、你有什么想问的？我就问了下公司那边的情况，这个自由发挥！ 最后我知道有二面的面试机会了。 10 来分钟不到，就再次打电话过来约了明早上午 10 点的视频面试。 视频二面二面面试官先打电话过来，然后加了个微信，开始微信视频面试 这个面试我也不太记得具体面试题目了，下面写的是大概方向的： 1、HashMap，源码级别的问了，包括为什么线程不安全 2、死锁 3、Synchronized 和 ReentrantLock 锁机制，怎么判断重入锁的，会不会是死锁？ 4、进程和线程的区别？ 5、进程之间如何保证同步？ 6、分布式锁 7、对象 GC 8、垃圾回收算法 9、JVM 参数 10、OOM 出现的有哪些场景？为什么会发生？ 11、JVM 内存结构说下吧 12、堆和栈的共享问题？ 13、有比较过 Http 和 RPC 吗？ 14、HttpClient 你说说里面的具体实现吧？（涉及了哪些东西） 15、那要你设计一个高性能的 Http ，你会怎么设计？ 二面微信视频面试只记得这么多了。 本文首发在我的知识星球，最近自己一直在写前段时间的所有面试情况，后面会一篇一篇分享在我的知识星球的，如果感兴趣，可以加入我的知识星球！ 知识星球更多面经文章： 1、蚂蚁金服电话一面 2、蚂蚁金服电话二面——后面聊的有点high 3、club factory 面经分享 4、作为面试官得到的经验 5、史上最强最全面经合集 6、公司需要什么样的人 7、如何介绍项目","tags":[{"name":"面经","slug":"面经","permalink":"http://yoursite.com/tags/面经/"}]},{"title":"20 个案例教你在 Java 8 中如何处理日期和时间?","date":"2018-06-19T16:00:00.000Z","path":"2018/06/20/java-8-date/","text":"前言前面一篇文章写了《SimpleDateFormat 如何安全的使用？》, 里面介绍了 SimpleDateFormat 如何处理日期／时间，以及如何保证线程安全，及其介绍了在 Java 8 中的处理时间／日期默认就线程安全的 DateTimeFormatter 类。那么 Java 8 中该怎么样处理生活中常见的一些日期／时间呢？比如：计算一周后的日期；计算一年前或一年后的日期；检查闰年等。 接下来创建了 20 个基于任务的实例来学习 Java 8 的新特性。从最简单创建当天的日期开始，然后创建时间及时区，接着模拟一个日期提醒应用中的任务——计算重要日期的到期天数，例如生日、纪念日、账单日、保费到期日、信用卡过期日等。 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/06/20/java-8-date/ 示例 1、在 Java 8 中获取今天的日期Java 8 中的 LocalDate 用于表示当天日期。和 java.util.Date 不同，它只有日期，不包含时间。当你仅需要表示日期时就用这个类。 12LocalDate now = LocalDate.now();System.out.println(now); 结果是： 12018-06-20 上面的代码创建了当天的日期，不含时间信息。打印出的日期格式非常友好，不像老的 Date 类打印出一堆没有格式化的信息。 示例 2、在 Java 8 中获取年、月、日信息LocalDate 类提供了获取年、月、日的快捷方法，其实例还包含很多其它的日期属性。通过调用这些方法就可以很方便的得到需要的日期信息，不用像以前一样需要依赖 java.util.Calendar 类了 12345LocalDate now = LocalDate.now();int year = now.getYear();int monthValue = now.getMonthValue();int dayOfMonth = now.getDayOfMonth();System.out.printf(\"year = %d, month = %d, day = %d\", year, monthValue, dayOfMonth); 结果是： 1year = 2018, month = 6, day = 20 示例 3、在 Java 8 中处理特定日期在第一个例子里，我们通过静态工厂方法 now() 非常容易地创建了当天日期，你还可以调用另一个有用的工厂方法LocalDate.of() 创建任意日期， 该方法需要传入年、月、日做参数，返回对应的 LocalDate 实例。这个方法的好处是没再犯老 API 的设计错误，比如年度起始于 1900，月份是从 0 开始等等。日期所见即所得，就像下面这个例子表示了 6 月 20 日，没有任何隐藏机关。 12LocalDate date = LocalDate.of(2018, 06, 20);System.out.println(date); 可以看到创建的日期完全符合预期，与写入的 2018 年 6 月 20 日完全一致。 示例 4、在 Java 8 中判断两个日期是否相等现实生活中有一类时间处理就是判断两个日期是否相等。你常常会检查今天是不是个特殊的日子，比如生日、纪念日或非交易日。这时就需要把指定的日期与某个特定日期做比较，例如判断这一天是否是假期。下面这个例子会帮助你用 Java 8 的方式去解决，你肯定已经想到了，LocalDate 重载了 equal 方法，请看下面的例子： 12345LocalDate now = LocalDate.now();LocalDate date = LocalDate.of(2018, 06, 20);if (date.equals(now)) &#123; System.out.println(\"同一天\");&#125; 这个例子中我们比较的两个日期相同。注意，如果比较的日期是字符型的，需要先解析成日期对象再作判断。 示例 5、在 Java 8 中检查像生日这种周期性事件Java 中另一个日期时间的处理就是检查类似每月账单、结婚纪念日、EMI日或保险缴费日这些周期性事件。如果你在电子商务网站工作，那么一定会有一个模块用来在圣诞节、感恩节这种节日时向客户发送问候邮件。Java 中如何检查这些节日或其它周期性事件呢？答案就是 MonthDay 类。这个类组合了月份和日，去掉了年，这意味着你可以用它判断每年都会发生事件。和这个类相似的还有一个 YearMonth 类。这些类也都是不可变并且线程安全的值类型。下面我们通过 MonthDay 来检查周期性事件： 123456789LocalDate now = LocalDate.now();LocalDate dateOfBirth = LocalDate.of(2018, 06, 20);MonthDay birthday = MonthDay.of(dateOfBirth.getMonth(), dateOfBirth.getDayOfMonth());MonthDay currentMonthDay = MonthDay.from(now);if (currentMonthDay.equals(birthday)) &#123; System.out.println(\"Happy Birthday\");&#125; else &#123; System.out.println(\"Sorry, today is not your birthday\");&#125; 结果：（注意：获取当前时间可能与你看的时候不对，所以这个结果可能和你看的时候运行结果不一样） 1Happy Birthday 只要当天的日期和生日匹配，无论是哪一年都会打印出祝贺信息。你可以把程序整合进系统时钟，看看生日时是否会受到提醒，或者写一个单元测试来检测代码是否运行正确。 示例 6、在 Java 8 中获取当前时间与 Java 8 获取日期的例子很像，获取时间使用的是 LocalTime 类，一个只有时间没有日期的 LocalDate 近亲。可以调用静态工厂方法 now() 来获取当前时间。默认的格式是 hh:mm:ss:nnn。 12LocalTime localTime = LocalTime.now();System.out.println(localTime); 结果： 113:35:56.155 可以看到当前时间就只包含时间信息，没有日期。 示例 7、如何在现有的时间上增加小时通过增加小时、分、秒来计算将来的时间很常见。Java 8 除了不变类型和线程安全的好处之外，还提供了更好的plusHours() 方法替换 add()，并且是兼容的。注意，这些方法返回一个全新的 LocalTime 实例，由于其不可变性，返回后一定要用变量赋值。 1234LocalTime localTime = LocalTime.now();System.out.println(localTime);LocalTime localTime1 = localTime.plusHours(2);//增加2小时System.out.println(localTime1); 结果： 1213:41:20.72115:41:20.721 可以看到，新的时间在当前时间 13:41:20.721 的基础上增加了 2 个小时。 示例 8、如何计算一周后的日期和上个例子计算两小时以后的时间类似，这个例子会计算一周后的日期。LocalDate 日期不包含时间信息，它的 plus()方法用来增加天、周、月，ChronoUnit 类声明了这些时间单位。由于 LocalDate 也是不变类型，返回后一定要用变量赋值。 1234LocalDate now = LocalDate.now();LocalDate plusDate = now.plus(1, ChronoUnit.WEEKS);System.out.println(now);System.out.println(plusDate); 结果： 122018-06-202018-06-27 可以看到新日期离当天日期是 7 天，也就是一周。你可以用同样的方法增加 1 个月、1 年、1 小时、1 分钟甚至一个世纪，更多选项可以查看 Java 8 API 中的 ChronoUnit 类。 示例 9、计算一年前或一年后的日期继续上面的例子，上个例子中我们通过 LocalDate 的 plus() 方法增加天数、周数或月数，这个例子我们利用 minus() 方法计算一年前的日期。 12345LocalDate now = LocalDate.now();LocalDate minusDate = now.minus(1, ChronoUnit.YEARS);LocalDate plusDate1 = now.plus(1, ChronoUnit.YEARS);System.out.println(minusDate);System.out.println(plusDate1); 结果： 122017-06-202019-06-20 示例 10、使用 Java 8 的 Clock 时钟类Java 8 增加了一个 Clock 时钟类用于获取当时的时间戳，或当前时区下的日期时间信息。以前用到System.currentTimeInMillis() 和 TimeZone.getDefault() 的地方都可用 Clock 替换。 1234Clock clock = Clock.systemUTC();Clock clock1 = Clock.systemDefaultZone();System.out.println(clock);System.out.println(clock1); 结果： 12SystemClock[Z]SystemClock[Asia/Shanghai] 示例 11、如何用 Java 判断日期是早于还是晚于另一个日期另一个工作中常见的操作就是如何判断给定的一个日期是大于某天还是小于某天？在 Java 8 中，LocalDate 类有两类方法 isBefore() 和 isAfter() 用于比较日期。调用 isBefore() 方法时，如果给定日期小于当前日期则返回 true。 12345678LocalDate tomorrow = LocalDate.of(2018,6,20);if(tomorrow.isAfter(now))&#123; System.out.println(\"Tomorrow comes after today\");&#125;LocalDate yesterday = now.minus(1, ChronoUnit.DAYS);if(yesterday.isBefore(now))&#123; System.out.println(\"Yesterday is day before today\");&#125; 在 Java 8 中比较日期非常方便，不需要使用额外的 Calendar 类来做这些基础工作了。 示例 12、在 Java 8 中处理时区Java 8 不仅分离了日期和时间，也把时区分离出来了。现在有一系列单独的类如 ZoneId 来处理特定时区，ZoneDateTime 类来表示某时区下的时间。这在 Java 8 以前都是 GregorianCalendar 类来做的。 1234ZoneId america = ZoneId.of(\"America/New_York\");LocalDateTime localtDateAndTime = LocalDateTime.now();ZonedDateTime dateAndTimeInNewYork = ZonedDateTime.of(localtDateAndTime, america );System.out.println(dateAndTimeInNewYork); 示例 13、如何表示信用卡到期这类固定日期，答案就在 YearMonth与 MonthDay 检查重复事件的例子相似，YearMonth 是另一个组合类，用于表示信用卡到期日、FD 到期日、期货期权到期日等。还可以用这个类得到 当月共有多少天，YearMonth 实例的 lengthOfMonth() 方法可以返回当月的天数，在判断 2 月有 28 天还是 29 天时非常有用。 1234YearMonth currentYearMonth = YearMonth.now();System.out.printf(\"Days in month year %s: %d%n\", currentYearMonth, currentYearMonth.lengthOfMonth());YearMonth creditCardExpiry = YearMonth.of(2018, Month.FEBRUARY);System.out.printf(\"Your credit card expires on %s %n\", creditCardExpiry); 结果： 12Days in month year 2018-06: 30Your credit card expires on 2018-02 示例 14、如何在 Java 8 中检查闰年LocalDate 类有一个很实用的方法 isLeapYear() 判断该实例是否是一个闰年。 示例 15、计算两个日期之间的天数和月数有一个常见日期操作是计算两个日期之间的天数、周数或月数。在 Java 8 中可以用 java.time.Period 类来做计算。下面这个例子中，我们计算了当天和将来某一天之间的月数。 123LocalDate date = LocalDate.of(2019, Month.MARCH, 20);Period period = Period.between(now, date);System.out.println(&quot;离下个时间还有&quot; + period.getMonths() + &quot; 个月&quot;); 示例 16、包含时差信息的日期和时间在 Java 8 中，ZoneOffset 类用来表示时区，举例来说印度与 GMT 或 UTC 标准时区相差 +05:30，可以通过ZoneOffset.of() 静态方法来获取对应的时区。一旦得到了时差就可以通过传入 LocalDateTime 和 ZoneOffset 来创建一个 OffSetDateTime 对象。 1234LocalDateTime datetime = LocalDateTime.of(2014, Month.JANUARY, 14,19,30);ZoneOffset offset = ZoneOffset.of(\"+05:30\");OffsetDateTime date = OffsetDateTime.of(datetime, offset); System.out.println(\"Date and Time with timezone offset in Java : \" + date); 示例 17、在 Java 8 中获取当前的时间戳如果你还记得 Java 8 以前是如何获得当前时间戳，那么现在你终于解脱了。Instant 类有一个静态工厂方法 now() 会返回当前的时间戳，如下所示： 12Instant timestamp = Instant.now();System.out.println(timestamp); 结果： 12018-06-20T06:35:24.881Z 时间戳信息里同时包含了日期和时间，这和 java.util.Date 很像。实际上 Instant 类确实等同于 Java 8 之前的 Date类，你可以使用 Date 类和 Instant 类各自的转换方法互相转换，例如：Date.from(Instant) 将 Instant 转换成java.util.Date，Date.toInstant() 则是将 Date 类转换成 Instant 类。 示例 18、在 Java 8 中如何使用预定义的格式化工具去解析或格式化日期在 Java 8 以前的世界里，日期和时间的格式化非常诡异，唯一的帮助类 SimpleDateFormat 也是非线程安全的，而且用作局部变量解析和格式化日期时显得很笨重。幸好线程局部变量能使它在多线程环境中变得可用，不过这都是过去时了。Java 8 引入了全新的日期时间格式工具，线程安全而且使用方便。它自带了一些常用的内置格式化工具。 参见我上一篇文章： 《SimpleDateFormat 如何安全的使用？》 示例 19、如何在 Java 中使用自定义格式化工具解析日期尽管内置格式化工具很好用，有时还是需要定义特定的日期格式。可以调用 DateTimeFormatter 的 ofPattern() 静态方法并传入任意格式返回其实例，格式中的字符和以前代表的一样，M 代表月，m 代表分。如果格式不规范会抛出 DateTimeParseException 异常，不过如果只是把 M 写成 m 这种逻辑错误是不会抛异常的。 参见我上一篇文章： 《SimpleDateFormat 如何安全的使用？》 示例 20、在 Java 8 中如何把日期转换成字符串上两个主要是从字符串解析日期。现在我们反过来，把 LocalDateTime 日期实例转换成特定格式的字符串。这是迄今为止 Java 日期转字符串最为简单的方式了。下面的例子将返回一个代表日期的格式化字符串。和前面类似，还是需要创建 DateTimeFormatter 实例并传入格式，但这回调用的是 format() 方法，而非 parse() 方法。这个方法会把传入的日期转化成指定格式的字符串。 123456789LocalDateTime arrivalDate = LocalDateTime.now();try &#123; DateTimeFormatter format = DateTimeFormatter.ofPattern(\"MMMdd yyyy hh:mm a\"); String landing = arrivalDate.format(format); System.out.printf(\"Arriving at : %s %n\", landing);&#125;catch (DateTimeException ex) &#123; System.out.printf(\"%s can't be formatted!%n\", arrivalDate); ex.printStackTrace();&#125; Java 8 日期时间 API 的重点通过这些例子，你肯定已经掌握了 Java 8 日期时间 API 的新知识点。现在来回顾一下这个优雅 API 的使用要点： 1）提供了 javax.time.ZoneId 获取时区。 2）提供了 LocalDate 和 LocalTime 类。 3）Java 8 的所有日期和时间 API 都是不可变类并且线程安全，而现有的 Date 和 Calendar API 中的 java.util.Date 和SimpleDateFormat 是非线程安全的。 4）主包是 java.time, 包含了表示日期、时间、时间间隔的一些类。里面有两个子包 java.time.format 用于格式化， java.time.temporal 用于更底层的操作。 5）时区代表了地球上某个区域内普遍使用的标准时间。每个时区都有一个代号，格式通常由区域/城市构成（Asia/Tokyo），在加上与格林威治或 UTC 的时差。例如：东京的时差是 +09:00。 6）OffsetDateTime 类实际上组合了 LocalDateTime 类和 ZoneOffset 类。用来表示包含和格林威治或 UTC 时差的完整日期（年、月、日）和时间（时、分、秒、纳秒）信息。 7）DateTimeFormatter 类用来格式化和解析时间。与 SimpleDateFormat 不同，这个类不可变并且线程安全，需要时可以给静态常量赋值。 DateTimeFormatter 类提供了大量的内置格式化工具，同时也允许你自定义。在转换方面也提供了 parse() 将字符串解析成日期，如果解析出错会抛出 DateTimeParseException。DateTimeFormatter 类同时还有format() 用来格式化日期，如果出错会抛出 DateTimeException异常。 8）再补充一点，日期格式“MMM d yyyy”和“MMM dd yyyy”有一些微妙的不同，第一个格式可以解析“Jan 2 2014”和“Jan 14 2014”，而第二个在解析“Jan 2 2014”就会抛异常，因为第二个格式里要求日必须是两位的。如果想修正，你必须在日期只有个位数时在前面补零，就是说“Jan 2 2014”应该写成 “Jan 02 2014”。 相关文章SimpleDateFormat 如何安全的使用？","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"SimpleDateFormat 如何安全的使用？","date":"2018-06-18T16:00:00.000Z","path":"2018/06/19/SimpleDateFormat/","text":"前言为什么会写这篇文章？因为这些天在看《阿里巴巴开发手册详尽版》，没看过的可以关注微信公众号：zhisheng，回复关键字：阿里巴巴开发手册详尽版 就可以获得。 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/06/19/SimpleDateFormat/ 在看的过程中有这么一条： 【强制】SimpleDateFormat 是线程不安全的类，一般不要定义为 static 变量，如果定义为 static，必须加锁，或者使用 DateUtils 工具类。 看到这条我立马就想起了我实习的时候有个项目里面就犯了这个错误，记得当时是这样写的： 1private static final SimpleDateFormat df = new SimpleDateFormat(\"yyyyMMddHHmmss\"); 所以才认真的去研究下这个 SimpleDateFormat，所以才有了这篇文章。 它是谁？想必大家对 SimpleDateFormat 并不陌生。SimpleDateFormat 是 Java 中一个非常常用的类，他是以区域敏感的方式格式化和解析日期的具体类。 它允许格式化 (date -&gt; text)、语法分析 (text -&gt; date)和标准化。 SimpleDateFormat 允许以任何用户指定的日期-时间格式方式启动。 但是，建议使用 DateFormat 中的 getTimeInstance、 getDateInstance 或 getDateTimeInstance 方法来创建一个日期-时间格式。 这几个方法会返回一个默认的日期／时间格式。 你可以根据需要用 applyPattern 方法修改格式方式。 日期时间格式日期和时间格式由 日期和时间模式字符串 指定。在 日期和时间模式字符串 中，未加引号的字母 ‘A’ 到 ‘Z’ 和 ‘a’ 到 ‘z’ 被解释为模式字母，用来表示日期或时间字符串元素。文本可以使用单引号 (‘) 引起来，以免进行解释。所有其他字符均不解释，只是在格式化时将它们简单复制到输出字符串。 简单的讲：这些 A ——Z，a —— z 这些字母(不被单引号包围的)会被特殊处理替换为对应的日期时间，其他的字符串还是原样输出。 日期和时间模式(注意大小写，代表的含义是不同的)如下： 怎么使用？日期／时间格式模版样例：（给的时间是：2001-07-04 12:08:56 U.S. Pacific Time time zone） 使用方法： 1234567891011121314151617181920212223import java.text.SimpleDateFormat;import java.util.Date;/** * Created by zhisheng_tian on 2018/6/19 */public class FormatDateTime &#123; public static void main(String[] args) &#123; SimpleDateFormat myFmt = new SimpleDateFormat(\"yyyy年MM月dd日 HH时mm分ss秒\"); SimpleDateFormat myFmt1 = new SimpleDateFormat(\"yy/MM/dd HH:mm\"); SimpleDateFormat myFmt2 = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");//等价于now.toLocaleString() SimpleDateFormat myFmt3 = new SimpleDateFormat(\"yyyy年MM月dd日 HH时mm分ss秒 E \"); SimpleDateFormat myFmt4 = new SimpleDateFormat(\"一年中的第 D 天 一年中第w个星期 一月中第W个星期 在一天中k时 z时区\"); Date now = new Date(); System.out.println(myFmt.format(now)); System.out.println(myFmt1.format(now)); System.out.println(myFmt2.format(now)); System.out.println(myFmt3.format(now)); System.out.println(myFmt4.format(now)); System.out.println(now.toGMTString()); System.out.println(now.toLocaleString()); System.out.println(now.toString()); &#125;&#125; 结果是： 123456782018年06月19日 23时10分05秒18/06/19 23:102018-06-19 23:10:052018年06月19日 23时10分05秒 星期二一年中的第 170 天 一年中第25个星期 一月中第4个星期 在一天中23时 CST时区19 Jun 2018 15:10:05 GMT2018-6-19 23:10:05Tue Jun 19 23:10:05 CST 2018 使用方法很简单，就是先自己定义好时间／日期模版，然后调用 format 方法（传入一个时间 Date 参数）。 上面的是日期转换成自己想要的字符串格式。下面反过来，将字符串类型装换成日期类型： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;/** * Created by zhisheng_tian on 2018/6/19 */public class StringFormatDate &#123; public static void main(String[] args) &#123; String time1 = \"2018年06月19日 23时10分05秒\"; String time2 = \"18/06/19 23:10\"; String time3 = \"2018-06-19 23:10:05\"; String time4 = \"2018年06月19日 23时10分05秒 星期二\"; SimpleDateFormat myFmt = new SimpleDateFormat(\"yyyy年MM月dd日 HH时mm分ss秒\"); SimpleDateFormat myFmt1 = new SimpleDateFormat(\"yy/MM/dd HH:mm\"); SimpleDateFormat myFmt2 = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");//等价于now.toLocaleString() SimpleDateFormat myFmt3 = new SimpleDateFormat(\"yyyy年MM月dd日 HH时mm分ss秒 E\"); Date date1 = null; try &#123; date1 = myFmt.parse(time1); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; System.out.println(date1); Date date2 = null; try &#123; date2 = myFmt1.parse(time2); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; System.out.println(date2); Date date3 = null; try &#123; date3 = myFmt2.parse(time3); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; System.out.println(date3); Date date4 = null; try &#123; date4 = myFmt3.parse(time4); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; System.out.println(date4); &#125;&#125; 结果是： 1234Tue Jun 19 23:10:05 CST 2018Tue Jun 19 23:10:00 CST 2018Tue Jun 19 23:10:05 CST 2018Tue Jun 19 23:10:05 CST 2018 这个转换方法也很简单。但是不要高兴的太早，主角不在这。 线程不安全 在 SimpleDateFormat 类的 JavaDoc 中，描述了该类不能够保证线程安全，建议为每个线程创建单独的日期／时间格式实例，如果多个线程同时访问一个日期／时间格式，它必须在外部进行同步。那么在多线程环境下调用 format() 和 parse() 方法应该使用同步代码来避免问题。下面我们通过一个具体的场景来一步步的深入学习和理解SimpleDateFormat 类。 1、每个线程创建单独的日期／时间格式实例 大量的创建 SimpleDateFormat 实例对象，然后再丢弃这个对象，占用大量的内存和 JVM 空间。 2、创建一个静态的 SimpleDateFormat 实例，在使用时直接使用这个实例进行操作（我当时就是这么干的😄） 123private static final SimpleDateFormat df = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");Date date = new Date();df.format(date); 当然，这个方法的确很不错，在大部分的时间里面都会工作得很好，但一旦在生产环境中一定负载情况下时，这个问题就出来了。他会出现各种不同的情况，比如转化的时间不正确，比如报错，比如线程被挂死等等。我们看下面的测试用例，拿事实说话： 1234567891011121314151617import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;/** * Created by zhisheng_tian on 2018/6/20 */public class DateUtils &#123; private static final SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); public static String formatDate(Date date) throws ParseException &#123; return sdf.format(date); &#125; public static Date parse(String strDate) throws ParseException &#123; return sdf.parse(strDate); &#125;&#125; 12345678910111213141516171819202122232425262728import java.text.ParseException;/** * Created by zhisheng_tian on 2018/6/20 */public class DateUtilsTest &#123; public static class TestSimpleDateFormatThreadSafe extends Thread &#123; @Override public void run() &#123; while (true) &#123; try &#123; this.join(2000); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; try &#123; System.out.println(this.getName() + \":\" + DateUtils.parse(\"2018-06-20 01:18:20\")); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; for (int i = 0; i &lt; 3; i++) &#123; new TestSimpleDateFormatThreadSafe().start(); &#125; &#125;&#125; 运行结果如下： 1234567891011121314151617181920212223242526Exception in thread \"Thread-0\" Exception in thread \"Thread-1\" java.lang.NumberFormatException: For input string: \"\" at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) at java.lang.Long.parseLong(Long.java:601) at java.lang.Long.parseLong(Long.java:631) at java.text.DigitList.getLong(DigitList.java:195) at java.text.DecimalFormat.parse(DecimalFormat.java:2051) at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869) at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) at java.text.DateFormat.parse(DateFormat.java:364) at com.zhisheng.demo.date.DateUtils.parse(DateUtils.java:19) at com.zhisheng.demo.date.DateUtilsTest$TestSimpleDateFormatThreadSafe.run(DateUtilsTest.java:19)java.lang.NumberFormatException: For input string: \".1818\" at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) at java.lang.Long.parseLong(Long.java:578) at java.lang.Long.parseLong(Long.java:631) at java.text.DigitList.getLong(DigitList.java:195) at java.text.DecimalFormat.parse(DecimalFormat.java:2051) at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:2162) at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) at java.text.DateFormat.parse(DateFormat.java:364) at com.zhisheng.demo.date.DateUtils.parse(DateUtils.java:19) at com.zhisheng.demo.date.DateUtilsTest$TestSimpleDateFormatThreadSafe.run(DateUtilsTest.java:19)Thread-2:Sat Jun 20 01:18:20 CST 2201Thread-2:Wed Jun 20 01:18:20 CST 2018Thread-2:Wed Jun 20 01:18:20 CST 2018Thread-2:Wed Jun 20 01:18:20 CST 2018 说明：Thread-1和Thread-0报java.lang.NumberFormatException: multiple points错误，直接挂死，没起来；Thread-2 虽然没有挂死，但输出的时间是有错误的，比如我们输入的时间是：2018-06-20 01:18:20 ，当会输出：Sat Jun 20 01:18:20 CST 2201 这样的灵异事件。 Why?为什么会出现线程不安全的问题呢？ 下面我们通过看 JDK 源码来看看为什么 SimpleDateFormat 和 DateFormat 类不是线程安全的真正原因： SimpleDateFormat 继承了 DateFormat，在 DateFormat 中定义了一个 protected 属性的 Calendar 类的对象：calendar。只是因为 Calendar 类的概念复杂，牵扯到时区与本地化等等，JDK 的实现中使用了成员变量来传递参数，这就造成在多线程的时候会出现错误。 在 SimpleDateFormat 中的 format 方法源码中： 123456789101112131415161718192021222324252627282930313233@Overridepublic StringBuffer format(Date date, StringBuffer toAppendTo,FieldPosition pos) &#123; pos.beginIndex = pos.endIndex = 0; return format(date, toAppendTo, pos.getFieldDelegate());&#125;// Called from Format after creating a FieldDelegateprivate StringBuffer format(Date date, StringBuffer toAppendTo,FieldDelegate delegate) &#123; // Convert input date to time field list calendar.setTime(date); boolean useDateFormatSymbols = useDateFormatSymbols(); for (int i = 0; i &lt; compiledPattern.length; ) &#123; int tag = compiledPattern[i] &gt;&gt;&gt; 8; int count = compiledPattern[i++] &amp; 0xff; if (count == 255) &#123; count = compiledPattern[i++] &lt;&lt; 16; count |= compiledPattern[i++]; &#125; switch (tag) &#123; case TAG_QUOTE_ASCII_CHAR: toAppendTo.append((char)count); break; case TAG_QUOTE_CHARS: toAppendTo.append(compiledPattern, i, count); i += count; break; default: subFormat(tag, count, delegate, toAppendTo, useDateFormatSymbols); break; &#125; &#125; return toAppendTo;&#125; calendar.setTime(date) 这条语句改变了 calendar，稍后，calendar 还会用到（在 subFormat 方法里），而这就是引发问题的根源。想象一下，在一个多线程环境下，有两个线程持有了同一个 SimpleDateFormat 的实例，分别调用format 方法： 12345线程 1 调用 format 方法，改变了 calendar 这个字段。线程 1 中断了。线程 2 开始执行，它也改变了 calendar。线程 2 中断了。线程 1 回来了 此时，calendar 已然不是它所设的值，而是走上了线程 2 设计的道路。如果多个线程同时争抢 calendar 对象，则会出现各种问题，时间不对，线程挂死等等。 分析一下 format 的实现，我们不难发现，用到成员变量 calendar，唯一的好处，就是在调用 subFormat 时，少了一个参数，却带来了许多的问题。其实，只要在这里用一个局部变量，一路传递下去，所有问题都将迎刃而解。 这个问题背后隐藏着一个更为重要的问题–无状态：无状态方法的好处之一，就是它在各种环境下，都可以安全的调用。衡量一个方法是否是有状态的，就看它是否改动了其它的东西，比如全局变量，比如实例的字段。format 方法在运行过程中改动了 SimpleDateFormat 的 calendar 字段，所以，它是有状态的。 这也同时提醒我们在开发和设计系统的时候注意下一下三点: 1.自己写公用类的时候，要对多线程调用情况下的后果在注释里进行明确说明 2.多线程环境下，对每一个共享的可变变量都要注意其线程安全性 3.我们的类和方法在做设计的时候，要尽量设计成无状态的 解决方法1、需要的时候创建新实例 说明：在需要用到 SimpleDateFormat 的地方新建一个实例，不管什么时候，将有线程安全问题的对象由共享变为局部私有都能避免多线程问题，不过也加重了创建对象的负担。在一般情况下，这样其实对性能影响比不是很明显的。 2、使用同步：同步 SimpleDateFormat 对象 1234567891011121314151617181920import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;public class DateSyncUtil &#123; private static SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); public static String formatDate(Date date) throws ParseException &#123; synchronized(sdf) &#123; return sdf.format(date); &#125; &#125; public static Date parse(String strDate) throws ParseException &#123; synchronized(sdf) &#123; return sdf.parse(strDate); &#125; &#125;&#125; 说明：当线程较多时，当一个线程调用该方法时，其他想要调用此方法的线程就要 block 等待，多线程并发量大的时候会对性能有一定的影响。 3、使用 ThreadLocal 12345678910111213141516171819202122import java.text.DateFormat;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;public class ConcurrentDateUtil &#123; private static ThreadLocal&lt;DateFormat&gt; threadLocal = new ThreadLocal&lt;DateFormat&gt;() &#123; @Override protected DateFormat initialValue() &#123; return new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); &#125; &#125;; public static Date parse(String dateStr) throws ParseException &#123; return threadLocal.get().parse(dateStr); &#125; public static String format(Date date) &#123; return threadLocal.get().format(date); &#125;&#125; 说明：使用 ThreadLocal, 也是将共享变量变为独享，线程独享肯定能比方法独享在并发环境中能减少不少创建对象的开销。如果对性能要求比较高的情况下，一般推荐使用这种方法。 Java 8 中的解决办法Java 8 提供了新的日期时间 API，其中包括用于日期时间格式化的 DateTimeFormatter，它与 SimpleDateFormat 最大的区别在于：DateTimeFormatter 是线程安全的，而 SimpleDateFormat 并不是线程安全。 DateTimeFormatter 如何使用： 解析日期 123String dateStr= \"2018年06月20日\";DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"yyyy年MM月dd日\"); LocalDate date= LocalDate.parse(dateStr, formatter); 日期转换为字符串 123LocalDateTime now = LocalDateTime.now(); DateTimeFormatter format = DateTimeFormatter.ofPattern(\"yyyy年MM月dd日 hh:mm a\");String nowStr = now .format(format); 由 DateTimeFormatter 的静态方法 ofPattern() 构建日期格式，LocalDateTime 和 LocalDate 等一些表示日期或时间的类使用 parse 和 format 方法把日期和字符串做转换。 使用新的 API，整个转换过程都不需要考虑线程安全的问题。 总结SimpleDateFormat 是线程不安全的类，多线程环境下注意线程安全问题，如果是 Java 8 ，建议使用 DateTimeFormatter 代替 SimpleDateFormat。 参考资料http://www.cnblogs.com/peida/archive/2013/05/31/3070790.html 相关文章20 个案例教你在 Java 8 中如何处理日期和时间?","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"苦逼的毕业论文经历","date":"2018-05-25T16:00:00.000Z","path":"2018/05/26/paper/","text":"背景最近一直在学校忙毕业论文的事，抱歉了，很长一段时间没更新文章了，今天星期六，昨天星期五，幸好所有资料都在周末前交齐了，昨晚还到参加班上的聚会，也喝了不少酒，但幸好没醉，不然今天肯定不会写下这篇文章的。这篇文章就把从接触到毕业论文指导老师到现在的这半年时间有关毕业论文的事都讲讲，希望能够留下点回忆！ 毕设小分队成立学校会给每个毕业导师安排带几个学生（7～8 人左右），但是在学校发出表格之前，谁也不知道自己将会由哪个导师带，甚至你会发现就算学校发出表格后看到自己导师名字后，自己都不知道这导师是谁？是男是女？（当某个导师的名字比较中性的时候）到底好不好相处？性格咋样？他的联系方式（手机 或者 QQ），WTF，一个都不知道？苦逼了，后面也是通过问辅导员才知道这些情况。后面我们几个人中就有一个带头建了个 QQ 群（毕设小分队），并把指导老师拉进群了。 就这样，我们终于开始了毕设小分队之旅！ 实习时的毕设状态2017.12月那时是还在公司实习的，到了毕业设计选题的时候，我们导师还好，给我们每个人很自由的选择，你可以自拟题目，也可以从导师那里挑选题目。不过我看了下老师给的题目，大都是深度学习、机器学习做的相关图像识别、推荐系统、搜索系统、人脸识别、网络流量监控、文本情感分析等高大上的课题。臣妾做不到，毕竟这都是研究生才会研究点方向，老师会发出来这些题目可能也是和她自己在研究学习的相关知识有关（因为我后来给导师检查我的系统的时候就是导师带的一个研究生检查的，期间那个学长和我说他最近就在做文本情感分析相关的一个项目，因为看到我项目中的中文分词器，所以还向我请教了下，我于是把我原来写过的一篇中英文分词器找出来给他一个个解释），所以导师希望在我们这里也能够有人去研究下这方面的知识，拓展下视野。但是由于在公司实习，工作之外腾出的时间确实有限，没这么多时间去学习这些特别新的东西，而且还要做出一定的成果出来，这还是有点挑战性的。导师说你可以自拟题目，或者把自己在公司做的项目优化后拿来做毕设项目。于是，我选择的是拿自己以前写的项目来当毕业论文项目了，因为感觉项目也还行，拿来做毕设完全可以了，所以后面自己也就比较轻松了。 毕设流程把自己的毕设题目报给老师后，老师会根据你的题目审核是否可以拿来当毕设。然后再要求你把毕业设计的开题报告书（论文题目、目的、意义、怎么做、做出什么效果、目标之类的）写详细上交上去。要经过导师和学校的审批后才能够正式开题。开题后确定后几乎就可以开始写自己的项目了。机智的我，那时比较轻松，哈哈哈，工作之外都撸代码和博客呢。差不多过了一个月后，导师就开始在群里问我们系统完成的怎么样了？然后要我们上交论文任务书，学校的这种材料有时候要的很急，不得不吐槽下，很头疼，自己有时候都一下子交不了。又过了一段时间就需要把论文初稿交上去，真是苦逼，那时还不知道论文的模版到底是咋样？该怎么写本科论文呢？搞得我自己那段时间花了周末两天和一天的上班的时间才勉强交了个一般的初稿，初稿中的各种图画的我是想哭。真多，还麻烦。幸好在那段时间公司的任务还不忙，所以在有时间在实习上班的时候也稍微完成下自己的初稿。 今年 3 ～ 4 月的时候，导师也在不断的叫我们把学校该交的实习每周报告交给她。这个实习还有实习申请表、实习鉴定表材料（要求盖章），这两个资料也是很重要的，如果后面没有这个材料都不能参加答辩的。 在快五月的时候，导师那时就天天晚上深夜在毕设小分队群里催同学们，你们的论文和系统都完成的咋样了？然后又催了要交论文中期报告（这个应该也蛮重要的，我们班两个同学好像就因为这个导致有个论文中期报告警告，吓得他们后来答辩的时候都怕自己挂了）。 过完劳动节后，再过了几天后，我就辞职了。辞职后在苏州玩了几天，期间也是在不断的修改自己的论文和优化些格式问题。 后来又浪回家了，在家呆了几天，也是改论文改到深夜，那段时间自己已经开始在把自己的论文初稿拿到些第三方免费的查重去查重网站去查重，然后根据查重后的结果，将一些重复的地方东改改，西凑凑，或者用自己的话写一遍。这样就可以减少点重复语句。 再然后就是 18 号赶回来学校了，19，20号刚好周末，又是不眠之夜，那两天不断查重，向别人请教，怎么加字数，自己一开始的字数好像还是不够的，学校要求的字数有点高，其他学校都是 8000 字，我们却要求 1.5 w，真是能扯啊，怎么可以扯出这么多字来。通过某人的经验之谈，我成功的扯了不少字，还不带重复的，😄。 然后 21 号周一拿给老师检查的时候就是看了下格式（老师不在办公室，她带的研究生给检查的），说你全文首行咋都没缩进呢（写博客写习惯了，谁还缩进啊），行间距比较大，图的标号错误，主要还是检查格式。当晚，又好好改了下，并和我同学讨论了下他的指导导师的要求有哪些，并也做了相应的修改。（因为我的答辩导师就有我那同学的指导老师）记得当晚好像改论文改到三点，幸好有人陪着我一边聊天一边改，不然早困了，睡了。 22 号一早就醒来了，然后就去打印店打印论文。这里想给的大家一个建议就是：如果你也要打印论文，最好早点去，因为毕业季，学校打印店几乎都是爆满，打印论文的非常多，还有学弟学妹打印各种考试卷子、资料啥的，反正人很多，估计要排队。指导老师也不是一直在办公室的，他们也是要上课的。所以最好在导师在办公室的时候能赶到，这样就不会错过了。这次老师检查论文就比较仔细了，论文一行行的找内容，看是否通顺？是否有不合适的地方？标点符号是不是多了或者少了？格式是不是还有问题？所以呢，这次又很惨，要改的地方很多，还包括流程图和 E-R 图要改的。苦逼了，下午回去租房宾馆的时候就开始拼命的改。改的差不多了，因为第二天要答辩了，所以就在看看自己的系统是否能够跑起来，有没有什么bug，结果还真发现几个小错误和一个大错误，小错误很快修复完善了，有一个 Redis 存数据再取出来的时候数据变化问题当然debug了很久没找到原因。找到深夜一点多，没解决，放弃了，第二天不演示这块。当晚怕自己明天项目启动的时候其他环境要一个个起，需要耗费不少时间，于是自己简单的写了个脚本，一键将自己项目的环境启动起来，这样就可以直接运行项目展示给答辩老师，节省时间，尤其是关键场合，怕掉链子。 23 号早上很早就去把自己的实习申请表、实习鉴定表、实习 20 周日志、实习终结、论文初稿这些资料打印，然后拿给导师查看和打分。（又找出问题来了，苦逼，记得那个早上现场在导师办公室用电脑改好后，检查完才拿去打印再拿回来给导师检查的，来回跑来跑去的真的很急，出了不少汗）注意：越是关键时刻，千万要顺着导师来，他说改哪里就改哪里，千万别刚，我一个同学的论文改的导师都发火了，差点没让我那同学参加答辩，直接进入二辩的。 答辩的时候先每个人介绍下自己的实习经历，听到不少牛逼的同学，进了不少厉害的公司，有的同学还当上了公司的项目组组长了，真厉害，一年的实习时间就混的这么好了。瞬间发现差距很大。 然后就是每个人的答辩了，我第三个，还好，答辩的过程问了我的问题和演示系统的时候都表现的蛮好的。就现场还问了我有个功能的代码是哪？（估计是想看下是不是自己写的项目）后面也看到有的同学项目竟然起不来的，或者回答不出老师的问题的。答辩过程中，答辩老师又给论文找了不少问题，又要苦逼的修改了。 当天就出了待定名单和要二辩的名单，速度还是很快。还好我没有，开森。本以为答辩好了，就可以松下一口气了，后面改好了论文后拿去给导师检查，她都说 OK 了，结果我就去把论文拿去胶装了，注意胶装顺序别搞错了。第二天把胶装好的论文和各种资料一起交给老师的时候，结果又挑出一个目录的问题，WTF，搞得我后面改好后去打印店重新胶装了遍（封皮从原来的上面扯下来胶装）。昨天签完字了，把论文的初稿、终稿、答辩记录、实习材料等材料一起装进档案袋了，这才安心了。 于是当晚就去参加了班上的毕业聚会。 感受这段时间真的是很累，每天熬夜到很晚，第二天早上很早就自然醒了。瘦了不少，牛仔裤的皮带我感觉都缩了一圈，右手的几个手指天天在电脑的触摸板上滑来滑去的，都脱了一层皮了，现在放上去都有点痛，打印论文好像打印了7份，烧钱啊。发现我的记忆还是可以的，这么多事竟然还能记得，搞得整篇文章有点像在记流水账，哈哈哈。反正也是记录下自己的论文答辩这段时间的经历。论文的格式很重要，不再像写博客那样随意，论文是需要以一种严谨的态度去对待的。 写着写着忘了说论文的查重了。一般你一开始最好先用第三方的免费查重下，然后修改。 我当时用的是：http://xueshu.baidu.com/usercenter/papercheck/ 这个地方有几个是可以第一次免费查重的，如果次数用完了，记得换个百度账号就又有好些次查重的机会。如果你实在是不放心，可以去学校的打印店问下是否有论文查重的，也是查的知网的库，和学校知网查重的区别不大，我比较自信，没花这个钱，毕竟好贵，😯我穷，哈哈哈。学校规定的是不能高于 30% 的查重率，建议还是自己把查重降低到 20% 以下然后再提交到学校知网去，不然超过的话是需要二次查重的。 还有一个感受就是：多和你的导师交流，遇到不会或者拿不准的最好在群里问他，然后他说的错误，你要及时更改好过来，说你问题的时候要学会脸上挂着笑容，嘻嘻嘻就过去了，然后记住该问题的错误，回去立马改好，及时拿材料给老师查阅修改好的论文。","tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]},{"title":"Spring Boot 2.0系列文章(七)：SpringApplication 深入探索","date":"2018-04-29T16:00:00.000Z","path":"2018/04/30/springboot_SpringApplication/","text":"关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/04/30/springboot_SpringApplication/ 前言在 Spring Boot 项目的启动类中常见代码如下： 123456@SpringBootApplicationpublic class SpringbotApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringbotApplication.class, args); &#125;&#125; 其中也就两个比较引人注意的地方： @SpringBootApplication SpringApplication.run() 对于第一个注解 @SpringBootApplication，我已经在博客 Spring Boot 2.0系列文章(六)：Spring Boot 2.0中SpringBootApplication注解详解 中详细的讲解了。接下来就是深入探究第二个了 SpringApplication.run() 。 换个姿势上面的姿势太简单了，只一行代码就完事了。 1SpringApplication.run(SpringbotApplication.class, args); 其实是支持做一些个性化的设置，接下来我们换个姿势瞧瞧： 123456789@SpringBootApplicationpublic class SpringbotApplication &#123; public static void main(String[] args) &#123; SpringApplication app = new SpringApplication(SpringbotApplication.class); // 自定义应用程序的配置 //app.setXxx() app.run(args) &#125;&#125; 没错，就是通过一个构造函数，然后设置相关的属性，从而达到定制化服务。有哪些属性呢？ 属性对应的 get／set 方法 看到没，还很多呢！ 举个例子：你想把 Spring Boot 项目的默认 Banner 换成你自己的，就需要在这里如下： 123456789101112131415161718public static void main(String[] args) &#123;// SpringApplication.run(Springboot2Application.class, args); SpringApplication application = new SpringApplication(Springboot2Application.class); application.setBanner((environment, sourceClass, out) -&gt; &#123; //这里打印一个logo System.out.println(\" _ _ _\\n\" + \" | | (_) | |\\n\" + \" ____| |__ _ ___ | |__ ___ _ __ __ _\\n\" + \"|_ /| '_ \\\\ | |/ __|| '_ \\\\ / _ \\\\| '_ \\\\ / _` |\\n\" + \" / / | | | || |\\\\__ \\\\| | | || __/| | | || (_| |\\n\" + \"/___||_| |_||_||___/|_| |_| \\\\___||_| |_| \\\\__, |\\n\" + \" __/ |\\n\" + \" |___/\\n\"); &#125;); application.setBannerMode(Banner.Mode.CONSOLE); //你还可以干其他的定制化初始设置 application.run(args);&#125; 现在重启项目，你就会发现，控制台的 logo 已经换成你自己的了。 当然了，你可能会觉得这样写有点复杂，嗯嗯，确实，这样硬编码在代码里确实不太友好。你还可以在src/main/resources路径下新建一个banner.txt文件，banner.txt中填写好需要打印的字符串内容即可。 从该类中可以看到在 Spring Boot 2 中引入了个新的 WebApplicationType 和 WebEnvironment。 确实，这也是 Spring Boot 2 中比较大的特性，它是支持响应式编程的。我之前在文章 Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 中也介绍过，以后有机会会介绍它的，这里我先卖个关子。 SpringApplication 初始化SpringApplication.run() 的实现才是我们要深入探究的主角，该方法代码如下： 123456789//静态方法，可用于使用默认配置运行 SpringApplicationpublic static ConfigurableApplicationContext run(Class&lt;?&gt; primarySource, String... args) &#123; return run(new Class&lt;?&gt;[] &#123; primarySource &#125;, args);&#125;public static ConfigurableApplicationContext run(Class&lt;?&gt;[] primarySources, String[] args) &#123; return new SpringApplication(primarySources).run(args);&#125; 在这个静态方法中，创建 SpringApplication 对象，并调用该对象的 run 方法。 123456789101112131415public SpringApplication(Class&lt;?&gt;... primarySources) &#123; this(null, primarySources);&#125;//创建一个 SpringApplication 实例，应用上下文会根据指定的主要资源加载 beans ，实例在调用 run 方法之前可以定制化@SuppressWarnings(&#123; \"unchecked\", \"rawtypes\" &#125;)public SpringApplication(ResourceLoader resourceLoader, Class&lt;?&gt;... primarySources) &#123; this.resourceLoader = resourceLoader; Assert.notNull(primarySources, \"PrimarySources must not be null\"); this.primarySources = new LinkedHashSet&lt;&gt;(Arrays.asList(primarySources)); this.webApplicationType = deduceWebApplicationType(); setInitializers((Collection) getSpringFactoriesInstances( ApplicationContextInitializer.class)); setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); this.mainApplicationClass = deduceMainApplicationClass();&#125; 首先是进入单个参数的构造方法，然后进入两参数的构造方法（ResourceLoader 为 null），然后进行初始化。 1、deduceWebApplicationType() : 推断应用的类型 ，创建的是一个 SERVLET 应用还是 REACTIVE应用或者是 NONE 1234567891011121314151617private static final String REACTIVE_WEB_ENVIRONMENT_CLASS = \"org.springframework.web.reactive.DispatcherHandler\";private static final String MVC_WEB_ENVIRONMENT_CLASS = \"org.springframework.web.servlet.DispatcherServlet\";private static final String[] WEB_ENVIRONMENT_CLASSES = &#123; \"javax.servlet.Servlet\", \"org.springframework.web.context.ConfigurableWebApplicationContext\" &#125;;private WebApplicationType deduceWebApplicationType() &#123; if (ClassUtils.isPresent(REACTIVE_WEB_ENVIRONMENT_CLASS, null) &amp;&amp; !ClassUtils.isPresent(MVC_WEB_ENVIRONMENT_CLASS, null)) &#123; return WebApplicationType.REACTIVE; //该程序是 REACTIVE 程序 &#125; for (String className : WEB_ENVIRONMENT_CLASSES) &#123; if (!ClassUtils.isPresent(className, null)) &#123; return WebApplicationType.NONE; //该程序为 NONE &#125; &#125; return WebApplicationType.SERVLET; //默认返回是 SERVLET 程序&#125; 2、setInitializers((Collection) getSpringFactoriesInstances(ApplicationContextInitializer.class))：初始化 classpath 下的所有的可用的 ApplicationContextInitializer。 1）、getSpringFactoriesInstances() 123456789101112131415161718192021222324252627282930313233private &lt;T&gt; Collection&lt;T&gt; getSpringFactoriesInstances(Class&lt;T&gt; type) &#123; return getSpringFactoriesInstances(type, new Class&lt;?&gt;[] &#123;&#125;);&#125;//获取所有的 Spring 工厂实例private &lt;T&gt; Collection&lt;T&gt; getSpringFactoriesInstances(Class&lt;T&gt; type,Class&lt;?&gt;[] parameterTypes, Object... args) &#123; ClassLoader classLoader = Thread.currentThread().getContextClassLoader(); // Use names and ensure unique to protect against duplicates Set&lt;String&gt; names = new LinkedHashSet&lt;&gt;(SpringFactoriesLoader.loadFactoryNames(type, classLoader)); //获取所有 Spring Factories 的名字 List&lt;T&gt; instances = createSpringFactoriesInstances(type, parameterTypes, classLoader, args, names); AnnotationAwareOrderComparator.sort(instances); //Spring 工厂实例排序 return instances;&#125;//根据读取到的名字创建对象（Spring 工厂实例）private &lt;T&gt; List&lt;T&gt; createSpringFactoriesInstances(Class&lt;T&gt; type, Class&lt;?&gt;[] parameterTypes, ClassLoader classLoader, Object[] args, Set&lt;String&gt; names) &#123; List&lt;T&gt; instances = new ArrayList&lt;&gt;(names.size()); for (String name : names) &#123; try &#123; Class&lt;?&gt; instanceClass = ClassUtils.forName(name, classLoader); Assert.isAssignable(type, instanceClass); Constructor&lt;?&gt; constructor = instanceClass.getDeclaredConstructor(parameterTypes); T instance = (T) BeanUtils.instantiateClass(constructor, args); instances.add(instance); &#125; catch (Throwable ex) &#123; throw new IllegalArgumentException( \"Cannot instantiate \" + type + \" : \" + name, ex); &#125; &#125; return instances;&#125; 上面的 SpringFactoriesLoader.loadFactoryNames() ，是从 META-INF/spring.factories 的资源文件中，读取 key 为org.springframework.context.ApplicationContextInitializer 的 value。 而 spring.factories 的部分内容如下： 可以看到，最近的得到的，是 ConfigurationWarningsApplicationContextInitializer，ContextIdApplicationContextInitializer，DelegatingApplicationContextInitializer，ServerPortInfoApplicationContextInitializer 这四个类的名字。 2）、setInitializers()： 12345public void setInitializers( Collection&lt;? extends ApplicationContextInitializer&lt;?&gt;&gt; initializers) &#123; this.initializers = new ArrayList&lt;&gt;(); this.initializers.addAll(initializers);&#125; 所以，这里 setInitializers() 所得到的成员变量 initializers 就被初始化为ConfigurationWarningsApplicationContextInitializer，ContextIdApplicationContextInitializer，DelegatingApplicationContextInitializer，ServerPortInfoApplicationContextInitializer 这四个类的对象组成的 list。 3、setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class))：初始化 classpath 下的所有的可用的 ApplicationListener。 1）、getSpringFactoriesInstances() 和上面的类似，但是它是从 META-INF/spring.factories 的资源文件中，获取到 key 为 org.springframework.context.ApplicationListener 的 value。 2）、setListeners()： 1234public void setListeners(Collection&lt;? extends ApplicationListener&lt;?&gt;&gt; listeners) &#123; this.listeners = new ArrayList&lt;&gt;(); this.listeners.addAll(listeners);&#125; 所以，这里 setListeners() 所得到的成员变量 listeners 就被初始化为 ClearCachesApplicationListener，ParentContextCloserApplicationListener，FileEncodingApplicationListener，AnsiOutputApplicationListener ，ConfigFileApplicationListener，DelegatingApplicationListener，ClasspathLoggingApplicationListener，LoggingApplicationListener，LiquibaseServiceLocatorApplicationListener 这九个类的对象组成的 list。 4、deduceMainApplicationClass() ：根据调用栈，推断出 main 方法的类名 1234567891011121314private Class&lt;?&gt; deduceMainApplicationClass() &#123; try &#123; StackTraceElement[] stackTrace = new RuntimeException().getStackTrace(); for (StackTraceElement stackTraceElement : stackTrace) &#123; if (\"main\".equals(stackTraceElement.getMethodName())) &#123; return Class.forName(stackTraceElement.getClassName()); &#125; &#125; &#125; catch (ClassNotFoundException ex) &#123; // Swallow and continue &#125; return null;&#125; run 方法背后的秘密上面看完了构造方法后，已经初始化了一个 SpringApplication 对象，接下来调用其 run 方法，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445//运行 Spring 应用程序，创建并刷新一个新的 ApplicationContextpublic ConfigurableApplicationContext run(String... args) &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); ConfigurableApplicationContext context = null; Collection&lt;SpringBootExceptionReporter&gt; exceptionReporters = new ArrayList&lt;&gt;(); configureHeadlessProperty(); SpringApplicationRunListeners listeners = getRunListeners(args); listeners.starting(); try &#123; ApplicationArguments applicationArguments = new DefaultApplicationArguments( args); ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); configureIgnoreBeanInfo(environment); Banner printedBanner = printBanner(environment); context = createApplicationContext(); exceptionReporters = getSpringFactoriesInstances( SpringBootExceptionReporter.class, new Class[] &#123; ConfigurableApplicationContext.class &#125;, context); prepareContext(context, environment, listeners, applicationArguments, printedBanner); refreshContext(context); afterRefresh(context, applicationArguments); stopWatch.stop(); if (this.logStartupInfo) &#123; new StartupInfoLogger(this.mainApplicationClass) .logStarted(getApplicationLog(), stopWatch); &#125; listeners.started(context); callRunners(context, applicationArguments); &#125; catch (Throwable ex) &#123; handleRunFailure(context, ex, exceptionReporters, listeners); throw new IllegalStateException(ex); &#125; try &#123; listeners.running(context); &#125; catch (Throwable ex) &#123; handleRunFailure(context, ex, exceptionReporters, null); throw new IllegalStateException(ex); &#125; return context; &#125; 可变个数参数 args 即是我们整个应用程序的入口 main 方法的参数。StopWatch 是来自 org.springframework.util 的工具类，可以用来方便的记录程序的运行时间。 再来看看 1.5.12 与 2.0.1 版本的 run 方法 有什么不一样的地方？ 接下来好好分析上面新版本（2.0.1）的 run 方法的代码并配合比较旧版本（1.5.12）。 1、configureHeadlessProperty()：设置 headless 模式 1234567private static final String SYSTEM_PROPERTY_JAVA_AWT_HEADLESS = \"java.awt.headless\";private boolean headless = true;private void configureHeadlessProperty() &#123; System.setProperty(SYSTEM_PROPERTY_JAVA_AWT_HEADLESS, System.getProperty( SYSTEM_PROPERTY_JAVA_AWT_HEADLESS, Boolean.toString(this.headless)));&#125; 实际上是就是设置系统属性 java.awt.headless，该属性会被设置为 true。 2、getRunListeners()：加载 SpringApplicationRunListener 对象 12345678910111213141516171819 //TODO: xxxSpringApplicationRunListeners listeners = getRunListeners(args);//初始化监听器listeners.starting();try &#123; prepareContext(context, environment, listeners, applicationArguments, printedBanner); refreshContext(context); afterRefresh(context, applicationArguments); listeners.started(context); callRunners(context, applicationArguments);&#125;try &#123; listeners.running(context);&#125;private SpringApplicationRunListeners getRunListeners(String[] args) &#123; Class&lt;?&gt;[] types = new Class&lt;?&gt;[] &#123; SpringApplication.class, String[].class &#125;; return new SpringApplicationRunListeners(logger, getSpringFactoriesInstances( SpringApplicationRunListener.class, types, this, args));&#125; 上面的 getRunListeners() 中也利用 SpringFactoriesLoader 加载 META-INF/spring.factories 中 key 为 SpringApplicationRunListener 的值，然后再将获取到的值作为参数传递到 SpringApplicationRunListeners 的构造方法中去创建对象。 3、new DefaultApplicationArguments(args) ：获取启动时传入参数 args（main 方法传进来的参数） 并初始化为 ApplicationArguments 对象。 12345public DefaultApplicationArguments(String[] args) &#123; Assert.notNull(args, \"Args must not be null\"); this.source = new Source(args); this.args = args;&#125; 4、prepareEnvironment(listeners, applicationArguments)：根据 listeners 和 applicationArguments 配置SpringBoot 应用的环境。 1234567891011121314151617181920212223242526272829303132333435363738private ConfigurableEnvironment prepareEnvironment( SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments) &#123; // Create and configure the environment ConfigurableEnvironment environment = getOrCreateEnvironment(); configureEnvironment(environment, applicationArguments.getSourceArgs()); listeners.environmentPrepared(environment); bindToSpringApplication(environment); if (this.webApplicationType == WebApplicationType.NONE) &#123; environment = new EnvironmentConverter(getClassLoader()) .convertToStandardEnvironmentIfNecessary(environment); &#125; ConfigurationPropertySources.attach(environment); return environment;&#125;//如果 environment 不为空，直接 get 到，否则创建private ConfigurableEnvironment getOrCreateEnvironment() &#123; if (this.environment != null) &#123; return this.environment; &#125; if (this.webApplicationType == WebApplicationType.SERVLET) &#123; return new StandardServletEnvironment(); &#125; return new StandardEnvironment();&#125;//配置环境protected void configureEnvironment(ConfigurableEnvironment environment,String[] args) &#123; configurePropertySources(environment, args);//配置要使用的PropertySources configureProfiles(environment, args);//配置要使用的Profiles&#125;//将环境绑定到 SpringApplicationprotected void bindToSpringApplication(ConfigurableEnvironment environment) &#123; try &#123; Binder.get(environment).bind(\"spring.main\", Bindable.ofInstance(this)); &#125; catch (Exception ex) &#123; throw new IllegalStateException(\"Cannot bind to SpringApplication\", ex); &#125;&#125; 5、configureIgnoreBeanInfo(environment)：根据环境信息配置要忽略的 bean 信息 1234567891011public static final String IGNORE_BEANINFO_PROPERTY_NAME = \"spring.beaninfo.ignore\";private void configureIgnoreBeanInfo(ConfigurableEnvironment environment) &#123; if (System.getProperty( CachedIntrospectionResults.IGNORE_BEANINFO_PROPERTY_NAME) == null) &#123; Boolean ignore = environment.getProperty(\"spring.beaninfo.ignore\", Boolean.class, Boolean.TRUE); System.setProperty(CachedIntrospectionResults.IGNORE_BEANINFO_PROPERTY_NAME, ignore.toString()); &#125;&#125; 6、printBanner(environment)：打印标志，上面我已经说过了。 12345678910111213private Banner printBanner(ConfigurableEnvironment environment) &#123; if (this.bannerMode == Banner.Mode.OFF) &#123; //如果设置为 off，不打印 Banner return null; &#125; ResourceLoader resourceLoader = this.resourceLoader != null ? this.resourceLoader : new DefaultResourceLoader(getClassLoader()); SpringApplicationBannerPrinter bannerPrinter = new SpringApplicationBannerPrinter( resourceLoader, this.banner); if (this.bannerMode == Mode.LOG) &#123; return bannerPrinter.print(environment, this.mainApplicationClass, logger); &#125; return bannerPrinter.print(environment, this.mainApplicationClass, System.out);&#125; 7、createApplicationContext()：根据应用类型来确定该 Spring Boot 项目应该创建什么类型的 ApplicationContext ，默认情况下，如果没有明确设置的应用程序上下文或应用程序上下文类，该方法会在返回合适的默认值。 1234567891011121314151617181920212223242526public static final String DEFAULT_WEB_CONTEXT_CLASS = \"org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext\";public static final String DEFAULT_REACTIVE_WEB_CONTEXT_CLASS = \"org.springframework.boot.web.reactive.context.AnnotationConfigReactiveWebServerApplicationContext\";public static final String DEFAULT_CONTEXT_CLASS = \"org.springframework.context.annotation.AnnotationConfigApplicationContext\";protected ConfigurableApplicationContext createApplicationContext() &#123; Class&lt;?&gt; contextClass = this.applicationContextClass; if (contextClass == null) &#123; try &#123; switch (this.webApplicationType) &#123; //根据应用程序的类型来初始化容器 case SERVLET: //servlet 应用程序 contextClass = Class.forName(DEFAULT_WEB_CONTEXT_CLASS); break; case REACTIVE: //reactive 应用程序 contextClass = Class.forName(DEFAULT_REACTIVE_WEB_CONTEXT_CLASS); break; default: //默认 contextClass = Class.forName(DEFAULT_CONTEXT_CLASS); &#125; &#125; catch (ClassNotFoundException ex) &#123; throw new IllegalStateException( \"Unable create a default ApplicationContext,please specify an ApplicationContextClass\",ex); &#125; &#125; //最后通过Spring的工具类 BeanUtils 初始化容器类 bean return (ConfigurableApplicationContext) BeanUtils.instantiateClass(contextClass);&#125; 来看看在 1.5.12 中是怎么样的？ 8、exceptionReporters = getSpringFactoriesInstances( SpringBootExceptionReporter.class, new Class[] { ConfigurableApplicationContext.class }, context) 1234567891011private &lt;T&gt; Collection&lt;T&gt; getSpringFactoriesInstances(Class&lt;T&gt; type, Class&lt;?&gt;[] parameterTypes, Object... args) &#123; ClassLoader classLoader = Thread.currentThread().getContextClassLoader(); // Use names and ensure unique to protect against duplicates Set&lt;String&gt; names = new LinkedHashSet&lt;&gt;( SpringFactoriesLoader.loadFactoryNames(type, classLoader)); List&lt;T&gt; instances = createSpringFactoriesInstances(type, parameterTypes, classLoader, args, names);//根据类型 key 为 SpringBootExceptionReporter 去加载 AnnotationAwareOrderComparator.sort(instances);//对实例排序 return instances;&#125; 这里也是通过 SpringFactoriesLoader 加载 META-INF/spring.factories 中 key 为 SpringBootExceptionReporter 的全类名的 value 值。 9、prepareContext(context, environment, listeners, applicationArguments, printedBanner)：完成整个容器的创建与启动以及 bean 的注入功能。 12345678910111213141516171819202122232425262728//装配 Contextprivate void prepareContext(ConfigurableApplicationContext context, ConfigurableEnvironment environment, SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments, Banner printedBanner) &#123; //将之前准备好的 environment 设置给创建好的 ApplicationContext 使用 context.setEnvironment(environment); //1、 postProcessApplicationContext(context); //2、 applyInitializers(context); listeners.contextPrepared(context); if (this.logStartupInfo) &#123;//启动日志 logStartupInfo(context.getParent() == null); logStartupProfileInfo(context); &#125; // Add boot specific singleton beans context.getBeanFactory().registerSingleton(\"springApplicationArguments\", applicationArguments); if (printedBanner != null) &#123; context.getBeanFactory().registerSingleton(\"springBootBanner\", printedBanner); &#125; // Load the sources Set&lt;Object&gt; sources = getAllSources(); Assert.notEmpty(sources, \"Sources must not be empty\"); //3、 load(context, sources.toArray(new Object[0])); listeners.contextLoaded(context);&#125; 1）、postProcessApplicationContext(context) 12345678910111213141516171819public static final String CONFIGURATION_BEAN_NAME_GENERATOR = \"org.springframework.context.annotation.internalConfigurationBeanNameGenerator\";protected void postProcessApplicationContext(ConfigurableApplicationContext context) &#123; if (this.beanNameGenerator != null) &#123; context.getBeanFactory().registerSingleton( AnnotationConfigUtils.CONFIGURATION_BEAN_NAME_GENERATOR, this.beanNameGenerator); &#125; if (this.resourceLoader != null) &#123; if (context instanceof GenericApplicationContext) &#123; ((GenericApplicationContext) context) .setResourceLoader(this.resourceLoader); &#125; if (context instanceof DefaultResourceLoader) &#123; ((DefaultResourceLoader) context) .setClassLoader(this.resourceLoader.getClassLoader()); &#125; &#125;&#125; 该方法对 context 进行了预设置，设置了 ResourceLoader 和 ClassLoader，并向 bean 工厂中添加了一个beanNameGenerator 。 2）、applyInitializers(context) 12345678protected void applyInitializers(ConfigurableApplicationContext context) &#123; for (ApplicationContextInitializer initializer : getInitializers()) &#123; Class&lt;?&gt; requiredType = GenericTypeResolver.resolveTypeArgument( initializer.getClass(), ApplicationContextInitializer.class); Assert.isInstanceOf(requiredType, context, \"Unable to call initializer.\"); initializer.initialize(context); &#125;&#125; 在刷新之前将任何 ApplicationContextInitializer 应用于上下文 3)、load(context, sources.toArray(new Object[0])) 主要是加载各种 beans 到 ApplicationContext 对象中。 1234567891011121314protected void load(ApplicationContext context, Object[] sources) &#123; BeanDefinitionLoader loader = createBeanDefinitionLoader( //2 getBeanDefinitionRegistry(context), sources);// 1 if (this.beanNameGenerator != null) &#123; loader.setBeanNameGenerator(this.beanNameGenerator); &#125; if (this.resourceLoader != null) &#123; loader.setResourceLoader(this.resourceLoader); &#125; if (this.environment != null) &#123; loader.setEnvironment(this.environment); &#125; loader.load();//3&#125; (1)、getBeanDefinitionRegistry(context) 获取 bean 定义注册表 12345678910private BeanDefinitionRegistry getBeanDefinitionRegistry(ApplicationContext context) &#123; if (context instanceof BeanDefinitionRegistry) &#123; return (BeanDefinitionRegistry) context; &#125; if (context instanceof AbstractApplicationContext) &#123; return (BeanDefinitionRegistry) ((AbstractApplicationContext) context) .getBeanFactory(); &#125; throw new IllegalStateException(\"Could not locate BeanDefinitionRegistry\");&#125; (2)、createBeanDefinitionLoader() 通过 BeanDefinitionLoader 的构造方法把参数（注册表、资源）传进去，然后创建 BeanDefinitionLoader。 (3)、load() 把资源全部加载。 10、refreshContext(context) 12345678910111213141516private void refreshContext(ConfigurableApplicationContext context) &#123; refresh(context);//1 if (this.registerShutdownHook) &#123; try &#123; context.registerShutdownHook(); &#125; catch (AccessControlException ex) &#123; // Not allowed in some environments. &#125; &#125;&#125;//刷新底层的 ApplicationContextprotected void refresh(ApplicationContext applicationContext) &#123; Assert.isInstanceOf(AbstractApplicationContext.class, applicationContext); ((AbstractApplicationContext) applicationContext).refresh();&#125; refreshContext(context) 方法又调用了 refresh(context)。在调用了 refresh(context) 方法之后，调用了 registerShutdownHook 方法。继续看它的 refresh 方法： 123456789101112131415161718192021222324252627282930313233343536373839404142public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // Prepare this context for refreshing. prepareRefresh(); // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try &#123; // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); // Initialize message source for this context. initMessageSource(); // Initialize event multicaster for this context. initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. onRefresh(); // Check for listener beans and register them. registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); //1 // Last step: publish corresponding event. finishRefresh(); &#125; catch (BeansException ex) &#123; 。。。 // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; finally &#123; // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125;&#125; 到这里，我们就看见重点了，仔细看上的注释，正在做各种初始化工作，而今天我们关注的重点就是方法 finishBeanFactoryInitialization(beanFactory)。该方法进行了非懒加载 beans 的初始化工作。现在我们进入该方法内部，一探究竟。 看上图方法中的最后一步，调用了 beanFactory 的 preInstantiateSingletons() 方法。此处的 beanFactory 是哪个类的实例对象呢？ 可以看到 ConfigurableListableBeanFactory 接口的实现类只有 DefaultListableBeanFactory，我们看下实现类中的 preInstantiateSingletons 方法是怎么做的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public void preInstantiateSingletons() throws BeansException &#123; // Iterate over a copy to allow for init methods which in turn register new bean definitions. // While this may not be part of the regular factory bootstrap, it does otherwise work fine. List&lt;String&gt; beanNames = new ArrayList&lt;&gt;(this.beanDefinitionNames); // Trigger initialization of all non-lazy singleton beans... for (String beanName : beanNames) &#123; RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) &#123; if (isFactoryBean(beanName)) &#123; Object bean = getBean(FACTORY_BEAN_PREFIX + beanName); if (bean instanceof FactoryBean) &#123; final FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) bean; boolean isEagerInit; if (System.getSecurityManager() != null &amp;&amp; factory instanceof SmartFactoryBean)&#123; isEagerInit = AccessController.doPrivileged((PrivilegedAction&lt;Boolean&gt;) ((SmartFactoryBean&lt;?&gt;) factory)::isEagerInit, getAccessControlContext()); &#125; else &#123; isEagerInit = (factory instanceof SmartFactoryBean &amp;&amp; ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit()); &#125; if (isEagerInit) &#123; getBean(beanName); &#125; &#125; &#125; else &#123; getBean(beanName); &#125; &#125; &#125; // Trigger post-initialization callback for all applicable beans... for (String beanName : beanNames) &#123; Object singletonInstance = getSingleton(beanName); if (singletonInstance instanceof SmartInitializingSingleton) &#123; final SmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance; if (System.getSecurityManager() != null) &#123; AccessController.doPrivileged((PrivilegedAction&lt;Object&gt;) () -&gt; &#123; smartSingleton.afterSingletonsInstantiated(); return null; &#125;, getAccessControlContext()); &#125; else &#123; smartSingleton.afterSingletonsInstantiated(); &#125; &#125; &#125;&#125; 从上面的代码中可以看到很多调用了 getBean(beanName) 方法，跟踪此方法进去后，最终发现 getBean 调用了AbstractBeanFactory 类的 doGetBean(xxx) 方法，doGetBean(xxx) 方法中有这么一段代码： 但是 createBean() 方法并没有得到实现，实现类在 AbstractAutowireCapableBeanFactory 中。这才是创建 bean 的核心方法。 不知不觉，代码看的越来越深，感觉思维都差点回不去 run 方法了，切回大脑的上下文线程到 run 方法去。 11、afterRefresh(context, applicationArguments)：在上下文刷新后调用该方法，其内部没有做任何操作。 发现没做任何操作了之后，就觉得有点奇怪，所以把当前版本和 1.5.12 对比了下，发现： 在 1.5.12 中的 afterRefresh() 方法中调用了 callRunners() 方法，但是在 2.0.1 版本中的 run 方法中调用了 callRunners () 方法: 这里不得不说 SpringApplicationRunListeners 在 2.0.1 中的改变： 可以发现在 run 方法中，SpringApplicationRunListeners 监听器的状态花生了变化，这也是通过对比不同版本的代码才知道的区别，所以说我们看源码需要多对比着看。 so，我们来看下这个 SpringApplicationRunListener 这个接口： started 状态：The context has been refreshed and the application has started but CommandLineRunner and ApplicationRunner have not been called running 状态：Called immediately before the run method finishes, when the application context has been refreshed and all CommandLineRunner and ApplicationRunners have been called. 相关文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变 4、Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建 5、Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览 6、Spring Boot 2.0系列文章(六)：Spring boot 2.0 中 SpringBootApplication 注解详解 7、Spring Boot 2.0系列文章(七)：SpringApplication 深入探索 总结本文从源码级别分析了 Spring Boot 应用程序的启动过程，着重看了 SpringApplication 类中的构造函数的初始化和其 run 方法内部实现，并把涉及到的流程代码都过了一遍。 感悟：有时候跟代码跟着跟着，发现越陷越深，好难跳出来！后面还需多向别人请教阅读源码的技巧！ 最后虽然源码很难，但随着不断的探索，源码在你面前将会一览无遗，享受这种探索后的成就感！加油！骚年！ 自己本人能力有限，源码看的不多，上面如有不对的还请留言交流。","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}]},{"title":"分布式锁看这篇就够了","date":"2018-04-23T16:00:00.000Z","path":"2018/04/24/Distributed_lock/","text":"关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/04/24/Distributed_lock/ 什么是锁？ 在单进程的系统中，当存在多个线程可以同时改变某个变量（可变共享变量）时，就需要对变量或代码块做同步，使其在修改这种变量时能够线性执行消除并发修改变量。 而同步的本质是通过锁来实现的。为了实现多个线程在一个时刻同一个代码块只能有一个线程可执行，那么需要在某个地方做个标记，这个标记必须每个线程都能看到，当标记不存在时可以设置该标记，其余后续线程发现已经有标记了则等待拥有标记的线程结束同步代码块取消标记后再去尝试设置标记。这个标记可以理解为锁。 不同地方实现锁的方式也不一样，只要能满足所有线程都能看得到标记即可。如 Java 中 synchronize 是在对象头设置标记，Lock 接口的实现类基本上都只是某一个 volitile 修饰的 int 型变量其保证每个线程都能拥有对该 int 的可见性和原子修改，linux 内核中也是利用互斥量或信号量等内存数据做标记。 除了利用内存数据做锁其实任何互斥的都能做锁（只考虑互斥情况），如流水表中流水号与时间结合做幂等校验可以看作是一个不会释放的锁，或者使用某个文件是否存在作为锁等。只需要满足在对标记进行修改能保证原子性和内存可见性即可。 什么是分布式？分布式的 CAP 理论告诉我们: 任何一个分布式系统都无法同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance），最多只能同时满足两项。 目前很多大型网站及应用都是分布式部署的，分布式场景中的数据一致性问题一直是一个比较重要的话题。基于 CAP理论，很多系统在设计之初就要对这三者做出取舍。在互联网领域的绝大多数的场景中，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证最终一致性。 分布式场景 此处主要指集群模式下，多个相同服务同时开启. 在许多的场景中，我们为了保证数据的最终一致性，需要很多的技术方案来支持，比如分布式事务、分布式锁等。很多时候我们需要保证一个方法在同一时间内只能被同一个线程执行。在单机环境中，通过 Java 提供的并发 API 我们可以解决，但是在分布式环境下，就没有那么简单啦。 分布式与单机情况下最大的不同在于其不是多线程而是多进程。 多线程由于可以共享堆内存，因此可以简单的采取内存作为标记存储位置。而进程之间甚至可能都不在同一台物理机上，因此需要将标记存储在一个所有进程都能看到的地方。 什么是分布式锁？ 当在分布式模型下，数据只有一份（或有限制），此时需要利用锁的技术控制某一时刻修改数据的进程数。 与单机模式下的锁不仅需要保证进程可见，还需要考虑进程与锁之间的网络问题。（我觉得分布式情况下之所以问题变得复杂，主要就是需要考虑到网络的延时和不可靠。。。一个大坑） 分布式锁还是可以将标记存在内存，只是该内存不是某个进程分配的内存而是公共内存如 Redis、Memcache。至于利用数据库、文件等做锁与单机的实现是一样的，只要保证标记能互斥就行。 我们需要怎样的分布式锁？ 可以保证在分布式部署的应用集群中，同一个方法在同一时间只能被一台机器上的一个线程执行。 这把锁要是一把可重入锁（避免死锁） 这把锁最好是一把阻塞锁（根据业务需求考虑要不要这条） 这把锁最好是一把公平锁（根据业务需求考虑要不要这条） 有高可用的获取锁和释放锁功能 获取锁和释放锁的性能要好 基于数据库做分布式锁 基于乐观锁 基于表主键唯一做分布式锁利用主键唯一的特性，如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁，当方法执行完毕之后，想要释放锁的话，删除这条数据库记录即可。 上面这种简单的实现有以下几个问题： 这把锁强依赖数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。 这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁。 这把锁只能是非阻塞的，因为数据的 insert 操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作。 这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据中数据已经存在了。 这把锁是非公平锁，所有等待锁的线程凭运气去争夺锁。 在 MySQL 数据库中采用主键冲突防重，在大并发情况下有可能会造成锁表现象。 当然，我们也可以有其他方式解决上面的问题。 数据库是单点？搞两个数据库，数据之前双向同步，一旦挂掉快速切换到备库上。 没有失效时间？只要做一个定时任务，每隔一定时间把数据库中的超时数据清理一遍。 非阻塞的？搞一个 while 循环，直到 insert 成功再返回成功。 非重入的？在数据库表中加个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就可以了。 非公平的？再建一张中间表，将等待锁的线程全记录下来，并根据创建时间排序，只有最先创建的允许获取锁。 比较好的办法是在程序中生产主键进行防重。 基于表字段版本号做分布式锁这个策略源于 mysql 的 mvcc 机制，使用这个策略其实本身没有什么问题，唯一的问题就是对数据表侵入较大，我们要为每个表设计一个版本号字段，然后写一条判断 sql 每次进行判断，增加了数据库操作的次数，在高并发的要求下，对数据库连接的开销也是无法忍受的。 基于悲观锁 基于数据库排他锁做分布式锁在查询语句后面增加for update，数据库会在查询过程中给数据库表增加排他锁 (注意： InnoDB 引擎在加锁的时候，只有通过索引进行检索的时候才会使用行级锁，否则会使用表级锁。这里我们希望使用行级锁，就要给要执行的方法字段名添加索引，值得注意的是，这个索引一定要创建成唯一索引，否则会出现多个重载方法之间无法同时被访问的问题。重载方法的话建议把参数类型也加上。)。当某条记录被加上排他锁之后，其他线程无法再在该行记录上增加排他锁。 我们可以认为获得排他锁的线程即可获得分布式锁，当获取到锁之后，可以执行方法的业务逻辑，执行完方法之后，通过connection.commit()操作来释放锁。 这种方法可以有效的解决上面提到的无法释放锁和阻塞锁的问题。 阻塞锁？ for update语句会在执行成功后立即返回，在执行失败时一直处于阻塞状态，直到成功。 锁定之后服务宕机，无法释放？使用这种方式，服务宕机之后数据库会自己把锁释放掉。 但是还是无法直接解决数据库单点和可重入问题。 这里还可能存在另外一个问题，虽然我们对方法字段名使用了唯一索引，并且显示使用 for update 来使用行级锁。但是，MySQL 会对查询进行优化，即便在条件中使用了索引字段，但是否使用索引来检索数据是由 MySQL 通过判断不同执行计划的代价来决定的，如果 MySQL 认为全表扫效率更高，比如对一些很小的表，它就不会使用索引，这种情况下 InnoDB 将使用表锁，而不是行锁。如果发生这种情况就悲剧了。。。 还有一个问题，就是我们要使用排他锁来进行分布式锁的 lock，那么一个排他锁长时间不提交，就会占用数据库连接。一旦类似的连接变得多了，就可能把数据库连接池撑爆。 优缺点优点：简单，易于理解 缺点：会有各种各样的问题（操作数据库需要一定的开销，使用数据库的行级锁并不一定靠谱，性能不靠谱） 基于 Redis 做分布式锁基于 redis 的 setnx()、expire() 方法做分布式锁setnx()setnx 的含义就是 SET if Not Exists，其主要有两个参数 setnx(key, value)。该方法是原子的，如果 key 不存在，则设置当前 key 成功，返回 1；如果当前 key 已经存在，则设置当前 key 失败，返回 0。 expire()expire 设置过期时间，要注意的是 setnx 命令不能设置 key 的超时时间，只能通过 expire() 来对 key 设置。 使用步骤1、setnx(lockkey, 1) 如果返回 0，则说明占位失败；如果返回 1，则说明占位成功 2、expire() 命令对 lockkey 设置超时时间，为的是避免死锁问题。 3、执行完业务代码后，可以通过 delete 命令删除 key。 这个方案其实是可以解决日常工作中的需求的，但从技术方案的探讨上来说，可能还有一些可以完善的地方。比如，如果在第一步 setnx 执行成功后，在 expire() 命令执行成功前，发生了宕机的现象，那么就依然会出现死锁的问题，所以如果要对其进行完善的话，可以使用 redis 的 setnx()、get() 和 getset() 方法来实现分布式锁。 基于 redis 的 setnx()、get()、getset()方法做分布式锁这个方案的背景主要是在 setnx() 和 expire() 的方案上针对可能存在的死锁问题，做了一些优化。 getset()这个命令主要有两个参数 getset(key，newValue)。该方法是原子的，对 key 设置 newValue 这个值，并且返回 key 原来的旧值。假设 key 原来是不存在的，那么多次执行这个命令，会出现下边的效果： getset(key, “value1”) 返回 null 此时 key 的值会被设置为 value1 getset(key, “value2”) 返回 value1 此时 key 的值会被设置为 value2 依次类推！ 使用步骤 setnx(lockkey, 当前时间+过期超时时间)，如果返回 1，则获取锁成功；如果返回 0 则没有获取到锁，转向 2。 get(lockkey) 获取值 oldExpireTime ，并将这个 value 值与当前的系统时间进行比较，如果小于当前系统时间，则认为这个锁已经超时，可以允许别的请求重新获取，转向 3。 计算 newExpireTime = 当前时间+过期超时时间，然后 getset(lockkey, newExpireTime) 会返回当前 lockkey 的值currentExpireTime。 判断 currentExpireTime 与 oldExpireTime 是否相等，如果相等，说明当前 getset 设置成功，获取到了锁。如果不相等，说明这个锁又被别的请求获取走了，那么当前请求可以直接返回失败，或者继续重试。 在获取到锁之后，当前线程可以开始自己的业务处理，当处理完毕后，比较自己的处理时间和对于锁设置的超时时间，如果小于锁设置的超时时间，则直接执行 delete 释放锁；如果大于锁设置的超时时间，则不需要再锁进行处理。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import cn.com.tpig.cache.redis.RedisService;import cn.com.tpig.utils.SpringUtils;//redis分布式锁public final class RedisLockUtil &#123; private static final int defaultExpire = 60; private RedisLockUtil() &#123; // &#125; /** * 加锁 * @param key redis key * @param expire 过期时间，单位秒 * @return true:加锁成功，false，加锁失败 */ public static boolean lock(String key, int expire) &#123; RedisService redisService = SpringUtils.getBean(RedisService.class); long status = redisService.setnx(key, \"1\"); if(status == 1) &#123; redisService.expire(key, expire); return true; &#125; return false; &#125; public static boolean lock(String key) &#123; return lock2(key, defaultExpire); &#125; /** * 加锁 * @param key redis key * @param expire 过期时间，单位秒 * @return true:加锁成功，false，加锁失败 */ public static boolean lock2(String key, int expire) &#123; RedisService redisService = SpringUtils.getBean(RedisService.class); long value = System.currentTimeMillis() + expire; long status = redisService.setnx(key, String.valueOf(value)); if(status == 1) &#123; return true; &#125; long oldExpireTime = Long.parseLong(redisService.get(key, \"0\")); if(oldExpireTime &lt; System.currentTimeMillis()) &#123; //超时 long newExpireTime = System.currentTimeMillis() + expire; long currentExpireTime = Long.parseLong(redisService.getSet(key, String.valueOf(newExpireTime))); if(currentExpireTime == oldExpireTime) &#123; return true; &#125; &#125; return false; &#125; public static void unLock1(String key) &#123; RedisService redisService = SpringUtils.getBean(RedisService.class); redisService.del(key); &#125; public static void unLock2(String key) &#123; RedisService redisService = SpringUtils.getBean(RedisService.class); long oldExpireTime = Long.parseLong(redisService.get(key, \"0\")); if(oldExpireTime &gt; System.currentTimeMillis()) &#123; redisService.del(key); &#125; &#125;&#125; 123456789101112131415public void drawRedPacket(long userId) &#123; String key = \"draw.redpacket.userid:\" + userId; boolean lock = RedisLockUtil.lock2(key, 60); if(lock) &#123; try &#123; //领取操作 &#125; finally &#123; //释放锁 RedisLockUtil.unLock(key); &#125; &#125; else &#123; new RuntimeException(\"重复领取奖励\"); &#125;&#125; 基于 Redlock 做分布式锁Redlock 是 Redis 的作者 antirez 给出的集群模式的 Redis 分布式锁，它基于 N 个完全独立的 Redis 节点（通常情况下 N 可以设置成 5）。 算法的步骤如下： 1、客户端获取当前时间，以毫秒为单位。 2、客户端尝试获取 N 个节点的锁，（每个节点获取锁的方式和前面说的缓存锁一样），N 个节点以相同的 key 和 value 获取锁。客户端需要设置接口访问超时，接口超时时间需要远远小于锁超时时间，比如锁自动释放的时间是 10s，那么接口超时大概设置 5-50ms。这样可以在有 redis 节点宕机后，访问该节点时能尽快超时，而减小锁的正常使用。 3、客户端计算在获得锁的时候花费了多少时间，方法是用当前时间减去在步骤一获取的时间，只有客户端获得了超过 3 个节点的锁，而且获取锁的时间小于锁的超时时间，客户端才获得了分布式锁。 4、客户端获取的锁的时间为设置的锁超时时间减去步骤三计算出的获取锁花费时间。 5、如果客户端获取锁失败了，客户端会依次删除所有的锁。使用 Redlock 算法，可以保证在挂掉最多 2 个节点的时候，分布式锁服务仍然能工作，这相比之前的数据库锁和缓存锁大大提高了可用性，由于 redis 的高效性能，分布式缓存锁性能并不比数据库锁差。 但是，有一位分布式的专家写了一篇文章《How to do distributed locking》，质疑 Redlock 的正确性。 https://mp.weixin.qq.com/s/1bPLk_VZhZ0QYNZS8LkviA https://blog.csdn.net/jek123456/article/details/72954106 优缺点优点： 性能高 缺点： 失效时间设置多长时间为好？如何设置的失效时间太短，方法没等执行完，锁就自动释放了，那么就会产生并发问题。如果设置的时间太长，其他获取锁的线程就可能要平白的多等一段时间。 基于 redisson 做分布式锁redisson 是 redis 官方的分布式锁组件。GitHub 地址：https://github.com/redisson/redisson 上面的这个问题 ——&gt; 失效时间设置多长时间为好？这个问题在 redisson 的做法是：每获得一个锁时，只设置一个很短的超时时间，同时起一个线程在每次快要到超时时间时去刷新锁的超时时间。在释放锁的同时结束这个线程。 基于 ZooKeeper 做分布式锁zookeeper 锁相关基础知识 zk 一般由多个节点构成（单数），采用 zab 一致性协议。因此可以将 zk 看成一个单点结构，对其修改数据其内部自动将所有节点数据进行修改而后才提供查询服务。 zk 的数据以目录树的形式，每个目录称为 znode， znode 中可存储数据（一般不超过 1M），还可以在其中增加子节点。 子节点有三种类型。序列化节点，每在该节点下增加一个节点自动给该节点的名称上自增。临时节点，一旦创建这个 znode 的客户端与服务器失去联系，这个 znode 也将自动删除。最后就是普通节点。 Watch 机制，client 可以监控每个节点的变化，当产生变化会给 client 产生一个事件。 zk 基本锁 原理：利用临时节点与 watch 机制。每个锁占用一个普通节点 /lock，当需要获取锁时在 /lock 目录下创建一个临时节点，创建成功则表示获取锁成功，失败则 watch/lock 节点，有删除操作后再去争锁。临时节点好处在于当进程挂掉后能自动上锁的节点自动删除即取消锁。 缺点：所有取锁失败的进程都监听父节点，很容易发生羊群效应，即当释放锁后所有等待进程一起来创建节点，并发量很大。 zk 锁优化 原理：上锁改为创建临时有序节点，每个上锁的节点均能创建节点成功，只是其序号不同。只有序号最小的可以拥有锁，如果这个节点序号不是最小的则 watch 序号比本身小的前一个节点 (公平锁)。 步骤： 在 /lock 节点下创建一个有序临时节点 (EPHEMERAL_SEQUENTIAL)。 判断创建的节点序号是否最小，如果是最小则获取锁成功。不是则取锁失败，然后 watch 序号比本身小的前一个节点。 当取锁失败，设置 watch 后则等待 watch 事件到来后，再次判断是否序号最小。 取锁成功则执行代码，最后释放锁（删除该节点）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169import java.io.IOException;import java.util.ArrayList;import java.util.Collections;import java.util.List;import java.util.concurrent.CountDownLatch;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooDefs;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.data.Stat;public class DistributedLock implements Lock, Watcher&#123; private ZooKeeper zk; private String root = \"/locks\";//根 private String lockName;//竞争资源的标志 private String waitNode;//等待前一个锁 private String myZnode;//当前锁 private CountDownLatch latch;//计数器 private int sessionTimeout = 30000; private List&lt;Exception&gt; exception = new ArrayList&lt;Exception&gt;(); /** * 创建分布式锁,使用前请确认config配置的zookeeper服务可用 * @param config 127.0.0.1:2181 * @param lockName 竞争资源标志,lockName中不能包含单词lock */ public DistributedLock(String config, String lockName)&#123; this.lockName = lockName; // 创建一个与服务器的连接 try &#123; zk = new ZooKeeper(config, sessionTimeout, this); Stat stat = zk.exists(root, false); if(stat == null)&#123; // 创建根节点 zk.create(root, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE,CreateMode.PERSISTENT); &#125; &#125; catch (IOException e) &#123; exception.add(e); &#125; catch (KeeperException e) &#123; exception.add(e); &#125; catch (InterruptedException e) &#123; exception.add(e); &#125; &#125; /** * zookeeper节点的监视器 */ public void process(WatchedEvent event) &#123; if(this.latch != null) &#123; this.latch.countDown(); &#125; &#125; public void lock() &#123; if(exception.size() &gt; 0)&#123; throw new LockException(exception.get(0)); &#125; try &#123; if(this.tryLock())&#123; System.out.println(\"Thread \" + Thread.currentThread().getId() + \" \" +myZnode + \" get lock true\"); return; &#125; else&#123; waitForLock(waitNode, sessionTimeout);//等待锁 &#125; &#125; catch (KeeperException e) &#123; throw new LockException(e); &#125; catch (InterruptedException e) &#123; throw new LockException(e); &#125; &#125; public boolean tryLock() &#123; try &#123; String splitStr = \"_lock_\"; if(lockName.contains(splitStr)) throw new LockException(\"lockName can not contains \\\\u000B\"); //创建临时子节点 myZnode = zk.create(root + \"/\" + lockName + splitStr, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE,CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(myZnode + \" is created \"); //取出所有子节点 List&lt;String&gt; subNodes = zk.getChildren(root, false); //取出所有lockName的锁 List&lt;String&gt; lockObjNodes = new ArrayList&lt;String&gt;(); for (String node : subNodes) &#123; String _node = node.split(splitStr)[0]; if(_node.equals(lockName))&#123; lockObjNodes.add(node); &#125; &#125; Collections.sort(lockObjNodes); System.out.println(myZnode + \"==\" + lockObjNodes.get(0)); if(myZnode.equals(root+\"/\"+lockObjNodes.get(0)))&#123; //如果是最小的节点,则表示取得锁 return true; &#125; //如果不是最小的节点，找到比自己小1的节点 String subMyZnode = myZnode.substring(myZnode.lastIndexOf(\"/\") + 1); waitNode = lockObjNodes.get(Collections.binarySearch(lockObjNodes, subMyZnode) - 1); &#125; catch (KeeperException e) &#123; throw new LockException(e); &#125; catch (InterruptedException e) &#123; throw new LockException(e); &#125; return false; &#125; public boolean tryLock(long time, TimeUnit unit) &#123; try &#123; if(this.tryLock())&#123; return true; &#125; return waitForLock(waitNode,time); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return false; &#125; private boolean waitForLock(String lower, long waitTime) throws InterruptedException, KeeperException &#123; Stat stat = zk.exists(root + \"/\" + lower,true); //判断比自己小一个数的节点是否存在,如果不存在则无需等待锁,同时注册监听 if(stat != null)&#123; System.out.println(\"Thread \" + Thread.currentThread().getId() + \" waiting for \" + root + \"/\" + lower); this.latch = new CountDownLatch(1); this.latch.await(waitTime, TimeUnit.MILLISECONDS); this.latch = null; &#125; return true; &#125; public void unlock() &#123; try &#123; System.out.println(\"unlock \" + myZnode); zk.delete(myZnode,-1); myZnode = null; zk.close(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; &#125; public void lockInterruptibly() throws InterruptedException &#123; this.lock(); &#125; public Condition newCondition() &#123; return null; &#125; public class LockException extends RuntimeException &#123; private static final long serialVersionUID = 1L; public LockException(String e)&#123; super(e); &#125; public LockException(Exception e)&#123; super(e); &#125; &#125;&#125; 优缺点优点： 有效的解决单点问题，不可重入问题，非阻塞问题以及锁无法释放的问题。实现起来较为简单。 缺点： 性能上可能并没有缓存服务那么高，因为每次在创建锁和释放锁的过程中，都要动态创建、销毁临时节点来实现锁功能。ZK 中创建和删除节点只能通过 Leader 服务器来执行，然后将数据同步到所有的 Follower 机器上。还需要对 ZK的原理有所了解。 基于 Consul 做分布式锁DD 写过类似文章，其实主要利用 Consul 的 Key / Value 存储 API 中的 acquire 和 release 操作来实现。 文章地址：http://blog.didispace.com/spring-cloud-consul-lock-and-semphore/ 使用分布式锁的注意事项1、注意分布式锁的开销 2、注意加锁的粒度 3、加锁的方式 总结无论你身处一个什么样的公司，最开始的工作可能都需要从最简单的做起。不要提阿里和腾讯的业务场景 qps 如何大，因为在这样的大场景中你未必能亲自参与项目，亲自参与项目未必能是核心的设计者，是核心的设计者未必能独自设计。希望大家能根据自己公司业务场景，选择适合自己项目的方案。 参考资料http://www.hollischuang.com/archives/1716 http://www.spring4all.com/question/158 https://www.cnblogs.com/PurpleDream/p/5559352.html http://www.cnblogs.com/PurpleDream/p/5573040.html https://www.cnblogs.com/suolu/p/6588902.html","tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/数据库/"},{"name":"分布式锁","slug":"分布式锁","permalink":"http://yoursite.com/tags/分布式锁/"},{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"},{"name":"Zookeeper","slug":"Zookeeper","permalink":"http://yoursite.com/tags/Zookeeper/"}]},{"title":"Spring Boot 2.0系列文章(六)：Spring Boot 2.0中SpringBootApplication注解详解","date":"2018-04-18T16:00:00.000Z","path":"2018/04/19/SpringBootApplication-annotation/","text":"关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/04/19/SpringBootApplication-annotation/ 概述许多 Spring Boot 开发者喜欢他们的应用程序使用自动配置、组件扫描、并能够在他们的 “Application” 类上定义额外的配置。 可以使用一个 @SpringBootApplication 注解来启用这些功能。 123456789import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; @SpringBootApplication跟进去 @SpringBootApplication 注解可以发现下图： 其中标注的三个注解正能解决我们上面所说的三种功能，它们是： @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan 该接口除了上面三个注解外，还有四个方法如下： Class&lt;?&gt;[] exclude() default {}:根据 class 来排除，排除特定的类加入 spring 容器，传入参数 value 类型是 class 类型。 String[] excludeName() default {}:根据 class name 来排除，排除特定的类加入 spring 容器，传入参数 value 类型是 class 的全类名字符串数组。 String[] scanBasePackages() default {}:指定扫描包，参数是包名的字符串数组。 Class&lt;?&gt;[] scanBasePackageClasses() default {}:扫描特定的包，参数类似是 Class 类型数组。 就拿 scanBasePackages 来举个例子： 1@SpringBootApplication(scanBasePackages = &#123;\"com.zhisheng.controller\",\"com.zhisheng.model\"&#125;) 将不需要的 bean 排除在 spring 容器中，如何操作？看看官方的代码怎么用的： @SpringBootConfiguration @SpringBootConfiguration继承自@Configuration，二者功能也一致，标注当前类是配置类，并会将当前类内声明的一个或多个以@Bean注解标记的方法的实例纳入到srping容器中，并且实例名就是方法名。 虽说现在已经推荐使用 Spring Boot 里面的 @SpringBootConfiguration 注解，为了探个究竟，我们还是继续研究下 @Configuration 注解。 @Configuration@Configuration 标注在类上，相当于把该类作为 spring 的 xml 配置文件中的 &lt;beans&gt;，作用为：配置 spring 容器(应用上下文) 123&lt;beans&gt;&lt;/beans&gt; @Bean@Bean 标注在方法上(返回某个实例的方法)，等价于 spring 的 xml 配置文件中的&lt;bean&gt;，作用为：注册 bean 对象 可以看看这篇文章：https://www.ibm.com/developerworks/cn/webservices/ws-springjava/index.html @ComponentScan可以通过该注解指定扫描某些包下包含如下注解的均自动注册为 spring beans： @Component、@Service、 @Repository、 @Controller、@Entity 等等 例如： 1@ComponentScan(basePackages = &#123;\"com.zhisheng.controller\",\"com.zhisheng.model\"&#125;) 以前是在 xml 配置文件中设置如下标签：&lt;context:component-scan&gt;（用来扫描包配置） 除了可以使用 @ComponentScan 注解来加载我们的 bean，还可以在 Application 类中使用 @Import 指定该类。 例如： 1@Import(&#123;ConsulConfig.class, Log4jEndPointConfiguration.class&#125;) //直接 imoport 要引入的类 @EnableAutoConfiguration@EnableAutoConfiguration的作用启动自动的配置，@EnableAutoConfiguration注解的意思就是Springboot根据你添加的 jar 包来配置你项目的默认配置，比如根据spring-boot-starter-web ，来判断你的项目是否需要添加了webmvc和tomcat，就会自动的帮你配置 web 项目中所需要的默认配置。简单点说就是它会根据定义在 classpath 下的类，自动的给你生成一些 Bean，并加载到 Spring 的 Context 中。 可以看到 import 引入了 AutoConfigurationImportSelector 类。该类使用了 Spring Core 包的 SpringFactoriesLoader 类的 loadFactoryNamesof() 方法。 AutoConfigurationImportSelector 类实现了 DeferredImportSelector 接口，并实现了 selectImports 方法，用来导出Configuration 类。 1List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes); 导出的类是通过 SpringFactoriesLoader.loadFactoryNames() 读取了 ClassPath 下面的 META-INF/spring.factories 文件。 这个文件内容大致如下。 后面继续会写自动配置方面的博客，请继续关注！ 如果你发现自动装配的 Bean 不是你想要的，你也可以 disable 它。比如说，我不想要自动装配 Database 的那些Bean： 1@EnableAutoConfiguration(exclude = &#123;DataSourceAutoConfiguration.class&#125;) 相关文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变 4、Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建 5、Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览 6、Spring Boot 2.0系列文章(六)：Spring boot 2.0 中 SpringBootApplication 注解详解 7、Spring Boot 2.0系列文章(七)：SpringApplication 深入探索 总结本文主要讲了 SpringBootApplication 注解，然后展开写了其包含的三个注解 SpringBootConfiguration、ComponentScan、EnableAutoConfiguration 最后虽然源码很难，但随着不断的探索，源码在你面前将会一览无遗，享受这种探索后的成就感！加油！骚年！ 自己本人能力有限，源码看的不多，上面如有不对的还请留言交流。","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}]},{"title":"Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览","date":"2018-04-17T16:00:00.000Z","path":"2018/04/18/spring_boot2_project/","text":"关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/04/15/springboot2_code/ 项目结构 结构分析： Spring-boot-project 核心代码，代码量很多（197508 行） Spring-boot-samples 一些样例 demo，代码量不多（9685 行），蛮有用的 Spring-boot-samples-invoker 里面无代码 Spring-boot-tests 测试代码（1640 行） spring-boot-projectSpring-boot-project 下面有很多模块，如下： Spirng-boot 该模块 47760 行代码（含测试代码），Spring boot 主要的库，提供了支持 Spring Boot 其他部分的功能，其中包括了： 在SpringApplication类，提供静态便捷方法，可以很容易写一个独立的 Spring 应用程序。它唯一的工作就是创造并更新一个合适的 SpringApplicationContext 带有可选容器的嵌入式 Web 应用程序（Tomcat，Jetty 或 Undertow） 一流的外部配置支持 便捷ApplicationContext初始化程序，包括对敏感日志记录默认值的支持 spring-boot-actuator 该模块 18398 行代码（含测试代码），spring-boot-actuator 模块它完全是一个用于暴露自身信息的模块，提供了一个监控和管理生产环境的模块，可以使用 http、jmx、ssh、telnet 等管理和监控应用。审计（Auditing）、 健康（health）、数据采集（metrics gathering）会自动加入到应用里面。 spring-boot-actuator-autoconfigure 该模块 16721 行代码（含测试代码），Spring Boot Actuator 提供了额外的自动配置功能，可以在生产环境中实现可即时部署和支持的功能，从而装饰你的应用。例如，如果您正在编写 JSON Web 服务，那么它将提供服务器，安全性，日志记录，外部配置，管理端点，审计抽象等等功能。如果您想关闭内置功能，或者扩展或替换它们，它也会变得非常简单。 spring-boot-autoconfigure 该模块 51100 行代码（含测试代码）， Spring Boot 可以根据类路径的内容配置大部分常用应用程序。单个@EnableAutoConfiguration注释会触发 Spring上下文的自动配置。 自动配置尝试推断用户可能需要哪些 bean。例如，如果 HSQLDB在类路径中，并且用户尚未配置任何数据库连接，则他们可能需要定义内存数据库。当用户开始定义他们自己的 bean 时，自动配置将永远远离。 spring-boot-cli 该模块 9346 行代码（含测试代码），Spring 命令行应用程序编译并运行 Groovy 源代码，使得可以编写少量代码就能运行应用程序。Spring CLI 也可以监视文件，当它们改变时自动重新编译并重新启动。 spring-boot-dependencies 该模块里面没有源码，只有所有依赖和插件的版本号信息。 spring-boot-devtools 该模块 9418 行代码（含测试代码），spring-boot-devtools 模块来使 Spring Boot 应用支持热部署，提高开发者的开发效率，无需手动重启 Spring Boot 应用。 spring-boot-docs 该模块 671 行代码，springboot 参考文件。 spring-boot-parent 该模块是其他项目的 parent，该模块的父模块是 spring-boot-dependencies。 spring-boot-properties-migrator 该模块有 495 行代码，在 Spring Boot 2.0 中，许多配置属性被重新命名/删除，开发人员需要更新application.properties/ application.yml相应的配置。为了帮助你解决这一问题，Spring Boot 发布了一个新spring-boot-properties-migrator模块。一旦作为该模块作为依赖被添加到你的项目中，它不仅会分析应用程序的环境，而且还会在启动时打印诊断信息，而且还会在运行时为您暂时迁移属性。在您的应用程序迁移期间，这个模块是必备的，完成迁移后，请确保从项目的依赖关系中删除此模块。 spring-boot-starters Starter POMs 是由很多方便的依赖集合组成，如果你需要使用某种技术，通过添加少量的jar就可以把相关的依赖加入到项目中去。 虽然你看得到有这么多 starter，但是却没有一行 Java 代码，意不意外？ 这确实是 Spring Boot 自动配置的关键之处，后面我可以讲讲。 spring-boot-test测试代码！有 10980 行代码。 spring-boot-test-autoconfigure自动配置的测试代码，有 6063 行代码。 spring-boot-tools spring-boot-antlib Spring Boot AntLib 模块为 Apache Ant 提供了基本的 Spring Boot 支持。 您可以使用该模块创建可执行文件夹。 要使用该模块，您需要在 build.xml 中声明一个额外的 spring-boot 命名空间，如以下示例所示： 12345&lt;project xmlns:ivy=\"antlib:org.apache.ivy.ant\" xmlns:spring-boot=\"antlib:org.springframework.boot.ant\" name=\"myapp\" default=\"build\"&gt; ...&lt;/project&gt; 您需要记住使用 -lib 选项启动 Ant，如以下示例所示： 1ant -lib &lt;folder containing spring-boot-antlib-2.1.0.BUILD-SNAPSHOT.jar&gt; Spring-boot-autoconfigure-processor spring boot 自动配置的核心类 Spring-boot-configuration-metadata Spring boot 配置元数据 Spring-boot-configuration-processor spring boot 配置的核心 Spring-boot-gradle-plugin Spring Boot Gradle 插件在 Gradle 中提供了 Spring Boot 支持，可以打包成可执行 jar 或 war ，运行 Spring Boot 应用程序，并使用 spring-boot-dependencies 提供的依赖关系管理。 它需要 Gradle 4.0 或更高版本。 Spring-boot-maven-plugin Spring Boot Maven Plugin 在 Maven 中提供了 Spring Boot 支持，让您可以打包成可执行 jar 或 war 应用，并“就地”运行应用程序。 要使用它，你必须使用 Maven 3.2（或更高版本）。 Spring-boot-loader spring-boot-load 模块通过自定义 jar 包结构，自定义类加载器，优雅的实现了嵌套 jar 资源的加载，通过打包时候重新设置启动类和组织 jar 结构，通过运行时设置自定义加载器来实现嵌套 jar 资源加载。 Spring-boot-loader-tools spring-boot-load 模块的工具模块 Spring-boot-test-support 测试 spring-boot-samples 样例 demo 比较多，大家看源码的时候可以拿这些现成 demo 测试。 spring-boot-tests 相关文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变 4、Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建 5、Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览 6、Spring Boot 2.0系列文章(六)：Spring boot 2.0 中 SpringBootApplication 注解详解 7、Spring Boot 2.0系列文章(七)：SpringApplication 深入探索 总结本文主要分析了下 Spring boot 项目源码结构。包含 Spring boot 核心源码、样例 demo、测试。分析了项目的整体结构后，后面才能够有的放矢的去读源码。 最后虽然源码很难，但随着不断的探索，源码在你面前将会一览无遗，享受这种探索后的成就感！加油！骚年！","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}]},{"title":"Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建","date":"2018-04-14T16:00:00.000Z","path":"2018/04/15/springboot2_code/","text":"前提前几天面试的时候，被问过 Spring Boot 的自动配置源码怎么实现的，没看过源码的我只能投降👦了。 这不，赶紧来补补了，所以才有了这篇文章的出现，Spring Boot 2. 0 源码阅读环境的搭建中还遇到点问题，被坑死了，还好解决了，感谢群里的小伙伴！ 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/04/15/springboot2_code/ 项目下载从 https://github.com/spring-projects/spring-boot/releases 可以看到所有版本的下载地址，我这里选择的是 Spring Boot 2 中最新的 v2.0.1.RELEASE 版本，下载后，然后解压。获取代码之前，请先确保你的 JDK 版本是 1.8 以上哦。 项目编译进入 spring-boot-2.0.1.RELEASE 的目录下，执行下面的命令。 跳过测试用例编译1sudo mvn clean install -DskipTests -Pfast //跳过测试用例 跳过测试用例可以加快编译的速度。 先看下运行成功的效果： 只花了 6 分多钟就好了。 全量编译1sudo mvn -f spring-boot-project -Pfull clean install 全量编译竟然报错，一波未平，一波又起！ 看网上的解决方法是：在项目的 pom.xml 文件中的 &lt;properties&gt; 添加 &lt;javadocExecutable&gt; 123&lt;properties&gt; &lt;javadocExecutable&gt;$&#123;java.home&#125;/../bin/javadoc&lt;/javadocExecutable&gt;&lt;/properties&gt; 此方法虽然管用，但是只是临时的，需要对每个项目都进行添加。 问题产生的原因应该是，mvn 拿到的 JAVA_HOME 位置应该是 ${JAVA_HOME}/jre 而不是 jdk 位置。 后面又看官方的 README 上面写的执行命令： 1sudo mvn clean install 执行后也是有各种报错，尝试了很久解决，最后花了好几个小时才到下面这图： 太折腾人了，太麻烦了！ 暂时就不全量编译了，我们就直接把现在 跳过测试用例编译 后的项目导入到 IDEA 中去。 导入项目工程 导入后将那些测试的 module 标记为 maven 项目，然后后面自己再根据测试用例去跟源码吧。 导入后项目没出现报错，美滋滋，后面源码可以看起来。 遇到的坑在这之前，我自己创建项目 Spring Boot 2 项目都是失败的，maven 运行项目（mvn clean install）报错如下： 通过上图可以发现报错的罪魁祸首是由于找不到 org.yaml.snakeyaml 1.19 的包，这个依赖死活下不下来，苦逼了😢。 一开始以为是公司配的 maven setting.xml 文件有问题（公司私服有问题），导致我这个 org.yaml.snakeyaml 1.19 的包一直下载不来。后来我叫群里的好友帮忙测试下能不能创建 Spring Boot 2 项目，结果他们都行的。我就换成了他们阿里云镜像的 setting 文件，结果在我这还是不行的。真是醉了，我干脆直接叫他把 maven 本地仓库中的 org.yaml.snakeyaml 1.19 整个包都发给我，结果再次创建 Spring Boot 2 项目就能成功了。美滋滋😄！ 然后就蹭着现在环境 OK，开始搭建我的 Spring Boot 2 源码阅读环境！ 相关文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变 4、Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建 5、Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览 6、Spring Boot 2.0系列文章(六)：Spring boot 2.0 中 SpringBootApplication 注解详解 7、Spring Boot 2.0系列文章(七)：SpringApplication 深入探索 最后源码不骗人，多看看！","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}]},{"title":"Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变","date":"2018-04-12T16:00:00.000Z","path":"2018/04/13/Spring_Boot_2.0_Configuration_Changelog/","text":"前提好久没更新文章了，本来打算在毕业之前不更新了，这里，对不住了，我又更新了。😝😝 之前翻译了两篇 Spring Boot 2.0 的文章，Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 和 Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 今天就继续详细探究 Spring Boot 2.0 里面的改变。 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/04/13/Spring_Boot_2.0_Configuration_Changelog/ 配置改变配置属性在 1.5.10.RELEASE 和 2.0.0.RELEASE 两个版本之间的改变： 启用键下面的表是 2.0.0.RELEASE 版本中的弃用键： Key Replacement（替代） 原因 spring.datasource.hikari.initialization-fail-fast spring.main.web-environment spring.main.web-application-type 新键下面的表是 2.0.0.RELEASE 版本中新的键： Key Default value（默认值） 描述 logging.file.max-history 0 要保存的归档日志文件的最大值 logging.file.max-size 10MB 日志文件最大容量 logging.pattern.dateformat yyyy-MM-dd HH:mm:ss.SSS 日志的日期格式 management.endpoint.auditevents.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.auditevents.enabled true 是否启用 auditevents 端点 management.endpoint.beans.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.beans.enabled true 是否启用 bean 端点 management.endpoint.conditions.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.conditions.enabled true 是否启用 conditions 端点 management.endpoint.configprops.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.configprops.enabled true 是否启用 configprops 端点 management.endpoint.configprops.keys-to-sanitize password,secret,key,token, .credentials.,vcap_services Keys that should be sanitized management.endpoint.env.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.env.enabled true 是否启用 env 端点 management.endpoint.env.keys-to-sanitize password,secret,key,token, .credentials.,vcap_services Keys that should be sanitized. management.endpoint.flyway.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.flyway.enabled true 是否启用 flyway 端点 management.endpoint.health.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.health.enabled true 是否启用 health 端点 management.endpoint.health.roles 角色用于确定用户是否有权显示详细信息 management.endpoint.health.show-details never 何时显示完整的健康详情 management.endpoint.heapdump.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.heapdump.enabled true 是否启用 heapdump 端点 management.endpoint.httptrace.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.httptrace.enabled true 是否启用 httptrace 端点 management.endpoint.info.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.info.enabled true 是否启用 info 端点 management.endpoint.jolokia.config Jolokia 设置 management.endpoint.jolokia.enabled true 是否启用 jolokia 端点 management.endpoint.liquibase.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.liquibase.enabled true 是否启用 liquibase 端点 management.endpoint.logfile.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.logfile.enabled true 是否启用 logfile 端点 management.endpoint.logfile.external-file 要访问的外部日志文件 management.endpoint.loggers.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.loggers.enabled true 是否启用 loggers 端点 management.endpoint.mappings.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.mappings.enabled true 是否启用 mappings 端点 management.endpoint.metrics.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.metrics.enabled true 是否启用 metrics 端点 management.endpoint.prometheus.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.prometheus.enabled true 是否启用 prometheus 端点 management.endpoint.scheduledtasks.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.scheduledtasks.enabled true 是否启用 scheduledtasks 端点 management.endpoint.sessions.enabled true 是否启用 sessions 端点 management.endpoint.shutdown.enabled false 是否启用 shutdown 端点 management.endpoint.threaddump.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.threaddump.enabled true 是否启用 threaddump 端点 management.endpoints.enabled-by-default 是否启用或者关闭所有的端点 management.endpoints.jmx.domain org.springframework.boot 端点 JMX 域名 management.endpoints.jmx.exposure.exclude 应排除的端点 ID management.endpoints.jmx.exposure.include * 应包含的端点 ID 或全部 * management.endpoints.jmx.static-names 追加到所有表示端点的 MBean 的ObjectName 的静态属性. management.endpoints.jmx.unique-names false 是否确保 ObjectNames 在发生冲突时被修改 management.endpoints.web.base-path /actuator Web 端点的基本路径 management.endpoints.web.cors.allow-credentials 是否支持凭证 management.endpoints.web.cors.allowed-headers Comma-separated list of headers to allow in a request. ‘*’ allows all headers. management.endpoints.web.cors.allowed-methods Comma-separated list of methods to allow. ‘*’ allows all methods. management.endpoints.web.cors.allowed-origins Comma-separated list of origins to allow. ‘*’ allows all origins. management.endpoints.web.cors.exposed-headers Comma-separated list of headers to include in a response. management.endpoints.web.cors.max-age 1800s How long the response from a pre-flight request can be cached by clients. management.endpoints.web.exposure.exclude Endpoint IDs that should be excluded. management.endpoints.web.exposure.include health,info Endpoint IDs that should be included or ‘*’ for all. management.endpoints.web.path-mapping Mapping between endpoint IDs and the path that should expose them. management.health.influxdb.enabled true Whether to enable InfluxDB health check. management.health.neo4j.enabled true Whether to enable Neo4j health check. management.health.status.http-mapping Mapping of health statuses to HTTP status codes. management.metrics.binders.files.enabled true Whether to enable files metrics. management.metrics.binders.integration.enabled true Whether to enable Spring Integration metrics. management.metrics.binders.jvm.enabled true Whether to enable JVM metrics. management.metrics.binders.logback.enabled true Whether to enable Logback metrics. management.metrics.binders.processor.enabled true Whether to enable processor metrics. management.metrics.binders.uptime.enabled true Whether to enable uptime metrics. management.metrics.distribution.percentiles Specific computed non-aggregable percentiles to ship to the backend for meter IDs starting-with the specified name. management.metrics.distribution.percentiles-histogram Whether meter IDs starting-with the specified name should be publish percentile histograms. management.metrics.distribution.sla Specific SLA boundaries for meter IDs starting-with the specified name. management.metrics.enable Whether meter IDs starting-with the specified name should be enabled. management.metrics.export.atlas.batch-size 10000 Number of measurements per request to use for this backend. management.metrics.export.atlas.config-refresh-frequency 10s Frequency for refreshing config settings from the LWC service. management.metrics.export.atlas.config-time-to-live 150s Time to live for subscriptions from the LWC service. management.metrics.export.atlas.config-uri http://localhost:7101/ lwc/api/v1/expressions/local-dev URI for the Atlas LWC endpoint to retrieve current subscriptions. management.metrics.export.atlas.connect-timeout 1s Connection timeout for requests to this backend. management.metrics.export.atlas.enabled true Whether exporting of metrics to this backend is enabled. management.metrics.export.atlas.eval-uri http://localhost:7101/ lwc/api/v1/evaluate URI for the Atlas LWC endpoint to evaluate the data for a subscription. management.metrics.export.atlas.lwc-enabled false Whether to enable streaming to Atlas LWC. management.metrics.export.atlas.meter-time-to-live 15m Time to live for meters that do not have any activity. management.metrics.export.atlas.num-threads 2 Number of threads to use with the metrics publishing scheduler. management.metrics.export.atlas.read-timeout 10s Read timeout for requests to this backend. management.metrics.export.atlas.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.atlas.uri http://localhost:7101/ api/v1/publish URI of the Atlas server. management.metrics.export.datadog.api-key Datadog API key. management.metrics.export.datadog.application-key Datadog application key. management.metrics.export.datadog.batch-size 10000 Number of measurements per request to use for this backend. management.metrics.export.datadog.connect-timeout 1s Connection timeout for requests to this backend. management.metrics.export.datadog.descriptions true Whether to publish descriptions metadata to Datadog. management.metrics.export.datadog.enabled true Whether exporting of metrics to this backend is enabled. management.metrics.export.datadog.host-tag instance Tag that will be mapped to “host” when shipping metrics to Datadog. management.metrics.export.datadog.num-threads 2 Number of threads to use with the metrics publishing scheduler. management.metrics.export.datadog.read-timeout 10s Read timeout for requests to this backend. management.metrics.export.datadog.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.datadog.uri https://app.datadoghq.com URI to ship metrics to. management.metrics.export.ganglia.addressing-mode multicast UDP addressing mode, either unicast or multicast. management.metrics.export.ganglia.duration-units milliseconds Base time unit used to report durations. management.metrics.export.ganglia.enabled true Whether exporting of metrics to Ganglia is enabled. management.metrics.export.ganglia.host localhost Host of the Ganglia server to receive exported metrics. management.metrics.export.ganglia.port 8649 Port of the Ganglia server to receive exported metrics. management.metrics.export.ganglia.protocol-version 3.1 Ganglia protocol version. management.metrics.export.ganglia.rate-units seconds Base time unit used to report rates. management.metrics.export.ganglia.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.ganglia.time-to-live 1 Time to live for metrics on Ganglia. management.metrics.export.graphite.duration-units milliseconds Base time unit used to report durations. management.metrics.export.graphite.enabled true Whether exporting of metrics to Graphite is enabled. management.metrics.export.graphite.host localhost Host of the Graphite server to receive exported metrics. management.metrics.export.graphite.port 2004 Port of the Graphite server to receive exported metrics. management.metrics.export.graphite.protocol pickled Protocol to use while shipping data to Graphite. management.metrics.export.graphite.rate-units seconds Base time unit used to report rates. management.metrics.export.graphite.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.graphite.tags-as-prefix `` For the default naming convention, turn the specified tag keys into part of the metric prefix. management.metrics.export.influx.auto-create-db true Whether to create the Influx database if it does not exist before attempting to publish metrics to it. management.metrics.export.influx.batch-size 10000 Number of measurements per request to use for this backend. management.metrics.export.influx.compressed true Whether to enable GZIP compression of metrics batches published to Influx. management.metrics.export.influx.connect-timeout 1s Connection timeout for requests to this backend. management.metrics.export.influx.consistency one Write consistency for each point. management.metrics.export.influx.db mydb Tag that will be mapped to “host” when shipping metrics to Influx. management.metrics.export.influx.enabled true Whether exporting of metrics to this backend is enabled. management.metrics.export.influx.num-threads 2 Number of threads to use with the metrics publishing scheduler. management.metrics.export.influx.password Login password of the Influx server. management.metrics.export.influx.read-timeout 10s Read timeout for requests to this backend. management.metrics.export.influx.retention-policy Retention policy to use (Influx writes to the DEFAULT retention policy if one is not specified). management.metrics.export.influx.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.influx.uri http://localhost:8086 URI of the Influx server. management.metrics.export.influx.user-name Login user of the Influx server. management.metrics.export.jmx.enabled true Whether exporting of metrics to JMX is enabled. management.metrics.export.jmx.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.newrelic.account-id New Relic account ID. management.metrics.export.newrelic.api-key New Relic API key. management.metrics.export.newrelic.batch-size 10000 Number of measurements per request to use for this backend. management.metrics.export.newrelic.connect-timeout 1s Connection timeout for requests to this backend. management.metrics.export.newrelic.enabled true Whether exporting of metrics to this backend is enabled. management.metrics.export.newrelic.num-threads 2 Number of threads to use with the metrics publishing scheduler. management.metrics.export.newrelic.read-timeout 10s Read timeout for requests to this backend. management.metrics.export.newrelic.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.newrelic.uri https://insights-collector .newrelic.com URI to ship metrics to. management.metrics.export.prometheus.descriptions true Whether to enable publishing descriptions as part of the scrape payload to Prometheus. management.metrics.export.prometheus.enabled true Whether exporting of metrics to Prometheus is enabled. management.metrics.export.prometheus.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.signalfx.access-token SignalFX access token. management.metrics.export.signalfx.batch-size 10000 Number of measurements per request to use for this backend. management.metrics.export.signalfx.connect-timeout 1s Connection timeout for requests to this backend. management.metrics.export.signalfx.enabled true Whether exporting of metrics to this backend is enabled. management.metrics.export.signalfx.num-threads 2 Number of threads to use with the metrics publishing scheduler. management.metrics.export.signalfx.read-timeout 10s Read timeout for requests to this backend. management.metrics.export.signalfx.source Uniquely identifies the app instance that is publishing metrics to SignalFx. management.metrics.export.signalfx.step 10s Step size (i.e. reporting frequency) to use. management.metrics.export.signalfx.uri https://ingest.signalfx.com URI to ship metrics to. management.metrics.export.simple.enabled true Whether, in the absence of any other exporter, exporting of metrics to an in-memory backend is enabled. management.metrics.export.simple.mode cumulative Counting mode. management.metrics.export.simple.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.statsd.enabled true Whether exporting of metrics to StatsD is enabled. management.metrics.export.statsd.flavor datadog StatsD line protocol to use. management.metrics.export.statsd.host localhost Host of the StatsD server to receive exported metrics. management.metrics.export.statsd.max-packet-length 1400 Total length of a single payload should be kept within your network’s MTU. management.metrics.export.statsd.polling-frequency 10s How often gauges will be polled. management.metrics.export.statsd.port 8125 Port of the StatsD server to receive exported metrics. management.metrics.export.statsd.publish-unchanged-meters true Whether to send unchanged meters to the StatsD server. management.metrics.export.statsd.queue-size 2147483647 Maximum size of the queue of items waiting to be sent to the StatsD server. management.metrics.export.wavefront.api-token API token used when publishing metrics directly to the Wavefront API host. management.metrics.export.wavefront.batch-size 10000 Number of measurements per request to use for this backend. management.metrics.export.wavefront.connect-timeout 1s Connection timeout for requests to this backend. management.metrics.export.wavefront.enabled true Whether exporting of metrics to this backend is enabled. management.metrics.export.wavefront.global-prefix Global prefix to separate metrics originating from this app’s white box instrumentation from those originating from other Wavefront integrations when viewed in the Wavefront UI. management.metrics.export.wavefront.num-threads 2 Number of threads to use with the metrics publishing scheduler. management.metrics.export.wavefront.read-timeout 10s Read timeout for requests to this backend. management.metrics.export.wavefront.source Unique identifier for the app instance that is the source of metrics being published to Wavefront. management.metrics.export.wavefront.step 10s Step size (i.e. reporting frequency) to use. management.metrics.export.wavefront.uri https://longboard.wavefront.com URI to ship metrics to. management.metrics.use-global-registry true Whether auto-configured MeterRegistry implementations should be bound to the global static registry on Metrics. management.metrics.web.client.max-uri-tags 100 Maximum number of unique URI tag values allowed. management.metrics.web.client.requests-metric-name http.client.requests Name of the metric for sent requests. management.metrics.web.server.auto-time-requests true Whether requests handled by Spring MVC or WebFlux should be automatically timed. management.metrics.web.server.requests-metric-name http.server.requests Name of the metric for received requests. management.server.add-application-context-header false Add the “X-Application-Context” HTTP header in each response. management.server.address Network address to which the management endpoints should bind. management.server.port Management endpoint HTTP port (uses the same port as the application by default). management.server.servlet.context-path Management endpoint context-path (for instance,/management). management.server.ssl.ciphers management.server.ssl.client-auth management.server.ssl.enabled management.server.ssl.enabled-protocols management.server.ssl.key-alias management.server.ssl.key-password management.server.ssl.key-store management.server.ssl.key-store-password management.server.ssl.key-store-provider management.server.ssl.key-store-type management.server.ssl.protocol management.server.ssl.trust-store management.server.ssl.trust-store-password management.server.ssl.trust-store-provider management.server.ssl.trust-store-type management.trace.http.enabled true Whether to enable HTTP request-response tracing. management.trace.http.include request-headers,response-headers, cookies,errors Items to be included in the trace. server.error.include-exception false Include the “exception” attribute. server.http2.enabled server.jetty.accesslog.append false Append to log. server.jetty.accesslog.date-format dd/MMM/yyyy:HH:mm:ss Z Timestamp format of the request log. server.jetty.accesslog.enabled false Enable access log. server.jetty.accesslog.extended-format false Enable extended NCSA format. server.jetty.accesslog.file-date-format Date format to place in log file name. server.jetty.accesslog.filename Log filename. server.jetty.accesslog.locale Locale of the request log. server.jetty.accesslog.log-cookies false Enable logging of the request cookies. server.jetty.accesslog.log-latency false Enable logging of request processing time. server.jetty.accesslog.log-server false Enable logging of the request hostname. server.jetty.accesslog.retention-period 31 Number of days before rotated log files are deleted. server.jetty.accesslog.time-zone GMT Timezone of the request log. server.servlet.application-display-name application Display name of the application. server.servlet.context-parameters Servlet context init parameters. server.servlet.context-path Context path of the application. server.servlet.jsp.class-name server.servlet.jsp.init-parameters server.servlet.jsp.registered server.servlet.path / Path of the main dispatcher servlet. server.servlet.session.cookie.comment server.servlet.session.cookie.domain server.servlet.session.cookie.http-only server.servlet.session.cookie.max-age server.servlet.session.cookie.name server.servlet.session.cookie.path server.servlet.session.cookie.secure server.servlet.session.persistent server.servlet.session.store-dir server.servlet.session.timeout server.servlet.session.tracking-modes server.tomcat.max-http-header-size 0 Maximum size, in bytes, of the HTTP message header. server.tomcat.resource.cache-ttl Time-to-live of the static resource cache. server.tomcat.use-relative-redirects Whether HTTP 1.1 and later location headers generated by a call to sendRedirect will use relative or absolute redirects. server.undertow.eager-filter-init true Whether servlet filters should be initialized on startup. spring.banner.charset UTF-8 Banner file encoding. spring.banner.image.height Height of the banner image in chars (default based on image height). spring.banner.image.invert false Whether images should be inverted for dark terminal themes. spring.banner.image.location classpath:banner.gif Banner image file location (jpg or png can also be used). spring.banner.image.margin 2 Left hand image margin in chars. spring.banner.image.width 76 Width of the banner image in chars. spring.banner.location classpath:banner.txt Banner text resource location. spring.batch.initialize-schema embedded Database schema initialization mode. spring.cache.redis.cache-null-values true Allow caching null values. spring.cache.redis.key-prefix Key prefix. spring.cache.redis.time-to-live Entry expiration. spring.cache.redis.use-key-prefix true Whether to use the key prefix when writing to Redis. spring.config.additional-location Config file locations used in addition to the defaults. spring.data.cassandra.connect-timeout Socket option: connection time out. spring.data.cassandra.pool.heartbeat-interval 30s Heartbeat interval after which a message is sent on an idle connection to make sure it’s still alive. spring.data.cassandra.pool.idle-timeout 120s Idle timeout before an idle connection is removed. spring.data.cassandra.pool.max-queue-size 256 Maximum number of requests that get queued if no connection is available. spring.data.cassandra.pool.pool-timeout 5000ms Pool timeout when trying to acquire a connection from a host’s pool. spring.data.cassandra.read-timeout Socket option: read time out. spring.data.cassandra.repositories.type auto Type of Cassandra repositories to enable. spring.data.couchbase.repositories.type auto Type of Couchbase repositories to enable. spring.data.mongodb.repositories.type auto Type of Mongo repositories to enable. spring.data.neo4j.auto-index none Auto index mode. spring.data.web.pageable.default-page-size 20 Default page size. spring.data.web.pageable.max-page-size 2000 Maximum page size to be accepted. spring.data.web.pageable.one-indexed-parameters false Whether to expose and assume 1-based page number indexes. spring.data.web.pageable.page-parameter page Page index parameter name. spring.data.web.pageable.prefix `` General prefix to be prepended to the page number and page size parameters. spring.data.web.pageable.qualifier-delimiter _ Delimiter to be used between the qualifier and the actual page number and size properties. spring.data.web.pageable.size-parameter size Page size parameter name. spring.data.web.sort.sort-parameter sort Sort parameter name. spring.datasource.hikari.initialization-fail-timeout spring.datasource.hikari.metrics-tracker-factory spring.datasource.hikari.scheduled-executor spring.datasource.hikari.scheduled-executor-service spring.datasource.hikari.schema spring.datasource.initialization-mode embedded Initialize the datasource with available DDL and DML scripts. spring.devtools.restart.log-condition-evaluation-delta true Whether to log the condition evaluation delta upon restart. spring.flyway.baseline-description spring.flyway.baseline-on-migrate spring.flyway.baseline-version spring.flyway.check-location true Whether to check that migration scripts location exists. spring.flyway.clean-disabled spring.flyway.clean-on-validation-error spring.flyway.dry-run-output spring.flyway.enabled true 是否启用 flyway. spring.flyway.encoding spring.flyway.error-handlers spring.flyway.group spring.flyway.ignore-future-migrations spring.flyway.ignore-missing-migrations spring.flyway.init-sqls SQL statements to execute to initialize a connection immediately after obtaining it. spring.flyway.installed-by spring.flyway.locations spring.flyway.mixed spring.flyway.out-of-order spring.flyway.password 如果您想让 Flyway 创建自己的DataSource，可以使用 JDBC 密码 spring.flyway.placeholder-prefix spring.flyway.placeholder-replacement spring.flyway.placeholder-suffix spring.flyway.placeholders spring.flyway.repeatable-sql-migration-prefix spring.flyway.schemas spring.flyway.skip-default-callbacks spring.flyway.skip-default-resolvers spring.flyway.sql-migration-prefix spring.flyway.sql-migration-separator spring.flyway.sql-migration-suffix spring.flyway.sql-migration-suffixes spring.flyway.table spring.flyway.target spring.flyway.undo-sql-migration-prefix spring.flyway.url 要迁移的数据库的 JDBC URL spring.flyway.user 登录要迁移数据库的用户名 spring.flyway.validate-on-migrate spring.gson.date-format 序列化 Date 对象时使用的格式 spring.gson.disable-html-escaping Whether to disable the escaping of HTML characters such as ‘&lt;’, ‘&gt;’, etc. spring.gson.disable-inner-class-serialization Whether to exclude inner classes during serialization. spring.gson.enable-complex-map-key-serialization Whether to enable serialization of complex map keys (i.e. non-primitives). spring.gson.exclude-fields-without-expose-annotation Whether to exclude all fields from consideration for serialization or deserialization that do not have the “Expose” annotation. spring.gson.field-naming-policy Naming policy that should be applied to an object’s field during serialization and deserialization. spring.gson.generate-non-executable-json Whether to generate non executable JSON by prefixing the output with some special text. spring.gson.lenient Whether to be lenient about parsing JSON that doesn’t conform to RFC 4627. spring.gson.long-serialization-policy Serialization policy for Long and long types. spring.gson.pretty-printing Whether to output serialized JSON that fits in a page for pretty printing. spring.gson.serialize-nulls Whether to serialize null fields. spring.influx.password Influx 登录用户名密码 spring.influx.url InfluxDB 数据库 URL spring.influx.user Influx 登录用户名 spring.integration.jdbc.initialize-schema embedded Database schema initialization mode. spring.integration.jdbc.schema classpath:org/springframework/ integration/jdbc/schema-@@platform@@.sql Path to the SQL file to use to initialize the database schema. spring.jdbc.template.fetch-size -1 Number of rows that should be fetched from the database when more rows are needed. spring.jdbc.template.max-rows -1 Maximum number of rows. spring.jdbc.template.query-timeout Query timeout. spring.jpa.mapping-resources Mapping resources (equivalent to “mapping-file” entries in persistence.xml). spring.jta.atomikos.datasource.concurrent-connection-validation spring.jta.atomikos.properties.allow-sub-transactions true Specify whether sub-transactions are allowed. spring.jta.atomikos.properties.default-max-wait-time-on-shutdown How long should normal shutdown (no-force) wait for transactions to complete. spring.jta.atomikos.properties.recovery.delay 10000ms Delay between two recovery scans. spring.jta.atomikos.properties.recovery.forget-orphaned-log-entries-delay 86400000ms Delay after which recovery can cleanup pending (‘orphaned’) log entries. spring.jta.atomikos.properties.recovery.max-retries 5 Number of retry attempts to commit the transaction before throwing an exception. spring.jta.atomikos.properties.recovery.retry-interval 10000ms Delay between retry attempts. spring.kafka.admin.client-id ID to pass to the server when making requests. spring.kafka.admin.fail-fast false Whether to fail fast if the broker is not available on startup. spring.kafka.admin.properties Additional admin-specific properties used to configure the client. spring.kafka.admin.ssl.key-password Password of the private key in the key store file. spring.kafka.admin.ssl.keystore-location Location of the key store file. spring.kafka.admin.ssl.keystore-password Store password for the key store file. spring.kafka.admin.ssl.truststore-location Location of the trust store file. spring.kafka.admin.ssl.truststore-password Store password for the trust store file. spring.kafka.consumer.properties Additional consumer-specific properties used to configure the client. spring.kafka.consumer.ssl.key-password Password of the private key in the key store file. spring.kafka.consumer.ssl.keystore-location Location of the key store file. spring.kafka.consumer.ssl.keystore-password Store password for the key store file. spring.kafka.consumer.ssl.truststore-location Location of the trust store file. spring.kafka.consumer.ssl.truststore-password Store password for the trust store file. spring.kafka.jaas.control-flag required Control flag for login configuration. spring.kafka.jaas.enabled false Whether to enable JAAS configuration. spring.kafka.jaas.login-module com.sun.security.auth .module.Krb5LoginModule Login module. spring.kafka.jaas.options Additional JAAS options. spring.kafka.listener.client-id Prefix for the listener’s consumer client.id property. spring.kafka.listener.idle-event-interval Time between publishing idle consumer events (no data received). spring.kafka.listener.log-container-config Whether to log the container configuration during initialization (INFO level). spring.kafka.listener.monitor-interval Time between checks for non-responsive consumers. spring.kafka.listener.no-poll-threshold Multiplier applied to “pollTimeout” to determine if a consumer is non-responsive. spring.kafka.listener.type single Listener type. spring.kafka.producer.properties Additional producer-specific properties used to configure the client. spring.kafka.producer.ssl.key-password Password of the private key in the key store file. spring.kafka.producer.ssl.keystore-location Location of the key store file. spring.kafka.producer.ssl.keystore-password Store password for the key store file. spring.kafka.producer.ssl.truststore-location Location of the trust store file. spring.kafka.producer.ssl.truststore-password Store password for the trust store file. spring.kafka.producer.transaction-id-prefix When non empty, enables transaction support for producer. spring.ldap.anonymous-read-only false Whether read-only operations should use an anonymous environment. spring.liquibase.change-log classpath:/db/changelog/ db.changelog-master.yaml Change log configuration path. spring.liquibase.check-change-log-location true Whether to check that the change log location exists. spring.liquibase.contexts Comma-separated list of runtime contexts to use. spring.liquibase.default-schema Default database schema. spring.liquibase.drop-first false Whether to first drop the database schema. spring.liquibase.enabled true Whether to enable Liquibase support. spring.liquibase.labels Comma-separated list of runtime labels to use. spring.liquibase.parameters Change log parameters. spring.liquibase.password Login password of the database to migrate. spring.liquibase.rollback-file File to which rollback SQL is written when an update is performed. spring.liquibase.url JDBC URL of the database to migrate. spring.liquibase.user Login user of the database to migrate. spring.main.web-application-type Flag to explicitly request a specific type of web application. spring.messages.cache-duration Loaded resource bundle files cache duration. spring.messages.use-code-as-default-message false Whether to use the message code as the default message instead of throwing a “NoSuchMessageException”. spring.mvc.contentnegotiation.favor-parameter false Whether a request parameter (“format” by default) should be used to determine the requested media type. spring.mvc.contentnegotiation.favor-path-extension false Whether the path extension in the URL path should be used to determine the requested media type. spring.mvc.contentnegotiation.media-types Map file extensions to media types for content negotiation. spring.mvc.contentnegotiation.parameter-name Query parameter name to use when “favor-parameter” is enabled. spring.mvc.pathmatch.use-registered-suffix-pattern false Whether suffix pattern matching should work only against extensions registered with “spring.mvc.contentnegotiation.media-types.*”. spring.mvc.pathmatch.use-suffix-pattern false Whether to use suffix pattern match (“.*”) when matching patterns to requests. spring.quartz.jdbc.initialize-schema embedded Database schema initialization mode. spring.quartz.jdbc.schema classpath:org/quartz/impl/ jdbcjobstore/ tables_@@platform@@.sql Path to the SQL file to use to initialize the database schema. spring.quartz.job-store-type memory Quartz job store type. spring.quartz.properties Additional Quartz Scheduler properties. spring.rabbitmq.listener.direct.acknowledge-mode Acknowledge mode of container. spring.rabbitmq.listener.direct.auto-startup true Whether to start the container automatically on startup. spring.rabbitmq.listener.direct.consumers-per-queue Number of consumers per queue. spring.rabbitmq.listener.direct.default-requeue-rejected Whether rejected deliveries are re-queued by default. spring.rabbitmq.listener.direct.idle-event-interval How often idle container events should be published. spring.rabbitmq.listener.direct.prefetch Number of messages to be handled in a single request. spring.rabbitmq.listener.direct.retry.enabled false Whether publishing retries are enabled. spring.rabbitmq.listener.direct.retry.initial-interval 1000ms Duration between the first and second attempt to deliver a message. spring.rabbitmq.listener.direct.retry.max-attempts 3 Maximum number of attempts to deliver a message. spring.rabbitmq.listener.direct.retry.max-interval 10000ms Maximum duration between attempts. spring.rabbitmq.listener.direct.retry.multiplier 1 Multiplier to apply to the previous retry interval. spring.rabbitmq.listener.direct.retry.stateless true Whether retries are stateless or stateful. spring.rabbitmq.listener.type simple Listener container type. spring.rabbitmq.ssl.key-store-type PKCS12 Key store type. spring.rabbitmq.ssl.trust-store-type JKS Trust store type. spring.rabbitmq.template.exchange `` Name of the default exchange to use for send operations. spring.rabbitmq.template.routing-key `` Value of a default routing key to use for send operations. spring.reactor.stacktrace-mode.enabled false Whether Reactor should collect stacktrace information at runtime. spring.redis.jedis.pool.max-active 8 Maximum number of connections that can be allocated by the pool at a given time. spring.redis.jedis.pool.max-idle 8 Maximum number of “idle” connections in the pool. spring.redis.jedis.pool.max-wait -1ms Maximum amount of time a connection allocation should block before throwing an exception when the pool is exhausted. spring.redis.jedis.pool.min-idle 0 Target for the minimum number of idle connections to maintain in the pool. spring.redis.lettuce.pool.max-active 8 Maximum number of connections that can be allocated by the pool at a given time. spring.redis.lettuce.pool.max-idle 8 Maximum number of “idle” connections in the pool. spring.redis.lettuce.pool.max-wait -1ms Maximum amount of time a connection allocation should block before throwing an exception when the pool is exhausted. spring.redis.lettuce.pool.min-idle 0 Target for the minimum number of idle connections to maintain in the pool. spring.redis.lettuce.shutdown-timeout 100ms Shutdown timeout. spring.resources.cache.cachecontrol.cache-private Indicate that the response message is intended for a single user and must not be stored by a shared cache. spring.resources.cache.cachecontrol.cache-public Indicate that any cache may store the response. spring.resources.cache.cachecontrol.max-age Maximum time the response should be cached, in seconds if no duration suffix is not specified. spring.resources.cache.cachecontrol.must-revalidate Indicate that once it has become stale, a cache must not use the response without re-validating it with the server. spring.resources.cache.cachecontrol.no-cache Indicate that the cached response can be reused only if re-validated with the server. spring.resources.cache.cachecontrol.no-store Indicate to not cache the response in any case. spring.resources.cache.cachecontrol.no-transform Indicate intermediaries (caches and others) that they should not transform the response content. spring.resources.cache.cachecontrol.proxy-revalidate Same meaning as the “must-revalidate” directive, except that it does not apply to private caches. spring.resources.cache.cachecontrol.s-max-age Maximum time the response should be cached by shared caches, in seconds if no duration suffix is not specified. spring.resources.cache.cachecontrol.stale-if-error Maximum time the response may be used when errors are encountered, in seconds if no duration suffix is not specified. spring.resources.cache.cachecontrol.stale-while-revalidate Maximum time the response can be served after it becomes stale, in seconds if no duration suffix is not specified. spring.resources.cache.period Cache period for the resources served by the resource handler. spring.security.filter.dispatcher-types async,error,request Security filter chain dispatcher types. spring.security.filter.order -100 Security filter chain order. spring.security.oauth2.client.provider OAuth provider details. spring.security.oauth2.client.registration OAuth client registrations. spring.security.user.name user Default user name. spring.security.user.password Password for the default user name. spring.security.user.roles Granted roles for the default user name. spring.servlet.multipart.enabled true Whether to enable support of multipart uploads. spring.servlet.multipart.file-size-threshold 0 Threshold after which files are written to disk. spring.servlet.multipart.location Intermediate location of uploaded files. spring.servlet.multipart.max-file-size 1MB Max file size. spring.servlet.multipart.max-request-size 10MB Max request size. spring.servlet.multipart.resolve-lazily false Whether to resolve the multipart request lazily at the time of file or parameter access. spring.session.jdbc.cleanup-cron 0 * * * * * Cron expression for expired session cleanup job. spring.session.jdbc.initialize-schema embedded Database schema initialization mode. spring.session.mongodb.collection-name sessions Collection name used to store sessions. spring.session.redis.cleanup-cron 0 * * * * * Cron expression for expired session cleanup job. spring.session.servlet.filter-dispatcher-types async,error,request Session repository filter dispatcher types. spring.session.servlet.filter-order Session repository filter order. spring.thymeleaf.enable-spring-el-compiler false Enable the SpringEL compiler in SpringEL expressions. spring.thymeleaf.reactive.chunked-mode-view-names Comma-separated list of view names (patterns allowed) that should be the only ones executed in CHUNKED mode when a max chunk size is set. spring.thymeleaf.reactive.full-mode-view-names Comma-separated list of view names (patterns allowed) that should be executed in FULL mode even if a max chunk size is set. spring.thymeleaf.reactive.max-chunk-size 0 Maximum size of data buffers used for writing to the response, in bytes. spring.thymeleaf.reactive.media-types Media types supported by the view technology. spring.thymeleaf.servlet.content-type text/html Content-Type value written to HTTP responses. spring.webflux.date-format Date format to use. spring.webflux.static-path-pattern /** Path pattern used for static resources. spring.webservices.wsdl-locations Comma-separated list of locations of WSDLs and accompanying XSDs to be exposed as beans. Key Replacement（替代） 原因 banner.charset spring.banner.charset banner.image.height spring.banner.image.height banner.image.invert spring.banner.image.invert banner.image.location spring.banner.image.location banner.image.margin spring.banner.image.margin banner.image.width spring.banner.image.width banner.location spring.banner.location endpoints.actuator.enabled actuator 端点不再可用 endpoints.actuator.path actuator 端点不再可用 endpoints.actuator.sensitive actuator 端点不再可用 endpoints.auditevents.enabled management.endpoint. auditevents.enabled endpoints.auditevents.path management.endpoints.web.path-mapping.auditevents endpoints.auditevents.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.autoconfig.enabled management.endpoint. conditions.enabled endpoints.autoconfig.id 端点标识符不再可定制 endpoints.autoconfig.path management.endpoints.web.path-mapping.conditions endpoints.autoconfig.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.beans.enabled management.endpoint.beans.enabled endpoints.beans.id 端点标识符不再可定制 endpoints.beans.path management.endpoints.web.path-mapping.beans endpoints.beans.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.configprops.enabled management.endpoint. configprops.enabled endpoints.configprops.id 端点标识符不再可定制 endpoints.configprops.keys-to-sanitize management.endpoint. configprops.keys-to-sanitize endpoints.configprops.path management.endpoints.web.path-mapping.configprops endpoints.configprops.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.cors.allow-credentials management.endpoints. web.cors.allow-credentials endpoints.cors.allowed-headers management.endpoints. web.cors.allowed-headers endpoints.cors.allowed-methods management.endpoints. web.cors.allowed-methods endpoints.cors.allowed-origins management.endpoints. web.cors.allowed-origins endpoints.cors.exposed-headers management.endpoints. web.cors.exposed-headers endpoints.cors.max-age management.endpoints. web.cors.max-age endpoints.docs.curies.enabled docs 端点不再可用 endpoints.docs.enabled docs 端点不再可用 endpoints.docs.path docs 端点不再可用 endpoints.docs.sensitive docs 端点不再可用 endpoints.dump.enabled management.endpoint. threaddump.enabled endpoints.dump.id 端点标识符不再可定制 endpoints.dump.path management.endpoints.web.path-mapping.dump endpoints.dump.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.enabled management.endpoints. enabled-by-default endpoints.env.enabled management.endpoint.env.enabled endpoints.env.id 端点标识符不再可定制 endpoints.env.keys-to-sanitize management.endpoint. env.keys-to-sanitize endpoints.env.path management.endpoints. web.path-mapping.env endpoints.env.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.flyway.enabled management.endpoint.flyway.enabled endpoints.flyway.id 端点标识符不再可定制 endpoints.flyway.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.health.enabled management.endpoint.health.enabled endpoints.health.id 端点标识符不再可定制 endpoints.health.mapping management.health.status.http-mapping endpoints.health.path management.endpoints.web.path-mapping.health endpoints.health.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.health.time-to-live management.endpoint. health.cache.time-to-live endpoints.heapdump.enabled management.endpoint.heapdump.enabled endpoints.heapdump.path management.endpoints.web.path-mapping.heapdump endpoints.heapdump.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.hypermedia.enabled Actuator 中的 Hypermedia 不再可用 endpoints.info.enabled management.endpoint.info.enabled endpoints.info.id 端点标识符不再可定制 endpoints.info.path management.endpoints.web.path-mapping.info endpoints.info.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.jmx.domain management.endpoints.jmx.domain endpoints.jmx.enabled management.endpoints. jmx.exposure.exclude endpoints.jmx.static-names management.endpoints. jmx.static-names endpoints.jmx.unique-names management.endpoints. jmx.unique-names endpoints.jolokia.enabled management.endpoint. jolokia.enabled endpoints.jolokia.path management.endpoints.web.path-mapping.jolokia endpoints.jolokia.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.liquibase.enabled management.endpoint. liquibase.enabled endpoints.liquibase.id 端点标识符不再可定制 endpoints.liquibase.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.logfile.enabled management.endpoint. logfile.enabled endpoints.logfile.external-file management.endpoint. logfile.external-file endpoints.logfile.path management.endpoints.web.path-mapping.logfile endpoints.logfile.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.loggers.enabled management.endpoint. loggers.enabled endpoints.loggers.id 端点标识符不再可定制 endpoints.loggers.path management.endpoints.web.path-mapping.loggers endpoints.loggers.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.mappings.enabled management.endpoint. mappings.enabled endpoints.mappings.id 端点标识符不再可定制 endpoints.mappings.path management.endpoints.web.path-mapping.mappings endpoints.mappings.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.metrics.enabled management.endpoint.metrics.enabled endpoints.metrics.filter.counter-submissions Metrics support 现在使用千分尺 endpoints.metrics.filter.enabled Metrics support 现在使用千分尺 endpoints.metrics.filter.gauge-submissions Metrics support 现在使用千分尺 endpoints.metrics.id 端点标识符不再可定制 endpoints.metrics.path management.endpoints.web.path-mapping.metrics endpoints.metrics.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.shutdown.enabled management.endpoint. shutdown.enabled endpoints.shutdown.id 端点标识符不再可定制 endpoints.shutdown.path management.endpoints.web.path-mapping.shutdown endpoints.shutdown.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.trace.enabled management.endpoint. httptrace.enabled endpoints.trace.filter.enabled management.trace.http.enabled endpoints.trace.id 端点标识符不再可定制 endpoints.trace.path management.endpoints.web.path-mapping.httptrace endpoints.trace.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 error.path Path of the error controller. flyway.baseline-description spring.flyway.baseline-description flyway.baseline-on-migrate spring.flyway.baseline-on-migrate flyway.baseline-version spring.flyway.baseline-version flyway.check-location spring.flyway.check-location flyway.clean-on-validation-error spring.flyway. clean-on-validation-error flyway.enabled spring.flyway.enabled flyway.encoding spring.flyway.encoding flyway.ignore-failed-future-migration flyway.init-sqls spring.flyway.init-sqls flyway.locations spring.flyway.locations flyway.out-of-order spring.flyway.out-of-order flyway.password spring.flyway.password flyway.placeholder-prefix spring.flyway.placeholder-prefix flyway.placeholder-replacement spring.flyway.placeholder-replacement flyway.placeholder-suffix spring.flyway.placeholder-suffix flyway.placeholders spring.flyway.placeholders flyway.schemas spring.flyway.schemas flyway.sql-migration-prefix spring.flyway.sql-migration-prefix flyway.sql-migration-separator spring.flyway.sql-migration-separator flyway.sql-migration-suffix spring.flyway.sql-migration-suffixes flyway.table spring.flyway.table flyway.target spring.flyway.target flyway.url spring.flyway.url flyway.user spring.flyway.user flyway.validate-on-migrate spring.flyway.validate-on-migrate jolokia.config management.endpoint.jolokia.config liquibase.change-log spring.liquibase.change-log liquibase.check-change-log-location spring.liquibase.check-change-log-location liquibase.contexts spring.liquibase.contexts liquibase.default-schema spring.liquibase.default-schema liquibase.drop-first spring.liquibase.drop-first liquibase.enabled spring.liquibase.enabled liquibase.labels spring.liquibase.labels liquibase.parameters spring.liquibase.parameters liquibase.password spring.liquibase.password liquibase.rollback-file spring.liquibase.rollback-file liquibase.url spring.liquibase.url liquibase.user spring.liquibase.user management.add-application-context-header management.server.add-application-context-header management.address management.server.address management.context-path management.server. servlet.context-path management.port management.server.port management.security.enabled 现在提供全局 security 自动配置。 management.security.roles security 自动配置不再可定制 management.security.sessions security 自动配置不再可定制 management.shell.auth.jaas.domain CRaSH 支持不再可用 management.shell.auth.key.path CRaSH 支持不再可用 management.shell.auth.simple.user.name CRaSH 支持不再可用 management.shell.auth.simple.user.password CRaSH 支持不再可用 management.shell.auth.spring.roles CRaSH 支持不再可用 management.shell.auth.type CRaSH 支持不再可用 management.shell.ssh.auth-timeout CRaSH 支持不再可用 management.shell.ssh.enabled CRaSH 支持不再可用 management.shell.ssh.idle-timeout CRaSH 支持不再可用 management.shell.ssh.key-path CRaSH 支持不再可用 management.shell.ssh.port CRaSH 支持不再可用 management.shell.telnet.enabled CRaSH 支持不再可用 management.shell.telnet.port CRaSH 支持不再可用 management.ssl.ciphers management.server.ssl.ciphers management.ssl.client-auth management.server.ssl.client-auth management.ssl.enabled management.server.ssl.enabled management.ssl.enabled-protocols management.server.ssl.enabled-protocols management.ssl.key-alias management.server.ssl.key-alias management.ssl.key-password management.server.ssl.key-password management.ssl.key-store management.server.ssl.key-store management.ssl.key-store-password management.server.ssl.key-store-password management.ssl.key-store-provider management.server.ssl.key-store-provider management.ssl.key-store-type management.server.ssl.key-store-type management.ssl.protocol management.server.ssl.protocol management.ssl.trust-store management.server.ssl.trust-store management.ssl.trust-store-password management.server.ssl.trust-store-password management.ssl.trust-store-provider management.server.ssl.trust-store-provider management.ssl.trust-store-type management.server.ssl.trust-store-type management.trace.include management.trace.http.include security.basic.authorize-mode security 自动配置不再可定制 security.basic.enabled security 自动配置不再可定制 security.basic.path security 自动配置不再可定制 security.basic.realm security 自动配置不再可定制 security.enable-csrf security 自动配置不再可定制 security.filter-dispatcher-types spring.security. filter.dispatcher-types security.filter-order spring.security.filter.order security.headers.cache security 自动配置不再可定制 security.headers.content-security-policy security 自动配置不再可定制 security.headers.content-security-policy-mode security 自动配置不再可定制 security.headers.content-type security 自动配置不再可定制 security.headers.frame security 自动配置不再可定制 security.headers.hsts security 自动配置不再可定制 security.headers.xss security 自动配置不再可定制 security.ignored security 自动配置不再可定制 security.oauth2. authorization.check-token-access Spring Security 访问规则用于检查令牌端点（例如，SpEL表达式，如“isAuthenticated（）”） security.oauth2.authorization.realm 客户端身份验证的领域名称 security.oauth2.authorization.token-key-access Spring Security访问规则用于检查令牌端点（例如，SpEL表达式，如“isAuthenticated（）”） security.oauth2.client.access-token-uri security.oauth2.client.access-token-validity-seconds security.oauth2.client.additional-information security.oauth2.client.authentication-scheme security.oauth2.client.authorities security.oauth2.client.authorized-grant-types security.oauth2.client.auto-approve-scopes security.oauth2.client.client-authentication-scheme security.oauth2.client.client-id security.oauth2.client.client-secret security.oauth2.client.grant-type security.oauth2.client.id security.oauth2.client.pre- established-redirect-uri security.oauth2.client.refresh-token-validity-seconds security.oauth2.client.registered-redirect-uri security.oauth2.client.resource-ids security.oauth2.client.scope security.oauth2.client.token-name security.oauth2.client.use-current-uri security.oauth2.client.user-authorization-uri security.oauth2.resource.filter-order 0 用于验证令牌的过滤器链的顺序。 security.oauth2.resource.id 资源的标识符 security.oauth2.resource.jwk.key-set-uri 获取验证密钥以验证 JWT 令牌的 URI security.oauth2.resource.jwt.key-uri JWT 令牌的 URI security.oauth2.resource.jwt.key-value JWT 令牌的验证密钥 security.oauth2.resource.prefer-token-info true 使用令牌信息，可以设置为 false以使用用户信息 security.oauth2.resource.service-id resource security.oauth2.resource.token-info-uri token decoding 端点的 URI security.oauth2.resource.token-type 使用 userInfoUri 时要发送的令牌类型。 security.oauth2.resource.user-info-uri 用户端点的 URI security.oauth2.sso.filter-order 如果不提供显式的WebSecurityConfigurerAdapter，则应用过滤器顺序（在这种情况下，可以改为提供顺序）。 security.oauth2.sso.login-path /login 登录页面的路径，即触发重定向到 OAuth2 授权服务器的页面。 security.require-ssl security 自动配置已不再可定制 security.sessions security 自动配置已不再可定制 security.user.name spring.security.user.name security.user.password spring.security.user.password security.user.role spring.security.user.roles server.context-parameters server.servlet.context-parameters server.context-path server.servlet.context-path server.display-name server.servlet. application-display-name server.jsp-servlet.class-name server.servlet.jsp.class-name server.jsp-servlet.init-parameters server.servlet.jsp.init-parameters server.jsp-servlet.registered server.servlet.jsp.registered server.servlet-path server.servlet.path server.session.cookie.comment server.servlet.session.cookie.comment server.session.cookie.domain server.servlet.session.cookie.domain server.session.cookie.http-only server.servlet. session.cookie.http-only server.session.cookie.max-age server.servlet.session.cookie.max-age server.session.cookie.name server.servlet.session.cookie.name server.session.cookie.path server.servlet.session.cookie.path server.session.cookie.secure server.servlet.session.cookie.secure server.session.persistent server.servlet.session.persistent server.session.store-dir server.servlet.session.store-dir server.session.timeout server.servlet.session.timeout server.session.tracking-modes server.servlet.session.tracking-modes spring.activemq.pool.configuration.block-if-session-pool-is-full spring.activemq.pool.configuration.block-if-session-pool-is-full-timeout spring.activemq.pool.configuration.connection-factory spring.activemq.pool.configuration.create-connection-on-startup spring.activemq.pool.configuration.expiry-timeout spring.activemq.pool.configuration.idle-timeout spring.activemq.pool.configuration.max-connections spring.activemq.pool.configuration.maximum-active-session-per-connection spring.activemq.pool.configuration.properties spring.activemq.pool.configuration.reconnect-on-exception spring.activemq.pool.configuration.time-between-expiration-check-millis spring.activemq.pool.configuration.use-anonymous-producers spring.application.index 应用程序上下文 ID 默认情况下是唯一的 spring.batch.initializer.enabled spring.batch.initialize-schema spring.cache.guava.spec 用于创建缓存的规范 spring.cache.hazelcast.config 用于初始化 Hazelcast 的配置文件的位置 spring.data.cassandra.connect-timeout-millis spring.data. cassandra.connect-timeout spring.data.cassandra.read-timeout-millis spring.data.cassandra.read-timeout spring.data.cassandra.repositories.enabled spring.data.cassandra. repositories.type spring.data.couchbase.repositories.enabled spring.data.couchbase. repositories.type spring.data.mongodb.repositories.enabled spring.data.mongodb. repositories.type spring.data.neo4j.compiler 从 Neo4j 3 开始不再支持 spring.datasource.dbcp.access-to-underlying-connection-allowed spring.datasource.dbcp.connection-init-sqls spring.datasource.dbcp.default-auto-commit spring.datasource.dbcp.default-catalog spring.datasource.dbcp.default-read-only spring.datasource.dbcp.default-transaction-isolation spring.datasource.dbcp.driver-class-name spring.datasource.dbcp.initial-size spring.datasource.dbcp.log-abandoned spring.datasource.dbcp.login-timeout spring.datasource.dbcp.max-active spring.datasource.dbcp.max-idle spring.datasource.dbcp.max-open-prepared-statements spring.datasource.dbcp.max-wait spring.datasource.dbcp.min-evictable-idle-time-millis spring.datasource.dbcp.min-idle spring.datasource.dbcp.num-tests-per-eviction-run spring.datasource.dbcp.password spring.datasource.dbcp.pool-prepared-statements spring.datasource.dbcp.remove-abandoned spring.datasource.dbcp.remove-abandoned-timeout spring.datasource.dbcp.test-on-borrow spring.datasource.dbcp.test-on-return spring.datasource.dbcp.test-while-idle spring.datasource.dbcp.time-between-eviction-runs-millis spring.datasource.dbcp.url spring.datasource.dbcp.username spring.datasource.dbcp.validation-query spring.datasource.dbcp.validation-query-timeout spring.datasource.hikari.connection-customizer-class-name spring.datasource.initialize spring.datasource. initialization-mode spring.devtools.remote.debug.enabled 远程 debug 已不再支持 spring.devtools.remote.debug.local-port 远程 debug 已不再支持 spring.http.multipart.enabled spring.servlet.multipart.enabled spring.http.multipart.file-size-threshold spring.servlet.multipart.file-size-threshold spring.http.multipart.location spring.servlet.multipart.location spring.http.multipart.max-file-size spring.servlet.multipart.max-file-size spring.http.multipart.max-request-size spring.servlet. multipart.max-request-size spring.http.multipart.resolve-lazily spring.servlet.multipart.resolve-lazily spring.jpa.hibernate.naming.strategy Hibernate 4 的自动配置已不再提供 spring.jta.atomikos.properties.console-log-level warn spring.messages.cache-seconds spring.messages.cache-duration spring.metrics.export.aggregate.key-pattern Metrics support 现在使用千分尺 spring.metrics.export.aggregate.prefix Metrics support 现在使用千分尺 spring.metrics.export.delay-millis Metrics support 现在使用千分尺 spring.metrics.export.enabled Metrics support 现在使用千分尺 spring.metrics.export.excludes Metrics support 现在使用千分尺 spring.metrics.export.includes Metrics support 现在使用千分尺 spring.metrics.export.redis.key Metrics support 现在使用千分尺 spring.metrics.export.redis.prefix Metrics support 现在使用千分尺 spring.metrics.export.send-latest Metrics support 现在使用千分尺 spring.metrics.export.statsd.host management.metrics. export.statsd.host spring.metrics.export.statsd.port management.metrics. export.statsd.port spring.metrics.export.statsd.prefix Metrics support 现在使用千分尺 spring.metrics.export.triggers Metrics support 现在使用千分尺 spring.mobile.devicedelegatingviewresolver.enable-fallback false 启用对回退解决方案的支持 spring.mobile.devicedelegatingviewresolver.enabled false 启用 device 视图解析器 spring.mobile.devicedelegatingviewresolver.mobile-prefix mobile/ 用于查看移动设备名称的前缀 spring.mobile.devicedelegatingviewresolver.mobile-suffix `` 附加后缀以查看移动设备的名称 spring.mobile.devicedelegatingviewresolver.normal-prefix `` 用于查看普通设备名称的前缀. spring.mobile.devicedelegatingviewresolver.normal-suffix `` 附加后缀以查看普通设备的名称 spring.mobile.devicedelegatingviewresolver.tablet-prefix tablet/ 前缀预设为查看平板电脑设备的名称 spring.mobile.devicedelegatingviewresolver.tablet-suffix `` 附加后缀以查看平板电脑设备的名称 spring.mobile.sitepreference.enabled true 启用 SitePreferenceHandler. spring.mvc.media-types spring.mvc. contentnegotiation.media-types spring.rabbitmq.listener.acknowledge-mode spring.rabbitmq.listener.auto-startup spring.rabbitmq.listener.concurrency spring.rabbitmq.listener.default-requeue-rejected spring.rabbitmq.listener.idle-event-interval spring.rabbitmq.listener.max-concurrency spring.rabbitmq.listener.prefetch spring.rabbitmq.listener.retry.enabled false 是否启用发布重试 spring.rabbitmq.listener.retry.initial-interval 1000 第一次和第二次尝试发布或传递讯息的时间间隔 spring.rabbitmq.listener.retry.max-attempts 3 尝试发布或传递邮件的最大次数 spring.rabbitmq.listener.retry.max-interval 10000 尝试之间的最大间隔 spring.rabbitmq.listener.retry.multiplier 1 于先前重试间隔的倍数 spring.rabbitmq.listener.retry.stateless true 无论重试是无状态还是有状态 spring.rabbitmq.listener.transaction-size spring.redis.pool.max-active spring.redis.jedis.pool.max-idle spring.redis.pool.max-idle spring.redis.jedis.pool.max-idle spring.redis.pool.max-wait spring.redis.jedis.pool.max-wait spring.redis.pool.min-idle spring.redis.jedis.pool.min-idle spring.resources.cache-period spring.resources.cache.period spring.sendgrid.password 不再支持使用用户名和密码 ( 使用 spring.sendgrid.api-key 代替 ) spring.sendgrid.username 不再支持使用用户名和密码 ( 使用 spring.sendgrid.api-key 代替 ) spring.session.jdbc.initializer.enabled spring.session. jdbc.initialize-schema spring.session.mongo.collection-name spring.session.mongodb.collection-name spring.social.auto-connection-views false 为支持的生产者启用连接状态视图 spring.social.facebook.app-id Application id. spring.social.facebook.app-secret Application secret. spring.social.linkedin.app-id Application id. spring.social.linkedin.app-secret Application secret. spring.social.twitter.app-id Application id. spring.social.twitter.app-secret Application secret. spring.thymeleaf.content-type spring.thymeleaf. servlet.content-type 相关文章相关文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变 4、Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建 5、Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览 6、Spring Boot 2.0系列文章(六)：Spring boot 2.0 中 SpringBootApplication 注解详解 最后英文参考：https://github.com/spring-projects/spring-boot/wiki/Spring-Boot-2.0-Configuration-Changelog","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}]},{"title":"写这么多系列博客，怪不得找不到女朋友","date":"2018-03-26T16:00:00.000Z","path":"2018/03/27/blogs/","text":"前提好几周没更新博客了，对不断支持我博客的童鞋们说声：“抱歉了！”。自己这段时间确实比较忙，而且还在抽空完成学校的毕业设计。今天晚上抽空把大学期间写过的博客弄一个系列文章合集，算是对大学这四年的一个总结，证明自己没白过。 熟悉我的人都知道我写博客的时间比较早，而且坚持的时间也比较久，一直到现在也是一直保持着更新状态。最早最早开始写博客是在 CSDN 上写的，然后在简书也写过一段时间，后来放弃了简书转战了掘金，以下图片是自己在掘金这一年的成果，快 1.5 万关注了，哈哈哈。 去年过年前还收到掘金送来的专属礼物，真是激动，感谢掘金，希望越办越好！ 细数文章后，发现自己在实习的这段时间写的博客也挺多的，而且质量还比较高，经常上开发者头条、掘金等平台的首页推荐。在此，感谢实习期间组内大佬们的各种帮助！ 这里再次说下写博客的好处： 很好的用来总结自己所学的知识 遇到那么一群也写博客的大佬，有共同话题聊了 面试加分（在简历上放上自己的个人网站链接，面试官就可以更好的了解你，知道你所学知识的深度和广度） 不要小看你的每一篇不起眼博客，用一个蚂蚁金服大佬跟我说的话叫做：厚积薄发！ 不多说了，如果想和我交流的可以加我 qq 群：528776268 和我的微信：zhisheng_tian 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/03/27/blogs/ 系列文章合集Spring Boot 系列文章1、Spring Boot系列文章（一）：SpringBoot Kafka 整合使用 2、Spring Boot系列文章（二）：SpringBoot Admin 使用指南 3、Spring Boot系列文章（三）：SpringBoot RabbitMQ 整合使用 4、Spring Boot系列文章（四）：SpringBoot ActiveMQ 整合使用 5、Spring Boot系列文章（五）：SpringBoot RabbitMQ 整合进阶版 6、Spring Boot系列文章（六）：SpringBoot RocketMQ 整合使用和监控 7、更多请期待 Spring Boot 2.0 系列文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、后面绝对有更多文章出现的 Docker 系列文章1、Docker系列文章（一）：基于 Harbor 搭建 Docker 私有镜像仓库 2、Docker系列文章（二）：Mac 安装 Docker 及常用命令 3、同样，后面也会持续更新 ElasticSearch 系列文章1、Elasticsearch 系列文章（一）：Elasticsearch 默认分词器和中分分词器之间的比较及使用方法 2、 Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程 3、Elasticsearch 系列文章（三）：ElasticSearch 集群监控 4、Elasticsearch 系列文章（四）：ElasticSearch 单个节点监控 5、Elasticsearch 系列文章（五）：ELK 实时日志分析平台环境搭建 6、也有更多深入的文章 搭建博客系列文章1、利用Github Page 搭建个人博客网站 2、Github pages + Hexo 博客 yilia 主题使用畅言评论系统 3、Hexo + yilia 搭建博客可能会遇到的所有疑问 4、Hexo + yilia 主题实现文章目录 5、这个系列看情况，可能还会有 Java 系列文章1、关于String s = new String(“xyz”); 创建几个对象的问题 2、《Java 多线程编程核心技术》学习笔记及总结 3、从对象深入分析 Java 中实例变量和类变量的区别 4、深度探究Java 中 finally 语句块 5、解决jdk1.8中发送邮件失败（handshake_failure）问题 6、深入分析 Java Web 中的中文编码问题 7、奇怪的Java题：为什么128 == 128返回为False，而127 == 127会返回为True? 8、Java读取文件 9、HashMap、Hashtable、HashSet 和 ConcurrentHashMap 的比较 10、Java连接Oracle数据库的三种连接方式 11、Java NIO 系列教程 12、《疯狂 Java 突破程序员基本功的 16 课》读书笔记 13、详细深入分析 Java ClassLoader 工作机制 14、详解 Filter 过滤器 15、Java IO流学习超详细总结（图文并茂） 16、通过源码详解 Servlet 17、Java 性能调优需要格外注意的细节 18、Java 线程池艺术探索 19、JVM性能调优监控工具jps、jstack、jmap、jhat、jstat等使用详解 20、这个必须的持续更新下去 Maven 系列文章1、Centos7 搭建最新 Nexus3 Maven 私服 2、Maven 中 dependencies 与 dependencyManagement 的区别 Kafka 系列文章1、Kafka 安装及快速入门 2、Spring Boot系列文章（一）：SpringBoot Kafka 整合使用 Mybatis 系列文章1、通过项目逐步深入了解Mybatis&lt;一&gt; 2、通过项目逐步深入了解Mybatis&lt;二&gt; 3、通过项目逐步深入了解Mybatis&lt;三&gt; 4、通过项目逐步深入了解Mybatis（四)/) 5、MyBatis的foreach语句详解 6、期待它的源码解析文章吗？ Nginx 系列文章1、Ubuntu16.10 安装 Nginx 2、Nginx 基本知识快速入门 Python 爬虫系列文章1、Python爬虫实战之爬取百度贴吧帖子 2、Pyspider框架 —— Python爬虫实战之爬取 V2EX 网站帖子 3、Python爬虫实战之爬取糗事百科段子 4、这个估计得等有机会再次学 Python 时再写 RocketMQ 系列文章1、RocketMQ系列文章（一）：RocketMQ 初探 2、RocketMQ系列文章（二）：RocketMQ 安装及快速入门 3、RocketMQ系列文章（三）：RocketMQ 简单的消息示例 4、Spring Boot系列文章（六）：SpringBoot RocketMQ 整合使用和监控 Spring MVC 系列文章1、Spring MVC系列文章（一）：Spring MVC + Hibernate JPA + Bootstrap 搭建的博客系统 Demo 2、Spring MVC系列文章（二）：Spring MVC+Hibernate JPA搭建的博客系统项目中所遇到的坑 3、Spring MVC系列文章（三）：看透 Spring MVC 源代码分析与实践 —— 网站基础知识 4、Spring MVC系列文章（四）：看透 Spring MVC 源代码分析与实践 —— 俯视 Spring MVC 5、Spring MVC系列文章（五）：看透 Spring MVC 源代码分析与实践 —— Spring MVC 组件分析 6、通过项目逐步深入了解Spring MVC（一） Netty 系列文章1、Netty系列文章（一）：Netty 源码阅读之初始环境搭建 2、这个系列迟早会更新的。 前端系列文章1、Bootstrap入门需掌握的知识点（一） 2、Bootstrap入门需掌握的知识点（二） 3、使用 CodeMirror 打造属于自己的在线代码编辑器 4、AJAX 学习 5、前端渣渣这个也要慢慢学习这块 面试经验系列1、秋招第一站 —— 亚信科技 2、秋招第二站 —— 内推爱奇艺（一面二面） 3、秋招第三站 —— 内推阿里（一面） 4、 面试过阿里等互联网大公司，我知道了这些套路 5、那些年我看过的书 —— 致敬我的大学生活 —— Say Good Bye ！ 其他还有一些其他方面的技术文章，算不是系列文章，比较零散，还有就是一些随笔文章，就不把它们放在合集里了。 总结用自己的一句话：坑要一个个填，路要一步步走！前人栽树，后人乘凉，学会感恩！ 建了个不错的微信群，如果有感兴趣的可以加我微信，对我回复 加群 ，然后会拉你进群交流。","tags":[{"name":"博客合集","slug":"博客合集","permalink":"http://yoursite.com/tags/博客合集/"}]},{"title":"Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南","date":"2018-03-05T16:00:00.000Z","path":"2018/03/06/SpringBoot2-Migration-Guide/","text":"前提希望本文档将帮助您把应用程序迁移到 Spring Boot 2.0。 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/03/06/SpringBoot2-Migration-Guide/ 在你开始之前首先，Spring Boot 2.0 需要 Java 8 或更高版本。不再支持 Java 6 和 7 了。 在 Spring Boot 2.0 中，许多配置属性被重新命名/删除，开发人员需要更新application.properties/ application.yml相应的配置。为了帮助你解决这一问题，Spring Boot 发布了一个新spring-boot-properties-migrator模块。一旦作为该模块作为依赖被添加到你的项目中，它不仅会分析应用程序的环境，而且还会在启动时打印诊断信息，而且还会在运行时为您暂时迁移属性。在您的应用程序迁移期间，这个模块是必备的： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-properties-migrator&lt;/artifactId&gt;&lt;/dependency&gt; 注意：完成迁移后，请确保从项目的依赖关系中删除此模块。 构建您的 Spring Boot 应用程序Spring Boot Maven 插件为了保持了一致性，并且避免与其他插件发生冲突，现在暴露的插件配置属性都以一个spring-boot前缀开始。 例如，以下命令prod使用命令行启用配置文件 1mvn spring-boot:run -Dspring-boot.run.profiles=prod Surefire 默认值以前的 include/exclude 模式已与最新的 Surefire 默认设置保持一致。如果依赖于此插件，需要相应地更新插件配置。之前对应的配置如下： 12345678910111213&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;includes&gt; &lt;include&gt;**/*Tests.java&lt;/include&gt; &lt;include&gt;**/*Test.java&lt;/include&gt; &lt;/includes&gt; &lt;excludes&gt; &lt;exclude&gt;**/Abstract*.java&lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt;&lt;/plugin&gt; PS: 如果您使用 JUnit 5，则应将 Surefire 降级到 2.19.1。该**/*Tests.java版本不包含此模式，因此如果您依赖该模式，请确保将其添加到您的配置中。 Spring Boot Gradle 插件Spring Boot 的 Gradle 插件在很大程度上已被重写，有了重大的改进。您可以在其参考文献和API文档中阅读关于插件功能的更多信息。 依赖管理Spring Boot 的 Gradle 插件不再自动应用依赖管理插件。相反，Spring Boot 的插件现在可以通过导入正确版本的spring-boot-dependencies BOM 来应用依赖管理插件。当依赖管理被配置的时候，这一点会让你有更多的控制权。 对于大多数应用程序，使用应用依赖管理插件就足够了： 12apply plugin: &apos;org.springframework.boot&apos;apply plugin: &apos;io.spring.dependency-management&apos; // &lt;-- add this to your build.gradle 注意：依赖管理插件仍然是 spring-boot-gradle-plugin 的传递依赖项，所以不需要在 buildscript 配置中将其列为类路径依赖项。 建立可执行的 Jars 和 Wars bootRepackage 任务已经被替换成 bootJar 和 bootWar 任务，分别用于构建可执行的 jar 包和 war包。 配置更新BootRun，BootJar和BootWar任务现在都使用mainClassName的属性来配置主类的名称。这使得三个特定于引导的任务相互一致，并将其与 Gradle 自己的应用程序插件进行对齐。 Spring Boot 特性默认动态代理策略Spring Boot 默认使用 CGLIB 做动态代理代理(基于类的动态代理)，包括对 AOP 的支持。如果你需要基于接口的动态代理，你需要将spring.aop.proxy-target-class 设置为false。 SpringApplicationWeb 环境Spring Boot 应用程序现在可以在更多模式下运行，因此spring.main.web-environment现在不推荐使用，spring.main.web-application-type属性可以提供更多的支持。 如果您想确保应用程序不启动 Web 服务器，则必须将该属性更改为： 1spring.main.web-application-type=none 注意：可以通过 SpringApplication 的 setWebApplicationType 方法实现。 Spring Boot 应用程序事件更改我们已经添加了一个新事件ApplicationStartedEvent。 ApplicationStartedEvent在上下文刷新之后但在任何应用程序和命令行参数被调用之前发送。 ApplicationReadyEvent在任何应用程序和命令行参数被调用后发送。它表示应用程序已准备好为请求提供服务。 请参阅更新的参考文档。 Banner在我们限制 Spring Boot 使用的根名称空间的数量的过程中，与标志相关的属性已被重定位到spring.banner。 外部化配置轻松的绑定有关宽松绑定的规则已经收紧。我们假设一个现有的acme.my-project.my-name属性： 所有前缀必须是 kebab格式（小写，连字符分隔）acme.myProject或acme.my_project无效 - 您必须acme.my-project在此处使用。 属性名称可以使用 kebab-case（my-name），camel-case（myName）或 snake-case（my_name）。 环境属性（来自操作系统环境变量）必须使用通常的大写下划线格式，下划线只能用于分隔键的各个部分ACME_MYPROJECT_MYNAME。 这种新的放松绑定具有以下几个优点： 无需担心密钥的结构@ConditionalOnProperty：只要密钥是以规范格式定义的，支持的松散变体就可以透明地工作。如果您正在使用该prefix属性，则现在只需使用name或value属性即可放置完整密钥。 RelaxedPropertyResolver不再可以Environment自动处理：env.getProperty(&quot;com.foo.my-bar&quot;)将找到一个com.foo.myBar属性。 该org.springframework.boot.bind软件包不再可用，并被新的宽松绑定规则所取代。特别是，RelaxedDataBinder朋友已被新的BinderAPI 取代。以下样品MyProperties从app.acme前缀中进行绑定。 123MyProperties target = Binder.get(environment) .bind(\"app.acme\", MyProperties.class) .orElse(null); 由于现在内置了轻松绑定，因此只要使用其中一种支持的格式，就可以请求任何属性而不必关心案例： 123FlagType flagType = Binder.get(environment) .bind(\"acme.app.my-flag\", FlagType.class) .orElse(FlagType.DEFAULT); @ConfigurationProperties 验证如果您想打开验证，现在必须为您的@ConfigurationProperties对象添加注释@Validated。 配置位置spring.config.location配置的方式已被修复; 它提前将一个位置添加到默认位置列表中，现在它将替换默认位置。如果你是按照以前的方式进行处理，现在应该使用它spring.config.additional-location进行替换。 开发 Web 应用程序嵌入式容器包装结构为了支持响应式用例，嵌入式容器包结构已经被大幅度的重构。 EmbeddedServletContainer已被重新命名为，WebServer并且该org.springframework.boot.context.embedded包已被重新定位到org.springframework.boot.web.embedded。例如，如果您使用TomcatEmbeddedServletContainerFactory回调接口定制嵌入式 Tomcat 容器，则应该使用TomcatServletWebServerFactory。 特定于 Servlet 的服务器属性许多server.* 属性 ( Servlet 特有的) 已经转移到server.servlet： 旧的属性 新的属性 server.context-parameters.* server.servlet.context-parameters.* server.context-path server.servlet.context-path server.jsp.class-name server.servlet.jsp.class-name server.jsp.init-parameters.* server.servlet.jsp.init-parameters.* server.jsp.registered server.servlet.jsp.registered server.servlet-path server.servlet.path Web Starter 作为传递依赖以前有几个 Spring Boot starter 是依赖于 Spring MVC 而传递的spring-boot-starter-web。在 Spring WebFlux 新的支持下，spring-boot-starter-mustache，spring-boot-starter-freemarker并spring-boot-starter-thymeleaf不再依赖它。开发者有责任选择和添加spring-boot-starter-web或spring-boot-starter-webflux。 模板引擎Mustache 模板曾经的文件扩展名是.html，现在的扩展名为 .mustache ，与官方规范和大多数 IDE 插件一致。您可以通过更改spring.mustache.suffix配置键来覆盖此新的默认值。 Jackson / JSON 支持在 2.0 中，我们改变了 Jackson 配置的默认值，将 ISO-8601 字符串 写为 JSR-310 日期 。如果你想回到以前的行为，你可以添加spring.jackson.serialization.write-dates-as-timestamps=true到你的配置。 新的spring-boot-starter-json starter 收集了必要的位去读写 JSON。它不仅提供了jackson-databind，而且提供了和 Java8 一起运作的时候相当有用的组件：jackson-datatype-jdk8, jackson-datatype-jsr310 和 jackson-module-parameter-names。如果你曾经手动地依赖这些组件，现在可以依赖这个新的 starter 取代。 Spring MVC 路径匹配默认行为更改我们已决定在 Spring MVC 应用程序中更改后缀路径匹配的默认值（请参阅＃11105）。按照 Spring Framework 中记录的最佳实践，此功能不再默认启用。 如果您的应用程序希望将请求&quot;GET /projects/spring-boot.json&quot;映射到@GetMapping(&quot;/projects/spring-boot&quot;)映射，则此更改会影响您。 有关此更多信息以及如何减轻此更改，请查阅Spring Boot中有关路径匹配和内容协商的参考文档。 Servlet 过滤器Servlet 过滤器的默认调度程序类型现在是DipatcherType.REQUEST; 这使 Spring Boot 的默认值与 Servlet 规范的默认值一致。如果您希望将过滤器映射到其他调度程序类型，请使用FilterRegistrationBean注册您的过滤器。 注意：Spring Security 和 Spring Session 过滤器配置 ASYNC, ERROR以及 REQUEST 调度类型。 RestTemplateBuilder该requestFactory(ClientHttpRequestFactory)方法已被新requestFactory(Supplier&lt;ClientHttpRequestFactory&gt; requestFactorySupplier)方法所取代。Supplier允许构建器生成的每个模板使用它自己的请求工厂，从而避免共享工厂可能导致的副作用。见＃11255。 WebJars 定位器Spring Boot 1.x 使用并提供依赖关系管理org.webjars:webjars-locator。webjars-locator是一个“命名不佳的库……包装webjars-locator-core项目”。org.webjars:webjars-locator应该更新依赖项来org.webjars:webjars-locator-core代替使用。 SecuritySpring Boot 2 极大地简化了默认的安全配置，并使添加定制安全变得简单。Spring Boot 现在具有一种行为，只要您添加自己的 WebSecurityConfigurerAdapter 就会退出，而不是进行多种与安全性相关的自动配置。 如果您使用以下任何属性，则会受到影响： 123456789101112131415security.basic.authorize-modesecurity.basic.enabledsecurity.basic.pathsecurity.basic.realmsecurity.enable-csrfsecurity.headers.cachesecurity.headers.content-security-policysecurity.headers.content-security-policy-modesecurity.headers.content-typesecurity.headers.framesecurity.headers.hstssecurity.headers.xsssecurity.ignoredsecurity.require-sslsecurity.sessions 默认安全安全自动配置不再公开选项，并尽可能使用 Spring Security 默认值。一个明显的副作用是使用 Spring Security 的内容协商进行授权（表单登录）。 默认用户默认情况下，Spring Boot 使用生成的密码配置单个用户。用户可以使用 spring.security.user.* 属性进行配置。要进一步定制用户或添加其他用户，您将不得不公开一个UserDetailsServicebean。 AuthenticationManager Bean如果您想将 Spring Security AuthenticationManager作为 bean 公开，请覆盖authenticationManagerBean您的方法WebSecurityConfigurerAdapter并为其添加注释@Bean。 OAuth2从功能的 Spring Security OAuth 项目 迁移到核心 Spring Security。不再为依赖关系提供依赖管理，Spring Boot 2 通过 Spring Security 5 提供 OAuth 2.0 客户端支持。 如果您依赖尚未迁移的 Spring Security OAuth 功能，则需要在其他 jar 上添加依赖项，请查看文档以获取更多详细信息。我们还继续支持 Spring Boot 1.5，以便旧版应用程序可以继续使用它，直到提供升级路径。 执行器安全执行器不再有单独的安全自动配置（management.security.*属性消失）。sensitive每个端点的标志也没有在安全配置中变得更加明确。如果您依赖于此行为，则需要创建或调整您的安全配置，以保护您选择角色的端点。 例如，假设以下配置： 123endpoints.flyway.sensitive=falseendpoints.info.sensitive=truemanagement.security.roles=MY_ADMIN 12345http .authorizeRequests() .requestMatchers(EndpointRequest.to(\"health\", \"flyway\")).permitAll() .requestMatchers(EndpointRequest.toAnyEndpoint()).hasRole(\"MY_ADMIN\") ... 需要注意的是在2.x，health和info在默认情况下启用（与health默认情况下不显示其细节）。为了与这些新的默认值一致，health已被添加到第一个匹配器。 使用 SQL 数据库配置数据源默认连接池已从 Tomcat 切换到 HikariCP。如果您过去spring.datasource.type在基于 Tomcat 的应用程序中强制使用 Hikari，现在可以删除重写。 特别是，如果你有这样的设置： 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.apache.tomcat&lt;/groupId&gt; &lt;artifactId&gt;tomcat-jdbc&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.zaxxer&lt;/groupId&gt; &lt;artifactId&gt;HikariCP&lt;/artifactId&gt;&lt;/dependency&gt; 现在可以这样修改： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt; WARN 消息隐含的’打开在视图’从现在起，未明确启用的应用程序spring.jpa.open-in-view将在启动过程中收到警告消息。虽然这种行为是一种友好的默认行为，但如果您没有完全意识到为您做了什么，这可能会导致问题。此消息可确保您了解可在查看呈现期间执行数据库查询。如果你没有问题，你可以明确地配置这个属性来消除警告信息。 JPA 和 Spring Data在 Spring Boot 1.x 中，一些用户正在扩展HibernateJpaAutoConfiguration以将高级自定义应用于自动配置EntityManagerFactory。为了防止发生这种错误的用例，Spring Boot 2 中不再可能扩展它。 为了支持这些用例，现在可以定义一个HibernatePropertiesCustomizerbean，它可以完全控制 Hibernate 属性，包括注册在上下文中声明为 bean 的 Hibernate 拦截器的能力。 FlywayFlyway 配置键被移动到spring命名空间（即spring.flyway） 升级到 Spring Boot 2 将会将 Flyway 升级3.x到5.x。为确保模式升级顺利进行，请按照以下说明操作： 首先将您的1.5.xSpring Boot 应用程序升级到 Flyway 4，请参阅Maven和Gradle的说明。 一旦您的架构升级到了 Flyway 4，升级到 Spring Boot 2 并再次运行迁移以将您的应用程序移植到 Flyway 5。 LiquibaseLiquibase 配置键被移动到spring命名空间（即spring.liquibase） 数据库初始化基本DataSource初始化现在仅针对嵌入式数据源启用，并将在您使用生产数据库时立即关闭。新的spring.datasource.initialization-mode（替换spring.datasource.initialize）提供更多的控制。 更新默认的’创建 - 删除’处理spring.jpa.hibernate.ddl-auto 属性默认为只有在没有使用 Liquibase 或 Flyway 等模式管理器时才使用嵌入式数据库进行创建。一旦检测到模式管理器，默认更改为 none。 整合 NoSQLRedis现在使用的是 Lettuce 而不是 Jedis 作为 Redis 驱动程序spring-boot-starter-redis。如果您使用更高级别的Spring Data 构造，则应该发现变化是透明的。我们仍然支持 Jedis，如果您愿意，通过排除 io.lettuce：lettuce-core并添加 redis.clients：jedis，则可以自由切换依赖项。 ElasticsearchElasticsearch 已经升级到 6.0+。与 Elastic 宣布嵌入式 Elasticsearch 不再受支持一致，自动配置NodeClient已被删除。TransportClient可以通过使用spring.data.elasticsearch.cluster-nodes提供要连接的一个或多个节点的地址来自动配置。 高速缓存用于缓存的专用 Hazelcast 自动配置。 无法自动配置常规HazelcastInstance和专用HazelcastInstance缓存。因此，该spring.cache.hazelcast.config属性已不再可用。 批量在启动时执行批处理作业的 CommandLineRunner 的顺序为 0。 测试Mockito 1.xMockito 1.x 不再支持@MockBean和@SpyBean。如果你不用spring-boot-starter-test来管理你的依赖关系，你应该升级到 Mockito 2.x. Spring Boot ActuatorSpring Boot 2 为 Actuator 带来了重要变化，无论是内部还是面向用户，请查阅参考指南中的更新部分和新的Actuator API文档。 您应该期望编程模型，配置密钥和某些端点的响应格式发生变化。Actuator 现在在 Spring MVC，Spring WebFlux 和Jersey 上得到本地支持。 构建Actuator 的代码分为两个模块：现有的spring-boot-actuator和新的spring-boot-actuator-autoconfigure。如果您使用原始模块（spring-boot-actuator）导入 actuator，请考虑使用spring-boot-starter-actuator启动器替代它。 Keys 的配置结构Endpoints 基础配置 key 已经统一： 旧的属性 新的属性 endpoints.&lt;id&gt;.* management.endpoint.&lt;id&gt;.* endpoints.cors.* management.endpoints.web.cors.* endpoints.jmx.* management.endpoints.jmx.* management.address management.server.address management.context-path management.server.servlet.context-path management.ssl.* management.server.ssl.* management.port management.server.port 基本路径所有 endpoints 默认情况下都已移至 /actuator。 我们修改了 management.server.servlet.context-path 的含义：它现在是 server.servlet.context-path 的端点管理的等价替代（只有在设置了 management.server.port 时才有效）。另外，您还可以使用新的单独属性 management.endpoints.web.base-path 为管理端点设置基本路径。 例如，如果你设置management.server.servlet.context-path=/management和management.endpoints.web.base-path=/application，你就可以在下面的路径到达终点健康：/management/application/health。 如果你想恢复 1.x 的行为（即具有/health代替/actuator/health），设置以下属性： 1management.endpoints.web.base-path=/ 审计事件 API 更改AuditEventRepository 现在有一个包含所有可选参数的单一方法。 Endpoints要通过 HTTP 使执行器端点可用，它需要同时启用和公开。默认： 无论您的应用程序中是否存在和配置 Spring Security，只有端点/health和/info端点都是暴露的。 所有端点，但/shutdown已启用。 您可以按如下方式公开所有端点： 1management.endpoints.web.exposure.include=* 您可以通过以下方式显式启用/shutdown端点： 1management.endpoint.shutdown.enabled=true 要公开所有（已启用）网络端点除env端点之外： 12management.endpoints.web.exposure.include=*management.endpoints.web.exposure.exclude=env Endpoint changes 1.x 端点 2.0 端点（改变） /actuator 不再可用。 但是，在 management.endpoints.web.base-path 的根目录中有一个映射，它提供了到所有暴露端点的链接。 /auditevents 该after参数不再需要 /autoconfig 重命名为 /conditions /docs 不再可用 /health 现在有一个 management.endpoint.health.show-details 选项 never, always, when-authenticated，而不是依靠 sensitive 标志来确定 health 端点是否必须显示全部细节。 默认情况下，/actuator/health公开并且不显示细节。 /trace 重命名为 /httptrace 端点属性已更改如下： endpoints.&lt;id&gt;.enabled 已经转移到了 management.endpoint.&lt;id&gt;.enabled endpoints.&lt;id&gt;.id 没有替换（端点的 ID 不再可配置） endpoints.&lt;id&gt;.sensitive没有替代品（请参见执行器安全） endpoints.&lt;id&gt;.path 已经转移到了 management.endpoints.web.path-mapping.&lt;id&gt; 端点格式/actuator/mappings 端点大改变JSON 格式已经更改为现在正确地包含有关上下文层次结构，多个DispatcherServlets，部署的 Servlet 和 Servlet 过滤器的信息。详情请参阅＃9979。 Actuator API 文档的相关部分提供了一个示例文档。 /actuator/httptrace 端点大改变响应的结构已经过改进，以反映端点关注跟踪 HTTP 请求 - 响应交换的情况。 迁移自定义端点如果您有自定义执行器端点，请查看专用博客文章。该团队还撰写了一个 wiki 页面，介绍如何将现有的执行器端点迁移到新的基础架构。 MetricsSpring Boot 自己的指标已被支持取代，包括自动配置，用于 icrometer 和 dimensional 指标。 设置 icrometer如果您的 Spring Boot 2.0 应用程序已依赖于 Actuator，则 icrometer 已在此处并自动配置。如果您希望将度量标准导出到 Prometheus，Atlas 或 Datadog 等外部注册表，Micrometer 将为许多注册表提供依赖关系; 您可以使用spring.metrics.*属性配置您的应用程序以导出到特定的注册表。 迁移定制计数器/量表您可以通过以下方式创建各种指标，而不是在应用程序代码中注入CounterService或GaugeService的实例： 注入MeterRegistry和调用方法。 直接调用静态方法Counter featureCounter = Metrics.counter(&quot;feature&quot;);。 开发者工具热拔插由于 Spring Loaded 项目被搁置，它在 Spring Boot 的支持已被删除。我们建议使用 Devtools。 Devtools 远程调试隧道已经从 Devtools 中删除了对通过 HTTP 进行隧道远程调试的支持。 已删除的功能以下功能不再可用： CRaSH 支持 Spring Mobile 的自动配置和依赖关系管理。 Spring Social 的自动配置和依赖关系管理。 依赖关系管理commons-digester。 依赖版本以下库的最低支持版本已更改： Elasticsearch 5.6 Gradle 4 Hibernate 5.2 Jetty 9.4 Spring Framework 5 Spring Security 5 Tomcat 8.5 参考资料https://github.com/spring-projects/spring-boot/wiki/Spring-Boot-2.0-Migration-Guide 相关文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变 4、Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建 5、Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览 6、Spring Boot 2.0系列文章(六)：Spring boot 2.0 中 SpringBootApplication 注解详解 7、Spring Boot 2.0系列文章(七)：SpringApplication 深入探索","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}]},{"title":"Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解","date":"2018-03-05T16:00:00.000Z","path":"2018/03/06/SpringBoot2-new-features/","text":"背景在 3 月 1 号，Spring Boot2.0.0.RELEASE正式发布，这是 Spring Boot1.0 发布 4 年之后第一次重大修订，因此有多的新功能和特性值得大家期待！下面带大家了解下 Spring Boot 2.0 中的新特性。 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/03/06/SpringBoot2-new-features/ 从 Spring Boot 1.5 升级由于 Spring Boot 2.0 的改变幅度有点大，所以升级现有的程序可能会比平常更大一些。 如果你还在考虑是否要升级，这里推荐 DD 的博客文章：Spring Boot 2.0 正式发布，升还是不升呢？ 如果要升级可以参考我的另外一篇文章：Spring Boot 2.0 迁移指南 如果您目前正在运行较早版本的 Spring Boot，我们强烈建议您在迁移到 Spring Boot 2.0 之前先升级到 Spring Boot 1.5。 新的和值得注意的特性小技巧：检查 配置更改日志 来获取配置更改的完整描述。 起码 JDK 8 和支持 JDK 9Spring Boot 2.0 要求 Java 8 作为最低版本。许多现有的 API 已更新，以利用 Java 8 的特性，例如：接口上的默认方法，函数回调以及新的 API，如javax.time。如果您当前正在使用 Java 7 或更早版本，则在开发 Spring Boot 2.0 应用程序之前，您需要升级您的 JDK。 Spring Boot 2.0 通过了在 JDK 9 下的测试，可以在 JDK 9 下正常运行，。我们所有的 jar 包都在模块系统兼容性的清单中附带了自动模块名称条目。 第三方库的升级Spring Boot 2.0 建立在 Spring Framework 5 之上，并且需要 Spring Framework 5 。你可以通过 What’s New in Spring Framework 5.x 了解 Spring 5 的新特性。并在继续之前查看其升级指南 Upgrading to Spring Framework 5.x 。 我们已尽可能升级到其他第三方库的最新稳定版本。 本版本中一些显着的依赖性升级包括： Tomcat 8.5 Flyway 5 Hibernate 5.2 Thymeleaf 3 Reactive SpringSpring 产品组合中的许多项目现在都为开发反应式应用程序提供一流的支持。反应性应用程序是完全异步和非阻塞的。它们旨在用于事件循环执行模型（而不是更传统的每个请求线程执行模型）。Spring 框架参考文档中的“Web 反应堆栈”部分为这个主题提供了一个很好的入门。 Spring Boot 2.0 通过自动配置和启动器 POM 完全支持反应式应用。Spring Boot 的内部本身也在必要时进行了更新，以提供反应性的反应（最明显的是我们的嵌入式服务器支持）。 Spring WebFlux＆WebFlux.fnSpring WebFlux 是 Spring MVC 的完全非阻塞反应式替代方案。Spring Boot 为基于注释的 Spring WebFlux 应用程序以及 WebFlux.fn 提供了自动配置，WebFlux.fn 提供了更实用的样式 API。 要开始，请添加 spring-boot-starter-webflux 到 POM，它将提供由嵌入式 Netty 服务器支持的 Spring WebFlux。 Reactive Spring Data在底层技术支持的情况下，Spring Data 还为反应式应用程序提供支持。目前 Cassandra，MongoDB，Couchbase 和 Redis 都有反应式 API 支持。 Spring Boot 包含针对这些技术的特殊 starter-POMs，可为您提供启动所需的一切。例如，spring-boot-starter-data-mongodb-reactive包括对反应性 mongo 驱动程序和项目反应堆的依赖性。 Reactive Spring SecuritySpring Boot 2.0 可以充分利用 Spring Security 5.0 来保护您的反应式应用程序。当 Spring Security 位于类路径中时，会为 WebFlux 应用程序提供自动配置。 使用 WebFlux 的 Spring Security 访问规则可以通过SecurityWebFilterChain。如果你之前整合过 Spring MVC 和 Spring Security，应该会感到非常熟悉。有关更多详细信息，请参阅 Spring Boot 参考文档和 Spring Security 文档。 嵌入式 Netty 服务器由于 WebFlux 不依赖于 Servlet API，我们现在可以首次为 Netty 作为嵌入式服务器提供支持。该spring-boot-starter-webflux 启动 POM 将拉取 Netty 4.1 和 Ractor Netty 。 注意：您只能将 Netty 用作反应式服务器。不提供阻止 servlet API 支持。 HTTP/2 支持为 Tomcat，Undertow 和 Jetty 提供 HTTP / 2 支持。支持取决于所选的 Web 服务器和应用程序环境（因为 JDK 8 不支持该协议）。 如何配置 HTTP／2，请参考 官方文档 。 配置属性的绑定在 Spring Boot 2.0 中，用于绑定Environment属性的机制@ConfigurationProperties已经完全彻底修改。我们借此机会收紧了松散绑定的规则，并修复了 Spring Boot 1.x 中的许多不一致之处。 新的BinderAPI 也可以@ConfigurationProperties直接在你自己的代码之外使用。例如，下面将结合到List的PersonName对象： 123List&lt;PersonName&gt; people = Binder.get(environment) .bind(\"my.property\", Bindable.listOf(PersonName.class)) .orElseThrow(IllegalStateException::new); 配置源可以像这样在 YAML 中表示： 123456my: property: - first-name: zhisheng last-name: tian - first-name: zhisheng last-name: tian 有关更新绑定规则的更多信息，请参阅此Wiki页面。 配置起源YAML 文件和被 Spring Boot 加载的 Properties 文件现在包含Origin信息，可帮助您跟踪项目从何处加载的信息。有些 Spring Boot 特性利用了这个信息可以在适当的时候展示出来。 例如，BindException绑定失败时抛出的类是一个OriginProvider。这意味着原始信息可以很好地从故障分析器中显示出来。 另一个例子是env执行器端点，当它有可用时包含了原始信息。下面的代码片断显示该spring.security.user.name属性来自 jar 包中的 application.properties 文件的第 1行，第 27 列。 123456789&#123; \"name\": \"applicationConfig: [classpath:/application.properties]\", \"properties\": &#123; \"spring.security.user.name\": &#123; \"value\": \"user\", \"origin\": \"class path resource [application.properties]:1:27\" &#125; &#125;&#125; 转换器支持Binding 利用了一个新的 ApplicationConversionService 类，它提供了一些对属性绑定特别有用的额外转换器。最引人注目的是转换器的Duration类型和分隔字符串。 该Duration转换器允许在任一 ISO-8601 格式中指定的持续时间，或作为一个简单的字符串（例如10m，10 分钟）。现有的属性已更改为始终使用Duration。该@DurationUnit注释通过设置如果没有指定所使用的单元确保向后兼容性。例如，Spring Boot 1.5 中需要秒数的属性现在必须@DurationUnit(ChronoUnit.SECONDS)确保一个简单的值，例如10实际使用的值10s。 分隔字符串转换允许您将简单绑定String到Collection或Array不必分割逗号。例如，LDAP base-dn 属性用 @Delimiter(Delimiter.NONE)，所以 LDAP DN（通常包含逗号）不会被错误解释。 Gradle 插件Spring Boot 的 Gradle 插件已在很大程度上进行了重新编写，以实现许多重大改进。您可以在其参考文献和 API 文档中阅读关于插件功能的更多信息。 Spring Boot 现在需要 Gradle 4.x. 如果您要升级使用 Gradle 的项目，请查看迁移指南。 KotlinSpring Boot 2.0 现在包含对 Kotlin 1.2.x 的支持，并提供了runApplication ，一个使用 Kotlin 运行 Spring Boot 应用程序的方法。我们还公开和利用了 Kotlin 对其他 Spring 项目（如Spring Framework，Spring Data 和 Reactor）已添加到其最近版本中的支持。 有关更多信息，请参阅参考文档的Kotlin支持部分。 Actuator 改进在 Spring Boot 2.0 中 Actuator endpoints 有很大的改进。所有 HTTP Actuator endpoints 现在都在该/actuator路径下公开，并且生成的 JSON 有效负载得到了改进。 我们现在也不会在默认情况下暴露很多端点。如果您要升级现有的 Spring Boot 1.5 应用程序，请务必查看迁移指南并特别注意该management.endpoints.web.exposure.include属性。 Actuator JSONSpring Boot 2.0 改进了从许多端点返回的 JSON 有效负载。 现在许多端点都具有更精确地反映底层数据的 JSON。例如，/actuator/conditions终端（/autoconfig在Spring Boot 1.5中）现在有一个顶级contexts密钥来将结果分组ApplicationContext。 现在还使用 Spring REST Docs 生成了广泛的 REST API 文档，并随每个版本发布。 Jersey and WebFlux 支持除了支持 Spring MVC 和 JMX，您现在可以在开发 Jersey 或 WebFlux 应用程序时访问执行器端点。Jersey 支持通过自定义 Jersey 提供Resource，WebFlux 使用自定义HandlerMapping。 Hypermedia links该/actuator端点现在提供了一个 HAL 格式的响应提供链接到所有活动端点（即使你没有 Spring HATEOAS 在classpath）。 Actuator @Endpoints为了支持 Spring MVC，JMX，WebFlux 和 Jersey，我们为 Actuator @Endpoints 开发了一种新的编程模型。该@Endpoint注解可以与@ReadOperation，@WriteOperation 和 @DeleteOperation 组合使用开发 endpoints。 您还可以使用@EndpointWebExtension或@EndpointJmxExtension编写技术特定的增强功能到 endpoints。详细信息请参阅更新的参考文档。 MicrometerSpring Boot 2.0 不再提供自己的指标 API。相反，我们依靠 micrometer.io 来满足所有应用程序监视需求。 Micrometer 包括尺寸指标的支持，当与尺寸监测系统配对时，尺寸指标可以有效访问特定的指定度量标准，并且可以在其尺寸范围内向下钻取。 指标可以输出到各种系统和开箱即用的 Spring Boot 2.0，为 Atlas，Datadog，Ganglia，Graphite，Influx，JMX，New Relic，Prometheus，SignalFx，StatsD 和 Wavefront 提供支持。另外还可以使用简单的内存中度量标准。 集成随 JVM 指标（包括 CPU，内存，线程和 GC），Logback，Tomcat，Spring MVC＆提供RestTemplate。 有关更多详细信息，请参阅参考文档的更新“指标”部分。 数据支持除了上面提到的 Reactive Spring Data 支持外，在数据领域还进行了其他一些更新和改进。 HikariCPSpring Boot 2.0 中的默认数据库池技术已从 Tomcat Pool 切换到 HikariCP。我们发现 Hakari 提供了卓越的性能，我们的许多用户更喜欢 Tomcat Pool。 初始化数据库初始化逻辑在 Spring Boot 2.0 中已经合理化。Spring Batch，Spring Integration，Spring Session 和 Quartz的初始化现在仅在使用嵌入式数据库时才会默认发生。该enabled属性已被替换为更具表现力枚举。例如，如果你想一直执行 Spring Batch 的初始化，您可以设置spring.batch.initialize-schema=always。 如果 Flyway 或 Liquibase 正在管理您的 DataSource 的模式，并且您正在使用嵌入式数据库，Spring Boot 现在会自动关闭 Hibernate 的自动 DDL 功能。 JOOQSpring Boot 2.0 现在基于 DataSource 自动检测 JOOQ 方言（类似于为 JPA 方言所做的）。@JooqTest是新引入的注解用来简化那些只有 JOOQ 必须被使用的测试。 JdbcTemplateSpring Boot 自动配置的 JdbcTemplate 现在可以通过 spring.jdbc.template 属性进行自定义。此外，NamedParameterJdbcTemplate自动配置的内容会重用JdbcTemplate。 Spring Data Web 配置Spring Boot 公开了一个新的spring.data.web配置名称空间，可以轻松配置分页和排序。 InfluxDBSpring Boot 现在自动配置开源时间序列数据库 InfluxDB。要启用 InfluxDB 支持，您需要设置一个spring.influx.url属性，并将其包含influxdb-java在您的类路径中。 Flyway/Liquibase 灵活配置如果仅提供自定义url或user属性，则 Flyway 和 Liquibase 的自动配置现在将重用标准数据源属性，而不是忽略它们。这使您可以创建一个自定义的数据源，仅用于所需信息的迁移。 Hibernate现在支持自定义 Hibernate 命名策略。对于高级场景，现在可以在上下文中定义ImplicitNamingStrategy或PhysicalNamingStrategy使用常规 bean。 现在也可以通过公开HibernatePropertiesCustomizerbean 来更加细致地定制 Hibernate 使用的属性。 MongoDB 客户端自定义现在可以通过定义一个类型的 bean 来为 Spring Boot 自动配置的 Mongo 客户端应用高级定制MongoClientSettingsBuilderCustomizer。 Redis现在可以使用spring.cache.redis.*属性配置 Redis 的缓存默认值。 Web除了上面提到的 WebFlux 和 WebFlux.fn 支持之外，还在开发 Web 应用程序时进行了以下改进。 上下文路径记录当使用嵌入式容器时，当您的应用程序启动时，上下文路径将与 HTTP 端口一起记录。例如，嵌入式 Tomcat 现在看起来像这样： 1Tomcat 在端口上启动：8080（http），其上下文路径为 &apos;/foo&apos; Web过滤器初始化Web 过滤器现在在所有支持的容器上急切地初始化。 ThymeleafThymeleaf 初始化现在包括thymeleaf-extras-java8time，提供javax.time类型支持。 JSON 支持新的spring-boot-starter-json起始者收集必要的位以读取和写入 JSON。它不仅提供了jackson-databind与Java8 工作时，也是有用的模块：jackson-datatype-jdk8，jackson-datatype-jsr310和jackson-module-parameter-names。这个新的起动器现在被用于jackson-databind之前定义的地方。 如果您更喜欢 Jackson 之外的其他产品，我们对 GSON 的支持在 Spring Boot 2.0 已经大大提高。我们还引入了对 JSON-B 的支持（包括 JSON-B 测试支持）。 Quartz自动配置支持目前包含了 Quartz Scheduler。我们还添加了新的spring-boot-starter-quartz 初始化 POM。 您可以使用内存JobStores中或完整的基于 JDBC 的存储。所有JobDetail，Calendar并Trigger从你的 Spring应用程序上下文豆将自动注册Scheduler。 有关更多详细信息，请阅读参考文档的新“Quartz Scheduler”部分。 测试对 Spring Boot 2.0 中提供的测试支持进行了一些补充和调整： @WebFluxTest已添加新注释以支持 WebFlux 应用程序的“切片”测试。 Converter和GenericConverter豆类现在自动扫描@WebMvcTest和@WebFluxTest。 @AutoConfigureWebTestClient已经添加了一个注释来提供一个WebTestClientbean 供测试使用。注释会自动应用于@WebFluxTest测试。 增加了一个新的ApplicationContextRunner测试实用程序，可以很容易地测试您的自动配置。我们已将大部分内部测试套件移至此新模型。详细信息请参阅更新的文档。 其它除了上面列出的变化外，还有很多小的调整和改进，包括： @ConditionalOnBean现在在确定是否满足条件时使用逻辑AND而不是逻辑OR。 无条件类现在包含在自动配置报告中。 该springCLI 应用程序现在包括encodepassword可用于创建 Spring Security 的兼容散列密码命令。 计划任务（即 @EnableScheduling）可以使用scheduledtasks执行器端点进行审查。 该loggers驱动器终端现在允许你重新设置一个记录器级别为它的默认。 Spring Session 用户现在可以通过sessions执行器端点查找和删除会话。 使用spring-boot-starter-parent现在基于 Maven 的应用程序-parameters默认使用标志。 我们的构建现在使用 concourse 的 CI 和我们的项目 POM 文件已被重构，使它们更简单的。 动画 ASCII 艺术最后，为了好玩，Spring Boot 2.0 现在支持动画 GIF 横幅。 参考资料https://github.com/spring-projects/spring-boot/wiki/Spring-Boot-2.0-Release-Notes 相关文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变 4、Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建 5、Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览 6、Spring Boot 2.0系列文章(六)：Spring boot 2.0 中 SpringBootApplication 注解详解 7、Spring Boot 2.0系列文章(七)：SpringApplication 深入探索","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}]},{"title":"小马哥 《Java 微服务实践 - Spring Boot 系列》强烈推荐","date":"2018-03-03T16:00:00.000Z","path":"2018/03/04/springboot-vedio/","text":"视频内容Java 微服务实践 - Spring Boot 为系列讲座，二十节专题直播，时长高达50个小时，包括目前最流行技术，深入源码分析，授人以渔的方式，帮助初学者深入浅出地掌握，为高阶从业人员抛砖引玉。 讲师介绍 小马哥，一线互联网公司技术专家，十余年 Java EE 从业经验，架构师、微服务布道师。目前主要负责微服务技术实施、架构衍进、基础设施构建等。重点关注云计算、微服务以及软件架构等领域。通过 SUN Java（SCJP、SCWCD、SCBCD）以及 Oracle OCA 等的认证。 视频列表1、Java 微服务实践 - Spring Boot 系列（一）初体验 2、Java 微服务实践 - Spring Boot 系列（二） Web篇（上） 3、Java 微服务实践 - Spring Boot 系列（三）Web篇（中） 4、 Java 微服务实践 - Spring Boot 系列（四）Web篇（下） 5、Java 微服务实践 - Spring Boot 系列（五）嵌入式Web容器 6、Java 微服务实践 - Spring Boot 系列（六）数据库 JDBC 7、 Java 微服务实践 - Spring Boot 系列（七）MyBatis 8、 Java 微服务实践 - Spring Boot 系列（八）JPA 9、 Java 微服务实践 - Spring Boot 系列（九）NoSQL 10、 Java 微服务实践 - Spring Boot 系列（十）缓存 11、 Java 微服务实践 - Spring Boot 系列（十一）消息 12、 Java 微服务实践 - Spring Boot 系列（十二）验证 13、Java 微服务实践 - Spring Boot 系列（十三）WebSocket 14、 Java 微服务实践- Spring Boot 系列（十四）WebService 15、 Java 微服务实践 - Spring Boot 系列（十五）安全 16、Java 微服务实践 - Spring Boot 系列（十六）日志 17、Java 微服务实践 - Spring Boot 系列（十七）监管 18、Java 微服务实践 - Spring Boot 系列（十八）配置 19、 Java 微服务实践 - Spring Boot 系列（十九）测试 20、Java 微服务实践 - Spring Boot 系列（二十）自定义启动器 购买链接你可以使用下面链接购买（有优惠的哦） https://segmentfault.com/ls/1650000011063780?r=bPrFW3 或者你也可以扫描下面的二维码购买（同样也有优惠的哦） 课程评价 1、小马哥讲的好详细，以前似是而非的东东，现在都懂了2、学到了怎么使用，也学到了怎么自定义校验，知其然也知其所以然3、6666 起飞了 websocket 之前看了写 这次更加深入和标准化 感谢小马哥带来的干货4、深入浅出，思路清晰。5、深入浅出啊。非常好，而且如果有问题，不管简单还是难的，都从不同的角度和层次来解答了。学到的东西非常多。谢谢。 以上评价摘自 segmentfault 相关课程Java 微服务实践 - Spring Cloud 系列 优惠地址：https://segmentfault.com/ls/1650000011386794?r=bPrFW3 另外：还有这个 Java 微服务实践 - Spring Boot / Spring Cloud，算是两个的合集了，合买的话优惠更大。 优惠地址：https://segmentfault.com/ls/1650000011387052?r=bPrFW3 关注我 建了个不错的微信群，如果有感兴趣的可以加我微信，然后拉你进群交流。","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"},{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"}]},{"title":"小马哥 《Java 微服务实践 - Spring Cloud 系列》强烈推荐","date":"2018-03-03T16:00:00.000Z","path":"2018/03/04/springcloud-vedio/","text":"视频内容Spring Cloud 系列课程致力于以实战的方式覆盖所有功能特性，结合小马哥十余年的学习方法和工作经验，体会作者设计意图。结合源码加深理解，最终达到形成系统性的知识和技术体系的目的。 讲师介绍 小马哥，一线互联网公司技术专家，十余年 Java EE 从业经验，架构师、微服务布道师。目前主要负责微服务技术实施、架构衍进、基础设施构建等。重点关注云计算、微服务以及软件架构等领域。通过 SUN Java（SCJP、SCWCD、SCBCD）以及 Oracle OCA 等的认证。 视频列表1、Java 微服务实践 - Spring Cloud 系列（一）云原生应用 2、Java 微服务实践 - Spring Cloud 系列（二）配置客户端 3、Java 微服务实践 - Spring Cloud 系列（三）配置服务器 4、Java 微服务实践 - Spring Cloud 系列（四）服务发现/注册 5、Java 微服务实践 - Spring Cloud 系列（五）高可用服务治理 6、Java 微服务实践 - Spring Cloud 系列（六）负载均衡 7、Java 微服务实践 - Spring Cloud 系列（七）Ribbon 源码 8、Java 微服务实践 - Spring Cloud 系列（八）服务短路 9、Java 微服务实践 - Spring Cloud 系列（九）Hystrix源码 10、Java 微服务实践 - Spring Cloud 系列（十）服务调用 11、Java 微服务实践 - Spring Cloud 系列（十一）服务网关 12、Java 微服务实践 - Spring Cloud 系列（十二）消息驱动整合 13、Java 微服务实践 - Spring Cloud 系列（十三）Binder实现 14、Java 微服务实践 - Spring Cloud 系列（十四）消息总线 15、Java 微服务实践 - Spring Cloud 系列（十五）分布式应用跟踪 16、Java 微服务实践 - Spring Cloud 系列（十六）系列回顾 购买链接你可以使用下面链接购买（有优惠的哦） https://segmentfault.com/ls/1650000011386794?r=bPrFW3 或者你也可以扫描下面的二维码购买（同样也有优惠的哦） 课程评价 1、小马哥讲的很好，实践和原理一起结合讲，理解的很深刻。期待小马哥的下一个高并发的系列讲座，还有那本万众瞩目的新书2、这些系列课程怎么说呢，就我来说其实学的不是知识，是学习方法。小马哥厉害的，给个赞。3、满满的赞，社会套路太多，须加强学习，期待下期与小马哥相逢4、基本是都是守者直播时间，跟着 Spring Boot 系列和 Spring Cloud 系列的课程，小马哥讲得很有启发性。讲一种技术的从哪里来，解决了什么问题，以及优缺点。讲源码又不仅限制于源码。总的来说值得花时间来学习和讨论的。值得推荐，五星好评。5、可以的，解答问题很耐心。从你这学到了浏览JSR等之类的规范，在用一些框架的时候发现国外的，很多都实现了JSR，比如spring batch，而国内的话，个人觉得这方面稍微差了点，这方面我们还要努力，加油！ 以上评价摘自 segmentfault 相关课程Java 微服务实践 - Spring Boot 系列 优惠地址：https://segmentfault.com/ls/1650000011063780?r=bPrFW3 另外：还有这个 Java 微服务实践 - Spring Boot / Spring Cloud，算是两个的合集了，合买的话优惠更大。 优惠地址：https://segmentfault.com/ls/1650000011387052?r=bPrFW3 关注我 建了个不错的微信群，如果有感兴趣的可以加我微信，然后拉你进群交流。","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"微服务","slug":"微服务","permalink":"http://yoursite.com/tags/微服务/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://yoursite.com/tags/SpringCloud/"}]},{"title":"《深入理解 Java 内存模型》读书笔记","date":"2018-02-27T16:00:00.000Z","path":"2018/02/28/Java-Memory-Model/","text":"前提《深入理解 Java 内存模型》程晓明著，该书在以前看过一遍，现在学的东西越多，感觉那块越重要，于是又再细看一遍，于是便有了下面的读书笔记总结。全书页数虽不多，内容讲得挺深的。细看的话，也是挺花时间的，看完收获绝对挺大的。也建议 Java 开发者都去看看。里面主要有 Java 内存模型的基础、重排序、顺序一致性、Volatile 关键字、锁、final。本文参考书中内容。 关注我如果你想查看这本书可以关注我的公众号: zhisheng ，然后里面回复关键字 JMM 可以查看我分享的百度云链接。 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/02/28/Java-Memory-Model/ 基础并发编程的模型分类在并发编程需要处理的两个关键问题是：线程之间如何通信 和 线程之间如何同步。 通信通信 是指线程之间以何种机制来交换信息。在命令式编程中，线程之间的通信机制有两种：共享内存 和 消息传递。 在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。 在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显式进行通信。 同步同步 是指程序用于控制不同线程之间操作发生相对顺序的机制。 在共享内存的并发模型里，同步是显式进行的。程序员必须显式指定某个方法或某段代码需要在线程之间互斥执行。 在消息传递的并发模型里，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。 Java 的并发采用的是共享内存模型，Java 线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。 Java 内存模型的抽象在 Java 中，所有实例域、静态域 和 数组元素存储在堆内存中，堆内存在线程之间共享。局部变量、方法定义参数 和 异常处理器参数 不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。 Java 线程之间的通信由 Java 内存模型（JMM）控制。JMM 决定了一个线程对共享变量的写入何时对另一个线程可见。从抽象的角度来看，JMM 定义了线程与主内存之间的抽象关系：线程之间的共享变量存储在主内存中，每一个线程都有一个自己私有的本地内存，本地内存中存储了该变量以读／写共享变量的副本。本地内存是 JMM 的一个抽象概念，并不真实存在。 JMM 抽象示意图： 从上图来看，如果线程 A 和线程 B 要通信的话，要如下两个步骤： 1、线程 A 需要将本地内存 A 中的共享变量副本刷新到主内存去 2、线程 B 去主内存读取线程 A 之前已更新过的共享变量 步骤示意图： 举个例子： 本地内存 A 和 B 有主内存共享变量 X 的副本。假设一开始时，这三个内存中 X 的值都是 0。线程 A 正执行时，把更新后的 X 值（假设为 1）临时存放在自己的本地内存 A 中。当线程 A 和 B 需要通信时，线程 A 首先会把自己本地内存 A 中修改后的 X 值刷新到主内存去，此时主内存中的 X 值变为了 1。随后，线程 B 到主内存中读取线程 A 更新后的共享变量 X 的值，此时线程 B 的本地内存的 X 值也变成了 1。 整体来看，这两个步骤实质上是线程 A 再向线程 B 发送消息，而这个通信过程必须经过主内存。JMM 通过控制主内存与每个线程的本地内存之间的交互，来为 Java 程序员提供内存可见性保证。 重排序在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三类： 1、编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 2、指令级并行的重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 3、内存系统的重排序。由于处理器使用缓存和读／写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 从 Java 源代码到最终实际执行的指令序列，会分别经历下面三种重排序： 上面的这些重排序都可能导致多线程程序出现内存可见性问题。对于编译器，JMM 的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM 的处理器重排序规则会要求 Java 编译器在生成指令序列时，插入特定类型的内存屏障指令，通过内存屏障指令来禁止特定类型的处理器重排序（不是所有的处理器重排序都要禁止）。 JMM 属于语言级的内存模型，它确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。 处理器重排序现代的处理器使用写缓冲区来临时保存向内存写入的数据。写缓冲区可以保证指令流水线持续运行，它可以避免由于处理器停顿下来等待向内存写入数据而产生的延迟。同时，通过以批处理的方式刷新写缓冲区，以及合并写缓冲区中对同一内存地址的多次写，可以减少对内存总线的占用。虽然写缓冲区有这么多好处，但每个处理器上的写缓冲区，仅仅对它所在的处理器可见。这个特性会对内存操作的执行顺序产生重要的影响：处理器对内存的读/写操作的执行顺序，不一定与内存实际发生的读/写操作顺序一致！ 举个例子： 假设处理器A和处理器B按程序的顺序并行执行内存访问，最终却可能得到 x = y = 0。具体的原因如下图所示： 处理器 A 和 B 同时把共享变量写入在写缓冲区中（A1、B1），然后再从内存中读取另一个共享变量（A2、B2），最后才把自己写缓冲区中保存的脏数据刷新到内存中（A3、B3）。当以这种时序执行时，程序就可以得到 x = y = 0 的结果。 从内存操作实际发生的顺序来看，直到处理器 A 执行 A3 来刷新自己的写缓存区，写操作 A1 才算真正执行了。虽然处理器 A 执行内存操作的顺序为：A1 -&gt; A2，但内存操作实际发生的顺序却是：A2 -&gt; A1。此时，处理器 A 的内存操作顺序被重排序了。 这里的关键是，由于写缓冲区仅对自己的处理器可见，它会导致处理器执行内存操作的顺序可能会与内存实际的操作执行顺序不一致。由于现代的处理器都会使用写缓冲区，因此现代的处理器都会允许对写-读操作重排序。 内存屏障指令为了保证内存可见性，Java 编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序。JMM 把内存屏障指令分为下列四类： 屏障类型 指令示例 说明 LoadLoad Barriers Load1; LoadLoad; Load2 确保 Load1 数据的装载，之前于 Load2 及所有后续装载指令的装载。 StoreStore Barriers Store1; StoreStore; Store2 确保 Store1 数据对其他处理器可见（刷新到内存），之前于 Store2 及所有后续存储指令的存储。 LoadStore Barriers Load1; LoadStore; Store2 确保 Load1 数据装载，之前于 Store2 及所有后续的存储指令刷新到内存。 StoreLoad Barriers Store1; StoreLoad; Load2 确保 Store1 数据对其他处理器变得可见（指刷新到内存），之前于 Load2 及所有后续装载指令的装载。StoreLoadBarriers 会使该屏障之前的所有内存访问指令（存储和装载指令）完成之后，才执行该屏障之后的内存访问指令。 happens-beforeJSR-133 内存模型使用 happens-before 的概念来阐述操作之间的内存可见性。在 JMM 中，如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须要存在 happens-before 关系。这里提到的两个操作既可以是在一个线程之内，也可以是在不同线程之间。 与程序员密切相关的 happens-before 规则如下： 程序顺序规则：一个线程中的每个操作，happens-before 于该线程中的任意后续操作。 监视器锁规则：对一个监视器的解锁，happens-before 于随后对这个监视器的加锁。 volatile 变量规则：对一个 volatile 域的写，happens-before 于任意后续对这个 volatile 域的读。 传递性：如果 A happens-before B，且 B happens-before C，那么 A happens-before C。 注意，两个操作之间具有 happens-before 关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before 仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前（the first is visible to and ordered before the second）。 happens-before 与 JMM 的关系如下图所示： 如上图所示，一个 happens-before 规则对应于一个或多个编译器和处理器重排序规则。 数据依赖性如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。数据依赖分下列三种类型： 名称 代码示例 说明 写后读 a = 1; b = a; 写一个变量之后，再读这个位置。 写后写 a = 1; a = 2; 写一个变量之后，再写这个变量。 读后写 a = b; b = 1; 读一个变量之后，再写这个变量。 上面三种情况，只要重排序两个操作的执行顺序，程序的执行结果将会被改变。 前面提到过，编译器和处理器可能会对操作做重排序。编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序。 注意，这里所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作，不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑。 as-if-serial 语义as-if-serial 语义的意思指：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器，runtime 和处理器都必须遵守 as-if-serial 语义。 为了遵守 as-if-serial 编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是如果操作之间没有数据依赖关系，这些操作就可能被编译器和处理器重排序。 举个例子： 123double pi = 3.14; //Adouble r = 1.0; //Bdouble area = pi * r * r; //C 上面三个操作的数据依赖关系如下图所示： 如上图所示，A 和 C 之间存在数据依赖关系，同时 B 和 C 之间也存在数据依赖关系。因此在最终执行的指令序列中，C 不能被重排序到 A 和 B 的前面（C 排到 A 和 B 的前面，程序的结果将会被改变）。但 A 和 B 之间没有数据依赖关系，编译器和处理器可以重排序 A 和 B 之间的执行顺序。下图是该程序的两种执行顺序： 在计算机中，软件技术和硬件技术有一个共同的目标：在不改变程序执行结果的前提下，尽可能的开发并行度。编译器和处理器遵从这一目标，从 happens-before 的定义我们可以看出，JMM 同样遵从这一目标。 重排序对多线程的影响举例： 123456789101112131415class Demo &#123; int a = 0; boolean flag = false; public void write() &#123; a = 1; //1 flag = true; //2 &#125; public void read() &#123; if(flag) &#123; //3 int i = a * a; //4 &#125; &#125;&#125; 由于操作 1 和 2 没有数据依赖关系，编译器和处理器可以对这两个操作重排序；操作 3 和操作 4 没有数据依赖关系，编译器和处理器也可以对这两个操作重排序。 1、当操作 1 和操作 2 重排序时，可能会产生什么效果？ 如上图所示，操作 1 和操作 2 做了重排序。程序执行时，线程 A 首先写标记变量 flag，随后线程 B 读这个变量。由于条件判断为真，线程 B 将读取变量 a。此时，变量 a 还根本没有被线程 A 写入，在这里多线程程序的语义被重排序破坏了！ 2、当操作 3 和操作 4 重排序时会产生什么效果（借助这个重排序，可以顺便说明控制依赖性）。 在程序中，操作 3 和操作 4 存在控制依赖关系。当代码中存在控制依赖性时，会影响指令序列执行的并行度。为此，编译器和处理器会采用猜测（Speculation）执行来克服控制相关性对并行度的影响。以处理器的猜测执行为例，执行线程 B 的处理器可以提前读取并计算 a * a，然后把计算结果临时保存到一个名为重排序缓冲（reorder buffer ROB）的硬件缓存中。当接下来操作 3 的条件判断为真时，就把该计算结果写入变量 i 中。 从图中我们可以看出，猜测执行实质上对操作3和4做了重排序。重排序在这里破坏了多线程程序的语义！ 在单线程程序中，对存在控制依赖的操作重排序，不会改变执行结果（这也是 as-if-serial 语义允许对存在控制依赖的操作做重排序的原因）；但在多线程程序中，对存在控制依赖的操作重排序，可能会改变程序的执行结果。 顺序一致性顺序一致性内存模型顺序一致性内存模型有两大特性： 一个线程中的所有操作必须按照程序的顺序来执行。 （不管程序是否同步）所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。 顺序一致性内存模型为程序员提供的视图如下： 在概念上，顺序一致性模型有一个单一的全局内存，这个内存通过一个左右摆动的开关可以连接到任意一个线程，同时每一个线程必须按照程序的顺序来执行内存读/写操作。从上面的示意图我们可以看出，在任意时间点最多只能有一个线程可以连接到内存。当多个线程并发执行时，图中的开关装置能把所有线程的所有内存读/写操作串行化。 举个例子： 假设有两个线程 A 和 B 并发执行。其中 A 线程有三个操作，它们在程序中的顺序是：A1 -&gt; A2 -&gt; A3。B 线程也有三个操作，它们在程序中的顺序是：B1 -&gt; B2 -&gt; B3。 假设这两个线程使用监视器锁来正确同步：A 线程的三个操作执行后释放监视器锁，随后 B 线程获取同一个监视器锁。那么程序在顺序一致性模型中的执行效果将如下图所示： 现在我们再假设这两个线程没有做同步，下面是这个未同步程序在顺序一致性模型中的执行示意图： 未同步程序在顺序一致性模型中虽然整体执行顺序是无序的，但所有线程都只能看到一个一致的整体执行顺序。以上图为例，线程 A 和 B 看到的执行顺序都是：B1 -&gt; A1 -&gt; A2 -&gt; B2 -&gt; A3 -&gt; B3。之所以能得到这个保证是因为顺序一致性内存模型中的每个操作必须立即对任意线程可见。 但是，在 JMM 中就没有这个保证。未同步程序在 JMM 中不但整体的执行顺序是无序的，而且所有线程看到的操作执行顺序也可能不一致。比如，在当前线程把写过的数据缓存在本地内存中，在还没有刷新到主内存之前，这个写操作仅对当前线程可见；从其他线程的角度来观察，会认为这个写操作根本还没有被当前线程执行。只有当前线程把本地内存中写过的数据刷新到主内存之后，这个写操作才能对其他线程可见。在这种情况下，当前线程和其它线程看到的操作执行顺序将不一致。 同步程序的顺序一致性效果下面我们对前面的示例程序用锁来同步，看看正确同步的程序如何具有顺序一致性。 请看下面的示例代码： 123456789101112131415class demo &#123; int a = 0; boolean flag = false; public synchronized void write() &#123; //获取锁 a = 1; flag = true; &#125; //释放锁 public synchronized void read() &#123; //获取锁 if(flag) &#123; int i = a; &#125; &#125; //释放锁&#125; 上面示例代码中，假设 A 线程执行 write() 方法后，B 线程执行 reade() 方法。这是一个正确同步的多线程程序。根据JMM规范，该程序的执行结果将与该程序在顺序一致性模型中的执行结果相同。下面是该程序在两个内存模型中的执行时序对比图： 在顺序一致性模型中，所有操作完全按程序的顺序执行。而在 JMM 中，临界区内的代码可以重排序（但 JMM 不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。JMM 会在退出临界区和进入临界区这两个关键时间点做一些特别处理，使得线程在这两个时间点具有与顺序一致性模型相同的内存视图。虽然线程 A 在临界区内做了重排序，但由于监视器的互斥执行的特性，这里的线程 B 根本无法“观察”到线程 A 在临界区内的重排序。这种重排序既提高了执行效率，又没有改变程序的执行结果。 从这里我们可以看到 JMM 在具体实现上的基本方针：在不改变（正确同步的）程序执行结果的前提下，尽可能的为编译器和处理器的优化打开方便之门。 未同步程序的执行特性未同步程序在 JMM 中的执行时，整体上是无序的，其执行结果无法预知。未同步程序在两个模型中的执行特性有下面几个差异： 顺序一致性模型保证单线程内的操作会按程序的顺序执行，而 JMM 不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。 顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而 JMM 不保证所有线程能看到一致的操作执行顺序。 JMM 不保证对 64 位的 long 型和 double 型变量的读/写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子 。 第三个差异与处理器总线的工作机制密切相关。在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过总线事务来完成的。总线事务包括读事务和写事务。读事务从内存传送数据到处理器，写事务从处理器传递数据到内存，每个事务会读／写内存中一个或多个物理上连续的字。总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其它所有的处理器和 I／O 设备执行内存的读／写。 总线的工作机制： 如上图所示，假设处理器 A、B、和 C 同时向总线发起总线事务，这时总线仲裁会对竞争作出裁决，假设总线在仲裁后判定处理器 A 在竞争中获胜（总线仲裁会确保所有处理器都能公平的访问内存）。此时处理器 A 继续它的总线事务，而其它两个处理器则要等待处理器 A 的总线事务完成后才能开始再次执行内存访问。假设在处理器 A 执行总线事务期间（不管这个总线事务是读事务还是写事务），处理器 D 向总线发起了总线事务，此时处理器 D 的这个请求会被总线禁止。 总线的这些工作机制可以把所有处理器对内存的访问以串行化的方式来执行；在任意时间点，最多只能有一个处理器能访问内存。这个特性确保了单个总线事务之中的内存读/写操作具有原子性。 在一些 32 位的处理器上，如果要求对 64 位数据的写操作具有原子性，会有比较大的开销。为了照顾这种处理器，Java 语言规范鼓励但不强求 JVM 对 64 位的 long 型变量和 double 型变量的写具有原子性。当 JVM 在这种处理器上运行时，会把一个 64 位 long/ double 型变量的写操作拆分为两个 32 位的写操作来执行。这两个 32 位的写操作可能会被分配到不同的总线事务中执行，此时对这个 64 位变量的写将不具有原子性。 当单个内存操作不具有原子性，将可能会产生意想不到后果。请看下面示意图： 如上图所示，假设处理器 A 写一个 long 型变量，同时处理器 B 要读这个 long 型变量。处理器 A 中 64 位的写操作被拆分为两个 32 位的写操作，且这两个 32 位的写操作被分配到不同的写事务中执行。同时处理器 B 中 64 位的读操作被分配到单个的读事务中执行。当处理器 A 和 B 按上图的时序来执行时，处理器 B 将看到仅仅被处理器 A “写了一半“的无效值。 注意，在 JSR -133 之前的旧内存模型中，一个 64 位 long/ double 型变量的读/写操作可以被拆分为两个 32 位的读/写操作来执行。从 JSR -133 内存模型开始（即从JDK5开始），仅仅只允许把一个 64 位 long/ double 型变量的写操作拆分为两个 32 位的写操作来执行，任意的读操作在JSR -133中都必须具有原子性（即任意读操作必须要在单个读事务中执行）。 VolatileVolatile 特性举个例子： 123456789101112131415public class VolatileTest &#123; volatile long a = 1L; // 使用 volatile 声明 64 位的 long 型 public void set(long l) &#123; a = l; //单个 volatile 变量的写 &#125; public long get() &#123; return a; //单个 volatile 变量的读 &#125; public void getAndIncreament() &#123; a++; // 复合（多个） volatile 变量的读 /写 &#125;&#125; 假设有多个线程分别调用上面程序的三个方法，这个程序在语义上和下面程序等价： 1234567891011121314151617public class VolatileTest &#123; long a = 1L; // 64 位的 long 型普通变量 public synchronized void set(long l) &#123; //对单个普通变量的写用同一个锁同步 a = l; &#125; public synchronized long get() &#123; //对单个普通变量的读用同一个锁同步 return a; &#125; public void getAndIncreament() &#123; //普通方法调用 long temp = get(); //调用已同步的读方法 temp += 1L; //普通写操作 set(temp); //调用已同步的写方法 &#125;&#125; 如上面示例程序所示，对一个 volatile 变量的单个读/写操作，与对一个普通变量的读/写操作使用同一个锁来同步，它们之间的执行效果相同。 锁的 happens-before 规则保证释放锁和获取锁的两个线程之间的内存可见性，这意味着对一个 volatile 变量的读，总是能看到（任意线程）对这个 volatile 变量最后的写入。 锁的语义决定了临界区代码的执行具有原子性。这意味着即使是 64 位的 long 型和 double 型变量，只要它是 volatile变量，对该变量的读写就将具有原子性。如果是多个 volatile 操作或类似于 volatile++ 这种复合操作，这些操作整体上不具有原子性。 简而言之，volatile 变量自身具有下列特性： 可见性。对一个 volatile 变量的读，总是能看到（任意线程）对这个 volatile 变量最后的写入。 原子性：对任意单个 volatile 变量的读/写具有原子性，但类似于 volatile++ 这种复合操作不具有原子性。 volatile 写-读的内存定义 当写一个 volatile 变量时，JMM 会把该线程对应的本地内存中的共享变量值刷新到主内存。 当读一个 volatile 变量时，JMM 会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。 假设上面的程序 flag 变量用 volatile 修饰 volatile 内存语义的实现下面是 JMM 针对编译器制定的 volatile 重排序规则表： 为了实现 volatile 的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。 下面是基于保守策略的 JMM 内存屏障插入策略： 在每个 volatile 写操作的前面插入一个 StoreStore 屏障。 在每个 volatile 写操作的后面插入一个 StoreLoad 屏障。 在每个 volatile 读操作的后面插入一个 LoadLoad 屏障。 在每个 volatile 读操作的后面插入一个 LoadStore 屏障。 下面是保守策略下，volatile 写操作 插入内存屏障后生成的指令序列示意图： 下面是在保守策略下，volatile 读操作 插入内存屏障后生成的指令序列示意图： 上述 volatile 写操作和 volatile 读操作的内存屏障插入策略非常保守。在实际执行时，只要不改变 volatile 写-读的内存语义，编译器可以根据具体情况省略不必要的屏障。 锁锁释放和获取的内存语义当线程释放锁时，JMM 会把该线程对应的本地内存中的共享变量刷新到主内存中。 当线程获取锁时，JMM 会把该线程对应的本地内存置为无效。从而使得被监视器保护的临界区代码必须要从主内存中去读取共享变量。 锁内存语义的实现借助 ReentrantLock 来讲解，PS： 后面专门讲下这块（ReentrantLock、Synchronized、公平锁、非公平锁、AQS等），可以看看大明哥的博客：http://cmsblogs.com/?p=2210 concurrent 包的实现如果我们仔细分析 concurrent 包的源代码实现，会发现一个通用化的实现模式： 首先，声明共享变量为 volatile； 然后，使用 CAS 的原子条件更新来实现线程之间的同步； 同时，配合以 volatile 的读/写和 CAS 所具有的 volatile 读和写的内存语义来实现线程之间的通信。 AQS，非阻塞数据结构和原子变量类（java.util.concurrent.atomic 包中的类），这些 concurrent 包中的基础类都是使用这种模式来实现的，而 concurrent 包中的高层类又是依赖于这些基础类来实现的。从整体来看，concurrent 包的实现示意图如下： final对于 final 域，编译器和处理器要遵守两个重排序规则： 在构造函数内对一个 final 域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 初次读一个包含 final 域的对象的引用，与随后初次读这个 final 域，这两个操作之间不能重排序。 写 final 域的重排序规则写 final 域的重排序规则禁止把 final 域的写重排序到构造函数之外。这个规则的实现包含下面2个方面： JMM 禁止编译器把 final 域的写重排序到构造函数之外。 编译器会在 final 域的写之后，构造函数 return 之前，插入一个 StoreStore 屏障。这个屏障禁止处理器把 final 域的写重排序到构造函数之外。 读 final 域的重排序规则在一个线程中，初次读对象引用与初次读该对象包含的 final 域，JMM 禁止处理器重排序这两个操作（注意，这个规则仅仅针对处理器）。编译器会在读 final 域操作的前面插入一个 LoadLoad 屏障。 final 域是引用类型对于引用类型，写 final 域的重排序规则对编译器和处理器增加了如下约束： 在构造函数内对一个 final 引用的对象的成员域的写入，与随后在构造函数外把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 总结JMM，处理器内存模型与顺序一致性内存模型之间的关系JMM 是一个语言级的内存模型，处理器内存模型是硬件级的内存模型，顺序一致性内存模型是一个理论参考模型。下面是语言内存模型，处理器内存模型和顺序一致性内存模型的强弱对比示意图： JMM 的设计示意图 JMM 的内存可见性保证Java 程序的内存可见性保证按程序类型可以分为下列三类： 1.单线程程序。单线程程序不会出现内存可见性问题。编译器，runtime 和处理器会共同确保单线程程序的执行结果与该程序在顺序一致性模型中的执行结果相同。 2.正确同步的多线程程序。正确同步的多线程程序的执行将具有顺序一致性（程序的执行结果与该程序在顺序一致性内存模型中的执行结果相同）。这是 JMM 关注的重点，JMM通过限制编译器和处理器的重排序来为程序员提供内存可见性保证。 3.未同步/未正确同步的多线程程序。JMM 为它们提供了最小安全性保障：线程执行时读取到的值，要么是之前某个线程写入的值，要么是默认值（0，null，false）。 下图展示了这三类程序在 JMM 中与在顺序一致性内存模型中的执行结果的异同：","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"JMM","slug":"JMM","permalink":"http://yoursite.com/tags/JMM/"}]},{"title":"Spring Boot系列文章（六）：SpringBoot RocketMQ 整合使用和监控","date":"2018-02-06T16:00:00.000Z","path":"2018/02/07/SpringBoot-RocketMQ/","text":"前提通过前面两篇文章可以简单的了解 RocketMQ 和 安装 RocketMQ ，今天就将 SpringBoot 和 RocketMQ 整合起来使用。 相关文章1、SpringBoot Kafka 整合使用 2、SpringBoot RabbitMQ 整合使用 3、SpringBoot ActiveMQ 整合使用 4、Kafka 安装及快速入门 5、SpringBoot RabbitMQ 整合进阶版 6、RocketMQ 初探 7、RocketMQ 安装及快速入门 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/02/07/SpringBoot-RocketMQ/ 创建项目在 IDEA 创建一个 SpringBoot 项目，项目结构如下： pom 文件引入 RocketMQ 的一些相关依赖，最后的 pom 文件如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zhisheng&lt;/groupId&gt; &lt;artifactId&gt;rocketmq&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;rocketmq&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot RocketMQ&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-common&lt;/artifactId&gt; &lt;version&gt;4.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-client&lt;/artifactId&gt; &lt;version&gt;4.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 配置文件application.properties 中如下： 123456# 消费者的组名apache.rocketmq.consumer.PushConsumer=PushConsumer# 生产者的组名apache.rocketmq.producer.producerGroup=Producer# NameServer地址apache.rocketmq.namesrvAddr=localhost:9876 生产者1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.zhisheng.rocketmq.client;import org.apache.rocketmq.client.producer.DefaultMQProducer;import org.apache.rocketmq.common.message.Message;import org.apache.rocketmq.remoting.common.RemotingHelper;import org.springframework.beans.factory.annotation.Value;import org.springframework.stereotype.Component;import org.springframework.util.StopWatch;import javax.annotation.PostConstruct;/** * Created by zhisheng_tian on 2018/2/6 */@Componentpublic class RocketMQClient &#123; /** * 生产者的组名 */ @Value(\"$&#123;apache.rocketmq.producer.producerGroup&#125;\") private String producerGroup; /** * NameServer 地址 */ @Value(\"$&#123;apache.rocketmq.namesrvAddr&#125;\") private String namesrvAddr; @PostConstruct public void defaultMQProducer() &#123; //生产者的组名 DefaultMQProducer producer = new DefaultMQProducer(producerGroup); //指定NameServer地址，多个地址以 ; 隔开 producer.setNamesrvAddr(namesrvAddr); try &#123; /** * Producer对象在使用之前必须要调用start初始化，初始化一次即可 * 注意：切记不可以在每次发送消息时，都调用start方法 */ producer.start(); //创建一个消息实例，包含 topic、tag 和 消息体 //如下：topic 为 \"TopicTest\"，tag 为 \"push\" Message message = new Message(\"TopicTest\", \"push\", \"发送消息----zhisheng-----\".getBytes(RemotingHelper.DEFAULT_CHARSET)); StopWatch stop = new StopWatch(); stop.start(); for (int i = 0; i &lt; 10000; i++) &#123; SendResult result = producer.send(message); System.out.println(\"发送响应：MsgId:\" + result.getMsgId() + \"，发送状态:\" + result.getSendStatus()); &#125; stop.stop(); System.out.println(\"----------------发送一万条消息耗时：\" + stop.getTotalTimeMillis()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; producer.shutdown(); &#125; &#125;&#125; 消费者123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package com.zhisheng.rocketmq.server;import org.apache.rocketmq.client.consumer.DefaultMQPushConsumer;import org.apache.rocketmq.client.consumer.listener.ConsumeConcurrentlyStatus;import org.apache.rocketmq.client.consumer.listener.MessageListenerConcurrently;import org.apache.rocketmq.common.consumer.ConsumeFromWhere;import org.apache.rocketmq.common.message.MessageExt;import org.apache.rocketmq.remoting.common.RemotingHelper;import org.springframework.beans.factory.annotation.Value;import org.springframework.stereotype.Component;import javax.annotation.PostConstruct;/** * Created by zhisheng_tian on 2018/2/6 */@Componentpublic class RocketMQServer &#123; /** * 消费者的组名 */ @Value(\"$&#123;apache.rocketmq.consumer.PushConsumer&#125;\") private String consumerGroup; /** * NameServer 地址 */ @Value(\"$&#123;apache.rocketmq.namesrvAddr&#125;\") private String namesrvAddr; @PostConstruct public void defaultMQPushConsumer() &#123; //消费者的组名 DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(consumerGroup); //指定NameServer地址，多个地址以 ; 隔开 consumer.setNamesrvAddr(namesrvAddr); try &#123; //订阅PushTopic下Tag为push的消息 consumer.subscribe(\"TopicTest\", \"push\"); //设置Consumer第一次启动是从队列头部开始消费还是队列尾部开始消费 //如果非第一次启动，那么按照上次消费的位置继续消费 consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); consumer.registerMessageListener((MessageListenerConcurrently) (list, context) -&gt; &#123; try &#123; for (MessageExt messageExt : list) &#123; System.out.println(\"messageExt: \" + messageExt);//输出消息内容 String messageBody = new String(messageExt.getBody(), RemotingHelper.DEFAULT_CHARSET); System.out.println(\"消费响应：msgId : \" + messageExt.getMsgId() + \", msgBody : \" + messageBody);//输出消息内容 &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); return ConsumeConcurrentlyStatus.RECONSUME_LATER; //稍后再试 &#125; return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; //消费成功 &#125;); consumer.start(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 启动类123456789101112package com.zhisheng.rocketmq;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class RocketmqApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(RocketmqApplication.class, args); &#125;&#125; RocketMQ代码已经都写好了，接下来我们需要将与 RocketMQ 有关的启动起来。 启动 Name Server在前面文章中已经写过怎么启动，http://www.54tianzhisheng.cn/2018/02/06/RocketMQ-install/#%E5%90%AF%E5%8A%A8-NameServer 进入到目录 ： 1cd distribution/target/apache-rocketmq 启动： 123nohup sh bin/mqnamesrv &amp;tail -f ~/logs/rocketmqlogs/namesrv.log //通过日志查看是否启动成功 启动 Broker123nohup sh bin/mqbroker -n localhost:9876 &amp;tail -f ~/logs/rocketmqlogs/broker.log //通过日志查看是否启动成功 然后运行启动类，运行效果如下： 监控RocketMQ有一个对其扩展的开源项目 ocketmq-console ，如今也提交给了 Apache ，地址在：https://github.com/apache/rocketmq-externals/tree/master/rocketmq-console ，官方也给出了其支持的功能的中文文档：https://github.com/apache/rocketmq-externals/blob/master/rocketmq-console/doc/1_0_0/UserGuide_CN.md ， 那么该如何安装？ Docker 安装1、获取 Docker 镜像 1docker pull styletang/rocketmq-console-ng 2、运行，注意将你自己的 NameServer 地址替换下面的 127.0.0.1 1docker run -e &quot;JAVA_OPTS=-Drocketmq.namesrv.addr=127.0.0.1:9876 -Dcom.rocketmq.sendMessageWithVIPChannel=false&quot; -p 8080:8080 -t styletang/rocketmq-console-ng 非 Docker 安装我们 git clone 一份代码到本地： 123git clone https://github.com/apache/rocketmq-externals.gitcd rocketmq-externals/rocketmq-console/ 需要 jdk 1.7 以上。 执行以下命令： 1mvn spring-boot:run 或者 123mvn clean package -Dmaven.test.skip=truejava -jar target/rocketmq-console-ng-1.0.0.jar 注意： 1、如果你下载依赖缓慢，你可以重新设置 maven 的 mirror 为阿里云的镜像 12345678&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt;&lt;/mirrors&gt; 2、如果你使用的 RocketMQ 版本小于 3.5.8，如果您使用 rocketmq &lt; 3.5.8，请在启动 rocketmq-console-ng 时添加 -Dcom.rocketmq.sendMessageWithVIPChannel = false（或者您可以在 ops 页面中更改它） 3、更改 resource / application.properties 中的 rocketmq.config.namesrvAddr（或者可以在ops页面中更改它） 错误解决方法1、Docker 启动项目报错 org.apache.rocketmq.remoting.exception.RemotingConnectException: connect to &lt;null&gt; failed 将 Docker 启动命令改成如下以后： 1docker run -e &quot;JAVA_OPTS=-Drocketmq.config.namesrvAddr=127.0.0.1:9876 -Drocketmq.config.isVIPChannel=false&quot; -p 8080:8080 -t styletang/rocketmq-console-ng 报错信息改变了，新的报错信息如下： 123ERROR op=global_exception_handler_print_errororg.apache.rocketmq.console.exception.ServiceException: This date have&apos;t data! 看到网上有人也遇到这个问题，他们都通过自己的方式解决了，但是方法我都试了，不适合我。不得不说，阿里，你能再用心点吗？既然把 RocketMQ 捐给 Apache 了，这些文档啥的都必须更新啊，不要还滞后着呢，不然少不了被吐槽！ 搞了很久这种方法没成功，暂时放弃！mmp 2、非 Docker 安装，只好把源码编译打包了。 1) 注意需要修改如下图中的配置： 1234rocketmq.config.namesrvAddr=localhost:9876 //注意替换你自己的ip#如果你 rocketmq 版本小于 3.5.8 才需设置 `rocketmq.config.isVIPChannel` 为 false，默认是 true, 这个可以在源码中可以看到的rocketmq.config.isVIPChannel= 2) 执行以下命令： 1mvn clean package -Dmaven.test.skip=true 编译成功： 可以看到已经打好了 jar 包： 运行： 1java -jar rocketmq-console-ng-1.0.0.jar 成功，不报错了，开心😄，访问 http://localhost:8080/ 整个监控大概就是这些了。 然后我运行之前的 SpringBoot 整合项目，查看监控信息如下： 总结整篇文章讲述了 SpringBoot 与 RocketMQ 整合和 RocketMQ 监控平台的搭建。 参考文章1、http://www.ymq.io/2018/02/02/spring-boot-rocketmq-example/#%E6%96%B0%E5%8A%A0%E9%A1%B9%E7%9B%AE 2、GitHub 官方 README","tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"},{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://yoursite.com/tags/RocketMQ/"}]},{"title":"RocketMQ系列文章（三）：RocketMQ 简单的消息示例","date":"2018-02-06T16:00:00.000Z","path":"2018/02/07/rocketmq-example/","text":"使用 RocketMQ 以三种方式发送消息：可靠的同步，可靠的异步和单向传输。 可靠的同步传输应用：可靠的同步传输广泛应用于重要通知消息，短信通知，短信营销系统等。 12345678910111213141516171819202122public class SyncProducer &#123; public static void main(String[] args) throws Exception &#123; //Instantiate with a producer group name. DefaultMQProducer producer = new DefaultMQProducer(\"please_rename_unique_group_name\"); //Launch the instance. producer.start(); for (int i = 0; i &lt; 100; i++) &#123; //Create a message instance, specifying topic, tag and message body. Message msg = new Message(\"TopicTest\" /* Topic */, \"TagA\" /* Tag */, (\"Hello RocketMQ \" + i).getBytes(RemotingHelper.DEFAULT_CHARSET) /* Message body */ ); //Call send message to deliver message to one of brokers. SendResult sendResult = producer.send(msg); System.out.printf(\"%s%n\", sendResult); &#125; //Shut down once the producer instance is not longer in use. producer.shutdown(); &#125;&#125; 可靠的异步传输应用：异步传输一般用于响应时间敏感的业务场景。 12345678910111213141516171819202122232425262728293031public class AsyncProducer &#123; public static void main(String[] args) throws Exception &#123; //Instantiate with a producer group name. DefaultMQProducer producer = new DefaultMQProducer(\"ExampleProducerGroup\"); //Launch the instance. producer.start(); producer.setRetryTimesWhenSendAsyncFailed(0); for (int i = 0; i &lt; 100; i++) &#123; final int index = i; //Create a message instance, specifying topic, tag and message body. Message msg = new Message(\"TopicTest\", \"TagA\", \"OrderID188\", \"Hello world\".getBytes(RemotingHelper.DEFAULT_CHARSET)); producer.send(msg, new SendCallback() &#123; @Override public void onSuccess(SendResult sendResult) &#123; System.out.printf(\"%-10d OK %s %n\", index, sendResult.getMsgId()); &#125; @Override public void onException(Throwable e) &#123; System.out.printf(\"%-10d Exception %s %n\", index, e); e.printStackTrace(); &#125; &#125;); &#125; //Shut down once the producer instance is not longer in use. producer.shutdown(); &#125;&#125; 单向传输应用：单向传输用于需要中等可靠性的情况，例如日志收集。 123456789101112131415161718192021public class OnewayProducer &#123; public static void main(String[] args) throws Exception&#123; //Instantiate with a producer group name. DefaultMQProducer producer = new DefaultMQProducer(\"ExampleProducerGroup\"); //Launch the instance. producer.start(); for (int i = 0; i &lt; 100; i++) &#123; //Create a message instance, specifying topic, tag and message body. Message msg = new Message(\"TopicTest\" /* Topic */, \"TagA\" /* Tag */, (\"Hello RocketMQ \" + i).getBytes(RemotingHelper.DEFAULT_CHARSET) /* Message body */ ); //Call send message to deliver message to one of brokers. producer.sendOneway(msg); &#125; //Shut down once the producer instance is not longer in use. producer.shutdown(); &#125;&#125; 关注我 总结本文是 RocketMQ 的三种发送消息的方式。 转发请注明地址：http://www.54tianzhisheng.cn/2018/02/07/rocketmq-example/","tags":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://yoursite.com/tags/RocketMQ/"}]},{"title":"RocketMQ系列文章（二）：RocketMQ 安装及快速入门","date":"2018-02-05T16:00:00.000Z","path":"2018/02/06/RocketMQ-install/","text":"如果你对 RocketMQ 还没了解，建议先看下上一篇文章：RocketMQ 初探 安装条件 64位操作系统，建议使用 Linux / Unix / Mac; 64位JDK 1.8+; Maven 3.2.x 下载和构建从 https://www.apache.org/dyn/closer.cgi?path=rocketmq/4.2.0/rocketmq-all-4.2.0-source-release.zip 下载 4.2.0 的源码版本，执行以下命令来解压4.2.0源码版本并构建二进制文件。 12345unzip rocketmq-all-4.2.0-source-release.zipcd rocketmq-all-4.2.0/mvn -Prelease-all -DskipTests clean install -U 构建成功如下： 进入到目录 ： 1cd distribution/target/apache-rocketmq 启动 NameServer123nohup sh bin/mqnamesrv &amp;tail -f ~/logs/rocketmqlogs/namesrv.log 结果如下就代表启动成功了： 启动 Broker123nohup sh bin/mqbroker -n localhost:9876 &amp;tail -f ~/logs/rocketmqlogs/broker.log 结果如下就代表启动成功了：从日志中可以看到 broker 注册到了 nameserver 上了（localhost:9876） 发送和接收消息在发送/接收消息之前，我们需要告诉客户名称服务器的位置。RocketMQ 提供了多种方法来实现这一点。为了简单起见，我们使用环境变量NAMESRV_ADDR 发送消息123export NAMESRV_ADDR=localhost:9876sh bin/tools.sh org.apache.rocketmq.example.quickstart.Producer 接收消息1sh bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer 关闭服务器123sh bin/mqshutdown broker //停止 brokersh bin/mqshutdown namesrv //停止 nameserver 关闭成功后如下： 常用命令上面几个启动和关闭 name server 和 broker 的就不再说了， 查看集群情况 ./mqadmin clusterList -n 127.0.0.1:9876 查看 broker 状态 ./mqadmin brokerStatus -n 127.0.0.1:9876 -b 172.20.1.138:10911 (注意换成你的 broker 地址) 查看 topic 列表 ./mqadmin topicList -n 127.0.0.1:9876 查看 topic 状态 ./mqadmin topicStatus -n 127.0.0.1:9876 -t MyTopic (换成你想查询的 topic) 查看 topic 路由 ./mqadmin topicRoute -n 127.0.0.1:9876 -t MyTopic 关注我 总结本文是 RocketMQ 的安装及快速入门案例。 转发请注明地址：http://www.54tianzhisheng.cn/2018/02/06/RocketMQ-install/","tags":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://yoursite.com/tags/RocketMQ/"}]},{"title":"RocketMQ系列文章（一）：RocketMQ 初探","date":"2018-02-04T16:00:00.000Z","path":"2018/02/05/RocketMQ/","text":"介绍RocketMQ 是阿里开源的消息中间件，前不久捐献给了 Apache 。正如官网介绍如下：它是一个开源的分布式消息传递和流式数据平台。 特点如下： 产品发展历史大约经历了三个主要版本迭代 一、Metaq（Metamorphosis）1.x 由开源社区killme2008维护，开源社区非常活跃。 二、Metaq 2.x 于2012年10月份上线，在淘宝内部被广泛使用。 三、RocketMQ 3.x 基于公司内部开源共建原则，RocketMQ 项目只维护核心功能，且去除了所有其他运行时依赖，核心功能最简化。每个 BU 的个性化需求都在 RocketMQ 项目之上进行深度定制。RocketMQ 向其他 BU 提供的仅仅是 Jar 包，例如要定制一个 Broker，那么只需要依赖 rocketmq-broker 这个 jar 包即可，可通过 API 进行交互，如果定制 client，则依赖 rocketmq-client 这个 jar 包，对其提供的 api 进行再封装。 在 RocketMQ 项目基础上衍生的项目如下 com.taobao.metaq v3.0 = RocketMQ + 淘宝个性化需求 为淘宝应用提供消息服务 com.alipay.zpullmsg v1.0 =RocketMQ + 支付宝个性化需求 为支付宝应用提供消息服务 com.alibaba.commonmq v1.0 = Notify + RocketMQ + B2B个性化需求 为 B2B 应用提供消息服务 四、RocketMQ 3.x 目前它的最新版本是 4.2 版本。 概念专业术语Producer 消息生产者，负责产生消息，一般由业务系统负责产生消息。 Consumer 消息消费者，负责消费消息，一般是后台系统负责异步消费。 Push Consumer Consumer 的一种，应用通常向 Consumer 对象注册一个 Listener 接口，一旦收到消息，Consumer 对象立刻回调 Listener 接口方法。 Pull Consumer Consumer 的一种，应用通常主动调用 Consumer 的拉消息方法从 Broker 拉消息，主动权由应用控制。 Producer Group 一类 Producer 的集合名称，这类 Producer 通常发送一类消息，且发送逻辑一致。 Consumer Group 一类 Consumer 的集合名称，这类 Consumer 通常消费一类消息，且消费逻辑一致。 Broker 消息中转角色，负责存储消息，转发消息，一般也称为 Server。在 JMS 规范中称为 Provider。 架构 从这架构图中可以看到它主要由四部分组成：Producer（生产者）、NameServer、Broker、Consumer（消费者）。 Producer生产者支持分布式部署。分布式生产者通过多种负载均衡模式向 Broker 集群发送消息。发送过程支持快速失败并具有低延迟。 NameServer它提供轻量级服务发现和路由，每个 Name Server 记录完整的路由信息，提供相应的读写服务，支持快速存储扩展。主要包括两个功能： 代理管理， NameServer 接受来自 Broker 集群的注册，并提供检测代理是否存在的心跳机制。 路由管理，每个 NameServer 将保存有关代理群集的全部路由信息以及客户端查询的队列信息。 我们知道，RocketMQ客户端（生产者/消费者）将从NameServer查询队列路由信息，但客户端如何找到NameServer地址？ 将NameServer地址列表提供给客户端有四种方法： 编程方式，就像producer.setNamesrvAddr(&quot;ip:port&quot;)。 Java选项，使用rocketmq.namesrv.addr。 环境变量，使用NAMESRV_ADDR。 HTTP 端点。 BrokerBroker 通过提供轻量级的 Topic 和 Queue 机制来照顾消息存储。它们支持 Push 和 Pull 模式，包含容错机制（2个拷贝或者3个拷贝），并且提供了强大的峰值填充和以原始时间顺序累计数千亿条消息的能力。此外，broker 还提供灾难恢复，丰富的指标统计数据和警报机制，而传统的消息传递系统都缺乏这些机制。 如上图：Broker 服务器重要的子模块： 远程处理模块是 broker 的入口，处理来自客户的请求。 Client manager，管理客户（生产者/消费者）并维护消费者的主题订阅。 Store Service，提供简单的 API 来存储或查询物理磁盘中的消息。 HA 服务，提供主代理和从代理之间的数据同步功能。 索引服务，通过指定键为消息建立索引，并提供快速的消息查询。 Consumer消费者也支持 Push 和 Pull 模型中的分布式部署。它还支持群集消费和消息广播。它提供了实时的消息订阅机制，可以满足大多数消费者的需求。 关注我 总结本文是对 RocketMQ 的简单点了解，参考了官网介绍。 转载请注明地址：http://www.54tianzhisheng.cn/2018/02/05/RocketMQ/","tags":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://yoursite.com/tags/RocketMQ/"}]},{"title":"Spring Boot系列文章（五）：SpringBoot RabbitMQ 整合进阶版","date":"2018-01-27T16:00:00.000Z","path":"2018/01/28/RabbitMQ/","text":"消息中间件RabbitMQ 是消息中间件的一种, 消息中间件即分布式系统中完成消息的发送和接收的基础软件. 这些软件有很多, 包括 ActiveMQ ( apache 公司的), RocketMQ (阿里巴巴公司的, 现已经转让给 apache), 还有性能极高的 Kafka。 消息中间件的工作过程可以用生产者消费者模型来表示. 即生产者不断的向消息队列发送信息, 而消费者从消息队列中消费信息. 具体过程如下: 从上图可看出, 对于消息队列来说, 生产者,消息队列,消费者 是最重要的三个概念。生产者发消息到消息队列中去, 消费者监听指定的消息队列, 并且当消息队列收到消息之后, 接收消息队列传来的消息, 并且给予相应的处理. 消息队列常用于分布式系统之间互相信息的传递. RabbitMQ 工作原理对于 RabbitMQ 来说, 除了这三个基本模块以外, 还添加了一个模块, 即交换机(Exchange). 它使得生产者和消息队列之间产生了隔离, 生产者将消息发送给交换机,而交换机则根据调度策略把相应的消息转发给对应的消息队列. 那么 RabitMQ 的工作流程如下所示: 说一下交换机: 交换机的主要作用是接收相应的消息并且绑定到指定的队列. 交换机有四种类型, 分别为Direct, topic, headers, Fanout. Direct 是 RabbitMQ 默认的交换机模式,也是最简单的模式.即创建消息队列的时候,指定一个 BindingKey. 当发送者发送消息的时候, 指定对应的 Key. 当 Key 和消息队列的 BindingKey 一致的时候,消息将会被发送到该消息队列中. topic 转发信息主要是依据通配符, 队列和交换机的绑定主要是依据一种模式(通配符+字符串), 而当发送消息的时候, 只有指定的 Key 和该模式相匹配的时候, 消息才会被发送到该消息队列中. headers 也是根据一个规则进行匹配, 在消息队列和交换机绑定的时候会指定一组键值对规则, 而发送消息的时候也会指定一组键值对规则, 当两组键值对规则相匹配的时候, 消息会被发送到匹配的消息队列中. Fanout 是路由广播的形式, 将会把消息发给绑定它的全部队列, 即便设置了 key, 也会被忽略. 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/01/28/RabbitMQ/ SpringBoot 整合 RabbitMQ（Topic 转发模式）在上一篇文章中，我们也将 SpringBoot 和 RabbitMQ 整合过，不过那是使用 Direct 模式，文章地址是：SpringBoot RabbitMQ 整合使用 相关文章1、SpringBoot Kafka 整合使用 2、SpringBoot RabbitMQ 整合使用 3、SpringBoot ActiveMQ 整合使用 4、Kafka 安装及快速入门 整合接下来，我要带大家继续整合（Topic 转发模式）： 1、配置文件和 pom.xml 这些还都是一样的，我们不用再修改 2、启动类中创建 Queue 和 Exchange，并把 Queue 按照相应的规则绑定到交换机Queue 上。代码如下图： 1234567891011121314@Beanpublic Queue queue() &#123; return new Queue(\"rpc-queue-zhisheng\");&#125;@Beanpublic TopicExchange exchange() &#123; return new TopicExchange(\"rpc-exchange-zhisheng\");&#125;@Beanpublic Binding binding(Queue queue, TopicExchange exchange) &#123; return BindingBuilder.bind(queue).to(exchange).with(\"rpc-zhisheng\");&#125; 这里创建一个 Queue 和 Exchange ，然后绑定。 注意：上面代码中的 with(“rpc-zhisheng”) 这个 “zhisheng” 是 routingkey，RabbitMQ 将会根据这个参数去寻找有没有匹配此规则的队列，如果有，则会把消息发送给它，如果不止有一个，则会把消息分发给所有匹配的队列。 3、消息发送类 1234567891011121314151617181920212223package com.zhisheng.rabbitmq.rpc.client;import org.springframework.amqp.core.TopicExchange;import org.springframework.amqp.rabbit.core.RabbitTemplate;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;/** * Created by zhisheng_tian on 2018/1/25 */@Componentpublic class RabbitMQClient &#123; @Autowired private RabbitTemplate rabbitTemplate; @Autowired private TopicExchange exchange; public void send(String message) &#123; rabbitTemplate.convertAndSend(exchange.getName(), \"rpc-zhisheng\", message); &#125;&#125; 这里是发送消息的代码，“rpc-zhisheng” 就是上面我们设置的 routingkey。 4、消息接收端 12345678910111213141516package com.zhisheng.rabbitmq.rpc.server;import org.springframework.amqp.rabbit.annotation.RabbitListener;import org.springframework.stereotype.Component;/** * Created by zhisheng_tian on 2018/1/25 */@Componentpublic class RabbitMQServer &#123; @RabbitListener(queues = \"rpc-queue-zhisheng\") public void receive(String message) &#123; System.out.println(\"--------receive ------- \" + message); &#125;&#125; 5、启动类中注入 发送消息类，然后调用 send 方法 12345678910111213@Autowiredprivate RabbitMQClient client;@PostConstructpublic void init() &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); for (int i = 0; i &lt; 1000; i++) &#123; client.send(\" zhisheng, --------- send \" + i); &#125; stopWatch.stop(); System.out.println(\"总共耗时：\" + stopWatch.getTotalTimeMillis());&#125; 运行此 SpringBoot 项目，则可以发现结果如下： 这里测试的是匹配一个消息队列的情况，感兴趣的可以测试下匹配多个消息队列的。 SpringBoot 整合 RabbitMQ( Fanout Exchange 形式)Fanout Exchange 形式又叫广播形式。 任何发送到 Fanout Exchange 的消息都会被转发到与该 Exchange 绑定(Binding)的所有 Queue 上。 这种模式需要提前将 Exchange 与 Queue 进行绑定，一个 Exchange 可以绑定多个 Queue，一个 Queue 可以同多个 Exchange 进行绑定 这种模式不需要 RoutingKey 如果接受到消息的 Exchange 没有与任何 Queue 绑定，则消息会被抛弃。 1、消息发送类 12345678910111213141516171819package com.zhisheng.rabbitmq.rpc.client;import org.springframework.amqp.rabbit.core.RabbitTemplate;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;/** * Created by zhisheng_tian on 2018/1/25 */@Componentpublic class RabbitMQClient &#123; @Autowired private RabbitTemplate rabbitTemplate; public void send2(String message) &#123; rabbitTemplate.convertAndSend(\"fanout-exchange\", \"\", message); &#125;&#125; 这里可以不设置 routingkey 了。 2、启动类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package com.zhisheng.rabbitmq.rpc;import com.zhisheng.rabbitmq.rpc.client.RabbitMQClient;import org.springframework.amqp.core.Binding;import org.springframework.amqp.core.BindingBuilder;import org.springframework.amqp.core.FanoutExchange;import org.springframework.amqp.core.Queue;import org.springframework.amqp.support.converter.Jackson2JsonMessageConverter;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Qualifier;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.annotation.Bean;import javax.annotation.PostConstruct;@SpringBootApplicationpublic class RabbitmqRpcApplication &#123; @Autowired private RabbitMQClient client; @PostConstruct public void init() &#123; client.send2(\"zhisheng ++++++++++ send2 \"); &#125; public static void main(String[] args) &#123; SpringApplication.run(RabbitmqRpcApplication.class, args); &#125; @Bean(name = \"queue\") public Queue queue() &#123; return new Queue(\"rpc.queue\"); &#125; @Bean(name = \"queue2\") public Queue queue2() &#123; return new Queue(\"rpc.queue2\"); &#125; @Bean(name = \"queue3\") public Queue queue3() &#123; return new Queue(\"rpc.queue3\"); &#125; @Bean public FanoutExchange exchange() &#123; return new FanoutExchange(\"fanout-exchange\"); &#125; @Bean public Binding binding(@Qualifier(\"queue\") Queue queue, FanoutExchange exchange) &#123; return BindingBuilder.bind(queue).to(exchange); &#125; @Bean public Binding binding2(@Qualifier(\"queue2\") Queue queue, FanoutExchange exchange) &#123; return BindingBuilder.bind(queue).to(exchange); &#125; @Bean public Binding binding3(@Qualifier(\"queue3\") Queue queue, FanoutExchange exchange) &#123; return BindingBuilder.bind(queue).to(exchange); &#125; @Bean public Jackson2JsonMessageConverter messageConverter() &#123; return new Jackson2JsonMessageConverter(); &#125;&#125; 在启动类中我创建三个 Queue： rpc.queue, rpc.queue2 , rpc.queue3 也创建一个 FanoutExchange，并把这三个 Queue 绑定在同一个交换机 fanout-exchange 上面 注意：这个 fanout-exchange 交换机不知为啥，我自己在应用程序里创建，运行程序会出错，下面讲讲我是怎么解决的。 我是从 RabbitMQ 管理界面直接添加个 exchange 的。 3、消息接收类 123456789101112131415161718192021222324252627package com.zhisheng.rabbitmq.rpc.server;import org.springframework.amqp.rabbit.annotation.RabbitListener;import org.springframework.stereotype.Component;/** * Created by zhisheng_tian on 2018/1/25 */@Componentpublic class RabbitMQServer &#123; @RabbitListener(queues = \"rpc.queue\") public void receive(String message) &#123; System.out.println(\"--------receive ------- \" + message); &#125; @RabbitListener(queues = \"rpc.queue2\") public void receive2(String message) &#123; System.out.println(\"--------receive2 ------- \" + message); &#125; @RabbitListener(queues = \"rpc.queue3\") public void receive3(String message) &#123; System.out.println(\"--------receive3 ------- \" + message); &#125;&#125; 监听每个 Queue，并有一个方法输出对应接收到的消息。 4、运行项目 结果如上，每个队列都打印出自己收到的结果，同时我们看看这三个 Queue 是不是绑定到 Exchange 上呢？ 可以看到三个 Queue 都绑定在 Exchange 上了。 总结RabbitMQ 与 SpringBoot 整合就到这里为止了，后面如果有时间会深度研究 RabbitMQ 的。 还请继续关注我的博客：http://www.54tianzhisheng.cn/","tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://yoursite.com/tags/RabbitMQ/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}]},{"title":"Spring Boot系列文章（四）：SpringBoot ActiveMQ 整合使用","date":"2018-01-26T16:00:00.000Z","path":"2018/01/27/SpringBoot-ActiveMQ/","text":"介绍 ActiveMQ它是 Apache 出品，最流行的，能力强劲的开源消息总线。ActiveMQ 是一个完全支持 JMS1.1 和 J2EE 1.4 规范的 JMS Provider 实现，尽管 JMS 规范出台已经是很久的事情了，但是 JMS 在当今的J2EE应用中间仍然扮演着特殊的地位。—— 摘自百度百科，偷了个懒。 相关文章1、SpringBoot Kafka 整合使用 2、SpringBoot RabbitMQ 整合使用 安装 ActiveMQ同之前一样，直接在 Docker 里面玩吧。命令也是一行解决： 1docker run -d -p 8161:8161 -p 61616:61616 -e ACTIVEMQ_ADMIN_LOGIN=admin -e ACTIVEMQ_ADMIN_PASSWORD=admin --name activemq webcenter/activemq 简单解释下： 8186: 表示 ActiveMQ 控制台端口号，它和 RabbitMQ 一样都是有控制台的，可以登陆控制台进行操作的 61616 ： 表示 ActiveMQ 所监听的 TCP 端口号，应用程序可通过该端口号与 ActiveMQ 建立 TCP 连接 CTIVEMQ_ADMIN_LOGIN ：登陆控制台的用户名 ACTIVEMQ_ADMIN_PASSWORD ：登陆控制台的密码 执行后，可在浏览器输入 http://localhost:8161/ 查看控制台， 解释下上面图片中控制台这些按钮的基本信息： Home：查看 ActiveMQ 的常见信息 Queues：查看 ActiveMQ 的队列信息 Topics：查看 ActiveMQ 的主题信息 Subscribers：查看主题的订阅者信息 Connections：查看 ActiveMQ 客户端的连接信息 Network：查看 ActiveMQ 的网络信息 Scheduled：查看 ActiveMQ 的定时任务 Send：用于通过表单方式向队列或者主题发送具体的消息 整合IDEA 创建 SpringBoot 项目，因为 SpringBoot 已经内置了对 ActiveMQ 的支持，所以直接引入依赖 spring-boot-starter-activemq 就行。整体项目结构如下： 1、pom.xml 文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zhisheng&lt;/groupId&gt; &lt;artifactId&gt;activemq&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;activemq&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot ActiveMQ&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-activemq&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 2、配置文件 application.properties 123spring.activemq.broker-url=tcp://localhost:61616spring.activemq.user=adminspring.activemq.password=admin 3、发送消息类 12345678910111213141516package com.zhisheng.activemq.client;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.jms.core.JmsTemplate;import org.springframework.stereotype.Component;@Componentpublic class ActiveMQClient &#123; @Autowired private JmsTemplate jmsTemplate; public void send(String message) &#123; jmsTemplate.convertAndSend(\"zhisheng\", message); &#125;&#125; 同样，和 RabbitMQ 类似，不多说了。 4、消息接收类 123456789101112package com.zhisheng.activemq.server;import org.springframework.jms.annotation.JmsListener;import org.springframework.stereotype.Component;@Componentpublic class ActiveMQServer &#123; @JmsListener(destination = \"zhisheng\") public void receive(String message) &#123; System.out.println(\"收到的 message 是：\" + message); &#125;&#125; 5、注意 这个队列是不需要我们提前定义好的，它和 RabbitMQ 不一样，它会在我们需要的时候动态的创建。 运行12345678910111213141516171819202122232425262728293031package com.zhisheng.activemq;import com.zhisheng.activemq.client.ActiveMQClient;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.util.StopWatch;import javax.annotation.PostConstruct;@SpringBootApplicationpublic class ActivemqApplication &#123; @Autowired ActiveMQClient client; @PostConstruct public void init() &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); for (int i = 0; i &lt; 10000; i++) &#123; client.send(\"发送消息----zhisheng-----\"); &#125; stopWatch.stop(); System.out.println(\"发送消息耗时: \" + stopWatch.getTotalTimeMillis()); &#125; public static void main(String[] args) &#123; SpringApplication.run(ActivemqApplication.class, args); &#125;&#125; 发送一万条消息运行后需要的时间挺久的：73180 ms 比 RabbitMQ 发送 10000 条消息耗时 215 ms 不知道高出多少倍了，可见其性能并不高的。 关注我 最后转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/01/27/SpringBoot-ActiveMQ/","tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"},{"name":"ActiveMQ","slug":"ActiveMQ","permalink":"http://yoursite.com/tags/ActiveMQ/"}]},{"title":"Spring Boot系列文章（三）：SpringBoot  RabbitMQ 整合使用","date":"2018-01-25T16:00:00.000Z","path":"2018/01/26/SpringBoot-RabbitMQ/","text":"前提上次写了篇文章，《SpringBoot Kafka 整合使用》，阅读量还挺高的，于是想想还是把其他几种 MQ 也和 SpringBoot 整合使用下。 下面是四种比较流行的 MQ ： 后面都写写和 SpringBoot 整合的文章。 安装 RabbitMQ由于换 Mac 了，所以一些环境就直接在 Mac 搞，但是像安装 RabbitMQ 这些又会把自己电脑系统给搞的太乱，所以能在 Docker 里面安装就安装在 Docker，这次 RabbitMQ 我也直接在 Docker 里安装。 启动 Docker for Mac，如果没安装过的请看我上一篇文章：http://www.54tianzhisheng.cn/2018/01/25/Docker-install/ 当然你也可以在自己的 Linux 服务器或者虚拟机里启动安装 RabbitMQ 。 Docker 安装的话很简单，因为 RabbitMQ 官方已经提供了自己的 Docker 容器，只需要一行命令： 1docker run -d -p 15672:15672 -p 5672:5672 -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=admin --name rabbitmq rabbitmq:3-management 该镜像拥有一个基于 web 的控制台和 Http API。Http API 可以在地址看到如何使用：http://localhost:15672/api/ 讲解下上面命令行： 15672 ：表示 RabbitMQ 控制台端口号，可以在浏览器中通过控制台来执行 RabbitMQ 的相关操作。 5672 : 表示 RabbitMQ 所监听的 TCP 端口号，应用程序可通过该端口与 RabbitMQ 建立 TCP 连接，并完成后续的异步消息通信 RABBITMQ_DEFAULT_USER：用于设置登陆控制台的用户名，这里我设置 admin RABBITMQ_DEFAULT_PASS：用于设置登陆控制台的密码，这里我设置 admin 容器启动成功后，可以在浏览器输入地址：http://localhost:15672/ 访问控制台 登陆后： 简单描述下上图中中控制台的列表的作用： Overview ：用于查看 RabbitMQ 的一些基本信息（消息队列、消息发送速率、节点、端口和上下文信息等） Connections：用于查看 RabbitMQ 客户端的连接信息 Channels：用户查看 RabbitMQ 的通道信息 Exchange：用于查看 RabbitMQ 交换机 Queues：用于查看 RabbitMQ 的队列 Admin：用于管理用户，可增加用户 创建项目在 IDEA 中创建一个 SpringBoot 项目结构： SpringBoot 框架中已经内置了对 RabbitMQ 的支持，如果你看过官方文档的话，就可以看到的，我们需要把依赖 spring-boot-starter-amqp 引入就行。 1、 pom.xml 引入依赖后如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zhisheng&lt;/groupId&gt; &lt;artifactId&gt;rabbitmq&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;rabbitmq&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot RabbitMQ&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 2、application.properties 配置修改如下： 123spring.rabbitmq.addresses=localhost:5672spring.rabbitmq.username=adminspring.rabbitmq.password=admin 3、消息发送类 RabbitMQClient.java 12345678910111213141516171819package com.zhisheng.rabbitmq.client;import org.springframework.amqp.rabbit.core.RabbitTemplate;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;/** * Created by zhisheng_tian on 2018/1/23 */@Componentpublic class RabbitMQClient &#123; @Autowired private RabbitTemplate rabbitTemplate; public void send(String message) &#123; rabbitTemplate.convertAndSend(\"zhisheng\", message); &#125;&#125; 就这样，发送消息代码就实现了。 这里关键的代码为 rabbitTemplate.convertAndSend() 方法，zhisheng 这个是路由规则（routingKey），它的值表明将消息发送到指定的队列 zhisheng 中去，这里跟了下源码，发现 convertAndSend() 方法最后调用的方法其实是一个 doSend() 方法。 4、消息接收类 12345678910111213141516package com.zhisheng.rabbitmq.server;import org.springframework.amqp.rabbit.annotation.RabbitListener;import org.springframework.stereotype.Component;/** * Created by zhisheng_tian on 2018/1/23 */@Componentpublic class RabbitMQServer &#123; @RabbitListener(queues = \"zhisheng\") public void receive(String message) &#123; System.out.println(\"收到的 message 是：\" + message); &#125;&#125; 你看，这里就有个 RabbitListener 一直在监听着队列 zhisheng 。 当然这个队列是必须要我们自己在应用程序中创建好，它不会像我之前写的文章 《SpringBoot Kafka 整合使用》 中的 Kafka 一样，Kafka 它会在用到队列的时候动态的创建，不需要我们提前创建好。 那么在 RabbitMQ 中该如何创建队列呢？ 如上图所示：这样我们就创建好了一个 zhisheng 的队列，当程序开始运行时，消息接收类会持续监听队列 zhisheng 中即将到来的消息。 5、运行项目 需要在启动类中注入发送消息的类，并且提供 init 方法，在 init 方法中调用发送消息类的 send() 方法 1234@PostConstructpublic void init() &#123; rabbitMQClient.send(\"发送消息----zhisheng-----\");&#125; 需要注意的是：init() 方法带有 @PostConstruct 注解，被 @PostConstruct 修饰的方法会在构造函数之后执行。 启动项目就可以发现控制台已经接收到消息了。 6、单线程测试性能 看到上面图片中注释掉的代码没？那就是用来测试消息发送的性能的，我发送 10000 条消息看看总共耗时多少。 10000 条消息发送耗时：215ms。 这是在单线程下，下次可以和其他的 MQ 测试对比下，并且也可以在多线程的环境下测试性能。 同时从控制台可以看到发送的速率： 7、多线程测试性能 开了10 个线程，每个线程发送 10000 条消息。 init 方法代码如下： 1234567891011121314151617181920212223242526272829303132333435363738@PostConstruct public void init() &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); int threads = 10; ExecutorService executorService = Executors.newFixedThreadPool(threads); final CountDownLatch start = new CountDownLatch(1); final CountDownLatch end = new CountDownLatch(threads); for (int i = 0; i &lt; threads; i++) &#123; executorService.execute(() -&gt; &#123; try &#123; start.await(); for (int i1 = 0; i1 &lt; 10000; i1++) &#123; rabbitMQClient.send(\"发送消息----zhisheng-----\"); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; end.countDown(); &#125; &#125;); &#125; start.countDown(); try &#123; end.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; executorService.shutdown(); &#125; stopWatch.stop(); System.out.println(\"发送消息耗时：\" + stopWatch.getTotalTimeMillis()); &#125; 耗时：4063ms 控制台显示如下图： 8、注意 这里测试发送的消息直接是 String 类型的，你也可以测试下 Bean 类，这需要注意需要序列化。 关注我 最后转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/01/26/SpringBoot-RabbitMQ/","tags":[{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://yoursite.com/tags/RabbitMQ/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}]},{"title":"Docker系列文章（二）：Mac 安装 Docker 及常用命令","date":"2018-01-24T16:00:00.000Z","path":"2018/01/25/Docker-install/","text":"背景微服务 + 容器，完美的一对！必须得好好学习学习。 安装步骤Mac 下 Docker 的安装真心建议跟着官方的文档走一遍，官网已经讲的很详细了。 https://docs.docker.com/docker-for-mac/install/#what-to-know-before-you-install 使用 Docker for Machttps://docs.docker.com/docker-for-mac/#check-versions-of-docker-engine-compose-and-machine 配置 Docker 加速器Docker 加速器是什么，我需要使用吗？ 使用 Docker 的时候，需要经常从官方获取镜像，但是由于显而易见的网络原因，拉取镜像的过程非常耗时，严重影响使用 Docker 的体验。因此 DaoCloud 推出了加速器工具解决这个难题，通过智能路由和缓存机制，极大提升了国内网络访问 Docker Hub 的速度，目前已经拥有了广泛的用户群体，并得到了 Docker 官方的大力推荐。如果您是在国内的网络环境使用 Docker，那么 Docker 加速器一定能帮助到您。 注册 daocloud，然后在 mac 标签页复制加速器 url。 入门案例跟着下面的文章进行敲一遍，熟悉下 Docker 整个的使用。 https://www.jianshu.com/p/cf6e7248b6c7 Docker 常用命令下面列出些自己常用的命令，目的就是记录下来，以后忘记了，再拿来跟着敲就行！ 12345678910111213141516171819202122232425262728293031323334353637383940414243docker run -i -t &lt;image_name/continar_id&gt; /bin/bash 启动容器并启动bash（交互方式）docker run -d -it image_name 启动容器以后台方式运行(更通用的方式）docker ps 列出当前所有正在运行的containerdocker ps -a 列出所有的containerdocker ps -l 列出最近一次启动的containerdocker images 列出本地所有的镜像docker rmi imagesID 删除指定的镜像iddocker rm CONTAINER ID 删除指定的CONTAINER iddocker diff 镜像名 查看容器的修改部分docker kill CONTAINER ID 杀掉正在运行的容器docker logs 容器ID/name 可以查看到容器主程序的输出docker pull image_name 下载imagedocker push image_name 发布docker镜像docker version 查看docker版本docker info 查看docker系统的信息docker inspect 容器的id 可以查看更详细的关于某一个容器的信息docker run -d image-name 后台运行镜像docker search 镜像名 查找公共的可用镜像docker stop 容器名/容器 ID 终止运行的容器docker restart 容器名/容器 ID 重启容器docker commit 提交，创建个新镜像docker build [OPTIONS] PATH | URL | - 利用 Dockerfile 创建新镜像 关注我 最后转载请注明地址：http://www.54tianzhisheng.cn/2018/01/25/Docker-install/","tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"MacBook Pro 初体验","date":"2018-01-23T16:00:00.000Z","path":"2018/01/24/mac/","text":"背景 在 Mac 到手之前就在各种群里看到人说 Mac 多好用，也有很多人鼓吹过 Mac 的好处，最后也坚定我的年前目标了 —— 就是买台 Mac，之前请原谅我这个穷鬼，买不起，现在买了 Mac 后更加得体谅我这个穷鬼了，毕竟在上海这个城市，靠着实习工资买这种奢侈品，不容易啊😄 。废话不多说，如果愿意支持我的，请在文章底部扫描二维码，在此先谢谢了。 如何挑选？MacBook 主要分两系列：MacBook Air 和 MacBook Pro。 Air 的话个人感觉配置不高，如果是开发还是建议买 Pro 系列的。如今买的话，可能还会分 2015 款、2016 款、2017 款。每款中又分 内存大小（8/16g）、硬盘大小（128/256/512g）、CPU、处理器（i5/i7）、是否有TouchBar、显卡等。不同配置对应电脑的型号也是不一样的。下面直接上一张在我的特殊渠道里的报价表截图吧。（想了解的可以找我） 光这型号，不懂的人还真不会挑。 不得不说，苹果电脑真尼玛难挑啊，如果你是土豪，那不用挑了，直接上最高价钱的吧。 然后可以从配置中发现 2016 款和 2017 款变化真心不大，在同等配置下，2017 款几乎比 2016 款价格高个 3000 来块。 然后就是 512 G 硬盘比 256 G 也几乎贵个 2000 多。 16G 那是必须的上啊，标配了，8G 就不说了，太小了。 含 TouchBar，虽然确实用处不大，不过调音两还是不错的。高配都有 TouchBar 的。 出于 qiong ，我买了 2016 款，配置是： 不过现在 2016 款好像停产了。 到手2018.01.11 下午六点快递送到的，很开心。晚上拿回家拆箱，第一件事情就是检查序列号啊，上面的图片打码掉的就是序列号，这个序列号在电脑机身、系统、外牛皮癣盒都有的，可以在官网查询这个序列号，获得电脑的激活日期和剩余保修时间的。再就是查询电脑的电池循环次数了，我的是一次，一般好像是几次之内都是符合的。证明之前没被别人用过，这个数字我也忘记了。 熟悉系统Mac 系统是类 Unix 系统，其实我觉得到和 Ubuntu 系统挺像的，既有图形化界面，也可以命令行操作。熟悉过 Linux 的应该上手很快的。 安装软件可以在 Appstore 里面下载，也可以在一些软件的官网直接下载 mac 版的。安装也挺简单的。如果你不知道有什么软件可以安装，那么我这里给你份 Mac 软件参考列表：https://github.com/jaywcjlove/awesome-mac/blob/master/README-zh.md 当然了，上面的不一定全，具体用到其他的还是的自己去找对应的软件。 还有就是好多软件是收费的，在 Mac 上如果要下载的话，还的费点心思去破解，比如 Office、IDEA、Adobe 系列等，当然也不是鼓吹大家去破解，我们自己用用就行，虽说现在没钱支持，但是有钱的话还是支持下。我一个写博客的知道写博客的不容易，那写软件的更不容易了，能支持一两块也挺好的。 然后就是美化下我们的一些软件，比如我们的终端之类的、尽量使用 Homebrew 安装软件。当然这篇文章不会写这些的，改天专门写篇文章写这个话题。 还有就是在 Mac 上从新打造一个适合自己的新写作环境（软件、Hexo 写博客环境）。 最后体验了 Mac 也有一个多礼拜了，整体效果还是不错的，毕竟花了巨资呢，也算是完成了自己年前的小目标。先 bb 到这里吧。有时间再写点关于 Mac 上的东西，这次写的比较简单，这篇文章也是在 Mac 上写的第一篇文章。","tags":[{"name":"Mac","slug":"Mac","permalink":"http://yoursite.com/tags/Mac/"}]},{"title":"Spring Boot系列文章（二）：SpringBoot Admin 使用指南","date":"2018-01-16T16:00:00.000Z","path":"2018/01/17/SpringBoot-Admin/","text":"什么是 SpringBoot Admin？Spring Boot Admin 是一个管理和监控你的 Spring Boot 应用程序的应用程序。 这些应用程序通过 Spring Boot Admin Client（通过 HTTP）注册或者使用 Spring Cloud（例如 Eureka）发现。 UI只是 Spring Boot Actuator 端点上的一个 AngularJs 应用程序。 快速开始首先在 IDEA 创建一个 SpringBoot 项目，把它当作 server 端，工程如下： 然后在 pom.xml 中引入依赖： 12345678910&lt;dependency&gt; &lt;groupId&gt;de.codecentric&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-admin-server&lt;/artifactId&gt; &lt;version&gt;1.5.6&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;de.codecentric&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-admin-server-ui&lt;/artifactId&gt; &lt;version&gt;1.5.6&lt;/version&gt;&lt;/dependency&gt; 继续在启动类 SpringbootAdminApplication.java 中引入注解 @EnableAdminServer ，然后运行项目： 访问 http://localhost:8084/ 即可： 此时会发现没有任何应用程序的信息。 接下来我们新建一个 SpringBoot 项目，把它当作客户端程序，工程如下： 在 pom.xml 中添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;de.codecentric&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-admin-starter-client&lt;/artifactId&gt; &lt;version&gt;1.5.6&lt;/version&gt;&lt;/dependency&gt; 然后在 application.yml 中设置： spring.boot.admin.url=http:localhost:8094 用于将当前应用注册到 Spring Boot Admin。 还可以设置，spring.boot.admin.client.name: （应用程序的名字）不设置的话会有默认的名字 此时把两个项目运行起来： 点击图中的 detail 按钮：可以看到应用程序的健康值、内存、JVM、GC 等信息。 metrics 信息： 环境 信息： log 信息： JMX 信息： 线程 信息： Trace 追踪信息： 还可以下载 Heapdump 文件。 刚才首页的应用列表后面有个红色的 ×，我们可以将注册上去的应用移除，但是只要你不把程序停掉，它立马又会注册上去。 还有就是应用列表的 version 和 info 上面的图中为空，下面看看怎么把它变出来： 123info.groupId: @project.groupId@info.artifactId: @project.artifactId@info.version: @project.version@ 重新运行客户端程序，刷新页面可以发现： 还可以查询应用程序的事件变化： 客户端应用程序JMX bean管理要在管理界面中与JMX-beans进行交互，您必须在客户端应用程序中包含 Jolokia, pom.xml 加入依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.jolokia&lt;/groupId&gt; &lt;artifactId&gt;jolokia-core&lt;/artifactId&gt;&lt;/dependency&gt; 重启客户端程序后，就可以在这里与 JMX 做交互了： 还有很多 SpringBoot Admin 客户端配置选项： http://codecentric.github.io/spring-boot-admin/1.5.6/#spring-boot-admin-client 服务端程序也有些 SpringBoot Admin 服务端程序配置选项： http://codecentric.github.io/spring-boot-admin/1.5.6/#spring-boot-admin-server 官方文档里面还有些关于服务下线消息通知的知识，想了解的可以查看： http://codecentric.github.io/spring-boot-admin/1.5.6/#_notifications 关注我 参考文章http://codecentric.github.io/spring-boot-admin/1.5.6/ 最后转载请注明文章原始地址为：http://www.54tianzhisheng.cn/2018/01/17/SpringBoot-Admin/","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}]},{"title":"Lombok 看这篇就够了","date":"2018-01-08T16:00:00.000Z","path":"2018/01/09/lombok/","text":"前提自从进公司实习后，项目代码中能用 Lombok 的都用了，毕竟这么好的轮子要充分利用好。也可以减少一些 get/set/toString 方法的编写，虽说 IDEA 的插件可以自动生成 get/set/toString 方法，但是使用 Lombok 可以让代码更简洁。下面看看如何在 IDEA 中如何安装 Lombok： 安装打开 IDEA 的 Settings 面板，并选择 Plugins 选项，然后点击 “Browse repositories” 在输入框输入”lombok”，得到搜索结果，点击安装，然后安装提示重启 IDEA，安装成功; 引入依赖在自己的项目里添加 lombok 的编译支持，在 pom 文件里面添加 dependency 123456&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.18&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 怎么使用？在实体类上引入相关的注解就行： 有哪些注解？ @Data @Setter @Getter @Slf4j @AllArgsConstructor @NoArgsConstructor @EqualsAndHashCode @NonNull @Cleanup @ToString @RequiredArgsConstructor @Value @SneakyThrows @Synchronized 注解详解@Data 注解在 类 上；提供类所有属性的 get 和 set 方法，此外还提供了equals、canEqual、hashCode、toString 方法。 @Setter 注解在 属性 上；为单个属性提供 set 方法; 注解在 类 上，为该类所有的属性提供 set 方法， 都提供默认构造方法。 @Getter 注解在 属性 上；为单个属性提供 get 方法; 注解在 类 上，为该类所有的属性提供 get 方法，都提供默认构造方法。 @Slf4j 注解在 类 上；为类提供一个 属性名为 log 的日志对象，提供默认构造方法。 @AllArgsConstructor 注解在 类 上；为类提供一个全参的构造方法，加了这个注解后，类中不提供默认构造方法了。 @NoArgsConstructor 注解在 类 上；为类提供一个无参的构造方法。 @EqualsAndHashCode 注解在 类 上, 可以生成 equals、canEqual、hashCode 方法。 @NonNull 注解在 属性 上，会自动产生一个关于此参数的非空检查，如果参数为空，则抛出一个空指针异常，也会有一个默认的无参构造方法。 @Cleanup 这个注解用在 变量 前面，可以保证此变量代表的资源会被自动关闭，默认是调用资源的 close() 方法，如果该资源有其它关闭方法，可使用 @Cleanup(“methodName”) 来指定要调用的方法，也会生成默认的构造方法 @ToString 这个注解用在 类 上，可以生成所有参数的 toString 方法，还会生成默认的构造方法。 @RequiredArgsConstructor 这个注解用在 类 上，使用类中所有带有 @NonNull 注解的或者带有 final 修饰的成员变量生成对应的构造方法。 @Value 这个注解用在 类 上，会生成含所有参数的构造方法，get 方法，此外还提供了equals、hashCode、toString 方法。 @SneakyThrows 这个注解用在 方法 上，可以将方法中的代码用 try-catch 语句包裹起来，捕获异常并在 catch 中用 Lombok.sneakyThrow(e) 把异常抛出，可以使用 @SneakyThrows(Exception.class) 的形式指定抛出哪种异常，也会生成默认的构造方法。 @Synchronized 这个注解用在 类方法 或者 实例方法 上，效果和 synchronized 关键字相同，区别在于锁对象不同，对于类方法和实例方法，synchronized 关键字的锁对象分别是类的 class 对象和 this 对象，而 @Synchronized 的锁对象分别是 私有静态 final 对象 lock 和 私有 final 对象 lock，当然，也可以自己指定锁对象，此外也提供默认的构造方法。 总结以上注解可根据需要一起搭配使用！ 虽说轮子好，但是我们不仅要知其然，也要知其所以然！ 关注我 最后转载请注明原创地址：http://www.54tianzhisheng.cn/2018/01/07/lombok/","tags":[{"name":"lombok","slug":"lombok","permalink":"http://yoursite.com/tags/lombok/"}]},{"title":"Spring Boot系列文章（一）：SpringBoot Kafka 整合使用","date":"2018-01-04T16:00:00.000Z","path":"2018/01/05/SpringBoot-Kafka/","text":"前提假设你了解过 SpringBoot 和 Kafka。1、SpringBoot 如果对 SpringBoot 不了解的话，建议去看看 DD 大佬 和 纯洁的微笑 的系列博客。 2、Kafka Kafka 的话可以看看我前两天写的博客 ： Kafka 安装及快速入门 学习的话自己开台虚拟机自己手动搭建环境吧，有条件的买服务器。 注意：一定要亲自自己安装实践，接下来我们将这两个进行整合。 创建项目项目整体架构： 使用 IDEA 创建 SpringBoot 项目，这个很简单了，这里不做过多的讲解。 1、pom 文件代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zhisheng&lt;/groupId&gt; &lt;artifactId&gt;kafka-learning&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;kafka-learning&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot + kafka&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt; &lt;version&gt;1.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 主要引入了 spring-kafka 、lombok 、 gson 依赖。 2、消息实体类 Message.java 如下： 123456789@Datapublic class Message &#123; private Long id; //id private String msg; //消息 private Date sendTime; //时间戳&#125; 3、消息发送类 KafkaSender.java 12345678910111213141516171819@Component@Slf4jpublic class KafkaSender &#123; @Autowired private KafkaTemplate&lt;String, String&gt; kafkaTemplate; private Gson gson = new GsonBuilder().create(); //发送消息方法 public void send() &#123; Message message = new Message(); message.setId(System.currentTimeMillis()); message.setMsg(UUID.randomUUID().toString()); message.setSendTime(new Date()); log.info(\"+++++++++++++++++++++ message = &#123;&#125;\", gson.toJson(message)); kafkaTemplate.send(\"zhisheng\", gson.toJson(message)); &#125;&#125; 就这样，发送消息代码就实现了。 这里关键的代码为 kafkaTemplate.send() 方法，zhisheng 是 Kafka 里的 topic ，这个 topic 在 Java 程序中是不需要提前在 Kafka 中设置的，因为它会在发送的时候自动创建你设置的 topic， gson.toJson(message) 是消息内容，这里暂时先说这么多了，不详解了，后面有机会继续把里面源码解读写篇博客出来（因为中途碰到坑，老子跟了几遍源码）。 4、消息接收类 KafkaReceiver.java 12345678910111213141516171819@Component@Slf4jpublic class KafkaReceiver &#123; @KafkaListener(topics = &#123;\"zhisheng\"&#125;) public void listen(ConsumerRecord&lt;?, ?&gt; record) &#123; Optional&lt;?&gt; kafkaMessage = Optional.ofNullable(record.value()); if (kafkaMessage.isPresent()) &#123; Object message = kafkaMessage.get(); log.info(\"----------------- record =\" + record); log.info(\"------------------ message =\" + message); &#125; &#125;&#125; 客户端 consumer 接收消息特别简单，直接用 @KafkaListener 注解即可，并在监听中设置监听的 topic ，topics 是一个数组所以是可以绑定多个主题的，上面的代码中修改为 @KafkaListener(topics = {&quot;zhisheng&quot;,&quot;tian&quot;}) 就可以同时监听两个 topic 的消息了。需要注意的是：这里的 topic 需要和消息发送类 KafkaSender.java 中设置的 topic 一致。 5、启动类 KafkaApplication.java 123456789101112131415161718192021@SpringBootApplicationpublic class KafkaApplication &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext context = SpringApplication.run(KafkaApplication.class, args); KafkaSender sender = context.getBean(KafkaSender.class); for (int i = 0; i &lt; 3; i++) &#123; //调用消息发送类中的消息发送方法 sender.send(); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 6、配置文件 application.properties 1234567891011121314151617181920212223242526#============== kafka ===================# 指定kafka 代理地址，可以多个spring.kafka.bootstrap-servers=192.168.153.135:9092#=============== provider =======================spring.kafka.producer.retries=0# 每次批量发送消息的数量spring.kafka.producer.batch-size=16384spring.kafka.producer.buffer-memory=33554432# 指定消息key和消息体的编解码方式spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializerspring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer#=============== consumer =======================# 指定默认消费者group idspring.kafka.consumer.group-id=test-consumer-groupspring.kafka.consumer.auto-offset-reset=earliestspring.kafka.consumer.enable-auto-commit=truespring.kafka.consumer.auto-commit-interval=100# 指定消息key和消息体的编解码方式spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializerspring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer spring.kafka.bootstrap-servers 后面设置你安装的 Kafka 的机器 IP 地址和端口号 9092。 如果你只是简单整合下，其他的几个默认就好了。 Kafka 设置在你安装的 Kafka 目录文件下： 启动 zk使用安装包中的脚本启动单节点 Zookeeper 实例： 1bin/zookeeper-server-start.sh -daemon config/zookeeper.properties 启动 Kafka 服务使用 kafka-server-start.sh 启动 kafka 服务： 1bin/kafka-server-start.sh config/server.properties 启动成功后！ 千万注意： 记得将你的虚拟机或者服务器关闭防火墙或者开启 Kafka 的端口 9092。 运行 出现这就代表整合成功了！ 我们看下 Kafka 中的 topic 列表就 1bin/kafka-topics.sh --list --zookeeper localhost:2181 就会发现刚才我们程序中的 zhisheng 已经自己创建了。 关注我 最后转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/01/05/SpringBoot-Kafka/","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://yoursite.com/tags/Kafka/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://yoursite.com/tags/SpringBoot/"}]},{"title":"为什么要重新运营以前的公众号呢？","date":"2018-01-03T16:00:00.000Z","path":"2018/01/04/weixin/","text":"前提老读者可能会发现现在我的公众号已经改名了。（由 “猿blog” 变成 “zhisheng” 了，细心的童鞋会发现不仅名字变了， ID 也变了，但是图片还没改，暂时还没想到好的 logo 图片），下面说说为啥吧！ 听我瞎 BB 上图是两年前公众号群发的第一条信息，那时自己还是在学校，如今已经进入了社会，在公司实习了。记得当初开这个公众号的原因是因为几个年轻人有着梦想，打算一起做点东西，当时一腔热血的自己立马就先申请了个公众号，后来 “东西” 倒是没做，反倒是我自己慢慢的在微信公众号分享一些文章，然后那时自己也写博客（算算自己写博客应该快三年了，坚持真不易啊），所以偶尔也把自己的博客分享在微信公众号上。 但是好景不长，那时的微信公众号排版真尼玛难用的一批，作为一个理工科的男生，本来自己做事一般细心和耐心，无奈，把这么好的一个童鞋都给逼坏了。我现在还是得吐槽下，如今的微信公众号后台排版还是那么差。但是可能因为需求比较多了，所以就有人做出了工具（将 markdown 排版后在将整个样式复制粘贴到微信公众号后台），这样一篇排版还算不错的博客就出来了。 自己早就知道了这么个工具，以前看 DD 的博客的时候就发现了这个工具，但是很久没更新的微信公众号，自己也不怎么想再管理。 有人就要问了？那为啥现在又要开始跟新了呢？ 我只想说：“贱人就是矫情！！！又想瞎折腾下。”，反正自己的博客也在不断的更新，偶尔顺带把文章同步到微信公众号其实也是可以的。在学校的时候时间比较多，那时真的是时间比较多，后悔没好好坚持运营下来。现在工作了，自己工作之外的时间较少，除了学习，偶尔写写博客，娱乐时间比较少，都是大学时宅的。 前段时间被人 “忽悠” 说继续更新公众号，那时刚好也快 2018 年了，自己也想给自己定几个目标，在元旦的那天，想想还是继续更新微信公众号吧，所以你也看得到最近我的更新了，可能最近的更新比较有规律，因为这些文章大部分是之前就已经写好了的，已经发过在我的博客里了。估计把这些文章更新完后，就不会每天都更新我自己的文章了。 定位说下微信公众号的定位吧： 1、我的技术博客应该都会同步在这里的。 2、分享自己平时的随笔文章。（比如这篇。。。） 3、除了技术文章，当然还有平时自己的 奇淫技巧 （包括但不限于写作方式、推荐好用的软件等） 4、分享自己觉得不错的文章（别人的，尽量征得同意，一定会备注原创地址的） 5、如果你也写博客，但是阅读量很小的话，可以考虑自荐。（注：文章我可能会审批，必须要觉得不错的文章） 6、分享一些学习视频和书籍 7、后期可能会搞工作内推 。。。 暂时只想到这些了 问题来了因为工作了，所以时间少，运营这微信公众号可能需要花费我不少工作之外的时间。 如果可以的话，我希望能找到一个能帮我分担点的朋友。 注：无偿的，如果介意的话，下面就不用再看了。 说点简单的要求吧： 1、细心、耐心的 boy or girl 都行 2、起码要知道点编程方面的知识 3、能坚持下来 4、对新技术有敏感的嗅觉 5、最后一点就是你要有点时间了，希望不耽误你学习 再说下能给你带来的 好处 吧： 1、肯定能增加你的运营能力（再去互联网公司投运营岗位会有优势的） 2、本人一开始会亲自教授该怎么做，所以没经验的朋友不用担心 3、可以增加和大牛勾搭的机会，你们懂的。。 4、本人可以亲自传授经验（编程和生活点滴经验） 如果你有意愿的话，请加我 QQ ： 1041218129 聊聊吧 关注我","tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]},{"title":"Kafka 安装及快速入门","date":"2018-01-03T16:00:00.000Z","path":"2018/01/04/Kafka/","text":"介绍官网：http://kafka.apache.org/ Apache Kafka是分布式发布-订阅消息系统。它最初由LinkedIn公司开发，之后成为Apache项目的一部分。Kafka是一种快速、可扩展的、设计内在就是分布式的，分区的和可复制的提交日志服务。Apache Kafka与传统消息系统相比，有以下不同： 它被设计为一个分布式系统，易于向外扩展； 它同时为发布和订阅提供高吞吐量； 它支持多订阅者，当失败时能自动平衡消费者； 它将消息持久化到磁盘，因此可用于批量消费，例如ETL，以及实时应用程序。 安装 kafka下载地址：https://kafka.apache.org/downloads 1wget http://mirrors.shuosc.org/apache/kafka/1.0.0/kafka_2.11-1.0.0.tgz 解压：123tar -zxvf kafka_2.11-1.0.0.tgzcd /usr/local/kafka_2.11-1.0.0/ 修改 kafka-server 的配置文件 1vim /usr/local/kafka/config/server.properties 修改其中的： 12broker.id=1log.dir=/data/kafka/logs-1 功能验证：1、启动 zk使用安装包中的脚本启动单节点 Zookeeper 实例： 1bin/zookeeper-server-start.sh -daemon config/zookeeper.properties 2、启动Kafka 服务使用 kafka-server-start.sh 启动 kafka 服务： 1bin/kafka-server-start.sh config/server.properties 3、创建 topic使用 kafka-topics.sh 创建单分区单副本的 topic test： 1bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test 查看 topic 列表： 1bin/kafka-topics.sh --list --zookeeper localhost:2181 查询创建的 topic 列表报错： 解决方法: 1vim /etc/hosts 将 host 里的 12127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 修改为： 12127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 ip6-localhost ip6-localhost.localdomain localhost6 localhost6.localdomain6 方法参考：zookeeper unable to open socket to localhost/0:0:0:0:0:0:0:1:2181 再次查询就不报错了。 4、产生消息使用 kafka-console-producer.sh 发送消息： 1bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 5、消费消息使用 kafka-console-consumer.sh 接收消息并在终端打印： 1bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning 打开个新的命令窗口执行上面命令即可查看信息： 6、查看描述 topics 信息1bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test 结果： 12Topic:test PartitionCount:1 ReplicationFactor:1 Configs: Topic: test Partition: 0 Leader: 1 Replicas: 1 Isr: 1 第一行给出了所有分区的摘要，每个附加行给出了关于一个分区的信息。 由于我们只有一个分区，所以只有一行。 “Leader”: 是负责给定分区的所有读取和写入的节点。 每个节点将成为分区随机选择部分的领导者。 “Replicas”: 是复制此分区日志的节点列表，无论它们是否是领导者，或者即使他们当前处于活动状态。 “Isr”: 是一组“同步”副本。这是复制品列表的子集，当前活着并被引导到领导者。 集群配置Kafka 支持两种模式的集群搭建：可以在单机上运行多个 broker 实例来实现集群，也可在多台机器上搭建集群，下面介绍下如何实现单机多 broker 实例集群，其实很简单，只需要如下配置即可。 单机多broker 集群配置利用单节点部署多个 broker。 不同的 broker 设置不同的 id，监听端口及日志目录。 例如： 1234567cp config/server.properties config/server-2.propertiescp config/server.properties config/server-3.propertiesvim config/server-2.propertiesvim config/server-3.properties 修改 ： 12345broker.id=2listeners = PLAINTEXT://your.host.name:9093log.dir=/data/kafka/logs-2 和 12345broker.id=3listeners = PLAINTEXT://your.host.name:9094log.dir=/data/kafka/logs-3 启动Kafka服务： 123bin/kafka-server-start.sh config/server-2.properties &amp;bin/kafka-server-start.sh config/server-3.properties &amp; 至此，单机多broker实例的集群配置完毕。 多机多 broker 集群配置分别在多个节点按上述方式安装 Kafka，配置启动多个 Zookeeper 实例。 假设三台机器 IP 地址是 ： 192.168.153.135， 192.168.153.136， 192.168.153.137 分别配置多个机器上的 Kafka 服务，设置不同的 broker id，zookeeper.connect 设置如下: 1vim config/server.properties 里面的 zookeeper.connect 修改为： 1zookeeper.connect=192.168.153.135:2181,192.168.153.136:2181,192.168.153.137:2181 使用 Kafka Connect 来导入/导出数据从控制台写入数据并将其写回控制台是一个方便的起点，但您可能想要使用其他来源的数据或将数据从 Kafka 导出到其他系统。对于许多系统，您可以使用 Kafka Connect 来导入或导出数据，而不必编写自定义集成代码。 Kafka Connect 是 Kafka 包含的一个工具，可以将数据导入和导出到 Kafka。它是一个可扩展的工具，运行 连接器，实现与外部系统交互的自定义逻辑。在这个快速入门中，我们将看到如何使用简单的连接器运行 Kafka Connect，这些连接器将数据从文件导入到 Kafka topic，并将数据从 Kafka topic 导出到文件。 首先，我们将通过创建一些种子数据开始测试： 1echo -e &quot;zhisheng\\ntian&quot; &gt; test.txt 接下来，我们将启动两个以独立模式运行的连接器，这意味着它们将在单个本地专用进程中运行。我们提供三个配置文件作为参数。首先是 Kafka Connect 过程的配置，包含常见的配置，例如要连接的 Kafka 代理以及数据的序列化格式。其余的配置文件都指定一个要创建的连接器。这些文件包括唯一的连接器名称，要实例化的连接器类以及连接器所需的任何其他配置。 1bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties Kafka 附带的这些示例配置文件使用您之前启动的默认本地群集配置，并创建两个连接器：第一个是源连接器，用于读取输入文件中的行，并将每个连接生成为 Kafka topic，第二个为连接器它从 Kafka topic 读取消息，并在输出文件中产生每行消息。 在启动过程中，您会看到一些日志消息，其中一些指示连接器正在实例化。Kafka Connect 进程启动后，源连接器应该开始读取 test.txt topic connect-test，并将其生成 topic ，并且接收器连接器应该开始读取 topic 中的消息 connect-test 并将其写入文件 test.sink.txt。我们可以通过检查输出文件的内容来验证通过整个管道传输的数据： 数据存储在 Kafka topic 中 connect-test，因此我们也可以运行控制台使用者来查看 topic 中的数据 1bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic connect-test --from-beginning 连接器继续处理数据，所以我们可以将数据添加到文件中，并看到它在管道中移动： 1234echo zhishengtian&gt;&gt; test.txtecho zhishengtian2&gt;&gt; test.txtecho zhishengtian3&gt;&gt; test.txtecho zhishengtian4&gt;&gt; test.txt 使用 Kafka 流来处理数据Kafka Streams 是用于构建关键任务实时应用程序和微服务的客户端库，输入和/或输出数据存储在 Kafka 集群中。Kafka Streams 结合了在客户端编写和部署标准 Java 和 Scala 应用程序的简单性以及 Kafka 服务器端集群技术的优势，使这些应用程序具有高度可伸缩性，弹性，容错性，分布式等特性。 可参考官网入门案例：http://kafka.apache.org/10/documentation/streams/quickstart 参考1、在CentOS 7上安装Kafka 2、http://kafka.apache.org/10/documentation/streams/quickstart 关注我 最后转载请注明原创地址为：http://www.54tianzhisheng.cn/2018/01/04/Kafka/","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://yoursite.com/tags/Kafka/"}]},{"title":"Windows 下安装 Consul","date":"2017-12-26T16:00:00.000Z","path":"2017/12/27/consul-install/","text":"前提从刚工作就开始接触 Consul，中途自己也有两个项目和 Consul 有关，后面有机会再讲讲，网上关于这个的资料还比较少。因为明天有 Consul 的技术分享，所以自己今天下午在官网看了下相关的介绍。 介绍Consul 是一个支持多数据中心分布式高可用的服务发现和配置共享的服务软件, 由 HashiCorp 公司用 Go 语言开发, 基于 Mozilla Public License 2.0 的协议进行开源。Consul 支持健康检查, 并允许 HTTP 和 DNS 协议调用 API 存储键值对。命令行超级好用的虚拟机管理软件 vgrant 也是 HashiCorp 公司开发的产品。一致性协议采用 Raft 算法, 用来保证服务的高可用， 使用 GOSSIP 协议管理成员和广播消息, 并且支持 ACL 访问控制。 下载安装去官网下载：https://www.consul.io/downloads.html 得到一个 zip 压缩包 在你想要安装的位置解压就行，只有一个 consul.exe 文件（我的解压位置是：D:\\software） 设置环境变量（在 path 中新增一条）： D:\\software cmd 命令窗口启动： 1consul agent -dev consul 自带 UI 界面，打开网址：http://localhost:8500 ，可以看到当前注册的服务界面。 Consul 优势 使用 Raft 算法来保证一致性, 比复杂的 Paxos 算法更直接. 相比较而言, zookeeper 采用的是 Paxos, 而 etcd 使用的则是 Raft. 支持多数据中心，内外网的服务采用不同的端口进行监听。 多数据中心集群可以避免单数据中心的单点故障,而其部署则需要考虑网络延迟, 分片等情况等. zookeeper 和 etcd 均不提供多数据中心功能的支持. 支持健康检查. etcd 不提供此功能. 支持 http 和 dns 协议接口. zookeeper 的集成较为复杂, etcd 只支持 http 协议. 官方提供web管理界面, etcd 无此功能. 综合比较, Consul 作为服务注册和配置管理的新星, 比较值得关注和研究. 最后 本文首发于：zhisheng的博客 地址为：http://www.54tianzhisheng.cn/2017/12/27/consul-install/ 转载请注明地址！","tags":[{"name":"Consul","slug":"Consul","permalink":"http://yoursite.com/tags/Consul/"}]},{"title":"Elasticsearch 系列文章（五）：ELK 实时日志分析平台环境搭建","date":"2017-12-24T16:00:00.000Z","path":"2017/12/25/ELK/","text":"简单介绍ELK（ElasticSearch, Logstash, Kibana），三者组合在一起搭建实时的日志分析平台，目前好多公司都是这套！ Elasticsearch 是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful 风格接口，多数据源，自动搜索负载等。 Logstash 是一个完全开源的工具，他可以对你的日志进行收集、过滤，并将其存储供以后使用（如，搜索）。 Kibana 也是一个开源和免费的工具，它 Kibana 可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助您汇总、分析和搜索重要数据日志。 安装 ES。。。这个省略，不 bb 了，以前写过。。。传送门：http://www.54tianzhisheng.cn/2017/09/09/Elasticsearch-install/ 安装 LogstashELK 整套环境搭建版本很关键，最好全统一一个版本，否则出啥问题就不太好找了。这是我见过版本统一最严格的了。而已 ES 版本升了后，其他的都要都要升级，包括其插件。升级代价挺大的，最好一开始就定位好要安装哪个版本！ 在官网下好安装包后传到 Linux 上，这是速度最快的。 1234567891011121314151617181920212223242526在 /usr/local 目录下解压：tar -zxvf logstash-5.5.2.tar.gz进入解压后的目录：cd /usr/local/logstash-5.5.2/bin新增配置文件：vim logstash.conf增加：input&#123; file&#123; path =&gt; [&quot;/var/log/*.log&quot;] &#125;&#125;output&#123; elasticsearch&#123; hosts =&gt; [&quot;192.168.153.135:9200&quot;] index =&gt; &quot;logstash__log&quot; &#125;&#125; Logstash 的启动方式是： 123在 /usr/local/logstash-5.5.2/bin 目录下运行：./logstash -f logstash.conf 安装 Kibana同样，官网下好安装包，上传到 Linux。 1234567891011解压：tar -zxvf kibana-5.5.2-linux-x86_64.tar.gz修改配置文件 kibana-5.5.2/config/kibana.yml 如下：Server.host //配置机器ip/hostnameServer.name //此kibana服务的名称elasticsearch.url //es master节点url Kibana 启动方式： 123在 /usr/local/kibana-5.5.2/bin 目录下运行：./kibana Web界面访问: http://ip:5601 此时需要输入用户名和密码登录,默认分别是 elastic 和 changeme X-PackX-Pack 是一个 Elastic Stack 的扩展，将安全，警报，监控，报告和图形功能包含在一个易于安装的软件包中。 ES 和 Kibana 都可安装。 插件 x-pack-5.5.2.zip 依旧官网下。 ES 安装 X-Pack123cd /usr/local/elasticsearch/bin./elasticsearch-plugin install file:///opt/es/x-pack-5.5.2.zip 如果成功：显示如下 123456789101112131415161718192021222324252627282930[root@node1 bin]# ./elasticsearch-plugin install file:///opt/es/x-pack-5.5.2.zip-&gt; Downloading file:///opt/es/x-pack-5.5.2.zip[=================================================] 100%@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: plugin requires additional permissions @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@* java.io.FilePermission \\\\.\\pipe\\* read,write* java.lang.RuntimePermission accessClassInPackage.com.sun.activation.registries* java.lang.RuntimePermission getClassLoader* java.lang.RuntimePermission setContextClassLoader* java.lang.RuntimePermission setFactory* java.security.SecurityPermission createPolicy.JavaPolicy* java.security.SecurityPermission getPolicy* java.security.SecurityPermission putProviderProperty.BC* java.security.SecurityPermission setPolicy* java.util.PropertyPermission * read,write* java.util.PropertyPermission sun.nio.ch.bugLevel write* javax.net.ssl.SSLPermission setHostnameVerifierSee http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.htmlfor descriptions of what these permissions allow and the associated risks.Continue with installation? [y/N]y@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: plugin forks a native controller @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@This plugin launches a native controller that is not subject to the Javasecurity manager nor to system call filters.Continue with installation? [y/N]y-&gt; Installed x-pack Kibana 安装 X-Pack123cd /usr/local/kibana-5.5.2/bin./kibana-plugin install file:///opt/es/x-pack-5.5.2.zip 安装成功如下： 123456789[root@node1 bin]# ./kibana-plugin install file:///opt/es/x-pack-5.5.2.zipAttempting to transfer from file:///opt/es/x-pack-5.5.2.zipTransferring 159867054 bytes....................Transfer completeRetrieving metadata from plugin archiveExtracting plugin archiveExtraction completeOptimizing and caching browser bundles...Plugin installation complete 启用 x-pack 安全机制分别在 kibana.yml 和 elasticsearch.yml 中加入下行 1xpack.security.enabled: true 这样后，你再打开 ES 的 head 界面和 Kibana 管理界面就需要输入账号密码了。 上图右边是安装 X-Pack 后的，功能多了几个。 最后环境搭建很简单，后面如果有时间的话可以再讲讲在 Kibana 的 Dev Tools 上构建 ES 的 JSON 串来对 ES 进行操作。 我还写过 ES 相关的文章： 1、Elasticsearch 默认分词器和中分分词器之间的比较及使用方法 2、全文搜索引擎 Elasticsearch 集群搭建入门教程 3、ElasticSearch 集群监控 4、ElasticSearch 单个节点监控 结尾 本文首发于：zhisheng 的博客 转载请注明地址：http://www.54tianzhisheng.cn/2017/12/25/ELK/","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://yoursite.com/tags/ElasticSearch/"},{"name":"LogStash","slug":"LogStash","permalink":"http://yoursite.com/tags/LogStash/"},{"name":"Kibana","slug":"Kibana","permalink":"http://yoursite.com/tags/Kibana/"}]},{"title":"Hexo + yilia 搭建博客可能会遇到的所有疑问","date":"2017-12-17T16:00:00.000Z","path":"2017/12/18/hexo-yilia/","text":"前提为什么会再次写这篇博客？请看下图： 这是我博客搜索引擎的主要关键字。为什么会有这些关键字呢？ 我猜估计是曾经写了几篇关于搭建博客的文章，被搜索引擎收入了，所以搜索引擎才会将这些流量引导至我的博客，文章如下： 1、利用Github Page 搭建个人博客网站 2、Hexo + yilia 主题实现文章目录 3、Github pages + Hexo 博客 yilia 主题使用畅言评论系统 那还有这么多人搜索这些关键字？说明碰到问题的还有不少，所以才有了这篇文章的诞生！ 问题解答1、hexo yilia 文章目录 这个我以前写过一篇文章：Hexo + yilia 主题实现文章目录 那篇文章写了我那个版本的 yilia 怎么添加文章目录的，但是好像新版本的 yilia 已经自带了这个文章目录功能。所以如果你是使用的新版本的 yilia ，请不要做任何修改！但是前几天有人给我发了个图片，又好像有点区别，如果实在有不同的话，请加群 528776268 找我要我那个主题版本的所有配置文件。再次说明，我前端也不是很擅长，我写那篇文章也是参考其他博客的修改，所以无能为力了。有什么问题，建议直接在 yilia 主题的 GitHub 去找作者聊！ 2、Hexo yilia 随笔 随笔如下： 对此想说的就是，“随笔” 其实就是文章的一个 tags(标签)，如果你想把文章作为随笔的话，请在文章的首部写个 tags 为 “随笔” 的标签。如下图： 注意：- 后面有个空格。 3、yilia 主题分类实现 如果要有多个标签，可以如下图所示： 4、hexo yilia 设置文章显示长度，不展开全文 yilia 主题中可以用 &lt;!-- more --&gt; 截取文章的显示长度，如果你想在哪截取文章，就在那行使用该字符。 5、yilia 添加阅读量 我添加的是 “不蒜子” 计数，它可以区分 pv/uv 的统计方式，统计更精准，满足更多需求。有这个需求的可以去查找下博客怎么添加。（网上有很多这方面的博客） 6、yilia 主题使用 “畅言” 评论系统 参见我以前的文章： Github pages + Hexo 博客 yilia 主题使用畅言评论系统 7、hexo yilia 引入音乐 1&lt;iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"填写音乐链接地址\"&gt;&lt;/iframe&gt; 如下图，可以在网易云音乐里搜到你想要引入的音乐，然后点击如下的 “生成外链播放器” 即可： 8、hexo yilia 引入视频 hexo 支持 html 语法的，所以可以如上图这样引入视频！ 9、hexo yilia 相册 这个抱歉，我自己也没做这方面的功能，暂时不太清楚怎么实现。不过有文章写怎么实现，大家可以搜索下！ 10、hexo yilia 怎么写文章 我一般写文章就是先用本地 markdown 编辑器写好后，然后放在 hexo 的 source/_posts 目录下。 结尾好了，大概就这些问题，我也一一解答了，希望搭建博客的你可以看到这篇文章，让你少走点弯路，如果你也遇到过这些问题，还请你能分享下文章，让更多人避免入坑！ 本文地址是：Hexo + yilia 搭建博客可能会遇到的所有疑问 本文原创，转载请注明原创地址。 最后http://www.54tianzhisheng.cn/2017/12/18/hexo-yilia 这个链接是让推酷爬虫吞掉的，哈哈！","tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"},{"name":"yilia","slug":"yilia","permalink":"http://yoursite.com/tags/yilia/"}]},{"title":"谷歌开发者大会收获满满，不去真 “可惜” 了","date":"2017-12-12T16:00:00.000Z","path":"2017/12/13/Google-Developer-Days/","text":"全文图片较多，请在 WiFi 下阅读，土豪请随意！ 前提今年 Google 开发者大会再度来袭，大会将于 12 月 13 日和 14 日在上海举办，主题涵盖机器学习(Machine Learning)、Android、移动网络(Mobile Web)、TensorFlow、Firebase、云服务(Cloud)、AR/VR、设计(Design)以及更多开发者相关内容。 今天我就到走一遭，收获满满，都是用袋子提回来的，哈哈。下图为袋子： 再秀张图代表我去了： 入场 说下今天我参加的会场吧！ 会场主会场开幕 当然是用来用来做开发者开幕大会主题演讲的。相信不少没到现场的也看了直播。 拍了两张李飞飞演讲时的照片： 还有个妹子是讲 TensorFlow 的，全程中文，还贼 6，佩服！！！ 中途演讲还好几个，没拍照了。。。 中途茶歇：去外面看了下。 主会场演讲主题是：《渐进式网页应用：快速、集成、可靠并且具有吸引力》 这次坐的是前排，还拍了照，演讲人技巧很好，边演讲边带有身体动作的，而且还比较诙谐。 午餐胸牌上有13、14 号的午餐券，可以免费吃、免费拿，福利超好。 下午会场下午会场有点多，略略略。。。 都拍了点照，如果想要，可以加群：528776268 找我要、 晚餐诱惑颇大。。 我还喝了杯葡萄酒。。哈哈 另外除了照片，还拍了三个视频 回家吃饱喝足，回家拍了张照 礼物到家了，整理了下今天的礼物： 贴纸一张 小礼品一个 一个可 DIY 的音箱 一个定制的手提电脑包，质量很好。 AndroidThings 最后全文图片较多，谢谢阅读！自己收获也挺多的，明天还有一天，可惜不打算去了，转载请注明地址：http://www.54tianzhisheng.cn/2017/12/13/Google-Developer-Days/ 结尾这个是为了防爬虫写的，哈哈","tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]},{"title":"使用 CodeMirror 打造属于自己的在线代码编辑器","date":"2017-12-08T16:00:00.000Z","path":"2017/12/09/CodeMirror/","text":"前提写这个的目的是因为之前项目里用到过 CodeMirror，觉得作为一款在线代码编辑器还是不错，也看到过有些网站用到过在线代码编辑，当然我不知道他们是用什么做的，这里我把公司项目里用到的那部分抽出来，单独写篇博客，并把抽出来的那部分代码提交到 GitHub 去（地址），以防日后可能会再次用到（没准毕业设计里可能用的到）。 简单介绍CodeMirror 是一款在线的支持语法高亮的代码编辑器。官网： http://codemirror.net/ 可能光看官网，第一眼觉得那些在线编辑器有点丑，反正第一眼给我的感觉就是这样子，但是经过自己的细调，也能打造出一款精美的在线代码编辑器。 官网可以把它下载下来。 下载后，解压开得到的文件夹中，lib 下是放的是核心库和核心 css，mode 下放的是各种支持语言的语法定义，theme 目录下是支持的主题样式。一般在开发中，添加 lib 下的引用和 mode 下的引用就够了。 如何使用下面两个是使用 Code Mirror 必须引入的： 12&lt;link rel=\"stylesheet\" href=\"codemirror-5.31.0/lib/codemirror.css\"/&gt;&lt;script src=\"codemirror-5.31.0/lib/codemirror.js\"&gt;&lt;/script&gt; 接下来要引用的就是在 mode 目录下编辑器中要编辑的语言对应的 js 文件，这里以 Groovy 为例： 12&lt;!--groovy代码高亮--&gt;&lt;script src=\"codemirror-5.31.0/mode/groovy/groovy.js\"&gt;&lt;/script&gt; 如果你想让 Java 代码也支持代码高亮，则需要引入我从网上下载下来的 clike.js（我已经放到我的 GitHub 去了） 12&lt;!--Java代码高亮必须引入--&gt;&lt;script src=\"codemirror-5.31.0/clike.js\"&gt;&lt;/script&gt; 引用的文件用于支持对应语言的语法高亮。 然后前面说了第一次进入 Code Mirror 官网，觉得那些编辑器比较丑，那可能是主题比较丑，我这里推荐一款还不错的主题，只需按照如下引入即可： 12&lt;!--引入css文件，用以支持主题--&gt;&lt;link rel=\"stylesheet\" href=\"codemirror-5.31.0/theme/dracula.css\"/&gt; 如果你还想让你的编辑器支持代码行折叠，请按照如下进行操作： 123456&lt;!--支持代码折叠--&gt;&lt;link rel=\"stylesheet\" href=\"codemirror-5.31.0/addon/fold/foldgutter.css\"/&gt;&lt;script src=\"codemirror-5.31.0/addon/fold/foldcode.js\"&gt;&lt;/script&gt;&lt;script src=\"codemirror-5.31.0/addon/fold/foldgutter.js\"&gt;&lt;/script&gt;&lt;script src=\"codemirror-5.31.0/addon/fold/brace-fold.js\"&gt;&lt;/script&gt;&lt;script src=\"codemirror-5.31.0/addon/fold/comment-fold.js\"&gt;&lt;/script&gt; 是不是这样引入就好了呢，当然不是啦 创建编辑器在实际项目中，一般都不会直接把 body 整个内容作为编辑器的容器。而最常用的，是使用 textarea。这里我在 里使用个 textarea， 123&lt;!-- begin code --&gt;&lt;textarea class=\"form-control\" id=\"code\" name=\"code\"&gt;&lt;/textarea&gt;&lt;!-- end code--&gt; 接下来就是创建编辑器了。 123//根据DOM元素的id构造出一个编辑器var editor = CodeMirror.fromTextArea(document.getElementById(\"code\"), &#123;&#125;); 是不是有点单调？ 没错，我还可以在里面给他设置些属性：（充分利用我一开始引入的那些文件） 123456789mode: \"text/groovy\", //实现groovy代码高亮mode: \"text/x-java\", //实现Java代码高亮lineNumbers: true, //显示行号theme: \"dracula\", //设置主题lineWrapping: true, //代码折叠foldGutter: true,gutters: [\"CodeMirror-linenumbers\", \"CodeMirror-foldgutter\"],matchBrackets: true, //括号匹配//readOnly: true, //只读 如果需要查看更多属性，可以去官网查找，目前我只用到这些属性！ 下面也列举些吧： indentUnit: integer缩进单位，值为空格数，默认为2 。 smartIndent: boolean自动缩进，设置是否根据上下文自动缩进（和上一行相同的缩进量）。默认为true。 tabSize: integertab字符的宽度，默认为4 。 indentWithTabs: boolean在缩进时，是否需要把 n*tab宽度个空格替换成n个tab字符，默认为false 。 electricChars: boolean在输入可能改变当前的缩进时，是否重新缩进，默认为true （仅在mode支持缩进时有效）。 specialChars: RegExp需要被占位符(placeholder)替换的特殊字符的正则表达式。最常用的是非打印字符。默认为：/[\\u0000-\\u0019\\u00ad\\u200b-\\u200f\\u2028\\u2029\\ufeff]/。 specialCharPlaceholder: function(char) → Element这是一个接收由specialChars选项指定的字符作为参数的函数，此函数会产生一个用来显示指定字符的DOM节点。默认情况下，显示一个红点（•），这个红点有一个带有前面特殊字符编码的提示框。 rtlMoveVisually: booleanDetermines whether horizontal cursor movement through right-to-left (Arabic, Hebrew) text is visual (pressing the left arrow moves the cursor left) or logical (pressing the left arrow moves to the next lower index in the string, which is visually right in right-to-left text). The default is false on Windows, and true on other platforms.（这段完全不晓得搞啥子鬼） keyMap: string配置快捷键。默认值为default，即 codemorrir.js 内部定义。其它在key map目录下。 extraKeys: object给编辑器绑定与前面keyMap配置不同的快捷键。 lineWrapping: boolean在长行时文字是换行(wrap)还是滚动(scroll)，默认为滚动(scroll)。 lineNumbers: boolean是否在编辑器左侧显示行号。 firstLineNumber: integer行号从哪个数开始计数，默认为1 。 lineNumberFormatter: function(line: integer) → string使用一个函数设置行号。 gutters: array用来添加额外的gutter（在行号gutter前或代替行号gutter）。值应该是CSS名称数组，每一项定义了用于绘制gutter背景的宽度（还有可选的背景）。为了能明确设置行号gutter的位置（默认在所有其它gutter的右边），也可以包含CodeMirror-linenumbers类。类名是用于传给setGutterMarker的键名(keys)。 fixedGutter: boolean设置gutter跟随编辑器内容水平滚动（false）还是固定在左侧（true或默认）。 scrollbarStyle: string设置滚动条。默认为”native”，显示原生的滚动条。核心库还提供了”null”样式，此样式会完全隐藏滚动条。Addons可以设置更多的滚动条模式。 coverGutterNextToScrollbar: boolean当fixedGutter启用，并且存在水平滚动条时，在滚动条最左侧默认会显示gutter，当此项设置为true时，gutter会被带有CodeMirror-gutter-filler类的元素遮挡。inputStyle: string选择CodeMirror处理输入和焦点的方式。核心库定义了textarea和contenteditable输入模式。在移动浏览器上，默认是contenteditable，在桌面浏览器上，默认是textarea。在contenteditable模式下对IME和屏幕阅读器支持更好。 readOnly: boolean|string编辑器是否只读。如果设置为预设的值 “nocursor”，那么除了设置只读外，编辑区域还不能获得焦点。 showCursorWhenSelecting: boolean在选择时是否显示光标，默认为false。 lineWiseCopyCut: boolean启用时，如果在复制或剪切时没有选择文本，那么就会自动操作光标所在的整行。 undoDepth: integer最大撤消次数，默认为200（包括选中内容改变事件） 。 historyEventDelay: integer在输入或删除时引发历史事件前的毫秒数。 tabindex: integer编辑器的tabindex。 autofocus: boolean是否在初始化时自动获取焦点。默认情况是关闭的。但是，在使用textarea并且没有明确指定值的时候会被自动设置为true。 dragDrop: boolean是否允许拖放，默认为true。 allowDropFileTypes: array默认为null。当设置此项时，只接收包含在此数组内的文件类型拖入编辑器。文件类型为MIME名称。 cursorBlinkRate: number光标闪动的间隔，单位为毫秒。默认为530。当设置为0时，会禁用光标闪动。负数会隐藏光标。 cursorScrollMargin: number当光标靠近可视区域边界时，光标距离上方和下方的距离。默认为0 。 cursorHeight: number光标高度。默认为1，也就是撑满行高。对一些字体，设置0.85看起来会更好。 resetSelectionOnContextMenu: boolean设置在选择文本外点击打开上下文菜单时，是否将光标移动到点击处。默认为true。 workTime, workDelay: number通过一个假的后台线程高亮 workTime 时长，然后使用 timeout 休息 workDelay 时长。默认为200和300 。（完全不懂这个功能是在说啥） pollInterval: number指明CodeMirror向对应的textarea滚动（写数据）的速度（获得焦点时）。大多数的输入都是通过事件捕获，但是有的输入法（如IME）在某些浏览器上并不会生成事件，所以使用数据滚动。默认为100毫秒。 flattenSpans: boolean默认情况下，CodeMirror会将使用相同class的两个span合并成一个。通过设置此项为false禁用此功能。 addModeClass: boolean当启用时（默认禁用），会给每个标记添加额外的表示生成标记的mode的以cm-m开头的CSS样式类。例如，XML mode产生的标记，会添加cm-m-xml类。 maxHighlightLength: number当需要高亮很长的行时，为了保持响应性能，当到达某些位置时，编辑器会直接将其他行设置为纯文本(plain text)。默认为10000，可以设置为Infinity来关闭此功能。 viewportMargin: integer指定当前滚动到视图中内容上方和下方要渲染的行数。这会影响到滚动时要更新的行数。通常情况下应该使用默认值10。可以设置值为Infinity始终渲染整个文档。注意：这样设置在处理大文档时会影响性能。 如果你要设置代码框的大小该怎么做呢？ 1editor.setSize('800px', '950px'); //设置代码框的长宽 另外，如果你想给代码框赋值，该怎么办呢？ 12editor.setValue(\"\"); //给代码框赋值editor.getValue(); //获取代码框的值 如果你再想在其他地方设置新的属性，可以像下面这样写： 1editor.setOption(\"readOnly\", true); //类似这种 总结上面就大概讲了下 Code Mirror 怎么使用，那么我们来看看效果吧 我自我感觉还是可以的哈！ 里面所有涉及的代码在 GitHub 里可以下载：https://github.com/zhisheng17/CoderBlog/tree/master/CodeMirror 文章原创，转载务必请注明原创地址：http://www.54tianzhisheng.cn/2017/12/09/CodeMirror/ 最后fuck 无脑的推酷爬虫，竟然把我所有文章最后的原创链接都给去掉了，这是我现在想到的一种对策方法。任何其他形式的转载，也必须把我文章所有内容加上，不得做任何修改，否则请别转载了！","tags":[{"name":"前端","slug":"前端","permalink":"http://yoursite.com/tags/前端/"}]},{"title":"Netty系列文章（一）：Netty 源码阅读之初始环境搭建","date":"2017-12-07T16:00:00.000Z","path":"2017/12/08/netty-01-env/","text":"Netty 简介Netty 是由 JBOSS 提供的一个开源的 java 网络编程框架，主要是对 java 的 nio 包进行了再次封装。Netty 比 java 原生的nio 包提供了更加强大、稳定的功能和易于使用的 api。 netty 的作者是 Trustin Lee，这是一个韩国人，他还开发了另外一个著名的网络编程框架，mina。二者在很多方面都十分相似，它们的线程模型也是基本一致 。不过 netty 社区的活跃程度要 mina 高得多。版本选择： 3.x 目前企业使用最多的版本，最为稳定。例如dubbo使用的就是3.x版本 4.x 引入了内存池等重大特性，可以有效的降低GC负载，rocketmq使用的就是4.x 5.x 已经被废弃了，具体可参见 https://github.com/netty/netty/issues/4466 所以这里我搭建的源码阅读环境是存在的 4.1 版本。 准备工具 IDEA 2017 环境搭建在 IDEA 中导入项目地址：https://github.com/netty/netty.git ，然后就会自动下载项目所有的依赖，但是请注意： 必须在 IDEA 中将 Profiles 中的所有都勾选上，否则会导致很多 jar 包拉不下来，如下图： 然后就是耐心等待了，一直到所有的 jar 包拉取下来。 中途你可能会遇到如下问题： 这里的是 1.5 版本，导致我们如果想用些高级的语法会完全报错。 如果你把这个版本设置为 8 的版本后， 下面会提示你，项目是从 maven 导过来的，如果 maven 配置改变重新 reimport 后，任何在这里的改变都会丢失。 同时你会看到项目的 Java Compile 版本是 1.5 的，如下图： 同样，你在这里修改，如果 maven 配置改变重新 reimport 后，任何在这里的改变也都会丢失。我估计碰到这种问题的不少。 总结起来原因就是 maven 中的编译版本就是 1.5 的，所以才会导致这里的问题发生，如果想完全修改好（一劳永逸）。请直接对 pom 文件动刀，就是干！ 只需把大项目（netty-parent）的那个 pom.xml 修改个属性，把版本信息提高到 1.8。 在等待它拉取 jar 包吧 搞完了之后发现还有两个模块（netty-bom、netty-dev-tools）不能设置到 版本，只能手动的和上面那种设置 language level 和 Java compile 为 1.8 了。 最后你会发现这里的完全没有报错了，开心不？ 代码行数统计额，看到项目这么多子模块，你都不知道该从哪里下手开始看，那么我就写了个简单的 Java 脚本去大概的统计每个子项目代码的行数。先看看统计结果： 整个项目差不多 23 万。（过滤了空行、各种注释和 @Override 之后的 Java 代码行数），靠这个数字很吓人！ 来看看我的脚本代码吧： 123456789101112131415public static void main(String[] args) throws Exception &#123; long count = Files.walk(Paths.get(\"C:\\\\JetBrains\\\\IDEAProject\\\\netty\\\\transport-udt\")) // 递归获得项目目录下的所有文件 .filter(file -&gt; !Files.isDirectory(file)) // 筛选出文件 .filter(file -&gt; file.toString().endsWith(\".java\")) // 筛选出 java 文件 .flatMap(Try.of(file -&gt; Files.lines(file), Stream.empty())) // 将会抛出受检异常的 Lambda 包装为 抛出非受检异常的 Lambda .filter(line -&gt; !line.trim().isEmpty()) // 过滤掉空行 .filter(line -&gt; !line.trim().startsWith(\"//\")) //过滤掉 //之类的注释 .filter(line -&gt; !(line.trim().startsWith(\"/*\") &amp;&amp; line.trim().endsWith(\"*/\"))) //过滤掉/* */之类的注释 .filter(line -&gt; !(line.trim().startsWith(\"/*\") &amp;&amp; !line.trim().endsWith(\"*/\"))) //过滤掉以 /* 开头的注释（去除空格后的开头） .filter(line -&gt; !(!line.trim().startsWith(\"/*\") &amp;&amp; line.trim().endsWith(\"*/\"))) //过滤掉已 */ 结尾的注释 .filter(line -&gt; !line.trim().startsWith(\"*\")) //过滤掉 javadoc 中的文字注释 .filter(line -&gt; !line.trim().startsWith(\"@Override\")) //过滤掉方法上含 @Override 的 .count(); System.out.println(\"代码行数：\" + count);&#125; 后面我会把我阅读源码的中文注释及解析之类的更新到我的 GitHub 去（欢迎关注、我是来骗 star 的），https://github.com/zhisheng17/netty ，如果你不想去自己设置上面所说的这些（偷懒），那就直接 fork 我的这份吧！ 最后环境搭建就写到这里了，转载请注明地址：http://www.54tianzhisheng.cn/2017/12/08/netty-01-env/","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Netty","slug":"Netty","permalink":"http://yoursite.com/tags/Netty/"}]},{"title":"RestTemplate 详解","date":"2017-12-02T16:00:00.000Z","path":"2017/12/03/RestTemplate/","text":"背景这段时间自己做的项目中需要调用服务提供者的服务（接口），具体就是：我这边需要将页面所输入的 Groovy 脚本代码传给别人提供的服务接口，然后那边返回脚本编译的结果给我，我需要将编译结果展示在页面，用的就是 RestTemplate 了，那 RestTemplate 是什么呢？简单说就是：简化了发起 HTTP 请求以及处理响应的过程，并且支持 REST 。下文就稍微总结下。 如何使用先讲讲如何使用吧，我项目是 SpringBoot 项目，可以在启动类中加入： 1234@Beanpublic RestTemplate restTemplate() &#123; return new RestTemplate();&#125; 然后在 Controller 层中引入： 12@Autowiredprivate RestTemplate restTemplate; 接下来就可以在 Controller 中各个方法中使用 restTemplate 了，但是 restTemplate 里面有什么方法呢？ RestTemplate 内部方法 从图中 RestTemplate 可以看到有很多方法，我们可以提取出主要的几种方法是： GET POST PUT DELETE HEAD OPTIONS EXCHANGE EXECUTE 图片中依然可以知道 RestTemplate 类中的方法主要是来自接口 RestOperations，下面我们具体看看这些方法里面的具体实现与该如何使用。 Get 方法在 RestTemplate 中，发送一个 GET 请求，我们可以通过如下两种方式： getForEntity getForEntity 方法的返回值是一个ResponseEntity&lt;T&gt;，ResponseEntity&lt;T&gt;是 Spring 对 HTTP 请求响应的封装，包括了几个重要的元素，如响应码、contentType、contentLength、响应消息体等。比如下面一个例子： 1234567891011121314@RequestMapping(\"/gethello\")public String getHello() &#123; ResponseEntity&lt;String&gt; responseEntity = restTemplate.getForEntity(\"http://HELLO-SERVICE/hello\", String.class); String body = responseEntity.getBody(); HttpStatus statusCode = responseEntity.getStatusCode(); int statusCodeValue = responseEntity.getStatusCodeValue(); HttpHeaders headers = responseEntity.getHeaders(); StringBuffer result = new StringBuffer(); result.append(\"responseEntity.getBody()：\").append(body).append(\"&lt;hr&gt;\") .append(\"responseEntity.getStatusCode()：\").append(statusCode).append(\"&lt;hr&gt;\") .append(\"responseEntity.getStatusCodeValue()：\").append(statusCodeValue).append(\"&lt;hr&gt;\") .append(\"responseEntity.getHeaders()：\").append(headers).append(\"&lt;hr&gt;\"); return result.toString();&#125; 关于这段代码，说如下几点： getForEntity 的第一个参数为我要调用的服务的地址，这里我调用了服务提供者提供的 /hello 接口，注意这里是通过服务名调用而不是服务地址，如果写成服务地址就没法实现客户端负载均衡了。（备注：我项目中需要通过 ConsulClient 去获取服务名，然后在去获取服务的 IP 和 Port，并把它拼接起来组合成我的服务地址，所以就没法实现客户端的负载均衡了，如果要是实现负载均衡，可以在 SpringBoot 启动类的中加入注解 @LoadBalanced, 如下: 12345@Bean@LoadBalancedpublic RestTemplate restTemplate() &#123; return new RestTemplate();&#125; ） getForEntity 第二个参数 String.class 表示我希望返回的 body 类型是 String 拿到返回结果之后，将返回结果遍历打印出来 有时候我在调用服务提供者提供的接口时，可能需要传递参数，有两种不同的方式: 123456789101112@RequestMapping(\"/sayhello\")public String sayHello() &#123; ResponseEntity&lt;String&gt; responseEntity = restTemplate.getForEntity(\"http://HELLO-SERVICE/sayhello?name=&#123;1&#125;\", String.class, \"张三\"); return responseEntity.getBody();&#125;@RequestMapping(\"/sayhello2\")public String sayHello2() &#123; Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); map.put(\"name\", \"李四\"); ResponseEntity&lt;String&gt; responseEntity = restTemplate.getForEntity(\"http://HELLO-SERVICE/sayhello?name=&#123;name&#125;\", String.class, map); return responseEntity.getBody();&#125; 可以用一个数字做占位符，最后是一个可变长度的参数，来一 一替换前面的占位符 也可以前面使用 name={name} 这种形式，最后一个参数是一个 map，map 的 key 即为前边占位符的名字，map的 value 为参数值 第一个调用地址也可以是一个URI而不是字符串，这个时候我们构建一个URI即可，参数神马的都包含在URI中了，如下： 1234567@RequestMapping(\"/sayhello3\")public String sayHello3() &#123; UriComponents uriComponents = UriComponentsBuilder.fromUriString(\"http://HELLO-SERVICE/sayhello?name=&#123;name&#125;\").build().expand(\"王五\").encode(); URI uri = uriComponents.toUri(); ResponseEntity&lt;String&gt; responseEntity = restTemplate.getForEntity(uri, String.class); return responseEntity.getBody();&#125; 通过Spring中提供的UriComponents来构建Uri即可。 当然，服务提供者不仅可以返回String，也可以返回一个自定义类型的对象，比如我的服务提供者中有如下方法： 1234@RequestMapping(value = \"/getbook1\", method = RequestMethod.GET)public Book book1() &#123; return new Book(\"三国演义\", 90, \"罗贯中\", \"花城出版社\");&#125; 对于该方法我可以在服务消费者中通过如下方式来调用： 12345@RequestMapping(\"/book1\")public Book book1() &#123; ResponseEntity&lt;Book&gt; responseEntity = restTemplate.getForEntity(\"http://HELLO-SERVICE/getbook1\", Book.class); return responseEntity.getBody();&#125; 运行结果如下： getForObject ​ getForObject 函数实际上是对 getForEntity 函数的进一步封装，如果你只关注返回的消息体的内容，对其他信息都不关注，此时可以使用 getForObject，举一个简单的例子，如下： 12345@RequestMapping(\"/book2\")public Book book2() &#123; Book book = restTemplate.getForObject(\"http://HELLO-SERVICE/getbook1\", Book.class); return book;&#125; POST 方法在 RestTemplate 中，POST 请求可以通过如下三个方法来发起： postForEntity 该方法和get请求中的getForEntity方法类似，如下例子： 1234567@RequestMapping(\"/book3\")public Book book3() &#123; Book book = new Book(); book.setName(\"红楼梦\"); ResponseEntity&lt;Book&gt; responseEntity = restTemplate.postForEntity(\"http://HELLO-SERVICE/getbook2\", book, Book.class); return responseEntity.getBody();&#125; 方法的第一参数表示要调用的服务的地址 方法的第二个参数表示上传的参数 方法的第三个参数表示返回的消息体的数据类型 我这里创建了一个Book对象，这个Book对象只有name属性有值，将之传递到服务提供者那里去，服务提供者代码如下： 12345678@RequestMapping(value = \"/getbook2\", method = RequestMethod.POST)public Book book2(@RequestBody Book book) &#123; System.out.println(book.getName()); book.setPrice(33); book.setAuthor(\"曹雪芹\"); book.setPublisher(\"人民文学出版社\"); return book;&#125; 服务提供者接收到服务消费者传来的参数book，给其他属性设置上值再返回，调用结果如下： postForObject 如果你只关注，返回的消息体，可以直接使用postForObject。用法和getForObject一致。 postForLocation postForLocation 也是提交新资源，提交成功之后，返回新资源的 URI，postForLocation 的参数和前面两种的参数基本一致，只不过该方法的返回值为 URI ，这个只需要服务提供者返回一个 URI 即可，该 URI 表示新资源的位置。 PUT 方法 在 RestTemplate 中，PUT 请求可以通过 put 方法调用，put 方法的参数和前面介绍的 postForEntity 方法的参数基本一致，只是 put 方法没有返回值而已。举一个简单的例子，如下： 123456@RequestMapping(\"/put\")public void put() &#123; Book book = new Book(); book.setName(\"红楼梦\"); restTemplate.put(\"http://HELLO-SERVICE/getbook3/&#123;1&#125;\", book, 99);&#125; book对象是我要提交的参数，最后的99用来替换前面的占位符{1} DELETE 方法 delete 请求我们可以通过 delete 方法调用来实现，如下例子： 1234@RequestMapping(\"/delete\")public void delete() &#123; restTemplate.delete(\"http://HELLO-SERVICE/getbook4/&#123;1&#125;\", 100);&#125; HEADER 方法 返回资源的所有 HTTP headers。 OPTIONS 问可以执行哪些方法。 EXCHANGE 与其它接口的不同： 允许调用者指定HTTP请求的方法（GET,POST,PUT等） 可以在请求中增加body以及头信息，其内容通过参数 HttpEntity&lt;?&gt;requestEntity 描述 exchange支持‘含参数的类型’（即泛型类）作为返回类型，该特性通过 ParameterizedTypeReferenceresponseType 描述 EXECUTE细心的你，不知道有没有发现上面所有的方法内部返回值都调用了同一个方法 —— execute 方法。 下面我们来看看： 可以看到，Excute方法只是将 String 格式的 URI 转成了 java.net.URI，之后调用了doExecute方法。整个调用过程关键起作用的是 doExecute 方法 doExecute 方法 这里需要了解两个类： RequestCallback 和 ResponseExtractor RestTemplate 类中可以看到他们两的实现类。 RequestCallback ：用于操作请求头和body，在请求发出前执行。 该接口有两个实现类： AcceptHeaderRequestCallback 只处理请求头，用于getXXX()方法。 HttpEntityRequestCallback 继承于AcceptHeaderRequestCallback可以处理请求头和body，用于putXXX()、postXXX()和exchange()方法。 ResponseExtractor：解析HTTP响应的数据，而且不需要担心异常和资源的关闭 上面图纸这个实现类 ResponseEntityResponseExtractor 的作用是：使用 HttpMessageConverterExtractor 提取 body（委托模式），然后将 body 和响应头、状态封装成 ResponseEntity 对象。 最后转载请注明地址：http://www.54tianzhisheng.cn/2017/12/03/RestTemplate/ 参考资料1、https://www.cnblogs.com/caolei1108/p/6169950.html 2、https://segmentfault.com/a/1190000011093597 如果想和我进一步交流请关注：","tags":[{"name":"Spring","slug":"Spring","permalink":"http://yoursite.com/tags/Spring/"}]},{"title":"实习圈群里提问小记","date":"2017-12-01T16:00:00.000Z","path":"2017/12/02/wx-01/","text":"","tags":[{"name":"实习圈","slug":"实习圈","permalink":"http://yoursite.com/tags/实习圈/"}]},{"title":"Docker系列文章（一）：基于 Harbor 搭建 Docker 私有镜像仓库","date":"2017-11-25T16:00:00.000Z","path":"2017/11/26/Docker-harbor/","text":"什么是 Harbor？第一次使用这个的时候是刚进公司处理的第一个任务的时候，发现 Harbor 就是一个用于存储和分发 Docker 镜像的企业级Registry 服务器。网上找到一个 Harbor 的架构图： Harbor 是 VMware 公司开源的企业级 DockerRegistry 项目，项目地址为 https://github.com/vmware/harbor。其目标是帮助用户迅速搭建一个企业级的 Docker registry 服务。它以 Docker 公司开源的 registry 为基础，提供了管理UI，基于角色的访问控制(Role Based Access Control)，AD/LDAP集成、以及审计日志(Auditlogging) 等企业用户需求的功能，同时还原生支持中文。Harbor 的每个组件都是以 Docker 容器的形式构建的，使用 Docker Compose 来对它进行部署。 环境准备1、自己在腾讯云买的服务器（CentOS7.3） 2、Docker 版本：17.05.0-ce 3、Docker-compose：1.17.1 4、Harbor：1.1.2 安装 Docker因为系统是 CentOS 7.3 ，内核啥的都已经是 3.10，所以不用担心内核升级的问题，一些操作啥的在 7.x 上操作也很方便。 1234567891011121314151617181920212223yum update //系统版本更新vim /etc/yum.repos.d/docker.repo //添加以下内容[dockerrepo]name=Docker Repositorybaseurl=https://yum.dockerproject.org/repo/main/centos/7/enabled=1gpgcheck=1gpgkey=https://yum.dockerproject.org/gpg//下面安装 Docker 引擎yum install docker-engine -y//安装docker引擎，此步也可作为更新docker版本的操作：先#systemctl stop docker 停止docker服务，再#yum install docker-engine 更新docker版本systemctl enable docker.servicesystemctl start docker //启动docker守护进程docker info //查看docker运行情况docker -v //查看版本信息 修改 Docker 配置文件 /etc/default/docker 如下： 1DOCKER_OPTS=\"--registry-mirror=http://aad0405c.m.daocloud.io\" //换成国内的镜像加速源，不然拉取镜像简直龟速，不想在吐槽了 使用 service docker restart 重启 Docker 服务即可。 或者用官方提供的方式： 1curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://ef017c13.m.daocloud.io 安装 Docker-compose如果是想直接命令安装也行， 1234567891011121314下载指定版本的docker-composesudo curl -L https://github.com/docker/compose/releases/download/1.17.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose对二进制文件赋可执行权限chmod +x /usr/local/bin/docker-compose测试下docker-compose是否安装成功docker-compose --version出现如下docker-compose version 1.17.1, build 6d101fb 但是，这种方法简直龟速，幸好还有种方法， 见这里：https://docs.docker.com/compose/install/#install-compose 这种需要通过 Python 的 pip 安装 安装 pip123456789wget --no-check-certificate https://pypi.python.org/packages/source/s/setuptools/setuptools-1.4.2.tar.gztar -vxf setuptools-1.4.2.tar.gzcd setuptools-1.4.2python2.7 setup.py install //因为服务器自带 Python 2.7easy_install-2.7 pip 安装 docker compose123pip install docker-composedocker-compose --version //测试安装是否成功 安装 Harbor12345wget https://github.com/vmware/harbor/releases/download/v1.1.2/harbor-offline-installer-v1.1.2.tgz离线安装包,也是龟速，把这个下载链接用迅雷下载，速度却贼快，嘿嘿，然后再传到服务器上去，整个过程快很多！tar -zxvf harbor-offline-installer-v1.1.2.tgz 解压缩之后，进入目录下会看到 harbor.cfg 文件，该文件就是 Harbor 的配置文件。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495## Configuration file of Harbor# hostname设置访问地址，可以使用ip、域名，不可以设置为127.0.0.1或localhosthostname = 115.159.227.249 #这里我先配置我的服务器IP地址# 访问协议，默认是http，也可以设置https，如果设置https，则nginx ssl需要设置onui_url_protocol = http# mysql数据库root用户默认密码root123，实际使用时修改下db_password = root123#Maximum number of job workers in job servicemax_job_workers = 3#Determine whether or not to generate certificate for the registry&apos;s token.#If the value is on, the prepare script creates new root cert and private key#for generating token to access the registry. If the value is off the default key/cert will be used.#This flag also controls the creation of the notary signer&apos;s cert.customize_crt = on#The path of cert and key files for nginx, they are applied only the protocol is set to httpsssl_cert = /data/cert/server.crtssl_cert_key = /data/cert/server.key#The path of secretkey storagesecretkey_path = /data#Admiral&apos;s url, comment this attribute, or set its value to NA when Harbor is standaloneadmiral_url = NA#NOTES: The properties between BEGIN INITIAL PROPERTIES and END INITIAL PROPERTIES#only take effect in the first boot, the subsequent changes of these properties#should be performed on web ui#************************BEGIN INITIAL PROPERTIES************************#Email account settings for sending out password resetting emails.#Email server uses the given username and password to authenticate on TLS connections to host and act as identity.#Identity left blank to act as username.email_identity =email_server = smtp.mydomain.comemail_server_port = 25email_username = sample_admin@mydomain.comemail_password = abcemail_from = admin &lt;sample_admin@mydomain.com&gt;email_ssl = false##The initial password of Harbor admin, only works for the first time when Harbor starts.#It has no effect after the first launch of Harbor.# 启动Harbor后，管理员UI登录的密码，默认是Harbor12345harbor_admin_password = Harbor12345# 认证方式，这里支持多种认证方式，如LADP、本次存储、数据库认证。默认是db_auth，mysql数据库认证auth_mode = db_auth#The url for an ldap endpoint.ldap_url = ldaps://ldap.mydomain.com#A user&apos;s DN who has the permission to search the LDAP/AD server.#If your LDAP/AD server does not support anonymous search, you should configure this DN and ldap_search_pwd.#ldap_searchdn = uid=searchuser,ou=people,dc=mydomain,dc=com#the password of the ldap_searchdn#ldap_search_pwd = password#The base DN from which to look up a user in LDAP/ADldap_basedn = ou=people,dc=mydomain,dc=com#Search filter for LDAP/AD, make sure the syntax of the filter is correct.#ldap_filter = (objectClass=person)# The attribute used in a search to match a user, it could be uid, cn, email, sAMAccountName or other attributes depending on your LDAP/AD ldap_uid = uid#the scope to search for users, 1-LDAP_SCOPE_BASE, 2-LDAP_SCOPE_ONELEVEL, 3-LDAP_SCOPE_SUBTREEldap_scope = 3#Timeout (in seconds) when connecting to an LDAP Server. The default value (and most reasonable) is 5 seconds.ldap_timeout = 5# 是否开启自注册self_registration = on# Token有效时间，默认30分钟token_expiration = 30# 用户创建项目权限控制，默认是everyone（所有人），也可以设置为adminonly（只能管理员）project_creation_restriction = everyone#Determine whether the job service should verify the ssl cert when it connects to a remote registry.#Set this flag to off when the remote registry uses a self-signed or untrusted certificate.verify_remote_cert = on#************************END INITIAL PROPERTIES************************ 启动 harbor，修改完配置文件后，在的当前目录执行./install.sh，Harbor服务就会根据当期目录下的docker-compose.yml开始下载依赖的镜像，检测并按照顺序依次启动各个服务。 启动完成后，我们访问刚设置的 hostname 即可，http://115.159.227.249/，默认是80端口，如果端口占用，我们可以去修改docker-compose.yml文件中，对应服务的端口映射。 登录 Web Harbor , 输入用户名 admin，默认密码（或已修改密码）登录系统。 我们可以看到系统各个模块如下： 项目：新增/删除项目，查看镜像仓库，给项目添加成员、查看操作日志、复制项目等 日志：仓库各个镜像create、push、pull等操作日志 系统管理 用户管理：新增/删除用户、设置管理员等 复制管理：新增/删除从库目标、新建/删除/启停复制规则等 配置管理：认证模式、复制、邮箱设置、系统设置等 其他设置 用户设置：修改用户名、邮箱、名称信息 修改密码：修改用户密码 注意：非系统管理员用户登录，只能看到有权限的项目和日志，其他模块不可见。 我们要尝试下能不能把自己 Docker 里面的镜像 push 到 Harbor 的 library 里来（默认这个 library 项目是公开的，所有人都可以有读的权限，都不需要 docker login 进来，就可以拉取里面的镜像）。 注意： 为了后面留坑，我这里先 在自己的 docker.service 中添加仓库：（这是个坑，建议你先按照我说的做，不然下面可能会一直登录不上） 1234vim /usr/lib/systemd/system/docker.service里面的这行修改为：（其实就是添加 --insecure-registry 115.159.227.249 ）ExecStart=/usr/bin/dockerd --insecure-registry 115.159.227.249 添加完了后重新启动 docker： 1systemctl daemon-reload &amp;&amp; systemctl enable docker &amp;&amp; systemctl start docker 启动 docker 服务: 1service docker start 登录：（为了测试下能否登录成功） 12345admin登录$ docker login 115.159.227.249Username: adminPassword:Login Succeeded 打 tag 并 push 12345678910docker tag ubuntu:15.10 115.159.227.249/library/ubuntu:15.10 //给我的镜像打个 tagdocker push 115.159.227.249/library/ubuntuThe push refers to a repository [115.159.227.249/library/ubuntu]98d59071f692: Pushedaf288f00b8a7: Pushed4b955941a4d0: Pushedf121afdbbd5d: Pushed15.10: digest: sha256:ec89c4a90f45f5e103860191890f48d8379e0504a2881ff706aef0768dc0321b size: 1150 上传完毕后，登录Web Harbor，选择项目 library，就可以看到我刚 push 的镜像了。 同理，你也可以测试下从 Harbor pull 镜像到你的 Docker 中去，这里就不继续演示了。 最后转载请注明地址为：http://www.54tianzhisheng.cn/2017/11/26/Docker-harbor/","tags":[{"name":"Docker","slug":"Docker","permalink":"http://yoursite.com/tags/Docker/"}]},{"title":"谈谈我的理财","date":"2017-11-17T16:00:00.000Z","path":"2017/11/18/Money-management/","text":"背景最开始接触理财是去年的时候，在我的一个群里（几个好朋友），有个朋友他女朋友是学金融的，当时还开玩笑地说叫她带带我们怎么买股票、基金、黄金这些东西呢。后来在群里偶尔聊下这方面的东西！ 黄金那时我第才开始接触黄金，也是自己一个人买了点支付宝里面的存金宝，（不多，就几百块），后来慢慢的加仓和减仓，刚开始的时候，因为是刚上手这些东西，比较对这每天的数字增长和降低很在意，一天打开蚂蚁聚宝的次数很多，老是看着每天的实时参考金价，反正就是心理各种不踏实。就是那种患得患失的感觉，哈哈，我也不知道怎么形容了。。。😳 那时也不懂，老是追涨低抛，没有打算长期持有。附图： 现在看看这图，想想自己以前真傻。可以发现现在已经没买黄金了，对，在今年的时候主要是关注些基金！ 基金通过买黄金，发现，黄金增长下跌确实不怎么那么尽人意，只赚了一点点。然后慢慢在关注着基金，发现有些基金的还是收入效果还是很好的。 什么是基金呢？ 好像有很多种，但是我的理解是，我们散户有点闲钱，打算投资，又没时间去买股票，一是缺乏经验，不知道买哪支，而是没空去整天盯着股票的走势。这时就出现了一个平台，我们散户把钱投给这个平台，平台有专业的人去进行买股票，如买股票有盈利，则大家一起赚钱，如果亏，则一起亏，我对基金就是这样的理解，也不知道对不对？ 再说说我现在主要买的基金吧，看看收益图： 嘿嘿，这几个是我观察很久了，并觉得长期看好的基金了，当然以前也买过一两个基金，有个亏得不少，有涨有跌，现在觉得又要平常心，如果是长期看好的，没必要纠结这一两天的涨跌，等过段时间再来看看效果咋样（心态一定要好），如果遇到被套的话，有时也需要装死心态，哈哈！另外，我还自选了一批基金，正在观察中，等有时间把觉得还行的给统计下！ 股票这个不太懂，不过目前觉得我自己公司的股票也还不错，打算看什么时候有机会买点，这东西都是靠自己慢慢研究出来的，然后就是看看高人指点，我倒是关注了点微信公众号讲这方面的知识，微博也关注了几个，在蚂蚁财富里面也关注些，觉得有些还是很靠谱的，还是一样，有时间继续做个统计，然后发在我的小密圈里面。 友金所投了这个是因为进 stormzhang 的小密圈，通过注册并投资点可以免费进他的小密圈，不然得花 199 元，这算很优惠了。因为是新用户，所以这个收益很高，一个月期限，12% 的收益！ 余额宝支付宝里的，平常钱也一般放这里面，因为平时用支付宝比较多，放这里，可以付款的时候选择直接余额宝支付，另外还有差不多 4% 的收益，可以随时存取，比较方便！ 最后有时间将上面所说的：自选基金列表、微信公众号、微博这几个列表发在我的小密圈里。 最重要的话还是得说三遍： 投资有风险，需谨慎！ 投资有风险，需谨慎！ 投资有风险，需谨慎！","tags":[{"name":"投资理财","slug":"投资理财","permalink":"http://yoursite.com/tags/投资理财/"}]},{"title":"基于分布式环境下限流系统的设计","date":"2017-11-17T16:00:00.000Z","path":"2017/11/18/flow-control/","text":"前提业务背景就拿前些天的双十一的 “抢券活动” 来说，一般是设置整点开始抢的，你想想，淘宝的用户群体非常大，可以达到亿级别，而服务接口每秒能处理的量是有限的，那么这个时候问题就会出现，我们如何通过程序来控制用户抢券呢，于是就必须加上这个限流功能了。 生产环境1、服务接口所能提供的服务上限（limit）假如是 500次/s 2、用户请求接口的次数未知，QPS可能达到 800次/s，1000次/s，或者更高 3、当服务接口的访问频率超过 500次/s，超过的量将拒绝服务，多出的信息将会丢失 4、线上环境是多节点部署的，但是调用的是同一个服务接口 于是，为了保证服务的可用性，就要对服务接口调用的速率进行限制（接口限流）。 什么是限流？限流是对系统的出入流量进行控制，防止大流量出入，导致资源不足，系统不稳定。 限流系统是对资源访问的控制组件，控制主要的两个功能：限流策略和熔断策略，对于熔断策略，不同的系统有不同的熔断策略诉求，有的系统希望直接拒绝、有的系统希望排队等待、有的系统希望服务降级、有的系统会定制自己的熔断策略，这里只针对限流策略这个功能做详细的设计。 限流算法1、限制瞬时并发数Guava RateLimiter 提供了令牌桶算法实现：平滑突发限流(SmoothBursty)和平滑预热限流(SmoothWarmingUp)实现。 2、限制某个接口的时间窗最大请求数即一个时间窗口内的请求数，如想限制某个接口/服务每秒/每分钟/每天的请求数/调用量。如一些基础服务会被很多其他系统调用，比如商品详情页服务会调用基础商品服务调用，但是怕因为更新量比较大将基础服务打挂，这时我们要对每秒/每分钟的调用量进行限速；一种实现方式如下所示： 12345678910111213141516171819LoadingCache&lt;Long, AtomicLong&gt; counter = CacheBuilder.newBuilder() .expireAfterWrite(2, TimeUnit.SECONDS) .build(new CacheLoader&lt;Long, AtomicLong&gt;() &#123; @Override public AtomicLong load(Long seconds) throws Exception &#123; return new AtomicLong(0); &#125; &#125;);long limit = 1000;while(true) &#123; //得到当前秒 long currentSeconds = System.currentTimeMillis() / 1000; if(counter.get(currentSeconds).incrementAndGet() &gt; limit) &#123; System.out.println(\"限流了:\" + currentSeconds); continue; &#125; //业务处理&#125; 使用Guava的Cache来存储计数器，过期时间设置为2秒（保证1秒内的计数器是有的），然后我们获取当前时间戳然后取秒数来作为KEY进行计数统计和限流，这种方式也是简单粗暴，刚才说的场景够用了。 3、令牌桶 算法描述： 假如用户配置的平均发送速率为r，则每隔1/r秒一个令牌被加入到桶中 假设桶中最多可以存放b个令牌。如果令牌到达时令牌桶已经满了，那么这个令牌会被丢弃 当流量以速率v进入，从桶中以速率v取令牌，拿到令牌的流量通过，拿不到令牌流量不通过，执行熔断逻辑 属性 长期来看，符合流量的速率是受到令牌添加速率的影响，被稳定为：r 因为令牌桶有一定的存储量，可以抵挡一定的流量突发情况 M是以字节/秒为单位的最大可能传输速率。 M&gt;r T max = b/(M-r) 承受最大传输速率的时间 B max = T max * M 承受最大传输速率的时间内传输的流量 优点：流量比较平滑，并且可以抵挡一定的流量突发情况 4、Google guava 提供的工具库中 RateLimiter 类（内部也是采用令牌桶算法实现）最快的方式是使用 RateLimit 类，但是这仅限制在单节点，如果是分布式系统，每个节点的 QPS 是一样的，请求量到服务接口那的话就是 QPS * 节点数 了。所以这种方案在分布式的情况下不适用！ 5、基于 Redis 实现，存储两个 key，一个用于计时，一个用于计数。请求每调用一次，计数器增加 1，若在计时器时间内计数器未超过阈值，则可以处理任务。这种能够很好地解决了分布式环境下多实例所导致的并发问题。因为使用redis设置的计时器和计数器均是全局唯一的，不管多少个节点，它们使用的都是同样的计时器和计数器，因此可以做到非常精准的流控。 代码就不公布了，毕竟涉及公司隐私了。 最后参考文章： 基于Redis的限流系统的设计 感兴趣的可以看看别人的代码是怎么写的：https://github.com/wukq/rate-limiter 转载请注明文章地址为：http://www.54tianzhisheng.cn/2017/11/18/flow-control/","tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"},{"name":"流控","slug":"流控","permalink":"http://yoursite.com/tags/流控/"}]},{"title":"送你一份双十一剁手书单【墙裂推荐】","date":"2017-11-10T16:00:00.000Z","path":"2017/11/11/recommended-books/","text":"昨晚和朋友聊天说双十一都买啥，结果 TMD 竟然都是书籍，不愧是标准的程序猿。所以这里想推荐基佬的一份书单，方便大家在双十一剁剁剁，建议先收藏本书单，认真啃完一本再买下一本，扎实走完每一步，希望书单能在你想要进一步打怪升级的路上，给予些许帮助！ 书籍列表《Effective Java 中文版》 豆瓣评分：9.1【1235 人评价】 推荐理由：本书介绍了在Java编程中78条极具实用价值的经验规则，这些经验规则涵盖了大多数开发人员每天所面临的问题的解决方案。 友情提示：同推荐《重构 : 改善既有代码的设计》、《代码整洁之道》、《代码大全》，有一定的内容重叠。 《Java性能权威指南》 豆瓣评分：8.2【44 人评价】 推荐理由：市面上介绍Java的书有很多，但专注于Java性能的并不多，能游刃有余地展示Java性能优化难点的更是凤毛麟角，本书即是其中之一。通过使用JVM和Java平台，以及Java语言和应用程序接口，本书详尽讲解了Java性能调优的相关知识，帮助读者深入理解Java平台性能的各个方面，最终使程序如虎添翼。 《Spring揭秘》 豆瓣评分：9.0 【162 人评价】 推荐理由：Spring 使用者不得不读！ 推荐博客：Spring4All社区 推荐公众号：Spring4All社区 《SpringBoot揭秘》 豆瓣评分：6.8 【44 人评价】 推荐理由：《Spring揭秘》相同作者。SpringBoot 入门书籍。 作者博客：扶墙老师说：一个架构士的思考与沉淀 作者公众号：扶墙老师说 付费教程：《Java 微服务实践 - Spring Boot 系列》 《MyBatis技术内幕》 豆瓣评分：暂无 推荐理由：以MyBatis 3.4为基础，针对MyBatis的架构设计和实现细节进行了详细分析，其中穿插介绍了MyBatis源码中涉及的基础知识、设计模式以及笔者自己在实践中的思考。 作者博客：祖大俊的博客 《有效的单元测试》 豆瓣评分：7.4 【18 人评价】 推荐理由：Java 单元测试入门。 《Java并发编程实战》 豆瓣评分：9.0 【651 人评价】 推荐理由：本书深入浅出地介绍了Java线程和并发，是一本完美的Java并发参考手册。 推荐博客：并发编程网 推荐公众号：并发编程网 《Netty实战》 豆瓣评分：7.5【24 人评价】 豆瓣评分：8.1【83 人评价】 《Netty in Action》英文版 推荐理由：Netty之父”Trustin Lee作序推荐。 推荐公众号：Netty之家 《深入剖析Tomcat》 豆瓣评分：8.3【118 人评价】 豆瓣评分：8.9【73 人评价】 《How Tomcat Works》英文版 推荐理由：本书深入剖析Tomcat 4和Tomcat 5中的每个组件，并揭示其内部工作原理。通过学习本书，你将可以自行开发Tomcat组件，或者扩展已有的组件。 《Nginx 中文官方文档》 豆瓣评分：暂无 推荐理由：暂时未找到大家评价不错的 Nginx 实战相关书籍，先推荐看中文翻译的官方文档。如果你有合适的推荐，烦请告诉我。 《深入理解Nginx》 豆瓣评分：8.5【138 人评价】 推荐理由：书中首先通过介绍官方Nginx的基本用法和配置规则，帮助读者了解一般Nginx模块的用法，然后重点介绍了如何开发HTTP模块(含HTTP过滤模块)来得到定制化的Nginx，其中包括开发—个功能复杂的模块所需要了解的各种知识，并对内存池的实现细节及TCP协议进行了详细介绍；接着，综合Nginx框架代码分析了Nginx架构的设计理念和技巧，此外，还新增了如何在模块中支持HTTP变量，以及与slab共享内存等相关的内容，相信通过完善，可进一步帮助读者更好地开发出功能丰富、性能—流的Nginx模块。 友情提示：相对适用于 Nginx 开发者。Nginx 使用者可以了解。 《深入理解Java虚拟机：JVM高级特性与最佳实践》 豆瓣评分：8.9 【657 人评价】 推荐理由：不去了解 JVM 的工程师，和咸鱼有什么区别？ 推荐公众号：你假笨 推荐博客：你假笨@JVM 推荐小程序：JVMPocket 《Java核心技术系列：Java虚拟机规范（Java SE 8版）》 豆瓣评分：暂无评价 豆瓣评分：8.3 【27 人评价】《Java虚拟机规范(Java SE 7版)》 推荐理由：基于Java SE 8,Oracle官方发布，Java虚拟机技术创建人撰写，国内Java技术专家翻译，是深度了解Java虚拟机和Java语言实现细节的必读之作 推荐博客：占小狼的简书 推荐公众号：占小狼的博客 《Go语言编程》 豆瓣评分：7.1 【444 人评价】 推荐理由：这本书从整体的写作风格来说，会以介绍 Go 语言特性为主，示例则尽量采用作者平常的实践，而不是一个没有太大实际意义的语法示范样例。 友情提示：本书作者背景极强，许式伟为原金山WPS首席架构师、曾是盛大创新院研究员，目前是国内Go语言实践圈子公认的Go语言专家。 《 Go语言学习笔记》 豆瓣评分：8.4 【57 人评价】 推荐理由：基于Go1.6， 解析语言规范，深入剖析Go运行时源码 友情提示：雨痕大大，教科书级人物。 《MySQL技术内幕——InnoDB存储引擎》 豆瓣评分：8.6 【104 人评价】 推荐理由：从源代码的角度深度解析了InnoDB的体系结构、实现原理、工作机制，并给出了大量最佳实践，能帮助你系统而深入地掌握InnoDB，更重要的是，它能为你设计管理高性能、高可用的数据库系统提供绝佳的指导。 推荐公众号：DBAplus社群 《高性能MySQL》 豆瓣评分：9.3 【245 人评价】 推荐理由：对于想要了解MySQL性能提升的人来说，这是一本不可多得的书。书中没有各种提升性能的秘籍，而是深入问题的核心，详细的解释了每种提升性能的原理，从而可以使你四两拨千斤。授之于鱼不如授之于渔，这本书做到了。 推荐公众号：老叶茶馆 《高可用MySQL》 豆瓣评分：8.0 【87 人评价】 推荐理由：《高性能MySQL》的姊妹篇。 《MongoDB权威指南》 豆瓣评分：8.0 【69 人评价】 推荐理由：算是普通的参考书了，没有特别有深度的讲解。其实就是一本正常的介绍mongoDB是怎么用的，也可以作为nosql学习的入门。作为指南书，还是很合格的符合期望。 推荐博客：MongoDB 中文社区 推荐公众号：MongoDB 中文社区 《Redis开发与运维》 豆瓣评分：8.8 【41 人评价】 推荐理由：从开发、运维两个角度总结了Redis实战经验，深入浅出地剖析底层实现，包含大规模集群开发与运维的实际案例、应用技巧。全面覆盖Redis 基本功能及应用，图示丰富，讲解细腻。 推荐博客：Redis 中国用户组 推荐公众号：CRUG 《Redis设计与实现》 豆瓣评分：8.5 【427 人评价】 推荐理由：系统而全面地描述了 Redis 内部运行机制。图示丰富，描述清晰，并给出大量参考信息，是NoSQL数据库开发人员案头必备。 《NoSQL精粹》 豆瓣评分：8.2 【226 人评价】 推荐理由：书中全方位比较了关系型数据库与NoSQL数据库的异同；分别以Riak、MongoDB、Cassandra和Neo4J为代表，详细讲解了键值数据库、文档数据库、列族数据库和图数据库这4大类NoSQL数据库的优劣势、用法和适用场合；深入探讨了实现NoSQL数据库系统的各种细节，以及与关系型数据库的混用。 《ElasticSearch 可扩展的开源弹性搜索解决方案》 豆瓣评分：7.3 【23 人评价】 推荐理由：基于ElasticSearch 的0.2 版本，覆盖了ElasticSearch 各种功能和命令的应用，全面、详细地介绍了开源、分布式、RESTful，具有全文检索功能的搜索引擎ElasticSearch。 友情提示：本书 ElasticSearch 比较旧，不忍推荐。仅适合入门，有其他合适的 ElasticSearch 书籍，烦请告诉我。《Elasticsearch权威指南》中文版，目前正在翻译中。 推荐博客：Elastic 中文社区 《ELK Stack权威指南》 豆瓣评分：7.0 【10 人评价】 推荐理由：ELK stack是以Elasticsearch、Logstash、Kibana三个开源软件为主的数据处理工具链，是目前开源界最流行的实时数据分析解决方案，成为实时日志处理领域开源界的第一选择。 《ZooKeeper：分布式过程协同技术详解》 豆瓣评分：7.6 【49 人评价】 推荐理由：Zookeeper 入门 友情提示：翻译可能略显尴尬。 《从Paxos到Zookeeper分布式一致性原理与实践》 豆瓣评分：8.1 【187 人评价】 推荐理由：从分布式一致性的理论出发，向读者简要介绍几种典型的分布式一致性协议，以及解决分布式一致性问题的思路，其中重点讲解了Paxos和ZAB协议。同时，本书深入介绍了分布式一致性问题的工业解决方案——ZooKeeper，并着重向读者展示这一分布式协调框架的使用方法、内部实现及运维技巧，旨在帮助读者全面了解ZooKeeper，并更好地使用和运维ZooKeeper。 《RabbitMQ实战：高效部署分布式消息队列》 豆瓣评分：6.9 【47 人评价】 推荐理由：本书对RabbitMQ做了全面、翔实的讲解，体现了两位专家的真知灼见。本书首先介绍了有关MQ的历史，然后从基本的消息通信原理讲起，带领读者一路探索RabbitMQ的消息通信世界。 友情提示：本书 RabbitMQ 版本较旧。消息队列中间件 RabbitMQ、ActiveMQ、RocketMQ、Kafka 可以选择了解一下。 《Apache Kafka源码剖析》 豆瓣评分：7.8 【30 人评价】 推荐理由：以Kafka 0.10.0版本源码为基础，针对Kafka的架构设计到实现细节进行详细阐述。 《作业调度系统 Quartz 中文文档》 豆瓣评分：暂无 推荐理由：暂时未找到大家评价不错的 Quartz 实战相关书籍，先推荐看中文翻译的官方文档。如果你有合适的推荐，烦请告诉我。 友情提示：国内开源项目 Elastic-Job，XXL-Job 都可以选择了解。 《微服务设计》 豆瓣评分：8.1 【273 人评价】 推荐理由：通过Netflix等多个业界案例，从微服务架构演进到原理剖析，全面讲解建模集成部署等微服务所涉及的各种主题，微服务架构与实践指南。 《Spring Cloud微服务实战》 豆瓣评分：7.9【20 人评价】 推荐理由：从时下流行的微服务架构概念出发，详细介绍了Spring Cloud针对微服务架构中几大核心要素的解决方案和基础组件。对于各个组件的介绍，主要以示例与源码结合的方式来帮助读者更好地理解这些组件的使用方法以及运行原理。同时，在介绍的过程中，还包含了作者在实践中所遇到的一些问题和解决思路，可供读者在实践中作为参考。 作者博客：http://blog.didispace.com/ 作者公众号：didispace 《亿级流量网站架构核心技术》 豆瓣评分：7.6【57 人评价】 推荐理由：总结并梳理了亿级流量网站高可用和高并发原则，通过实例详细介绍了如何落地这些原则。本书分为四部分：概述、高可用原则、高并发原则、案例实战。 作者博客：开涛的博客 作者公众号：开涛的博客 《架构即未来：现代企业可扩展的Web架构、流程和组织》 豆瓣评分：8.7【77 人评价】 推荐理由：任何一个持续成长的公司最终都需要解决系统、组织和流程的扩展性问题。本书汇聚了作者从eBay、VISA、Salesforce.com到Apple超过30年的丰富经验， 全面阐释了经过验证的信息技术扩展方法，对所需要掌握的产品和服务的平滑扩展做了详尽的论述，并在第1版的基础上更新了扩展的策略、技术和案例。 《Maven 实战》 豆瓣评分：8.1【563 人评价】 推荐理由：国内最权威的Maven专家的力作，唯一一本哦！ 《Jenkins权威指南》 豆瓣评分：暂无评分 推荐理由：Jenkins 唯一实体书。 友情提示：内容相对比较旧，大多是过时的案例。建议，快速过一遍。Jenkins 方面无特别好的选择推荐书籍。可以选择 Google 一些教程。 《鸟哥的Linux私房菜 （基础学习篇）》 豆瓣评分：9.1【2269 人评价】 推荐理由：本书是最具知名度的Linux入门书《鸟哥的Linux私房菜基础学习篇》的最新版，全面而详细地介绍了Linux操作系统。 友情提示：内容非常全面，建议挑选和自己实际工作相关度较高的，其他部分有需要再阅读。 《鸟哥的Linux私房菜 （服务器架设篇）》 豆瓣评分：8.8 【198 人评价】 推荐理由：您已有Linux基础，想要进一步学习服务器架设？还想了解如何维护与管理您的服务器？本书是您最佳的选择。 《Zabbix企业级分布式监控系统》 豆瓣评分：7.6 【39 人评价】 推荐理由：本书从运维（OPS）角度对Zabbix的各项功能进行了详细介绍，以自动化运维视角为出发点，对Zabbix的安装和配置、自动化功能、监控告警、性能调优、Zabbix API、Zabbix协议、RPM安装包定制，结合SaltStack实现自动化配置管理等内容进行了全方位的深入剖析。 《第一本Docker书》 豆瓣评分：8.8 【63 人评价】 推荐理由：本书由Docker公司前服务与支持副总裁James Turnbull编写，是Docker开发指南。本书专注于Docker 1.9及以上版本，指导读者完成Docker的安装、部署、管理和扩展，带领读者经历从测试到生产的整个开发生命周期，让读者了解Docker适用于什么场景。 推荐博客：DockerOne 推荐公众号：DockerOne 《Docker——容器与容器云》 豆瓣评分：8.5 【99 人评价】 推荐理由：本书根据Docker 1.10版和Kubernetes 1.2版对第1版进行了全面更新，从实践者的角度出发，以Docker和Kubernetes为重点，沿着“基本用法介绍”到“核心原理解读”到“高级实践技巧”的思路，一本书讲透当前主流的容器和容器云技术，有助于读者在实际场景中利用Docker容器和容器云解决问题并启发新的思考。 《Kubernetes权威指南》 豆瓣评分：7.7【15 人评价】 推荐理由：Kubernetes重磅开山之作，针对Kubernetes v1.6和本书第2版进行大篇幅内容更新，全方位完美覆盖，可借鉴性极强。 推荐博客：Kubernetes 中文社区 推荐公众号：K8S 技术社区 《用Mesos框架构建分布式应用》 豆瓣评分：暂无评分 推荐理由：超级薄的一本书，看完之后，你会对 Mesos 会非常了解，并且极大可能性学会如何基于 Mesos 框架构建分布式应用。 《数据结构与算法分析：Java语言描述》 豆瓣评分：8.3【183 人评价】 推荐理由：本书是国外数据结构与算法分析方面的经典教材，使用卓越的Java编程语言作为实现工具讨论了数据结构（组织大量数据的方法）和算法分析（对算法运行时间的估计）。 友情提示：算法方法还有其他很好的书籍，例如《算法导论》、《算法（第四版）》，也可以选择阅读。重要的是，保持耐心，享受这个痛并快乐的过程。 《Head First 设计模式》 豆瓣评分：9.2【2394 人评价】 推荐理由：《Head First设计模式》(中文版)共有14章，每章都介绍了几个设计模式，完整地涵盖了四人组版本全部23个设计模式。 《HTTP权威指南》 豆瓣评分：8.7 【1126 人评价】 推荐理由：本书尝试着将HTTP中一些互相关联且常被误解的规则梳理清楚，并编写了一系列基于各种主题的章节，对HTTP各方面的特性进行了介绍。纵观全书，对HTTP“为什么”这样做进行了详细的解释，而不仅仅停留在它是“怎么做”的。 《TCP/IP详解 系列》 豆瓣评分：9.3 【1883 人评价】 推荐理由：完整而详细的TCP/IP协议指南。针对任何希望理解TCP/IP协议是如何实现的读者设计。 《Linux内核设计与实现》 豆瓣评分：8.7【286 人评价】 详细描述了Linux内核的主要子系统和特点，包括Linux内核的设计、实现和接口。从理论到实践涵盖了Linux内核的方方面面，可以满足读者的各种兴趣和需求。 友情提示：Linux内核方面不乏好书。本书篇幅方面较为合适。 《剑指Offer：名企面试官精讲典型编程题》 豆瓣评分：8.5【508 人评价】 推荐理由：剖析了80个典型的编程面试题，系统整理基础知识、代码质量、解题思路、优化效率和综合能力这5个面试要点。 推荐网站：牛客网-专业IT笔试面试备考平台 《程序员代码面试指南：IT名企算法与数据结构题目最优解》 豆瓣评分：8.4【32 人评价】 推荐理由：程序员刷题宝典！编程能力提升秘笈！精选IT名企真实代码面试题，全面覆盖算法与数据结构题型！ 《领域驱动设计》 豆瓣评分：9.0【115 人评价】 推荐理由：是领域驱动设计方面的经典之作。全书围绕着设计和开发实践，结合若干真实的项目案例，向读者阐述如何在真实的软件开发中应用领域驱动设计。 友情提示：理论的书籍往往较为枯燥，勤修内功是必须走的路。 《火球:UML大战需求分析》 豆瓣评分：7.9【115 人评价】 推荐理由：融合UML、非UML、需求分析及需求管理等各方面的知识，帮助读者解决UML业界问题、需求分析及需求管理问题。 友情提示：可能不是最好的 UML 书籍，但从是否能够阅读理解完的角度来说，本书可能是相对合适的。有兴趣的同学也可以看看《UML和模式应用》、《大象：Thinking in UML》。 TODO List待推荐主题书籍 TODO 《大数据日知录 架构与算法》TODO 《大型网站系统与Java中间件实践》TODO 《HotSpot实战》TODO 《垃圾回收的算法与实现》TODO 《彩色UML建模》TODO 《七周七并发模型》TODO 《Go程序设计语言》 [x] Go [ ] Node [x] Linux 内核 [x] 领域 [x] UML [x] Tomcat [x] SpringCloud [x] Java 基础 [x] Netty [x] MyBatis [x] 数据库 [x] MongoDB [x] Maven [x] DevOps [x] Linux 运维 [x] 面试 [x] 消息队列 [x] 设计模式 [x] 算法与数据结构 [x] Zookeeper [x] SpringBoot [x] Nginx [x] 定时任务 [x] 搜索引擎 [x] 协议 [x] 单元测试 [x] 重构 [x] 日志 [x] Docker [x] 监控 最后原文出处 http://www.iocoder.cn/Architecture/books-recommended/ 「芋道源码」欢迎转载，保留摘要，谢谢！","tags":[{"name":"书籍","slug":"书籍","permalink":"http://yoursite.com/tags/书籍/"}]},{"title":"Maven 中 dependencies 与 dependencyManagement 的区别","date":"2017-11-10T16:00:00.000Z","path":"2017/11/11/Maven-dependencies-dependencyManagement/","text":"前提这段时间项目中遇到过了一些 Jar 包冲突的问题，很多是由于我们项目模块很多的时候，用 Maven 管理不当导致的冲突问题，本文就这个问题参考网上的资料，于是总结下 Maven 中 dependencies 与 dependencyManagement 的区别。 假设项目结构如下： parent 为父模块，抽象出来管理子项目的公共依赖，为了项目的正确运行，必须让所有的子项目使用依赖项的统一版本，必须确保应用的各个项目的依赖项和版本一致，才能保证测试的和发布的是相同的结果。 dependencyManagement在项目的 parent 层，可以通过 dependencyManagement 元素来管理 jar 包的版本，让子项目中引用一个依赖而不用显示的列出版本号。 parent 中 pom.xml 123456789101112131415161718&lt;properties&gt; &lt;version.framework&gt;1.0-SNAPSHOT&lt;/version.framework&gt; &lt;javaee-api.version&gt;1.0-SNAPSHOT&lt;/javaee-api.version&gt;&lt;/properties&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.zhisheng&lt;/groupId&gt; &lt;artifactId&gt;framework-cache&lt;/artifactId&gt; &lt;version&gt;$&#123;version.framework&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax&lt;/groupId&gt; &lt;artifactId&gt;javaee-api&lt;/artifactId&gt; &lt;version&gt;$&#123;javaee-api.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; extendion 中的 pom.xml 123456789101112131415161718192021&lt;parent&gt; &lt;artifactId&gt;parent&lt;/artifactId&gt; &lt;groupId&gt;com.zhisheng&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../parent/pom.xml&lt;/relativePath&gt;&lt;/parent&gt;&lt;!--依赖关系--&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;javax&lt;/groupId&gt; &lt;artifactId&gt;javaee-api&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.zhisheng&lt;/groupId&gt; &lt;artifactId&gt;framework-cache&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 这样做的好处：统一管理项目的版本号，确保应用的各个项目的依赖和版本一致，才能保证测试的和发布的是相同的成果，因此，在顶层 pom 中定义共同的依赖关系。同时可以避免在每个使用的子项目中都声明一个版本号，这样想升级或者切换到另一个版本时，只需要在父类容器里更新，不需要任何一个子项目的修改；如果某个子项目需要另外一个版本号时，只需要在 dependencies 中声明一个版本号即可。子类就会使用子类声明的版本号，不继承于父类版本号。 我们知道 Maven 的继承和 Java 的继承一样，是无法实现多重继承的，如果10个、20个甚至更多模块继承自同一个模块，那么按照我们之前的做法，这个父模块的 dependencyManagement 会包含大量的依赖。如果你想把这些依赖分类以更清晰的管理，那就不可能了，import scope 依赖能解决这个问题。你可以把 dependencyManagement 放到单独的专门用来管理依赖的 POM 中，然后在需要使用依赖的模块中通过 import scope 依赖，就可以引入dependencyManagement。例如可以写这样一个用于依赖管理的 POM： 12345678910111213141516171819202122&lt;project&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zhisheng.sample&lt;/groupId&gt; &lt;artifactId&gt;sample-dependency-infrastructure&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactid&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.8.2&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactid&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.16&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt;&lt;/project&gt; 然后就可以通过非继承的方式来引入这段依赖管理配置： 1234567891011121314151617181920&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.zhisheng.sample&lt;/groupId&gt; &lt;artifactid&gt;sample-dependency-infrastructure&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactid&gt;junit&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactid&gt;log4j&lt;/artifactId&gt; &lt;/dependency&gt; 这样，父模块的 POM 就会非常干净，由专门的 packaging 为 pom 的 POM 来管理依赖，也契合的面向对象设计中的单一职责原则。此外，我们还能够创建多个这样的依赖管理 POM，以更细化的方式管理依赖。这种做法与面向对象设计中使用组合而非继承也有点相似的味道。 dependencies相对于 dependencyManagement，所有声明在父项目中 dependencies 里的依赖都会被子项目自动引入，并默认被所有的子项目继承。 区别 dependencies 即使在子项目中不写该依赖项，那么子项目仍然会从父项目中继承该依赖项（全部继承） dependencyManagement 里只是声明依赖，并不实现引入，因此子项目需要显示的声明需要用的依赖。如果不在子项目中声明依赖，是不会从父项目中继承下来的；只有在子项目中写了该依赖项，并且没有指定具体版本，才会从父项目中继承该项，并且 version 和 scope 都读取自父 pom; 另外如果子项目中指定了版本号，那么会使用子项目中指定的jar版本。 消除多模块插件配置重复与 dependencyManagement 类似的，我们也可以使用 pluginManagement 元素管理插件。一个常见的用法就是我们希望项目所有模块的使用 Maven Compiler Plugin 的时候，都使用 Java 1.8，以及指定 Java 源文件编码为 UTF-8，这时可以在父模块的 POM 中如下配置 pluginManagement： 12345678910111213141516&lt;build&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt;&lt;/build&gt; 这段配置会被应用到所有子模块的 maven-compiler-plugin 中，由于 Maven 内置了 maven-compiler-plugin 与生命周期的绑定，因此子模块就不再需要任何 maven-compiler-plugin 的配置了。 与依赖配置不同的是，通常所有项目对于任意一个依赖的配置都应该是统一的，但插件却不是这样，例如你可以希望模块 A 运行所有单元测试，模块 B 要跳过一些测试，这时就需要配置 maven-surefire-plugin 来实现，那样两个模块的插件配置就不一致了。这也就是说，简单的把插件配置提取到父 POM 的 pluginManagement 中往往不适合所有情况，那我们在使用的时候就需要注意了，只有那些普适的插件配置才应该使用 pluginManagement 提取到父 POM 中。 关于插件 pluginManagement，Maven 并没有提供与 import scope 依赖类似的方式管理，那我们只能借助继承关系，不过好在一般来说插件配置的数量远没有依赖配置那么多，因此这也不是一个问题。 最后你看到的只是冰山一角，更多请看书籍《Maven 实战》 。 转载请注明地址：http://www.54tianzhisheng.cn/2017/11/11/Maven-dependencies-dependencyManagement/","tags":[{"name":"Maven","slug":"Maven","permalink":"http://yoursite.com/tags/Maven/"}]},{"title":"小白谈数据脱敏","date":"2017-10-27T16:00:00.000Z","path":"2017/10/28/Data-Desensitization/","text":"什么是数据脱敏？百度百科是这样描述的： 数据脱敏是指对某些敏感信息通过脱敏规则进行数据的变形，实现敏感隐私数据的可靠保护。在涉及客户安全数据或者一些商业性敏感数据的情况下，在不违反系统规则条件下，对真实数据进行改造并提供测试使用，如身份证号、手机号、卡号、客户姓名、客户地址、等个人敏感信息都需要通过脱敏规则进行数据的变形，实现敏感隐私数据的可靠保护。这样就可以在开发、测试和其他非生产环境以及外包环境中可以安全的使用脱敏后的真实数据集。 生活中的常见例子1、火车票： 2、淘宝网页上的收获地址信息： 敏感数据梳理在进行数据脱敏之前我们应该要确定公司的哪些数据（哪些表、哪些字段）要作为脱敏的目标，下面从用户、公司、卖家方面分析： 1、用户：名字、手机号码、身份证号码、固定电话、收货地址、电子邮箱、银行卡号、密码等 2、卖家：名字、手机号码、身份证号码、固定电话等 3、公司：交易金额、优惠券码、充值码等 确定脱敏规则确定好了公司的哪些数据要作为脱敏目标后，我们就需要制定脱敏的规则（具体的实施方法）。 常见方法： 1、替换：如统一将女性用户名替换为F，这种方法更像“障眼法”，对内部人员可以完全保持信息完整性，但易破解。 2、重排：序号12345 重排为 54321，按照一定的顺序进行打乱，很像“替换”， 可以在需要时方便还原信息，但同样易破解。 3、加密：编号 12345 加密为 23456，安全程度取决于采用哪种加密算法，一般根据实际情况而定。 4、截断：13811001111 截断为 138，舍弃必要信息来保证数据的模糊性，是比较常用的脱敏方法，但往往对生产不够友好。（丢失字段的长度） 5、掩码: 123456 -&gt; 1xxxx6，保留了部分信息，并且保证了信息的长度不变性，对信息持有者更易辨别， 如火车票上得身份信息。（常用方法） 6、日期偏移取整：20130520 12:30:45 -&gt; 20130520 12:00:00，舍弃精度来保证原始数据的安全性，一般此种方法可以保护数据的时间分布密度。 目前我的脱敏规则想法是： 1、【中文姓名】只显示第一个汉字，其他隐藏为2个星号，比如：李** 2、【身份证号】显示最后四位，其他隐藏。共计18位或者15位，比如：*************1234 3、【固定电话】 显示后四位，其他隐藏，比如：*******3241 4、【手机号码】前三位，后四位，其他隐藏，比如：135****6810 5、【地址】只显示到地区，不显示详细地址，比如：上海徐汇区漕河泾开发区*** 6、【电子邮箱】 邮箱前缀仅显示第一个字母，前缀其他隐藏，用星号代替，@及后面的地址显示，比如：d**@126.com 7、【银行卡号】前六位，后四位，其他用星号隐藏每位1个星号，比如：6222600**********1234 8、【密码】密码的全部字符都用代替，比如：* 根据以上规则进行数据脱敏！ 具体思路目前是这样的： 从原数据源查询到的生产数据 ——&gt; 数据脱敏 ——&gt; 更新到目标数据源。 原数据源、目标数据源、需要脱敏的表、字段等都放在配置文件中，做到可扩展性！ 脱敏工具代码根据以上规则已经写好了一份简单的脱敏规则工具类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119/** * 数据脱敏工具类 * Created by zhisheng_tian on 2017/10/25. */public class DesensitizedUtils &#123; /** * 【中文姓名】只显示第一个汉字，其他隐藏为2个星号，比如：李** * * @param fullName * @return */ public static String chineseName(String fullName) &#123; if (StringUtils.isBlank(fullName)) &#123; return \"\"; &#125; String name = StringUtils.left(fullName, 1); return StringUtils.rightPad(name, StringUtils.length(fullName), \"*\"); &#125; /** * 【身份证号】显示最后四位，其他隐藏。共计18位或者15位，比如：*************1234 * * @param id * @return */ public static String idCardNum(String id) &#123; if (StringUtils.isBlank(id)) &#123; return \"\"; &#125; String num = StringUtils.right(id, 4); return StringUtils.leftPad(num, StringUtils.length(id), \"*\"); &#125; /** * 【固定电话】 显示后四位，其他隐藏，比如：*******3241 * * @param num * @return */ public static String fixedPhone(String num) &#123; if (StringUtils.isBlank(num)) &#123; return \"\"; &#125; return StringUtils.leftPad(StringUtils.right(num, 4), StringUtils.length(num), \"*\"); &#125; /** * 【手机号码】前三位，后四位，其他隐藏，比如：135****6810 * * @param num * @return */ public static String mobilePhone(String num) &#123; if (StringUtils.isBlank(num)) &#123; return \"\"; &#125; return StringUtils.left(num, 3).concat(StringUtils.removeStart(StringUtils.leftPad(StringUtils.right(num, 4), StringUtils.length(num), \"*\"), \"***\")); &#125; /** * 【地址】只显示到地区，不显示详细地址，比如：上海徐汇区漕河泾开发区*** * * @param address * @param sensitiveSize 敏感信息长度 * @return */ public static String address(String address, int sensitiveSize) &#123; if (StringUtils.isBlank(address)) &#123; return \"\"; &#125; int length = StringUtils.length(address); return StringUtils.rightPad(StringUtils.left(address, length - sensitiveSize), length, \"*\"); &#125; /** * 【电子邮箱】 邮箱前缀仅显示第一个字母，前缀其他隐藏，用星号代替，@及后面的地址显示，比如：d**@126.com * * @param email * @return */ public static String email(String email) &#123; if (StringUtils.isBlank(email)) &#123; return \"\"; &#125; int index = StringUtils.indexOf(email, \"@\"); if (index &lt;= 1) return email; else return StringUtils.rightPad(StringUtils.left(email, 1), index, \"*\").concat(StringUtils.mid(email, index, StringUtils.length(email))); &#125; /** * 【银行卡号】前六位，后四位，其他用星号隐藏每位1个星号，比如：6222600**********1234 * * @param cardNum * @return */ public static String bankCard(String cardNum) &#123; if (StringUtils.isBlank(cardNum)) &#123; return \"\"; &#125; return StringUtils.left(cardNum, 6).concat(StringUtils.removeStart(StringUtils.leftPad(StringUtils.right(cardNum, 4), StringUtils.length(cardNum), \"*\"), \"******\")); &#125; /** * 【密码】密码的全部字符都用*代替，比如：****** * * @param password * @return */ public static String password(String password) &#123; if (StringUtils.isBlank(password)) &#123; return \"\"; &#125; String pwd = StringUtils.left(password, 0); return StringUtils.rightPad(pwd, StringUtils.length(password), \"*\"); &#125;&#125; 最后转载请注明地址：http://www.54tianzhisheng.cn/2017/10/28/Data-Desensitization/","tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/数据库/"}]},{"title":"HBase 集群监控","date":"2017-10-20T16:00:00.000Z","path":"2017/10/21/HBase-metrics/","text":"为什么需要监控？为了保证系统的稳定性，可靠性，可运维性。 掌控集群的核心性能指标，了解集群的性能表现。 集群出现问题时及时报警，便于运维同学及时修复问题。 集群重要指标值异常时进行预警，将问题扼杀在摇篮中，不用等集群真正不可用时才采取行动。 当集群出现问题时，监控系统可以帮助我们更快的定位问题和解决问题 如何构建 HBase 集群监控系统？公司有自己的监控系统，我们所要做的就是将 HBase 中我们关心的指标项发送到监控系统去，问题就转换为我们开发，采集并返回哪些 HBase 集群监控指标项。 HBase 集群监控指标采集的监控数据主要包括以下几个方面：某台机器 OS 层面上的数据，例如 CPU、内存、磁盘、网络、load、网络流量等；某台 regionserver（或master）机器 jvm 的状态，例如关于线程的信息，GC 的次数和时间，内存使用状况，以及 ERROR、WARN、Fatal 事件出现的次数；regionserver（或 master）进程中的统计信息。 可以通过以下地址获取 HBase 提供的 JMX 信息的 web 页面 1http://your_master:60010/jmx //所有的bean JMX web 页面的数据格式是json格式，信息很多！ OS 监控数据HBase 中对于 OS 的监控数据，主要是 OperatingSystem 的对象来进行的，如下就是我提取出来的 JSON 信息， 1234567891011121314151617181920&#123; \"name\" : \"java.lang:type=OperatingSystem\", \"modelerType\" : \"com.sun.management.UnixOperatingSystem\", \"MaxFileDescriptorCount\" : 1000000, \"OpenFileDescriptorCount\" : 413, \"CommittedVirtualMemorySize\" : 1892225024, \"FreePhysicalMemorySize\" : 284946432, \"FreeSwapSpaceSize\" : 535703552, \"ProcessCpuLoad\" : 0.0016732901066722444, \"ProcessCpuTime\" : 59306210000000, \"SystemCpuLoad\" : 0.018197029910060655, \"TotalPhysicalMemorySize\" : 16660848640, \"TotalSwapSpaceSize\" : 536862720, \"AvailableProcessors\" : 8, \"Arch\" : \"amd64\", \"SystemLoadAverage\" : 0.0, \"Name\" : \"Linux\", \"Version\" : \"2.6.32-431.11.7.el6.ucloud.x86_64\", \"ObjectName\" : \"java.lang:type=OperatingSystem\" &#125; 其中比较重要的指标有 OpenFileDescriptorCount , FreePhysicalMemorySize , ProcessCpuLoad , SystemCpuLoad , AvailableProcessors , SystemLoadAverage JVM 监控数据Hbase 中对于 JVM 的监控数据，主要是 JvmMetrics 的对象来进行的，如下就是我提取出来的 JSON 信息， 12345678910111213141516171819202122232425262728293031&#123; \"name\" : \"Hadoop:service=HBase,name=JvmMetrics\", \"modelerType\" : \"JvmMetrics\", \"tag.Context\" : \"jvm\", \"tag.ProcessName\" : \"Master\", \"tag.SessionId\" : \"\", \"tag.Hostname\" : \"uhadoop-qrljqo-master2\", \"MemNonHeapUsedM\" : 53.846107, \"MemNonHeapCommittedM\" : 85.84375, \"MemNonHeapMaxM\" : 130.0, \"MemHeapUsedM\" : 79.05823, \"MemHeapCommittedM\" : 240.125, \"MemHeapMaxM\" : 989.875, \"MemMaxM\" : 989.875, \"GcCountParNew\" : 15190, \"GcTimeMillisParNew\" : 72300, \"GcCountConcurrentMarkSweep\" : 2, \"GcTimeMillisConcurrentMarkSweep\" : 319, \"GcCount\" : 15192, \"GcTimeMillis\" : 72619, \"ThreadsNew\" : 0, \"ThreadsRunnable\" : 21, \"ThreadsBlocked\" : 0, \"ThreadsWaiting\" : 144, \"ThreadsTimedWaiting\" : 18, \"ThreadsTerminated\" : 0, \"LogFatal\" : 0, \"LogError\" : 0, \"LogWarn\" : 0, \"LogInfo\" : 0 &#125; JvmMetrics 主要统计的信息包括：内存的使用状态信息；GC的统计信息；线程的统计信息；以及事件的统计信息。 内存的统计信息主要是：JVM 当前已经使用的 NonHeapMemory 的大小、以及配置的 NonHeapMemory 的大小；JVM 当前已经使用的 HeapMemory 的大小、以及配置的 HeapMemory 的大小； JVM 运行时的可以使用的最大的内存的大小。 GC 的统计较为简单，仅统计了进程在固定间隔内 GC 的次数和花费的总时间。 线程的统计，主要是统计进程内当前线程的处于 NEW 、RUNNABLE、BLOCKED、WAITING、TIMED_WAITING、TERMINATED 这六种状态下的线程数量。 对于事件的统计，主要统计固定时间间隔内的 Fatal、Error、Warn 以及 Info 的数量。(这块好像不怎么重要) Region Servers 健康你也可以通过如下地址： 1http://your_master:60010/jmx?qry=Hadoop:service=HBase,name=Master,sub=Server 获得到 Region Servers 健康值： 123456789101112131415161718&#123; \"name\" : \"Hadoop:service=HBase,name=Master,sub=Server\", \"modelerType\" : \"Master,sub=Server\", \"tag.liveRegionServers\" : \"xxx\", \"tag.deadRegionServers\" : \"\", \"tag.zookeeperQuorum\" : \"xxx\", \"tag.serverName\" : \"xxx2,60000,1495683310213\", \"tag.clusterId\" : \"e5e044a3-ef9f-48f7-ba63-637376f5fa90\", \"tag.isActiveMaster\" : \"true\", \"tag.Context\" : \"master\", \"tag.Hostname\" : \"xxx\", \"masterActiveTime\" : 1495683312239, \"masterStartTime\" : 1495683310213, \"averageLoad\" : 143.66666666666666, \"numRegionServers\" : 3, \"numDeadRegionServers\" : 0, \"clusterRequests\" : 1297834323 &#125; MemoryPool从全部的 JSON 值中你会看到很多种 MemoryPool 值，比如 Par Eden Space 、CMS Perm Gen、Par Survivor Space、CMS Old Gen、Code Cache ，按需获取吧。 总结任何一个服务的监控系统都是一个不断迭代，不断优化的过程，不可能一开始就做到最好。监控总是比问题发生来的更早一些，而每一次出问题，又进一步加强相应方面的监控，我们需要让监控系统从出问题时才报警到可能出现问题时就预警逐渐过渡，最终让监控系统成为我们保证系统稳定性的一个有力工具。 最后监控指标有很多，但请按需获取 ! 转载文章请注明原出处，谢谢支持！ http://www.54tianzhisheng.cn/2017/10/21/HBase-metrics/ 参考资料1、hbase性能监控（一） 2、hbase性能监控（二） 3、hbase性能监控（三） 4、HBase 集群监控系统构建 5、hbase jmx常用监控指标 推荐相关文章1、ElasticSearch 单个节点监控 2、ElasticSearch 集群监控","tags":[{"name":"HBase","slug":"HBase","permalink":"http://yoursite.com/tags/HBase/"}]},{"title":"Elasticsearch 系列文章（四）：ElasticSearch 单个节点监控","date":"2017-10-17T16:00:00.000Z","path":"2017/10/18/ElasticSearch-nodes-metrics/","text":"集群健康监控是对集群信息进行高度的概括，节点统计值 API 提供了集群中每个节点的统计值。节点统计值很多，在监控的时候仍需要我们清楚哪些指标是最值得关注的。 集群健康监控可以参考这篇文章：ElasticSearch 集群监控 节点信息 Node Info :1curl -XGET &apos;http://localhost:9200/_nodes&apos; 执行上述命令可以获取所有 node 的信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071_nodes: &#123; total: 2, successful: 2, failed: 0&#125;,cluster_name: \"elasticsearch\",nodes: &#123; MSQ_CZ7mTNyOSlYIfrvHag: &#123; name: \"node0\", transport_address: \"192.168.180.110:9300\", host: \"192.168.180.110\", ip: \"192.168.180.110\", version: \"5.5.0\", build_hash: \"260387d\", total_indexing_buffer: 103887667, roles:&#123;...&#125;, settings: &#123;...&#125;, os: &#123; refresh_interval_in_millis: 1000, name: \"Linux\", arch: \"amd64\", version: \"3.10.0-229.el7.x86_64\", available_processors: 4, allocated_processors: 4 &#125;, process: &#123; refresh_interval_in_millis: 1000, id: 3022, mlockall: false &#125;, jvm: &#123; pid: 3022, version: \"1.8.0_121\", vm_name: \"Java HotSpot(TM) 64-Bit Server VM\", vm_version: \"25.121-b13\", vm_vendor: \"Oracle Corporation\", start_time_in_millis: 1507515225302, mem: &#123; heap_init_in_bytes: 1073741824, heap_max_in_bytes: 1038876672, non_heap_init_in_bytes: 2555904, non_heap_max_in_bytes: 0, direct_max_in_bytes: 1038876672 &#125;, gc_collectors: [], memory_pools: [], using_compressed_ordinary_object_pointers: \"true\", input_arguments:&#123;&#125; &#125; thread_pool:&#123; force_merge: &#123;&#125;, fetch_shard_started: &#123;&#125;, listener: &#123;&#125;, index: &#123;&#125;, refresh: &#123;&#125;, generic: &#123;&#125;, warmer: &#123;&#125;, search: &#123;&#125;, flush: &#123;&#125;, fetch_shard_store: &#123;&#125;, management: &#123;&#125;, get: &#123;&#125;, bulk: &#123;&#125;, snapshot: &#123;&#125; &#125; transport: &#123;...&#125;, http: &#123;...&#125;, plugins: [], modules: [], ingest: &#123;...&#125; &#125; 上面是我已经简写了很多数据之后的返回值，但是指标还是很多，有些是一些常规的指标，对于监控来说，没必要拿取。从上面我们可以主要关注以下这些指标: 1os, process, jvm, thread_pool, transport, http, ingest and indices 节点统计 nodes-statistics节点统计值 API 可通过如下命令获取： 1GET /_nodes/stats 得到： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158_nodes: &#123; total: 2, successful: 2, failed: 0&#125;,cluster_name: \"elasticsearch\",nodes: &#123; MSQ_CZ7mTNyOSlYI0yvHag: &#123; timestamp: 1508312932354, name: \"node0\", transport_address: \"192.168.180.110:9300\", host: \"192.168.180.110\", ip: \"192.168.180.110:9300\", roles: [], indices: &#123; docs: &#123; count: 6163666, deleted: 0 &#125;, store: &#123; size_in_bytes: 2301398179, throttle_time_in_millis: 122850 &#125;, indexing: &#123;&#125;, get: &#123;&#125;, search: &#123;&#125;, merges: &#123;&#125;, refresh: &#123;&#125;, flush: &#123;&#125;, warmer: &#123;&#125;, query_cache: &#123;&#125;, fielddata: &#123;&#125;, completion: &#123;&#125;, segments: &#123;&#125;, translog: &#123;&#125;, request_cache: &#123;&#125;, recovery: &#123;&#125; &#125;, os: &#123; timestamp: 1508312932369, cpu: &#123; percent: 0, load_average: &#123; 1m: 0.09, 5m: 0.12, 15m: 0.08 &#125; &#125;, mem: &#123; total_in_bytes: 8358301696, free_in_bytes: 1381613568, used_in_bytes: 6976688128, free_percent: 17, used_percent: 83 &#125;, swap: &#123; total_in_bytes: 8455712768, free_in_bytes: 8455299072, used_in_bytes: 413696 &#125;, cgroup: &#123; cpuacct: &#123;&#125;, cpu: &#123; control_group: \"/user.slice\", cfs_period_micros: 100000, cfs_quota_micros: -1, stat: &#123;&#125; &#125; &#125;&#125;,process: &#123; timestamp: 1508312932369, open_file_descriptors: 228, max_file_descriptors: 65536, cpu: &#123; percent: 0, total_in_millis: 2495040 &#125;, mem: &#123; total_virtual_in_bytes: 5002465280 &#125;&#125;,jvm: &#123; timestamp: 1508312932369, uptime_in_millis: 797735804, mem: &#123; heap_used_in_bytes: 318233768, heap_used_percent: 30, heap_committed_in_bytes: 1038876672, heap_max_in_bytes: 1038876672, non_heap_used_in_bytes: 102379784, non_heap_committed_in_bytes: 108773376, pools: &#123; young: &#123; used_in_bytes: 62375176, max_in_bytes: 279183360, peak_used_in_bytes: 279183360, peak_max_in_bytes: 279183360 &#125;, survivor: &#123; used_in_bytes: 175384, max_in_bytes: 34865152, peak_used_in_bytes: 34865152, peak_max_in_bytes: 34865152 &#125;, old: &#123; used_in_bytes: 255683208, max_in_bytes: 724828160, peak_used_in_bytes: 255683208, peak_max_in_bytes: 724828160 &#125; &#125; &#125;, threads: &#123;&#125;, gc: &#123;&#125;, buffer_pools: &#123;&#125;, classes: &#123;&#125;&#125;, thread_pool: &#123; bulk: &#123;&#125;, fetch_shard_started: &#123;&#125;, fetch_shard_store: &#123;&#125;, flush: &#123;&#125;, force_merge: &#123;&#125;, generic: &#123;&#125;, get: &#123;&#125;, index: &#123; threads: 1, queue: 0, active: 0, rejected: 0, largest: 1, completed: 1 &#125; listener: &#123;&#125;, management: &#123;&#125;, refresh: &#123;&#125;, search: &#123;&#125;, snapshot: &#123;&#125;, warmer: &#123;&#125; &#125;, fs: &#123;&#125;, transport: &#123; server_open: 13, rx_count: 11696, rx_size_in_bytes: 1525774, tx_count: 10282, tx_size_in_bytes: 1440101928 &#125;, http: &#123; current_open: 4, total_opened: 23 &#125;, breakers: &#123;&#125;, script: &#123;&#125;, discovery: &#123;&#125;, ingest: &#123;&#125;&#125; 节点名是一个 UUID，上面列举了很多指标，下面讲解下： 索引部分 indices这部分列出了这个节点上所有索引的聚合过的统计值 ： docs 展示节点内存有多少文档，包括还没有从段里清除的已删除文档数量。 store 部分显示节点耗用了多少物理存储。这个指标包括主分片和副本分片在内。如果限流时间很大，那可能表明你的磁盘限流设置得过低。 indexing 显示已经索引了多少文档。这个值是一个累加计数器。在文档被删除的时候，数值不会下降。还要注意的是，在发生内部 索引操作的时候，这个值也会增加，比如说文档更新。 还列出了索引操作耗费的时间，正在索引的文档数量，以及删除操作的类似统计值。 get 显示通过 ID 获取文档的接口相关的统计值。包括对单个文档的 GET 和 HEAD 请求。 search 描述在活跃中的搜索（ open_contexts ）数量、查询的总数量、以及自节点启动以来在查询上消耗的总时间。用 query_time_in_millis / query_total 计算的比值，可以用来粗略的评价你的查询有多高效。比值越大，每个查询花费的时间越多，你应该要考虑调优了。 fetch 统计值展示了查询处理的后一半流程（query-then-fetch 里的 fetch ）。如果 fetch 耗时比 query 还多，说明磁盘较慢，或者获取了太多文档，或者可能搜索请求设置了太大的分页（比如， size: 10000 ）。 merges 包括了 Lucene 段合并相关的信息。它会告诉你目前在运行几个合并，合并涉及的文档数量，正在合并的段的总大小，以及在合并操作上消耗的总时间。 filter_cache 展示了已缓存的过滤器位集合所用的内存数量，以及过滤器被驱逐出内存的次数。过多的驱逐数 可能 说明你需要加大过滤器缓存的大小，或者你的过滤器不太适合缓存（比如它们因为高基数而在大量产生，就像是缓存一个 now 时间表达式）。 不过，驱逐数是一个很难评定的指标。过滤器是在每个段的基础上缓存的，而从一个小的段里驱逐过滤器，代价比从一个大的段里要廉价的多。有可能你有很大的驱逐数，但是它们都发生在小段上，也就意味着这些对查询性能只有很小的影响。 把驱逐数指标作为一个粗略的参考。如果你看到数字很大，检查一下你的过滤器，确保他们都是正常缓存的。不断驱逐着的过滤器，哪怕都发生在很小的段上，效果也比正确缓存住了的过滤器差很多。 field_data 显示 fielddata 使用的内存， 用以聚合、排序等等。这里也有一个驱逐计数。和 filter_cache 不同的是，这里的驱逐计数是很有用的：这个数应该或者至少是接近于 0。因为 fielddata 不是缓存，任何驱逐都消耗巨大，应该避免掉。如果你在这里看到驱逐数，你需要重新评估你的内存情况，fielddata 限制，请求语句，或者这三者。 segments 会展示这个节点目前正在服务中的 Lucene 段的数量。 这是一个重要的数字。大多数索引会有大概 50–150 个段，哪怕它们存有 TB 级别的数十亿条文档。段数量过大表明合并出现了问题（比如，合并速度跟不上段的创建）。注意这个统计值是节点上所有索引的汇聚总数。记住这点。 memory 统计值展示了 Lucene 段自己用掉的内存大小。 这里包括底层数据结构，比如倒排表，字典，和布隆过滤器等。太大的段数量会增加这些数据结构带来的开销，这个内存使用量就是一个方便用来衡量开销的度量值。 操作系统和进程部分OS 和 Process 部分基本是自描述的，不会在细节中展开讲解。它们列出来基础的资源统计值，比如 CPU 和负载。OS 部分描述了整个操作系统，而 Process 部分只显示 Elasticsearch 的 JVM 进程使用的资源情况。 这些都是非常有用的指标，不过通常在你的监控技术栈里已经都测量好了。统计值包括下面这些： CPU 负载 内存使用率 （mem.used_percent） Swap 使用率 打开的文件描述符 （open_file_descriptors） JVM 部分jvm 部分包括了运行 Elasticsearch 的 JVM 进程一些很关键的信息。 最重要的，它包括了垃圾回收的细节，这对你的 Elasticsearch 集群的稳定性有着重大影响。 123456789101112jvm: &#123; timestamp: 1508312932369, uptime_in_millis: 797735804, mem: &#123; heap_used_in_bytes: 318233768, heap_used_percent: 30, heap_committed_in_bytes: 1038876672, heap_max_in_bytes: 1038876672, non_heap_used_in_bytes: 102379784, non_heap_committed_in_bytes: 108773376, &#125;&#125; jvm 部分首先列出一些和 heap 内存使用有关的常见统计值。你可以看到有多少 heap 被使用了，多少被指派了（当前被分配给进程的），以及 heap 被允许分配的最大值。理想情况下，heap_committed_in_bytes 应该等于 heap_max_in_bytes 。如果指派的大小更小，JVM 最终会被迫调整 heap 大小——这是一个非常昂贵的操作。如果你的数字不相等，阅读 堆内存:大小和交换 学习如何正确的配置它。 heap_used_percent 指标是值得关注的一个数字。Elasticsearch 被配置为当 heap 达到 75% 的时候开始 GC。如果你的节点一直 &gt;= 75%，你的节点正处于 内存压力 状态。这是个危险信号，不远的未来可能就有慢 GC 要出现了。 如果 heap 使用率一直 &gt;=85%，你就麻烦了。Heap 在 90–95% 之间，则面临可怕的性能风险，此时最好的情况是长达 10–30s 的 GC，最差的情况就是内存溢出（OOM）异常。 线程池部分Elasticsearch 在内部维护了线程池。 这些线程池相互协作完成任务，有必要的话相互间还会传递任务。通常来说，你不需要配置或者调优线程池，不过查看它们的统计值有时候还是有用的，可以洞察你的集群表现如何。 每个线程池会列出已配置的线程数量（ threads ），当前在处理任务的线程数量（ active ），以及在队列中等待处理的任务单元数量（ queue ）。 如果队列中任务单元数达到了极限，新的任务单元会开始被拒绝，你会在 rejected 统计值上看到它反映出来。这通常是你的集群在某些资源上碰到瓶颈的信号。因为队列满意味着你的节点或集群在用最高速度运行，但依然跟不上工作的蜂拥而入。 这里的一系列的线程池，大多数你可以忽略，但是有一小部分还是值得关注的： indexing 普通的索引请求的线程池 bulk 批量请求，和单条的索引请求不同的线程池 get Get-by-ID 操作 search 所有的搜索和查询请求 merging 专用于管理 Lucene 合并的线程池 网络部分 transport 显示和 传输地址 相关的一些基础统计值。包括节点间的通信（通常是 9300 端口）以及任意传输客户端或者节点客户端的连接。如果看到这里有很多连接数不要担心；Elasticsearch 在节点之间维护了大量的连接。 http 显示 HTTP 端口（通常是 9200）的统计值。如果你看到 total_opened 数很大而且还在一直上涨，这是一个明确信号，说明你的 HTTP 客户端里有没启用 keep-alive 长连接的。持续的 keep-alive 长连接对性能很重要，因为连接、断开套接字是很昂贵的（而且浪费文件描述符）。请确认你的客户端都配置正确。 参考资料1、nodes-info 2、nodes-stats 3、ES监控指标 最后：转载请注明地址：http://www.54tianzhisheng.cn/2017/10/18/ElasticSearch-nodes-metrics/","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://yoursite.com/tags/ElasticSearch/"}]},{"title":"Elasticsearch 系列文章（三）：ElasticSearch 集群监控","date":"2017-10-14T16:00:00.000Z","path":"2017/10/15/ElasticSearch-cluster-health-metrics/","text":"最近在做 ElasticSearch 的信息（集群和节点）监控，特此稍微整理下学到的东西。这篇文章主要介绍集群的监控。 要监控哪些 ElasticSearch metrics Elasticsearch 提供了大量的 Metric，可以帮助您检测到问题的迹象，在遇到节点不可用、out-of-memory、long garbage collection times 的时候采取相应措施。但是指标太多了，有时我们并不需要这么多，这就需要我们进行筛选。 集群健康一个 Elasticsearch 集群至少包括一个节点和一个索引。或者它 可能有一百个数据节点、三个单独的主节点，以及一小打客户端节点——这些共同操作一千个索引（以及上万个分片）。 不管集群扩展到多大规模，你都会想要一个快速获取集群状态的途径。Cluster Health API 充当的就是这个角色。你可以把它想象成是在一万英尺的高度鸟瞰集群。它可以告诉你安心吧一切都好，或者警告你集群某个地方有问题。 让我们执行一下 cluster-health API 然后看看响应体是什么样子的： 1GET _cluster/health 和 Elasticsearch 里其他 API 一样，cluster-health 会返回一个 JSON 响应。这对自动化和告警系统来说，非常便于解析。响应中包含了和你集群有关的一些关键信息： 123456789101112&#123; \"cluster_name\": \"elasticsearch_zach\", \"status\": \"green\", \"timed_out\": false, \"number_of_nodes\": 1, \"number_of_data_nodes\": 1, \"active_primary_shards\": 10, \"active_shards\": 10, \"relocating_shards\": 0, \"initializing_shards\": 0, \"unassigned_shards\": 0&#125; 响应信息中最重要的一块就是 status 字段。状态可能是下列三个值之一 : status 含义 green 所有的主分片和副本分片都已分配。你的集群是 100% 可用的。 yellow 所有的主分片已经分片了，但至少还有一个副本是缺失的。不会有数据丢失，所以搜索结果依然是完整的。不过，你的高可用性在某种程度上被弱化。如果 更多的 分片消失，你就会丢数据了。把 yellow 想象成一个需要及时调查的警告。 red 至少一个主分片（以及它的全部副本）都在缺失中。这意味着你在缺少数据：搜索只能返回部分数据，而分配到这个分片上的写入请求会返回一个异常。 number_of_nodes 和 number_of_data_nodes 这个命名完全是自描述的。 active_primary_shards 指出你集群中的主分片数量。这是涵盖了所有索引的汇总值。 active_shards 是涵盖了所有索引的所有分片的汇总值，即包括副本分片。 relocating_shards 显示当前正在从一个节点迁往其他节点的分片的数量。通常来说应该是 0，不过在 Elasticsearch 发现集群不太均衡时，该值会上涨。比如说：添加了一个新节点，或者下线了一个节点。 initializing_shards 是刚刚创建的分片的个数。比如，当你刚创建第一个索引，分片都会短暂的处于 initializing 状态。这通常会是一个临时事件，分片不应该长期停留在 initializing状态。你还可能在节点刚重启的时候看到 initializing 分片：当分片从磁盘上加载后，它们会从initializing 状态开始。 unassigned_shards 是已经在集群状态中存在的分片，但是实际在集群里又找不着。通常未分配分片的来源是未分配的副本。比如，一个有 5 分片和 1 副本的索引，在单节点集群上，就会有 5 个未分配副本分片。如果你的集群是 red 状态，也会长期保有未分配分片（因为缺少主分片）。 集群统计集群统计信息包含 集群的分片数，文档数，存储空间，缓存信息，内存使用率，插件内容，文件系统内容，JVM 作用状况，系统 CPU，OS 信息，段信息。 查看全部统计信息命令： 1curl -XGET &apos;http://localhost:9200/_cluster/stats?human&amp;pretty&apos; 返回 JSON 结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177&#123; \"timestamp\": 1459427693515, \"cluster_name\": \"elasticsearch\", \"status\": \"green\", \"indices\": &#123; \"count\": 2, \"shards\": &#123; \"total\": 10, \"primaries\": 10, \"replication\": 0, \"index\": &#123; \"shards\": &#123; \"min\": 5, \"max\": 5, \"avg\": 5 &#125;, \"primaries\": &#123; \"min\": 5, \"max\": 5, \"avg\": 5 &#125;, \"replication\": &#123; \"min\": 0, \"max\": 0, \"avg\": 0 &#125; &#125; &#125;, \"docs\": &#123; \"count\": 10, \"deleted\": 0 &#125;, \"store\": &#123; \"size\": \"16.2kb\", \"size_in_bytes\": 16684, \"throttle_time\": \"0s\", \"throttle_time_in_millis\": 0 &#125;, \"fielddata\": &#123; \"memory_size\": \"0b\", \"memory_size_in_bytes\": 0, \"evictions\": 0 &#125;, \"query_cache\": &#123; \"memory_size\": \"0b\", \"memory_size_in_bytes\": 0, \"total_count\": 0, \"hit_count\": 0, \"miss_count\": 0, \"cache_size\": 0, \"cache_count\": 0, \"evictions\": 0 &#125;, \"completion\": &#123; \"size\": \"0b\", \"size_in_bytes\": 0 &#125;, \"segments\": &#123; \"count\": 4, \"memory\": \"8.6kb\", \"memory_in_bytes\": 8898, \"terms_memory\": \"6.3kb\", \"terms_memory_in_bytes\": 6522, \"stored_fields_memory\": \"1.2kb\", \"stored_fields_memory_in_bytes\": 1248, \"term_vectors_memory\": \"0b\", \"term_vectors_memory_in_bytes\": 0, \"norms_memory\": \"384b\", \"norms_memory_in_bytes\": 384, \"doc_values_memory\": \"744b\", \"doc_values_memory_in_bytes\": 744, \"index_writer_memory\": \"0b\", \"index_writer_memory_in_bytes\": 0, \"version_map_memory\": \"0b\", \"version_map_memory_in_bytes\": 0, \"fixed_bit_set\": \"0b\", \"fixed_bit_set_memory_in_bytes\": 0, \"file_sizes\": &#123;&#125; &#125;, \"percolator\": &#123; \"num_queries\": 0 &#125; &#125;, \"nodes\": &#123; \"count\": &#123; \"total\": 1, \"data\": 1, \"coordinating_only\": 0, \"master\": 1, \"ingest\": 1 &#125;, \"versions\": [ \"5.6.3\" ], \"os\": &#123; \"available_processors\": 8, \"allocated_processors\": 8, \"names\": [ &#123; \"name\": \"Mac OS X\", \"count\": 1 &#125; ], \"mem\" : &#123; \"total\" : \"16gb\", \"total_in_bytes\" : 17179869184, \"free\" : \"78.1mb\", \"free_in_bytes\" : 81960960, \"used\" : \"15.9gb\", \"used_in_bytes\" : 17097908224, \"free_percent\" : 0, \"used_percent\" : 100 &#125; &#125;, \"process\": &#123; \"cpu\": &#123; \"percent\": 9 &#125;, \"open_file_descriptors\": &#123; \"min\": 268, \"max\": 268, \"avg\": 268 &#125; &#125;, \"jvm\": &#123; \"max_uptime\": \"13.7s\", \"max_uptime_in_millis\": 13737, \"versions\": [ &#123; \"version\": \"1.8.0_74\", \"vm_name\": \"Java HotSpot(TM) 64-Bit Server VM\", \"vm_version\": \"25.74-b02\", \"vm_vendor\": \"Oracle Corporation\", \"count\": 1 &#125; ], \"mem\": &#123; \"heap_used\": \"57.5mb\", \"heap_used_in_bytes\": 60312664, \"heap_max\": \"989.8mb\", \"heap_max_in_bytes\": 1037959168 &#125;, \"threads\": 90 &#125;, \"fs\": &#123; \"total\": \"200.6gb\", \"total_in_bytes\": 215429193728, \"free\": \"32.6gb\", \"free_in_bytes\": 35064553472, \"available\": \"32.4gb\", \"available_in_bytes\": 34802409472 &#125;, \"plugins\": [ &#123; \"name\": \"analysis-icu\", \"version\": \"5.6.3\", \"description\": \"The ICU Analysis plugin integrates Lucene ICU module into elasticsearch, adding ICU relates analysis components.\", \"classname\": \"org.elasticsearch.plugin.analysis.icu.AnalysisICUPlugin\", \"has_native_controller\": false &#125;, &#123; \"name\": \"ingest-geoip\", \"version\": \"5.6.3\", \"description\": \"Ingest processor that uses looksup geo data based on ip adresses using the Maxmind geo database\", \"classname\": \"org.elasticsearch.ingest.geoip.IngestGeoIpPlugin\", \"has_native_controller\": false &#125;, &#123; \"name\": \"ingest-user-agent\", \"version\": \"5.6.3\", \"description\": \"Ingest processor that extracts information from a user agent\", \"classname\": \"org.elasticsearch.ingest.useragent.IngestUserAgentPlugin\", \"has_native_controller\": false &#125; ] &#125;&#125; 内存使用和 GC 指标在运行 Elasticsearch 时，内存是您要密切监控的关键资源之一。 Elasticsearch 和 Lucene 以两种方式利用节点上的所有可用 RAM：JVM heap 和文件系统缓存。 Elasticsearch 运行在Java虚拟机（JVM）中，这意味着JVM垃圾回收的持续时间和频率将成为其他重要的监控领域。 上面返回的 JSON，监控的指标有我个人觉得有这些： nodes.successful nodes.failed nodes.total nodes.mem.used_percent nodes.process.cpu.percent nodes.jvm.mem.heap_used 可以看到 JSON 文件是很复杂的，如果从这复杂的 JSON 中获取到对应的指标（key）的值呢，这里请看文章 ：JsonPath —— JSON 解析神器 最后这里主要讲下 ES 集群的一些监控信息，有些监控指标是个人觉得需要监控的，但是具体情况还是得看需求了。下篇文章主要讲节点的监控信息。转载请注明地址：http://www.54tianzhisheng.cn/2017/10/15/ElasticSearch-cluster-health-metrics/ 参考资料1、How to monitor Elasticsearch performance 2、ElasticSearch 性能监控 3、cluster-health 4、cluster-stats 相关阅读1、Elasticsearch 默认分词器和中分分词器之间的比较及使用方法 2、全文搜索引擎 Elasticsearch 集群搭建入门教程","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://yoursite.com/tags/ElasticSearch/"}]},{"title":"JsonPath —— JSON 解析神器","date":"2017-10-13T16:00:00.000Z","path":"2017/10/14/JsonPath/","text":"真乃神器也，再复杂的 Json 都能给你解析出来，非常方便的获取 JSON 的内容，很强大！语法简介 JsonPath 描述 $ 根节点 @ 当前节点 .or[] 子节点 .. 选择所有符合条件的节点 * 所有节点 [] 迭代器标示，如数组下标 [,] 支持迭代器中做多选 [start:end:step] 数组切片运算符 ?() 支持过滤操作 () 支持表达式计算 JSON 值： 1234567891011121314151617181920&#123; \"store\": &#123; \"book\": [ &#123; \"category\": \"reference\", \"author\": \"Nigel Rees\", \"title\": \"Sayings of the Century\", \"price\": 8.95 &#125;, &#123; \"category\": \"fiction\", \"author\": \"Evelyn Waugh\", \"title\": \"Sword of Honour\", \"price\": 12.99, \"isbn\": \"0-553-21311-3\" &#125; ], \"bicycle\": &#123; \"color\": \"red\", \"price\": 19.95 &#125; &#125;&#125; 导包：import com.jayway.jsonpath.JsonPath 解析代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344//输出book[0]的author值String author = JsonPath.read(json, \"$.store.book[0].author\");System.out.println(\"author\\t\"+author);//输出全部author的值，使用Iterator迭代List&lt;String&gt; authors = JsonPath.read(json, \"$.store.book[*].author\");System.out.println(\"authors\\t\"+authors);//输出book[*]中category == 'reference'的bookList&lt;Object&gt; books = JsonPath.read(json, \"$.store.book[?(@.category == 'reference')]\");System.out.println(\"books\\t\"+books);//输出book[*]中category == 'reference'的book或者List&lt;Object&gt; books2 = JsonPath.read(json, \"$.store.book[?(@.category == 'reference' || @.price&gt;10)]\");System.out.println(\"books2\\t\"+books2);//输出book[*]中category == 'reference'的book的authorList&lt;Object&gt; books1 = JsonPath.read(json, \"$.store.book[?(@.category == 'reference')].author\");System.out.println(\"books1\\t\"+books1);//输出book[*]中price&gt;10的bookList&lt;Object&gt; b1 = JsonPath.read(json, \"$.store.book[?(@.price&gt;10)]\");System.out.println(\"b1\"+b1);//输出book[*]中含有isbn元素的bookList&lt;Object&gt; b2 = JsonPath.read(json, \"$.store.book[?(@.isbn)]\");System.out.println(\"b2\"+b2);//输出该json中所有price的值List&lt;Double&gt; prices = JsonPath.read(json, \"$..price\");System.out.println(\"prices\"+prices);//输出该json中所有title的值List&lt;Double&gt; title = JsonPath.read(json, \"$..title\");System.out.println(\"title\"+title);//输出该json中book 0,1的值List&lt;Double&gt; book01 = JsonPath.read(json, \"$..book[0,1]\");System.out.println(\"book01\"+book01);/* //输出该json中book 0,1的值List&lt;Double&gt; book012 = JsonPath.read(json, \"$..book[-2:]\");System.out.println(\"book012\"+book012);*///可以提前编辑一个路径，并多次使用它JsonPath path = JsonPath.compile(\"$.store.book[*]\");List&lt;Object&gt; b3 = path.read(json);System.out.println(\"path\\t\"+path+\"\\n\"+b3); 用法比较简单，多使用几次就会使用了！文章主要参考网上！原谅很多天不跟博的我现在竟然这样水了这么一篇文章，哈哈！实在是忙！","tags":[{"name":"JSON","slug":"JSON","permalink":"http://yoursite.com/tags/JSON/"}]},{"title":"Centos7 搭建最新 Nexus3 Maven 私服","date":"2017-10-13T16:00:00.000Z","path":"2017/10/14/Nexus3-Maven/","text":"Maven 介绍Apache Maven 是一个创新的软件项目管理和综合工具。Maven 提供了一个基于项目对象模型（POM）文件的新概念来管理项目的构建，可以从一个中心资料片管理项目构建，报告和文件。Maven 最强大的功能就是能够自动下载项目依赖库。Maven 提供了开发人员构建一个完整的生命周期框架。开发团队可以自动完成项目的基础工具建设，Maven 使用标准的目录结构和默认构建生命周期。在多个开发团队环境时，Maven 可以设置按标准在非常短的时间里完成配置工作。由于大部分项目的设置都很简单，并且可重复使用，Maven 让开发人员的工作更轻松，同时创建报表，检查，构建和测试自动化设置。Maven 项目的结构和内容在一个 XML 文件中声明，pom.xml 项目对象模型（POM），这是整个 Maven 系统的基本单元。 Maven 提供了开发人员的方式来管理：1）Builds2）Documentation3）Reporting4）Dependencies5）SCMs6）Releases7）Distribution8）mailing list概括地说，Maven 简化和标准化项目建设过程。处理编译，分配，文档，团队协作和其他任务的无缝连接。Maven 增加可重用性并负责建立相关的任务。Maven 最初设计，是以简化 Jakarta Turbine 项目的建设。在几个项目，每个项目包含了不同的 Ant 构建文件。 JAR 检查到 CVS。Apache 组织开发 Maven 可以建立多个项目，发布项目信息，项目部署，在几个项目中 JAR 文件提供团队合作和帮助。 Maven 主要目标是提供给开发人员：1）项目是可重复使用，易维护，更容易理解的一个综合模型。2）插件或交互的工具，这种声明性的模式。 私服介绍私服是指私有服务器，是架设在局域网的一种特殊的远程仓库，目的是代理远程仓库及部署第三方构建。有了私服之后，当 Maven 需要下载构件时，直接请求私服，私服上存在则下载到本地仓库；否则，私服请求外部的远程仓库，将构件下载到私服，再提供给本地仓库下载。 Nexus 介绍Nexus 是一个强大的 Maven 仓库管理器，它极大地简化了本地内部仓库的维护和外部仓库的访问。如果使用了公共的 Maven 仓库服务器，可以从 Maven 中央仓库下载所需要的构件（Artifact），但这通常不是一个好的做法。正常做法是在本地架设一个 Maven 仓库服务器，即利用 Nexus 私服可以只在一个地方就能够完全控制访问和部署在你所维护仓库中的每个 Artifact。Nexus 在代理远程仓库的同时维护本地仓库，以降低中央仓库的负荷, 节省外网带宽和时间，Nexus 私服就可以满足这样的需要。Nexus 是一套 “开箱即用” 的系统不需要数据库，它使用文件系统加 Lucene 来组织数据。Nexus 使用 ExtJS 来开发界面，利用 Restlet 来提供完整的 REST APIs，通过 m2eclipse 与 Eclipse 集成使用。Nexus 支持 WebDAV 与 LDAP 安全身份认证。Nexus 还提供了强大的仓库管理功能，构件搜索功能，它基于 REST，友好的 UI 是一个 extjs 的 REST 客户端，它占用较少的内存，基于简单文件系统而非数据库。 为什么要构建 Nexus 私服？如果没有 Nexus 私服，我们所需的所有构件都需要通过 maven 的中央仓库和第三方的 Maven 仓库下载到本地，而一个团队中的所有人都重复的从 maven 仓库下载构件无疑加大了仓库的负载和浪费了外网带宽，如果网速慢的话，还会影响项目的进程。很多情况下项目的开发都是在内网进行的，连接不到 maven 仓库怎么办呢？开发的公共构件怎么让其它项目使用？这个时候我们不得不为自己的团队搭建属于自己的 maven 私服，这样既节省了网络带宽也会加速项目搭建的进程，当然前提条件就是你的私服中拥有项目所需的所有构件。 总之，在本地构建 nexus 私服的好处有：1）加速构建；2）节省带宽；3）节省中央 maven 仓库的带宽；4）稳定（应付一旦中央服务器出问题的情况）；5）控制和审计；6）能够部署第三方构件；7）可以建立本地内部仓库；8）可以建立公共仓库这些优点使得 Nexus 日趋成为最流行的 Maven 仓库管理器。 1. 安装 jdk1.8关于 jdk1.8 的安装, 在这里就不做赘述了 2. 安装 maven关于 maven 的安装, 本文在这里就不详细写了 3. 安装 nexus31. 下载 nexus-3.6.0-02-unix.tar.gz官网链接地址：https://www.sonatype.com/download-oss-sonatype 下载 linux 最新版本，直接下载速度可能很慢，建议用迅雷下载会快很多的。 2. 解压1tar -zxvf nexus-3.6.0-02-unix.tar.gz -C /usr/local/ 3. 启动 nexus312cd /usr/local/nexus-3.6.0-02/bin/./nexus run &amp; 稍等一会 (首次启动会比较慢), 当出现以下日志的时候表示启动成功! 12345-------------------------------------------------Started Sonatype Nexus OSS 3.6.0-02------------------------------------------------- 4. 开启远程访问端口关闭防火墙，并开启远程访问端口 8081 123vim /etc/sysconfig/iptables添加：-A INPUT -p tcp -m state --state NEW -m tcp --dport 8081 -j ACCEPT 5. 测试 123nexus3默认端口是:8081nexus3默认账号是:adminnexus3默认密码是:admin123 6. 设置开机自启动123ln -s /usr/local/nexus-3.6.0-02/bin/nexus /etc/init.d/nexus3chkconfig --add nexus3chkconfig nexus3 on 7. 修改 nexus3 的运行用户为 root1vim nexus.rc 12//设置run_as_user=&quot;root&quot; 8. 修改 nexus3 启动时要使用的 jdk 版本1vim nexus 第 14 行: 1INSTALL4J_JAVA_HOME_OVERRIDE=/usr/local/java/jdk1.8.0_144 9. 修改 nexus3 默认端口 (可选)12cd /usr/local/nexus-3.6.0-02/etc/vim nexus-default.properties 默认端口: 8081 1application-port=8081 10. 修改 nexus3 数据以及相关日志的存储位置 (可选)：12[root@MiWiFi-R3-srv bin]# cd /usr/local/nexus-3.6.0-02/bin/[root@MiWiFi-R3-srv bin]# vim nexus.vmoptions 123-XX:LogFile=./sonatype-work/nexus3/log/jvm.log-Dkaraf.data=./sonatype-work/nexus3-Djava.io.tmpdir=./sonatype-work/nexus3/tmp 出现上面 5 中的测试页面，说明配置 nexus 成功！ 点击右上角 “Log in”， 输入默认用户名 (admin) 和默认密码（admin123）登录 至此, nexus3_maven 的私服就搭建完成了!!! 可以点击上面的 “设置” 图标，在 “设置” 里可以添加用户、角色，对接 LDAP 等的设置，如下： 可以在 “管理” 里查看 nexus 的系统信息 注意：1.component name 的一些说明： 1）maven-central：maven 中央库，默认从 https://repo1.maven.org/maven2 / 拉取 jar 2）maven-releases：私库发行版 jar 3）maven-snapshots：私库快照（调试版本）jar 4）maven-public：仓库分组，把上面三个仓库组合在一起对外提供服务，在本地 maven 基础配置 settings.xml 中使用。 2.Nexus 默认的仓库类型有以下四种： 1）group(仓库组类型)：又叫组仓库，用于方便开发人员自己设定的仓库； 2）hosted(宿主类型)：内部项目的发布仓库（内部开发人员，发布上去存放的仓库）； 3）proxy(代理类型)：从远程中央仓库中寻找数据的仓库（可以点击对应的仓库的 Configuration 页签下 Remote Storage Location 属性的值即被代理的远程仓库的路径）； 4）virtual(虚拟类型)：虚拟仓库（这个基本用不到，重点关注上面三个仓库的使用）； 3.Policy(策略): 表示该仓库为发布 (Release) 版本仓库还是快照 (Snapshot) 版本仓库； 4.Public Repositories 下的仓库 1）3rd party: 无法从公共仓库获得的第三方发布版本的构件仓库，即第三方依赖的仓库，这个数据通常是由内部人员自行下载之后发布上去； 2）Apache Snapshots: 用了代理 ApacheMaven 仓库快照版本的构件仓库 3）Central: 用来代理 maven 中央仓库中发布版本构件的仓库 4）Central M1 shadow: 用于提供中央仓库中 M1 格式的发布版本的构件镜像仓库 5）Codehaus Snapshots: 用来代理 CodehausMaven 仓库的快照版本构件的仓库 6）Releases: 内部的模块中 release 模块的发布仓库，用来部署管理内部的发布版本构件的宿主类型仓库；release 是发布版本； 7）Snapshots: 发布内部的 SNAPSHOT 模块的仓库，用来部署管理内部的快照版本构件的宿主类型仓库；snapshots 是快照版本，也就是不稳定版本所以自定义构建的仓库组代理仓库的顺序为：Releases，Snapshots，3rd party，Central。也可以使用 oschina 放到 Central 前面，下载包会更快。 5.Nexus 默认的端口是 8081，可以在 etc/nexus-default.properties 配置中修改。 6.Nexus 默认的用户名密码是 admin/admin123 当遇到奇怪问题时，重启 nexus，重启后 web 界面要 1 分钟左右后才能访问。 8.Nexus 的工作目录是 sonatype-work（路径一般在 nexus 同级目录下）12345678[root@master-node local]# pwd/usr/local[root@master-node local]# ls nexus/bin deploy etc lib LICENSE.txt NOTICE.txt public system[root@master-node local]# ls sonatype-work/nexus3[root@master-node local]# ls sonatype-work/nexus3/backup blobs cache db elasticsearch etc generated-bundles health-check instances keystores lock log orient port tmp Nexus 仓库分类的概念：1）Maven 可直接从宿主仓库下载构件, 也可以从代理仓库下载构件, 而代理仓库间接的从远程仓库下载并缓存构件2）为了方便, Maven 可以从仓库组下载构件, 而仓库组并没有时间的内容 (下图中用虚线表示, 它会转向包含的宿主仓库或者代理仓库获得实际构件的内容). Nexus 的 web 界面功能介绍1.Browse Server Content 1.1 Search这个就是类似 Maven 仓库上的搜索功能，就是从私服上查找是否有哪些包。注意：1）在 Search 这级是支持模糊搜索的，如图所示： 2）如果进入具体的目录，好像不支持模糊搜索，如图所示： 1.2 Browse 1）Assets这是能看到所有的资源，包含 Jar，已经对 Jar 的一些描述信息。2）Components这里只能看到 Jar 包。 2.Server Adminstration And configuration看到这个选项的前提是要进行登录的，如上面已经介绍登陆方法，右上角点击 “Sign In” 的登录按钮，输入 admin/admin123, 登录成功之后，即可看到此功能，如图所示： 2.1 Blob Stores文件存储的地方，创建一个目录的话，对应文件系统的一个目录，如图所示： 2.2 Repositories 1）Proxy这里就是代理的意思，代理中央 Maven 仓库，当 PC 访问中央库的时候，先通过 Proxy 下载到 Nexus 仓库，然后再从 Nexus 仓库下载到 PC 本地。这样的优势只要其中一个人从中央库下来了，以后大家都是从 Nexus 私服上进行下来，私服一般部署在内网，这样大大节约的宽带。创建 Proxy 的具体步骤1 点击 “Create Repositories” 按钮 2 选择要创建的类型 3 填写详细信息Name：就是为代理起个名字Remote Storage: 代理的地址，Maven 的地址为: https://repo1.maven.org/maven2/Blob Store: 选择代理下载包的存放路径 2）HostedHosted 是宿主机的意思，就是怎么把第三方的 Jar 放到私服上。Hosted 有三种方式，Releases、SNAPSHOT、MixedReleases: 一般是已经发布的 Jar 包Snapshot: 未发布的版本Mixed：混合的Hosted 的创建和 Proxy 是一致的，具体步骤和上面基本一致。如下： 注意事项：Deployment Pollcy: 需要把策略改成 “Allow redeploy”。 3）Group能把两个仓库合成一个仓库来使用，目前没使用过，所以没做详细的研究。 2.3 Security这里主要是用户、角色、权限的配置（上面已经提到了在这里添加用户和角色等） 2.4 Support包含日志及数据分析。 2.5 System主要是邮件服务器，调度的设置地方这部分主要讲怎么和 Maven 做集成, 集成的方式主要分以下种情况：代理中央仓库、Snapshot 包的管理、Release 包的管理、第三方 Jar 上传到 Nexus 上。 代理中央仓库只要在 PMO 文件中配置私服的地址（比如 http://192.168.1.14:8081）即可，配置如下： 12345678910111213&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;maven-central&lt;/id&gt; &lt;name&gt;maven-central&lt;/name&gt; &lt;url&gt;http://192.168.1.14:8081/repository/maven-central/&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;/repository&gt;&lt;/repositories&gt; Snapshot 包的管理1）修改 Maven 的 settings.xml 文件，加入认证机制 12345&lt;servers&gt; &lt;server&gt;&lt;id&gt;nexus&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt; 2）修改工程的 Pom 文件 123456789101112&lt;distributionManagement&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus Snapshot&lt;/name&gt; &lt;url&gt;http://192.168.1.14:8081/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;site&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus Sites&lt;/name&gt; &lt;url&gt;dav:http://192.168.1.14:8081/repository/maven-snapshots/&lt;/url&gt; &lt;/site&gt;&lt;/distributionManagement&gt; 注意事项: 上面修改的 Pom 文件如截图中的名字要跟 / usr/local/maven/conf/settings.xml 文件中的名字一定要对应上。 3）上传到 Nexus 上 1– 项目编译成的 jar 是 Snapshot(POM 文件的头部) 1234&lt;groupId&gt;com.zhisheng&lt;/groupId&gt;&lt;artifactId&gt;test-nexus&lt;/artifactId&gt;&lt;version&gt;1.0.0-SHAPSHOT&lt;/version&gt;&lt;packaging&gt;jar&lt;/packaging&gt; 2– 使用 mvn deploy 命令运行即可（运行结果在此略过） 3– 因为 Snapshot 是快照版本，默认他每次会把 Jar 加一个时间戳，做为历史备份版本。 Releases 包的管理1）与 Snapshot 大同小异，只是上传到私服上的 Jar 包不会自动带时间戳2）与 Snapshot 配置不同的地方，就是工程的 PMO 文件，加入 repository 配置 1234567&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus Snapshot&lt;/name&gt; &lt;url&gt;http://192.168.1.14:8081/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt;&lt;/distributionManagement&gt; 3）打包的时候需要把 Snapshot 去掉 1234&lt;groupId&gt;com.zhisheng&lt;/groupId&gt;&lt;artifactId&gt;test-nexus&lt;/artifactId&gt;&lt;version&gt;1.0.0&lt;/version&gt;&lt;packaging&gt;jar&lt;/packaging&gt; | 第三方 Jar 上传到 Nexus[root@master-node src]# mvn deploy:deploy-file -DgroupId=org.jasig.cas.client -DartifactId=cas-client-core -Dversion=3.1.3 -Dpackag注意事项：-DrepositoryId=nexus 对应的就是 Maven 中 settings.xml 的认证配的名字。 最后搭建的时候是参考网上博客，写篇完整的博客再回馈给网上。转载请注明地址：http://www.54tianzhisheng.cn/2017/10/14/Nexus3-Maven/","tags":[{"name":"Maven","slug":"Maven","permalink":"http://yoursite.com/tags/Maven/"}]},{"title":"Google Guava 缓存实现接口的限流","date":"2017-09-22T16:00:00.000Z","path":"2017/09/23/Guava-limit/","text":"项目背景最近项目中需要进行接口保护，防止高并发的情况把系统搞崩，因此需要对一个查询接口进行限流，主要的目的就是限制单位时间内请求此查询的次数，例如 1000 次，来保护接口。参考了 开涛的博客聊聊高并发系统限流特技 ，学习了其中利用 Google Guava 缓存实现限流的技巧，在网上也查到了很多关于 Google Guava 缓存的博客，学到了好多，推荐一个博客文章：http://ifeve.com/google-guava-cachesexplained/, 关于 Google Guava 缓存的更多细节或者技术，这篇文章讲的很详细；这里我们并不是用缓存来优化查询，而是利用缓存，存储一个计数器，然后用这个计数器来实现限流。 效果实验1234567static LoadingCache&lt;Long, AtomicLong&gt; count = CacheBuilder.newBuilder().expireAfterWrite(1, TimeUnit.SECONDS).build(new CacheLoader&lt;Long, AtomicLong&gt;() &#123; @Override public AtomicLong load(Long o) throws Exception &#123; //System.out.println(\"Load call!\"); return new AtomicLong(0L); &#125; &#125;); 上面，我们通过 CacheBuilder 来新建一个 LoadingCache 缓存对象 count，然后设置其有效时间为 1 秒，即每 1 秒钟刷新一次；缓存中，key 为一个 long 型的时间戳类型，value 是一个计数器，使用原子性的 AtomicLong 保证自增和自减操作的原子性， 每次查询缓存时如果不能命中，即查询的时间戳不在缓存中，则重新加载缓存，执行 load 将当前的时间戳的计数值初始化为 0。这样对于每一秒的时间戳，能计算这一秒内执行的次数，从而达到限流的目的；这是要执行的一个 getCounter 方法： 123456public class Counter &#123; static int counter = 0; public static int getCounter() throws Exception&#123; return counter++; &#125;&#125; 现在我们创建多个线程来执行这个方法： 12345678910111213141516171819202122public class Test &#123; public static void main(String args[]) throws Exception &#123; for(int i = 0;i&lt;100;i++) &#123; new Thread()&#123; @Override public void run() &#123; try &#123; System.out.println(Counter.getCounter()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;.start(); &#125; &#125;&#125; 这样执行的话，执行结果很简单，就是很快地执行这个 for 循环，迅速打印 0 到 99 折 100 个数，不再贴出。这里的 for 循环执行 100 个进程时间是很快的，那么现在我们要限制每秒只能有 10 个线程来执行 getCounter() 方法，该怎么办呢，上面讲的限流方法就派上用场了： 12345678910111213141516171819202122public class Counter &#123; static LoadingCache&lt;Long, AtomicLong&gt; count = CacheBuilder.newBuilder().expireAfterWrite(1, TimeUnit.SECONDS).build(new CacheLoader&lt;Long, AtomicLong&gt;() &#123; @Override public AtomicLong load(Long o) throws Exception &#123; System.out.println(\"Load call!\"); return new AtomicLong(0L); &#125; &#125;); static long limits = 10; static int counter = 0; public static synchronized int getCounter() throws Exception&#123; while (true) &#123; //获取当前的时间戳作为key Long currentSeconds = System.currentTimeMillis() / 1000; if (count.get(currentSeconds).getAndIncrement() &gt; limits) &#123; continue; &#125; return counter++; &#125; &#125;&#125; 这样一来，就可以限制每秒的执行数了。对于每个线程，获取当前时间戳，如果当前时间 (当前这 1 秒) 内有超过 10 个线程正在执行，那么这个进程一直在这里循环，直到下一秒，或者更靠后的时间，重新加载，执行 load，将新的时间戳的计数值重新为 0。执行结果：每秒执行 11 个（因为从 0 开始），每一秒之后，load 方法会执行一次； 12345678910111213141516171819202122232425为了更加直观，我们可以让每个for循环sleep一段时间：public class Test &#123; public static void main(String args[]) throws Exception &#123; for(int i = 0;i&lt;100;i++) &#123; new Thread()&#123; @Override public void run() &#123; try &#123; System.out.println(Counter.getCounter()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;.start(); Thread.sleep(100); &#125; &#125;&#125; 在上述这样的情况下，一个线程如果遇到当前时间正在执行的线程超过 limit 值就会一直在 while 循环，这样会浪费大量的资源，我们在做限流的时候，如果出现这种情况，可以不进行 while 循环，而是直接抛出异常或者返回，来拒绝这次执行（查询），这样便可以节省资源。 最后本篇文章地址： http://www.54tianzhisheng.cn/2017/09/23/Guava-limit/","tags":[{"name":"Guava","slug":"Guava","permalink":"http://yoursite.com/tags/Guava/"}]},{"title":"面试过阿里等互联网大公司，我知道了这些套路","date":"2017-09-16T16:00:00.000Z","path":"2017/09/17/Interview-summary/","text":"前面感谢一波因为看到掘金在做秋招求职征文大赛，赞助商也有牛客网，自己前段时间也稍微写了篇博客总结我的大学生活，那些年我看过的书 —— 致敬我的大学生活 —— Say Good Bye ！ 博客中稍微简单的介绍了下自己的求职，重点是推荐了下我自己看过的那些书籍，对我帮助真的很大。 如今借这么个机会，回馈掘金和牛客网，想想自己这一年在掘金也写过不少文章，从 0 个粉丝到如今被 11047 人（截止写此篇文章时）关注，有点小激动，竟然这么多粉，也不知道真正活跃的用户有多少。不管怎样，这一年在掘金还是收获很多的，不仅可以阅读到很多大神的文章，学习新的知识，而且还遇到了好几个不错的哥们，如今平常也有和他们交流，比如 ：芋道源码 老哥人就很不错，在上海还和老哥见过面，吃过饭，平常对我帮助也很大，会推荐一些很有用的书籍给我看。欢迎大家关注他的博客：芋道源码的博客 ，里面有好几系列的源码分析博客文章呢。至于牛客网，我就更是老用户了，印象中好像是大一的时候注册的，那时有空的话就会去上面刷几道基础题，写写题解，坚持了好久了，如今早已是红名了。（其实是水出来的，哈哈）在牛客网遇到的大神也是超多，好多朋友几乎都是通过牛客网认识的，那时早的时候一起在一群讨论问题，别提那场面了，震惊，我等弱渣瑟瑟发抖。感谢叶神，左神，牛妹！ 说着说着，好像偏题了。 正式进入话题吧！ 正文开始本篇秋招求职征文主要分享如下几方面：招聘职位需求套路 、招聘面试的套路、简历撰写套路、简历投递套路 、找工作经历 、自己面试面经 、实习感悟、书籍推荐 、优秀网站推荐 、优秀博客推荐 、求职资料放送。 招聘职位需求套路摘举下几个公司的招聘需求：（from lagou） 1、Java开发校招生( 有赞 ) 职位诱惑：福利好待遇佳，技术氛围浓，有大牛带成长快职位描述： 有赞2018校招官方网申地址（请在官网投递，勿直接在Lagou上投递）：https://job.youzan.com/campus岗位职责 我们拥有世界级的 SaaS 电商解决方案，每天处理几百万订单、几亿条消息，并且量级不断攀升； 我们开放了有赞云，连接了数十万开发者，大大提升了 SaaS 对商家产生的价值； 我们正在新零售的潮流中激流勇进、开疆拓土，用产品技术撬动巨大的市场； 而你的工作，就是参与这些大流量系统的研发，哪怕提升1%的性能和稳定性都将是激动人心的时刻。 岗位要求 2018届本科及以上学历应届毕业生，计算机或者软件工程相关专业； 具备扎实的计算机基础知识，至少熟练使用一门主流开发语言； 积极参与开发实践，如果拥有引以为豪的项目经历则加分； 热衷数据结构与算法，如果一不小心在 ACM 赛场摘过金，夺过银则加分； 能在 Linux 上写任何脚本，比王者荣耀上手还快则加分； 快速学习新鲜事物，自我驱动追求卓越，积极应对问题和变化。 2、京东居家生活事业部-汽车用品招聘实习生（2018届） 职位诱惑：京东商城 职位描述：京东商城-汽车用品部门招聘实习生 我们需要这样的你： 2018届毕业生（本科或硕士均可） 学习能力强 担当、抗压、接受变化 能长期实习（优秀者有转正机会） 需要一个大的平台来展示和发挥自己的能力 你将收获： 重新认识快速成长的自己 一份世界500强的实习经历 一群优秀的伙伴 3、爱奇艺 Java 实习生 - 游戏事业部 要求：至少 6 个月以上每周三天以上实习。 本科以上学历，计算机、软件工程相关专业； 基础扎实，熟悉 Java 编程，熟悉 Spring、MyBatis 等框架优先； 熟悉 SQL 语句，熟练使用 MySQL 数据库； 良好的沟通、表达、协调能力，富有激情，学习能力强； 有 GitHub 账号或者技术博客优先； 热爱游戏行业优先。 这里随便找了三个，从招聘需求里看，好多公司目前招聘的话在招聘需求中并不怎么会写的很清楚，有的也不会说明要求的技术栈，这其实有时会对我们这种新人来说，有点不好的，这样的话我们就没有明确的目标去复习，还有就是一些加分项，其实也是有点帮助的。就比如有些招聘上面的说有优秀博客和 GitHub 者优先，这两点的话我们其实可以在大学慢慢积累出来的，对面试确实有帮助，我好些面试机会都是靠这两个的。还有套路就是，别光信他这招聘需求，进去面试可能就不问你这些方面的问题了，那些公司几乎都是这么个套路：面试造火箭，入职拧螺丝 ！ 进去公司之前可能需要你懂很多东西，但是进去的话还只是专门做一方面的东西。不管怎样，如果你有机会进去大公司的话（而且适合去），还是去大公司吧，出来大厂光环不少。 认真耐心地拧螺丝钉，说不定有机会去造大火箭——正规大公司的节奏。 短时间把螺丝拧出花，说不定有机会造小火箭——上升中创业公司的节奏。 招聘面试的套路参考：https://mp.weixin.qq.com/s/qRwDowetBkJqpeMeAZsIpA 一个在掘金上认识的老哥，在京东工作，写的不错，干脆分享下。大家可以去看他的博客，http://mindwind.me/ 当时我求职的时候通过作者博客也学到不少东西。 一次集中的扩招需求，有点像每年一度的晋升评审，都需要对大量的候选人进行定级评审，因为每一个新招聘的人员都会对其有一个定级的过程。 维度： 通用能力：考察其沟通表达、学习成长等 专业知识：考察其知识的掌握、深度、广度等 专业能力：考察其技能应用的能力和结果 工作业绩：考察其工作成果、产出、创新点等 价值观：考察其认知、理解、行为等 整个面试过程会包括下面几个部分： 自我介绍一开始的简短自我介绍，考察点在于对自我的总结、归纳和认知能力。观察其表达的逻辑性和清晰性，有个整体印象。 项目经历一般我不会专门问一些比较死的专业技术点之类的知识，都是套在候选人的项目经历和过往经验中穿插。通过其描述，来判断其掌握知识点的范围和深度，以及在实际的案例中如何运用这些知识与技能解决真正的问题的。 所以，不会有所谓的题库。每一个我决定面试的候选人，都是提前细读其简历，提炼场景和发掘需要问的问题，相当于面试前有个二三十分钟的备课过程，组织好面试时的交互过程与场景，以顺利达到我想要了解的点。 团队合作通常还会问候选人其所在团队中的角色，他们的工作模式、协作方式，并给出一些真实的场景化案例观察其应对的反应。评价一下关于他周围的同事、下属或领导，了解他在团队中的自我定位。这里的考察点是沟通协作方面的通用能力。 学习成长这个维度考察的关键点包括：成长潜力、职业生涯规划的清晰度。人与人之间成长速度的关键差距，我自己观察得出的结论在于：自驱力。而路径的清晰性，也是产生自驱的一个源动力，否则可能会感觉迷茫，而陷于困顿。 文化匹配这算是价值观的一部分吧。其实，这是最难考核的，我没有什么好方法，基本靠感觉。曾经有过好几次碰到经历和技能都不错的人，但总是感觉哪里不对，但又着急要人，就放进来了。但最终感觉是对的，合作很快就结束了，人也走了。 综合评价总结点评候选人的优势、劣势并进行技术定级，定级也没有绝对标准，而是相对的。我一般就是和周围觉得差不多级别的人的平均水准比较下，大概就会有一个技术级别的判断。 套路 招聘面试，其实是一个对人的筛选，而筛选的本质是匹配 —— 匹配人与职位。第一，你得非常清楚地理解，这个职位需要什么样属性的人。第二，确定你的候选人是否拥有这个职位要求的必须属性。那么，首先回答第一个问题，一般的职位需要什么样的属性？ 属性，又可以进一步拆解为三个层次。第一层次是「技能（Skills）」，技能是你习得的一种工具，就像程序员会用某种语言和框架来编写某类应用程序。第二层次是「能力（Abilities）」，能力是你运用工具的思考和行为方式，用同样的语言和框架编写同样程序的程序员能力可以差别很大。而第三层次是「价值观（Values）」，价值观是一个人根深蒂固的信念以及驱动行为的原因与动力所在。 简历撰写套路参考：https://mp.weixin.qq.com/s/3f8hGAQ-auLdkxkQ8XG3CQ 简历，是如此重要，它是获得一份满意工作的敲门砖，但不同的简历敲门的声响可不同。 但很多时候简历给人的感觉也似乎微不足道，因为没有人会真正细致的去读一份简历。而仅仅是快速的浏览一遍，就几乎同时对一个候选人形成了一种要么强烈，要么无感的印象。现实中的真实情况是，你的简历只有十几二十秒的时间窗口机会会被浏览到，然后就决定了能否进入下一步。 要让面试官看了你的简历后：知道你做过什么？看看技能、经历与岗位需求的匹配度，然后再问问你是谁？你通过简历散发出来的味道是什么感觉，我愿意和这样的人一起共事么？ 一份简历的最少必要内容包括： 个人信息 姓名 年龄 手机 邮箱 教育经历 博士（硕士、本科） 有多个全部写出来，最高学历写在上面 工作经历（最匹配职位需求的，挑选出来的 TOP3 的项目） 项目1 项目背景上下文（场景、问题） 你在其中的角色（职责、发挥的作用、结果度量） 与此项经历有关的知识与技能（技术栈） 项目2 项目3 附加信息 博客：持续有内容，不碎碎念 开源：GitHub 持续 commit 社区：有一定专业影响力的 书籍：用心写的 演讲：行业大会级别的 专利：凑数的就算了 论文：学术界比较有影响力的 爱好：真正的兴趣点 对于我们学生，缺乏工作经历，那就写写独特的学习或实习经历。同学们大家都共有的经历就不要随便写上去凑数了。对于学生，看重的是通用能力，学习能力，适应能力以及对工作的态度和热情。如果没有区分度高的经历，那么有作品也是很好的。比如将你的做的网站部署出来，把地址写在简历上。 关于技术栈部分的技术术语，很多程序员不太注意。比如，把 Java 写成 java 或 JAVA，Java 已是一个专有品牌名词，大小写要完全符合，这一点和 iOS 类似（i 小写，OS 大写）。另外，像 HTML，CSS 则全部大写，因为这是多个单词的缩写。一些小小的细节就能读出你的专业性和散发出来的味道。最后，技术术语不是罗列得多就好，不是真正熟练的技能，不要轻易写进简历。因为这将给你自己挖坑。你可以将你自己擅长的或者很熟的知识点写进去，有时想着重就加粗或者打个括号，这样可以挖坑给面试官，让他去问你熟悉的（前提要确保你真的能讲清楚，我试过这个方法很有效的）。 然后就是简历格式了，最好是 PDF 了，Word 在不同的电脑上的打开效果可能不一样，格式可能会变，况且有些人的电脑不一定装了 Word，不过我喜欢用 Markdown 写简历，简洁，适合程序员，然后把 Markdown 转换成 PDF 出来。 简历投递套路内推 有内推通道尽量走内推通道，不知道方便多少，而且成功几率也很大！找熟人，找学长学姐吧！牛客网讨论区很多内推帖子，可以去找找。不过今年的好多公司的内推通道都不咋管用了，套路越来越多了。记得去年好多公司内推都是免笔试，直接进入面试阶段，今年直接变成内推免简历筛选，进入笔试。因为现在的内推越来越不靠谱，直接面试的话，会增加公司的面试成本，干脆笔试再筛选一部分人。 拉勾网 拉勾上还是算不错的。 Boss 直聘 虽说前段时间出现了程序员找工作进入传销最后导致死亡的惨事发生，但是里面总比智联招聘和前程无忧靠谱点。因为智联招聘和前程无忧几乎被广告党和培训机构给占领了。 脉脉 里面招应届生和实习生比较少，但是也有，可以试试。 总之，简历投递给公司之前，请确认下这家公司到底咋样，先去百度了解下，别被坑了，每个平台都有一些居心不良的广告党等着你上钩，千万别上当！！！ 找工作经历这段经历，算是自己很难忘记的经历吧。既辛酸既充实的日子！也很感谢自己在这段时间的系统复习，感觉把自己的基础知识再次聚集在一起了，自己的能力在这一段时间提升的也很快。后面有机会的话我也想写一系列的相关文章，为后来准备工作（面试）的同学提供一些自己的帮助。自己在找工作的这段时间面过的公司也有几家大厂，但是结果都不是很好，对我自己有很大的压力，当时心里真的感觉 ：“自己真的有这么差”，为什么一直被拒，当时很怀疑自己的能力，自己也有总结原因。一是面试的时候自己准备的还不够充分，虽说自己脑子里对这些基础有点印象，但是面试的时候自己稍紧张下就描述不怎么清楚了，导致面试官觉得你可能广度够了，深度还不够（这是阿里面试官电话面试说的）；二是自己的表达能力还是有所欠缺，不能够将自己所要表达的东西说出来，这可能我要在后面加强的地方；三是我的学校问题。在面了几家公司失败后，终于面了家公司要我了，我也确定在这家公司了。很幸运，刚出来，就有一个很好（很负责）的架构师带我，这周就给了我一个很牛逼的项目给我看，里面新东西很多，说吃透了这个项目，以后绝对可以拿出去吹逼（一脸正经.jpg）。找工作期间，自己也经常去收集一些博客，并把它保存下来，这样能够让自己下次更好的系统复习，还在牛客网整理了很多面经，每天看几篇面经，知道面试一般问什么问题，都有啥套路，其实你看多了面经就会发现，面试考的题目几乎都差不多，区别不是很大。目前我的找工作经历就简短的介绍到这里了，如果感兴趣的话，可以加群：528776268 期待志同道合的你。 自己面试面经亚信地址：http://www.54tianzhisheng.cn/2017/08/04/yaxin/ 1）自我介绍（说到一个亮点：长期坚持写博客，面试官觉得这个习惯很好，算加分项吧） 2）看到简历项目中用到 Solr，详细的问了下 Solr（自己介绍了下 Solr 的使用场景和建立索引等东西） 3）项目里面写了一个 “ 敏感词和 JS 标签过滤防 XSS 攻击”，面试官让我讲了下这个 XSS 攻击，并且是怎样实现的 4）项目里写了支持 Markdown，问是不是自己写的解析代码，（回答不是，自己引用的是 GitHub上的一个开源项目解析的） 5）想问我前端的知识，我回复到：自己偏后端开发，前端只是了解，然后面试官就不问了 6）问我考不考研？ 7）觉得杭州怎么样？是打算就呆在杭州还是把杭州作为一个跳板？ 8）有啥小目标？以后是打算继续技术方向，还是先技术后管理（还开玩笑的说：是不是赚他几个亿，当时我笑了笑） 9）有啥兴趣爱好？ 总结：面试问的问题不算多，主要是通过简历上项目所涉及的东西提问的，如果自己不太会的切记不要写上去。面试主要考察你回答问题来判断你的逻辑是否很清楚。 爱奇艺地址：http://www.54tianzhisheng.cn/2017/08/04/iqiyi/ 笔试（半个小时）题目：（记得一些） 1、重载重写的区别？ 2、转发和重定向的区别？ 3、画下 HashMap 的结构图？HashMap 、 HashTable 和 ConcurrentHashMap 的区别？ 4、statement 和 preparedstatement 区别？ 5、JSP 中一个 中取值与直接取值的区别？会有什么安全问题？ 6、实现一个线程安全的单例模式 7、一个写 sql 语句的题目 8、自己实现一个 List，（主要实现 add等常用方法） 9、Spring 中 IOC 和 AOP 的理解？ 10、两个对象的 hashcode 相同，是否对象相同？equal() 相同呢？ 11、@RequestBody 和 @ResponseBody 区别？ 12、JVM 一个错误，什么情况下会发生？ 13、常用的 Linux 命令？ 第一轮面试（80 分钟）1、自我介绍 2、介绍你最熟悉的一个项目 3、讲下这个 XSS 攻击 4、HashMap 的结构？HashMap 、 HashTable 和 ConcurrentHashMap 的区别？ 5、HashMap 中怎么解决冲突的？（要我详细讲下） 6、ConcurrentHashMap 和 HashTable 中线程安全的区别？为啥建议用 ConcurrentHashMap ？能把 ConcurrentHashMap 里面的实现详细的讲下吗？ 7、Session 和 Cookie 的区别？ 8、你项目中登录是怎样做的，用的 Cookie 和 Session？ 9、讲讲你对 Spring 中的 IOC 和 AOP 的理解？ 10、问了好几个注解的作用？ 11、statement 和 preparedstatement 区别？ 12、$ 和 # 的区别？以及这两个在哪些地方用？ 13、前面项目介绍了数据是爬虫爬取过来的，那你讲讲你的爬虫是多线程的吧？ 14、讲讲 Python 中的多线程和 Java 中的多线程区别？ 15、自己刚好前几天在看线程池，立马就把面试官带到我熟悉的线程池，和面试官讲了下 JDK 自带的四种线程池、ThreadPoolExecutor 类中的最重要的构造器里面的七个参数，然后再讲了下线程任务进入线程池和核心线程数、缓冲队列、最大线程数量比较。 16、线程同步，你了解哪几种方式？ 17、讲下 Synchronized？ 18、讲下 RecentLock 可重入锁？ 什么是可重入锁？为什么要设计可重入锁？ 19、讲下 Volatile 吧？他是怎样做到同步的？ 20、Volatile 为什么不支持原子性？举个例子 21、Atomic 怎么设计的？（没看过源码，当时回答错了，后来才发现里面全部用 final 修饰的属性和方法） 22、问几个前端的标签吧？（问了一个不会，直接说明我偏后端，前端只是了解，后面就不问了） 23、SpringBoot 的了解？ 24、Linux 常用命令？ 25、JVM 里的几个问题？ 26、事务的特性？ 27、隔离级别？ 28、网络状态码？以 2、3、4、5 开头的代表什么意思。 29、并发和并行的区别？ 30、你有什么问题想问我的？ 一面面完后面试官和说这份试卷是用来考 1~3 年开发工作经验的，让我准备一下，接下来的二面。 第二轮面试（半个小时）1、一上来就问怎么简历名字都没有，我指了简历第一行的我的名字，还特意大写了，然后就问学校是不是在上海，我回答在南昌（感觉被鄙视了一波，后面我在回答问题的时候面试官就一直在玩手机，估计后面对我的印象就不是很好了） 2、自我介绍 3、说一说数据库建表吧（从范式讲） 4、讲讲多态？（这个我答出来了，可是面试官竟然说不是这样吧，可能面试官没听请，后面还说我是不是平时写多态比较少，感觉这个也让面试官对我印象减分） 5、将两个数转换（不借助第三个参数） 6、手写个插入排序吧（写完了和面试官讲了下执行流程） 7、讲讲你对 Spring 中的 IOC 和 AOP 的理解？ 8、问了几个常用的 Linux 命令？ 9、也问到多线程？和一面一样把自己最近看的线程池也讲了一遍 10、学 Java 多久了？ 11、你有什么想问的？ 总结：面试题目大概就是这么多了，有些问题自己也忘记了，面试题目顺序不一定是按照上面所写的。再次感谢爱奇艺的第一面面试官了，要不是他帮忙内推的，我可能还没有机会收到面试机会。自己接到爱奇艺面试邀请电话是星期一晚上快7点中的，之后加了面试官微信约好了星期四面试的（时间准备较短，之前没系统的复习过）。星期四一大早（5点就起床了），然后就收拾了下，去等公交车，转了两次车，然后再做地铁去爱奇艺公司的，总共路上花费时间四个多小时。总的来说，这次面试准备的时间不是很充裕，所以准备的个人觉得不是很好，通过这次的面试，发现面试还是比较注重基础和深度的，我也知道了自己的一些弱处，还需要在哪里加强，面试技巧上也要掌握些。为后面的其他公司继续做好充足的准备。加油！！！ 阿里地址：http://www.54tianzhisheng.cn/2017/08/04/alibaba/ （菜鸟网络部门）（49 分钟） 2017.08.02 晚上9点21打电话过来，预约明天什么时候有空面试，约好第二天下午两点。 2017.08.03 下午两点10分打过来了。 说看了我的博客和 GitHub，觉得我学的还行，知识广度都还不错，但是还是要问问具体情况，为什么没看到你春招的记录，什么原因没投阿里？非得说一个原因，那就是：我自己太菜了，不敢投。 1、先自我介绍 2、什么是多态？哪里体现了多态的概念？ 3、HashMap 源码分析，把里面的东西问了个遍？最后问是不是线程安全？引出 ConcurrentHashMap 4、ConcurrentHashMap 源码分析 5、类加载，双亲委托机制 6、Java内存模型（一开始说的不是他想要的，主要想问我堆和栈的细节） 7、垃圾回收算法 8、线程池，自己之前看过，所以说的比较多，最后面试官说了句：看你对线程池了解还是很深了 9、事务的四种特性 10、什么是死锁？ 11、乐观锁和悲观锁的策略 12、高可用网站的设计（有什么技术实现） 13、低耦合高内聚 14、设计模式了解不？你用过哪几种，为什么用，单例模式帮我们做什么东西？有什么好处？ 15、你参与什么项目中成长比较快？学到了什么东西，以前是没有学过的？ 16、项目中遇到的最大困难是怎样的？是怎么解决的？ 17、智力题（两根不均匀的香，点一头烧完要一个小时，怎么确定15分钟） 18、你有什么问题想要问我的？ 19、问了菜鸟网络他们部门主要做什么？ 20、对我这次面试做个评价：看了你博客和 GitHub，知道你对学习的热情还是很高的，花了不少功夫，后面有通知！ 总结：面试总的来说，第一次电话面试，感觉好紧张，好多问题自己会点，但是其中的细节没弄清楚，自己准备的也不够充分。面试官很友好，看到我紧张，也安慰我说不要紧，不管以后出去面试啥的，不需要紧张，公司问的问题可能很广，你只需要把你知道的说出来就行，不会的直接说不会就行。之前一直不敢投阿里，因为自己准备的完全不够充分，但是在朋友磊哥的帮助下，还是试了下，不管结果怎么样，经历过总比没有的好。 后面说有通知，结果并没有，只看到官网的投递按钮变灰了。在掘金上一个朋友（我隔壁学校的），当时看我挂了说要不要让他租一起的隔壁邻居再内推下淘宝，我想想还是算了，自己目前能力真的是有限，达不到进阿里的要求！不过还是要感谢那个哥们，人真的超级好，虽然我们未曾谋面，但是有机会的话，我一定会请你吃饭的。 哔哩哔哩首先直接根据简历项目开问，自我介绍都没有。 1、登录从前端到后端整个过程描述一遍？越详细越好，说到密码加密，网络传输，后台验证用户名和密码，Cookie 设置等。具体问我密码加密是前台还是后台加密，说了在后台加密？面试官说，那你做这个项目有什么意思？密码传输都是明文的，默认 HTTP 传递是明文传输，当时被面试官带进前台加密还是后台加密的沟里去了，没想到用 HTTPS ，后来后来的路上查了些资料才知道的，面试过程中他很想我说前台加密，但是前台加密算法那代码就摆在那里，很容易就给破解了吧，也没给点提示说 HTTPS，我只好投降 2、写一个查询的 sql 语句 3、线程同步的方法？Synchronized、Volatile、（面试官好像觉得 Volatile 不可以做到同步，我和他说了半天的 Volatile 原理 ，他竟然不认同，我开始怀疑他的实力了）、ThreadLocal、Atomic。 说到这些了，我当时竟然没把他带进我我给他挖的坑里去（线程池，之前好好研究过呢，可惜了） 4、Spring IOC 和 AOP 的理解？叫我写 AOP 的代码，我没写 5、JDK 动态代理和 Cglib 代理区别？ 5、你觉得项目里面你觉得哪些技术比较好？我指了两个，然后他也没有问下去。 6、解释下 XSS 攻击 7、Spring 和 SpringBoot 的区别？ 8、JVM 垃圾回收算法？分代中为什么要分三层？ 9、OOM 是什么？什么情况会发生？ 10、你觉得你有啥优点？ 然后就叫我等一会，一会有人事来通知我，结果过了一会人事叫我可以回去等通知了。 总结：到公司的时候已经一点多钟了，面试直接在一个很多人的地方（吃饭的地方）直接面的，周围还有人再吃饭，场景有点尴尬，面试过程感觉很随意，想到什么问题就问什么，完全没有衔接，问到的有些地方感觉面试官自己都不清楚，还怀疑我所说的，另外就是问题比较刁钻，总体技术也就那样吧！ 目前所在公司当时是我现在的老大（架构师）面的，先是电话面试过一次，问的问题也比较难，不过最后还是觉得我基础还是不错的。最后叫我去公司面试下，来到公司面试问的问题那就更难了，几乎好多都回答不出来，但是简单的说了下思路，最后再叫主任面试了下，问的问题就很简单了，最后就是 HR 面了，主要说了下工资问题和什么时候能报道！这几次面试的问题当时由于时间比较紧，也没去整理，现在也记不清楚了！目前自己已经工作了快一个月了，给的项目也完全是新东西，对我的挑战也很大，有时自己也确实不怎么知道，不过我老大很耐心的教我，对我也很不错，这也是我打算留在这里的原因，碰到个好老大不易！必须好好珍惜！ 实习感悟进公司是架构运维组中的 Java 实习开发，目前实习已经快一个月了，说实话，实习后才发现一天真的很忙，写下这篇征文也是在周末整理大晚上写的。刚进公司就给了一个 Consul 的服务发现与注册和健康检查的项目，里面涉及的东西有 Consul、Docker、Nginx、Lua、ElasticSearch 还有几个很轻量级的框架，对我来说几乎都是新东西，确实需要时间去了解，再优化和改里面的 bug 的过程中，幸好我老大和我理了几次思路，才让我对整个项目有所进展，后续继续是在优化这项目（可能以后这个项目的所有东西都是我来做）。在上海，住的地方离公司有一定的距离，上班几乎要一个小时，每天花在上班路上的时间很多，这也导致我每天感觉很忙。公司上班时间比较弹性，无打卡，虽说公司不加班，但是每天自己都不怎么会按点下班，自己也想在实习阶段多学点东西！这段时间也是最关键的时间，碰到个问题，要花好久时间才能解决，也有可能未必解决得了，有时觉得自己啥都不会，这么点东西都做不好，有点否定自己。这也确实是自己的技术知识栈缺乏，和自己学的 SSM、Spring Boot 这些都不相关，也不怎么写业务逻辑代码。所以感觉很痛苦，不像自己以前写的代码那样顺畅，当然可能是自己以前自己写的项目太 low 了。 看到掘金-凯伦征文中写到： 公司其实并不期望刚刚进来的你，能够创造多少价值。新人是要成长的，在成长期难免会遇到各种各样的小问题，这可能是大多数人的必经之路，因为你所看到的同事，他们都比你在工作领域待的时间更久，有更多的经验，可以把他们作为目标，但不要把他们作为现在自己的标准，那样会压力太大。 感觉这段话对我现在很受用！ 加油，好好挺过这个阶段，别轻易说放弃！ 书籍推荐大学，我不怎么喜欢玩游戏，自己也还算不怎么堕落吧，看了以下的一些书籍，算是对我后面写博客、找工作也有很大的帮助。如果你是大神，请忽略，如果你还是还在大学，和我一样不想把时间浪费在游戏上，可以看看我推荐的一些书籍，有想讨论的请在评论下留下你的评论或者加上面给的群号。 Java1、《Java 核心技术》卷一 、卷二 两本书，算是入门比较好的书籍了 2、《疯狂 Java 讲义》 很厚的一本书，里面的内容也是很注重基础了 3、《Java 并发编程的艺术》—— 方腾飞 、魏鹏、程晓明著 方腾飞 是并发编程网的创始人，里面的文章确实还不错，可以多看看里面的文章，收获绝对很大。 4、《 Java多线程编程核心技术》—— 高洪岩著 这本书也算是入门多线程编程的不错书籍，我之前还写了一篇读书笔记呢，《Java 多线程编程核心技术》学习笔记及总结 , 大家如果不想看书的可以去看我的笔记。 5、《Java 并发编程实战》 这本书讲的有点难懂啊，不过确实也是一本很好的书，以上三本书籍如果都弄懂了，我觉得你并发编程这块可能大概就 OK 了，然后再去看看线程池的源码，了解下线程池，我觉得那就更棒了。不想看的话，请看我的博客：Java 线程池艺术探索 我个人觉得还是写的很不错，那些大厂面试也几乎都会问线程池的东西，然后大概内容也就是我这博客写的 6、《Effective Java》中文版 第二版 算是 Java 的进阶书籍了，面试好多问题也是从这出来的 7、《深入理解 Java 虚拟机——JVM高级特性与最佳实践》第二版 这算是国内讲 JVM 最清楚的书了吧，目前还是只看了一遍，后面继续啃，大厂面试几乎也是都会考 JVM 的，阿里面 JVM 特别多，想进阿里的同学请一定要买这本书去看。 8、《深入分析Java Web技术内幕 修订版》许令波著 里面知识很广，每一章都是一个不同的知识，可见作者的优秀，不愧是阿里大神。 9、《大型网站系统与 Java 中间件实践》—— 曽宪杰 著 作者是前淘宝技术总监，见证了淘宝网的发展，里面的讲的内容也是很好，看完能让自己也站在高处去思考问题。 10、《大型网站技术架构 —— 核心原理与案例分析》 —— 李智慧 著 最好和上面那本书籍一起看，效果更好，两本看完了，提升思想的高度！ 11、《疯狂Java.突破程序员基本功的16课》 李刚 著 书中很注重 Java 的一些细节，讲的很深入，但是书中的错别字特多，可以看看我的读书笔记：《疯狂 Java 突破程序员基本功的 16 课》读书笔记 12、《Spring 实战》 Spring 入门书籍 13、《Spring 揭秘》—— 王福强 著 这本书别提多牛了，出版时期为 2009 年，豆瓣评分为 9.0 分，写的是真棒！把 Spring 的 IOC 和 AOP 特性写的很清楚，把 Spring 的来龙去脉讲的很全。墙裂推荐这本书籍，如果你想看 Spring，作者很牛，资深架构师，很有幸和作者有过一次交流，当时因为自己的一篇博客 Pyspider框架 —— Python爬虫实战之爬取 V2EX 网站帖子，竟然找到我想叫我去实习，可惜了，当时差点就跟着他混了。作者还有一本书 《Spring Boot 揭秘》。 14、《Spring 技术内幕》—— 深入解析 Spring 架构与设计原理 讲解 Spring 源码，深入了内部机制，个人觉得还是不错的。 15、Spring 官方的英文文档 这个别提了，很好，能看英文尽量看英文 16、《跟开涛学 Spring 3》 《跟开涛学 Spring MVC》 京东大神，膜 17、《看透springMvc源代码分析与实践》 算是把 Spring MVC 源码讲的很好的了 见我的笔记： 1、通过源码详解 Servlet 2 、看透 Spring MVC 源代码分析与实践 —— 网站基础知识 3 、看透 Spring MVC 源代码分析与实践 —— 俯视 Spring MVC 4 、看透 Spring MVC 源代码分析与实践 —— Spring MVC 组件分析 18、《Spring Boot 实战》 19、Spring Boot 官方 Reference Guide 网上好多写 SpringBoot 的博客，几乎和这个差不多。 20、《JavaEE开发的颠覆者: Spring Boot实战》 21、MyBatis 当然是官方的文档最好了，而且还是中文的。 自己也写过几篇文章，帮助过很多人入门，传送门： 1、通过项目逐步深入了解Mybatis（一）/) 2、通过项目逐步深入了解Mybatis（二）/) 3、通过项目逐步深入了解Mybatis（三）/) 4、通过项目逐步深入了解Mybatis（四）/) 22、《深入理解 Java 内存模型》—— 程晓明 著 我觉得每个 Java 程序员都应该了解下 Java 的内存模型，该书籍我看的是电子版的，不多，但是讲的却很清楚，把重排序、顺序一致性、Volatile、锁、final等写的很清楚。 Linux《鸟哥的Linux私房菜 基础学习篇(第三版) 》 鸟哥的Linux私房菜：服务器架设篇(第3版) 鸟哥的书 计算机网络《计算机网络第六版——谢希仁 编》 《计算机网络自顶向下方法》 计算机系统《代码揭秘：从C／C.的角度探秘计算机系统 —— 左飞》 《深入理解计算机系统》 《计算机科学导论_佛罗赞》 数据库《高性能MySQL》 《Mysql技术内幕InnoDB存储引擎》 Python这门语言语法很简单，上手快，不过我目前好久没用了，都忘得差不多了。当时是看的廖雪峰的 Python 博客 自己也用 Python 做爬虫写过几篇博客，不过有些是在前人的基础上写的。感谢那些栽树的人！ 工具Git ： 廖雪峰的 Git 教程 IDEA：IntelliJ IDEA 简体中文专题教程 Maven：《Maven实战》 其他《如何高效学习-斯科特杨》 教你怎样高效学习的 《软技能：代码之外的生存指南》 程序员除了写代码，还得懂点其他的软技能。 《提问的智慧“中文版”》 《How-To-Ask-Questions-The-Smart-Way》 作为程序员的你，一定要学会咋提问，不然别人都不想鸟你。 优秀网站推荐1、GitHub 别和我说不知道 2、InfoQ 文章很不错 3、CSDN 经常看博客专家的博客，里面大牛很多，传送门：zhisheng 4、知乎 多关注些大牛，看他们吹逼 5、掘金 自己也在上面写专栏，粉丝已经超过一万了，传送门 ：zhisheng 6、并发编程网 前面已经介绍 7、developerworks 上面的博客也很好 8、博客园 里面应该大牛也很多，不过自己没在上面写过博客 9、微信公众号 关注了很多人，有些人的文章确实很好，平时也经常看。 10、牛客网 刷笔试题不错的地方，里面大牛超多，怀念叶神和左神讲课的时候，还有很有爱的牛妹。 优秀博客推荐廖雪峰 Git 和 Python 入门文章就是从他博客看的 阮一峰的网络日志 酷壳-陈皓 RednaxelaFX R大，牛逼的不得了 江南白衣 老司机 stormzhang 人称帅逼张，微信公众号写的不错 你假笨 阿里搞 JVM 的，很厉害 占小狼 泥瓦匠BYSocket 崔庆才 写了好多 Python 爬虫相关的文章 纯洁的微笑 SpringBoot 系列不错，其他的文章自己看了感觉是自己喜欢的那种文笔 程序猿DD 周立 芋道源码的博客 好多系列的源码分析 zhisheng 这个是我不要脸，竟然把自己博客地址的写上去了 求职资料放送自己在准备找工作那段时间，系统的复习了下大学所学的知识，期间在网上参考了很多不错的博客，并收集下来了，个人觉得还是不错的，因为这是包含了自己的心血，所以一直没怎么送出来，只给过我的几个同学，还有就是一些学习视频和实战项目视频。借着这次征文的机会，我想送给那些有缘人，希望你或许是那种在求职道路上正在艰难走着的人；或许是大一大二的学弟学妹们却想好好学习，有个奋斗的目标，不堪在大学堕落的；或许是工作一两年后感觉基础还比较薄弱的。要资料的时候期望你能简单的介绍下自己，期望你！联系方式请看文章最下面。 最后送一句话，越努力，越幸运，祝早日成为大神！ 这些地方可以找到我： blog: http://www.54tianzhisheng.cn/ GitHub: https://github.com/zhisheng17 QQ 群：528776268","tags":[{"name":"面经","slug":"面经","permalink":"http://yoursite.com/tags/面经/"}]},{"title":"Linux 下 lua 开发环境安装及安装 luafilesystem","date":"2017-09-14T16:00:00.000Z","path":"2017/09/15/linux-lua-lfs-install/","text":"火云邪神语录：天下武功，无坚不破，唯快不破！Nginx 的看家本领就是速度，Lua 的拿手好戏亦是速度，这两者的结合在速度上无疑有基因上的优势。 最近一直再折腾这个，干脆就稍微整理下。以防后面继续跳坑！ 安装： 1.先安装 lua 的相关依赖安装 C 开发环境由于 gcc 包需要依赖 binutils 和 cpp 包，另外 make 包也是在编译中常用的，所以一共需要 9 个包来完成安装，因此我们只需要执行 9 条指令即可： 12345678910gcc：命令未找到（解决方法）yum install cppyum install binutilsyum install glibcyum install glibc-kernheadersyum install glibc-commonyum install glibc-develyum install gccyum install makeyum install readline-devel 2.安装 lua5.1.5下载地址：http://www.lua.org/ftp/ 1234567891011121314151617181920tar -zxvf lua-5.1.5.tar.gzcd lua-5.1.5vi Makefile设置 INSTALL_TOP= /usr/local/luamake linuxmake testmake installrm -rf /usr/bin/lualn -s /usr/local/lua/bin/lua /usr/bin/lualn -s /usr/local/lua/share/lua /usr/share/lua设置环境变量：vim /etc/profile添加：export LUA_HOME=/usr/local/luaexport PATH=$PATH:$LUA_HOME/bin环境变量生效：source /etc/profile 3、安装 luarocks是一个 Lua 包管理器，基于 Lua 语言开发，提供一个命令行的方式来管理 Lua 包依赖、安装第三方 Lua 包等。 地址： https://github.com/luarocks/luarocks 12345678910111213141516使用 luarocks-2.2.1 版本在我机器上没有问题，但是使用 luarocks-2.4.2 出现问题wget http://luarocks.org/releases/luarocks-2.2.1.tar.gztar -zxvf luarocks-2.2.1.tar.gzcd luarocks-2.2.1./configure --with-lua=/usr/local --with-lua-include=/usr/local/lua/include设置环境变量：export LUA_LUAROCKS_PATH=/usr/local/luarocks-2.2.1export PATH=$PATH:$LUA_LUAROCKS_PATHmake &amp; make install 4、安装 luafilesystem是一个用于 lua 进行文件访问的库，可以支持 lua 5.1 和 lua5.2，且是跨平台的，在为 lua 安装 lfs 之前需要先安装luarocks。因为自己的需求刚好需要这模块。 地址：https://github.com/keplerproject/luafilesystem 文档： http://keplerproject.github.io/luafilesystem/index.html 1luarocks install luafilesystem 5、测试测试 lua 是否安装成功 lua -v 结果： 1Lua 5.1.5 Copyright (C) 1994-2012 Lua.org, PUC-Rio 测试 luafilesystem 是否安装成功 a.lua 123456789local lfs = require&quot;lfs&quot;function Rreturn(filePath) local time = os.date(&quot;%a, %d %b %Y %X GMT&quot;, lfs.attributes(filePath).modification) --打印文件的修改时间 print(time)endRreturn(&quot;/opt/lua/a.txt&quot;) a.txt 123abc 运行： 1lua a.lua 结果： 1Tue, 12 Sep 2017 18:43:13 GMT 出现打印出时间的结果就意味着已经安装好了。 当然以上这是在 Linux 安装的， Windows 上的其实比这还简单了，但是安装 luafilesystem 的话需要自己去下载个 lfs.dll ，然后把这个放到 lua 的安装路径去。很简单的，这里就不细说了。 出现过的错误：123456789101112[root@n1 lua-5.1.5]# make linux testcd src &amp;&amp; make linuxmake[1]: Entering directory `/opt/lua-5.1.5/src&apos;make all MYCFLAGS=-DLUA_USE_LINUX MYLIBS=&quot;-Wl,-E -ldl -lreadline -lhistory -lncurses&quot;make[2]: Entering directory `/opt/lua-5.1.5/src&apos;gcc -O2 -Wall -DLUA_USE_LINUX -c -o lapi.o lapi.cmake[2]: gcc：命令未找到make[2]: *** [lapi.o] 错误 127make[2]: Leaving directory `/opt/lua-5.1.5/src&apos;make[1]: *** [linux] 错误 2make[1]: Leaving directory `/opt/lua-5.1.5/src&apos;make: *** [linux] 错误 2 原因：最开始的那些依赖没安装","tags":[{"name":"lua","slug":"lua","permalink":"http://yoursite.com/tags/lua/"}]},{"title":"Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程","date":"2017-09-08T16:00:00.000Z","path":"2017/09/09/Elasticsearch-install/","text":"介绍ElasticSearch 是一个基于 Lucene 的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于 RESTful web 接口。Elasticsearch 是用 Java 开发的，并作为 Apache 许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。基百科、Stack Overflow、Github 都采用它。 本文从零开始，讲解如何使用 Elasticsearch 搭建自己的全文搜索引擎。每一步都有详细的说明，大家跟着做就能学会。 环境1、VMware 2、Centos 6.6 3、Elasticsearch 5.5.2 4、JDK 1.8 VMware 安装以及在 VMware 中安装 Centos 这个就不说了，环境配置直接默认就好，不过分配给机器的内存最好设置大点（建议 2G）， 使用 dhclient 命令来自动获取 IP 地址，查看获取的 IP 地址则使用命令 ip addr 或者 ifconfig ，则会看到网卡信息和 lo 卡信息。 给虚拟机额中的 linux 设置固定的 ip（因为后面发现每次机器重启后又要重新使用 dhclient 命令来自动获取 IP 地址） 1vim /etc/sysconfig/network-scripts/ifcfg-eth0 修改： 12onboot=yesbootproto=static 增加：（下面可设置可不设置） 123IPADDR=192.168.1.113 网卡IP地址GATEWAY=192.168.1.1NETMASK=255.255.255.0 设置好之后，把网络服务重启一下， service network restart 修改 ip 地址参考： http://jingyan.baidu.com/article/e4d08ffdd417660fd3f60d70.html 大环境都准备好了，下面开始安装步骤： 安装 JDK 1.8先卸载自带的 openjdk，查找 openjdk 1rpm -qa | grep java 卸载 openjdk 12yum -y remove java-1.7.0-openjdk-1.7.0.65-2.5.1.2.el65.x8664yum -y remove java-1.6.0-openjdk-1.6.0.0-11.1.13.4.el6.x86_64 解压 JDK 安装包： 附上jdk1.8的下载地址：http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 解压完成后配置一下环境变量就 ok 1、在/usr/local/下创建Java文件夹 12cd /usr/local/ 进入目录mkdir java 新建java目录 2、文件夹创建完毕，把安装包拷贝到 Java 目录中，然后解压 jdk 到当前目录 12cp /usr/jdk-8u144-linux-x64.tar.gz /usr/local/java/ **注意匹配你自己的文件名** 拷贝到java目录tar -zxvf jdk-8u144-linux-x64.tar.gz 解压到当前目录（Java目录） 3、解压完之后，Java目录中会出现一个jdk1.8.0_144的目录，这就解压完成了。之后配置一下环境变量。编辑/etc/下的profile文件，配置环境变量 12345678vi /etc/profile 进入profile文件的编辑模式在最后边追加一下内容(**配置的时候一定要根据自己的目录情况而定哦！**) JAVA_HOME=/usr/local/java/jdk1.8.0_144 CLASSPATH=$JAVA_HOME/lib/ PATH=$PATH:$JAVA_HOME/bin export PATH JAVA_HOME CLASSPATH 之后保存并退出文件之后。 让文件生效：source /etc/profile 在控制台输入Java 和 Java -version 看有没有信息输出，如下： java -version 123java version &quot;1.8.0_144&quot; Java(TM) SE Runtime Environment (build 1.8.0_60-b27) Java HotSpot(TM) Client VM (build 25.60-b23, mixed mode) 能显示以上信息，就说明 JDK 安装成功啦 安装 Maven因为后面可能会用到 maven ，先装上这个。 1、下载 maven 1wget http://mirrors.hust.edu.cn/apache/maven/maven-3/3.2.5/binaries/apache-maven-3.2.5-bin.tar.gz 2、解压至 /usr/local 目录 1tar -zxvf apache-maven-3.2.5-bin.tar.gz 3、配置公司给的配置 替换成公司给的 setting.xml 文件，修改关于本地仓库的位置, 默认位置: ${user.home}/.m2/repository 4、配置环境变量etc/profile 最后添加以下两行 12export MAVEN_HOME=/usr/local/apache-maven-3.2.5export PATH=$&#123;PATH&#125;:$&#123;MAVEN_HOME&#125;/bin 5、测试 123[root@localhost ~]# mvn -vApache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T09:29:23-08:00)Maven home: /usr/local/apache-maven-3.2.5 VMware 虚拟机里面的三台机器 IP 分别是： 123192.168.153.133192.168.153.134192.168.153.132 配置 hosts在 /etc/hosts下面编写：ip node 节点的名字（域名解析） 1vim /etc/hosts 新增： 123192.168.153.133 es1192.168.153.134 es2192.168.153.132 es3 设置 SSH 免密码登录安装expect命令 ： yum -y install expect 将 ssh_p2p.jar 随便解压到任何目录下： (这个 jar 包可以去网上下载) 1unzip ssh_p2p.zip 修改 resource 的 ip 值 1vim /ssh_p2p/deploy_data/resource （各个节点和账户名，密码，free代表相互都可以无密码登陆） 123456#设置为你每台虚拟机的ip地址，用户名，密码address=(&quot;192.168.153.133,root,123456,free&quot;&quot;192.168.153,134,root,123456,free&quot;&quot;192.168.153.132,root,123456,free&quot;) 修改 start.sh 的运行权限 1chmod u+x start.sh 运行 1./start.sh 测试： ssh ip地址 （测试是否可以登录） 安装 ElasticSearch下载地址： https://www.elastic.co/downloads/elasticsearch 123wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.2.tar.gzcd /usr/localtar -zxvf elasticsearch-5.5.2.tar.gz su tzs 切换到 tzs 用户下 ( 默认不支持 root 用户) sh /usr/local/elasticsearch/bin/elasticsearch -d 其中 -d 表示后台启动 在 vmware 上测试是否成功：curl http://localhost:9200/ 出现如上图这样的效果，就代表已经装好了。 elasticsearch 默认 restful-api 的端口是 9200 不支持 IP 地址，也就是说无法从主机访问虚拟机中的服务，只能在本机用 http://localhost:9200 来访问。如果需要改变，需要修改配置文件 /usr/local/elasticsearch/config/elasticsearch.yml 文件，加入以下两行： 12network.bind_host: 0.0.0.0network.publish_host: _nonloopback:ipv4 或去除 network.host 和 http.port 之前的注释，并将 network.host 的 IP 地址修改为本机外网 IP。然后重启，Elasticsearch 关闭方法（输入命令：ps -ef | grep elasticsearch ，找到进程，然后 kill 掉就行了。 如果外网还是不能访问，则有可能是防火墙设置导致的 ( 关闭防火墙：service iptables stop ) 修改配置文件：vim config/elasticsearch.yml cluster.name : my-app (集群的名字，名字相同的就是一个集群) node.name : es1 （节点的名字, 和前面配置的 hosts 中的 name 要一致） path.data: /data/elasticsearch/data （数据的路径。没有要创建（mkdir -p /data/elasticsearch/{data,logs}），并且给执行用户权限 chown tzs /data/elasticsearch/{data,logs} -R ）path.logs: /data/elasticsearch/logs （数据 log 信息的路径，同上）network.host: 0.0.0.0 //允许外网访问，也可以是自己的ip地址http.port: 9200 //访问的端口discovery.zen.ping.unicast.hosts: [“192.168.153.133”, “192.168.153.134”, “192.168.153.132”] //各个节点的ip地址 记得需要添加上：（这个是安装 head 插件要用的， 目前不需要）http.cors.enabled: truehttp.cors.allow-origin: “*” 最后在外部浏览器的效果如下图： 安装 IK 中文分词可以自己下载源码使用 maven 编译，当然如果怕麻烦可以直接下载编译好的 https://github.com/medcl/elasticsearch-analysis-ik/releases/tag/v5.5.2 注意下载对应的版本放在 plugins 目录下 解压 unzip elasticsearch-analysis-ik-5.5.2.zip 在 es 的 plugins 下新建 ik 目录 mkdir ik 将刚才解压的复制到ik目录下 cp -r elasticsearch/* ik 删除刚才解压后的 12rm -rf elasticsearchrm -rf elasticsearch-analysis-ik-5.5.2.zip IK 带有两个分词器ik_max_word ：会将文本做最细粒度的拆分；尽可能多的拆分出词语 ik_smart：会做最粗粒度的拆分；已被分出的词语将不会再次被其它词语占有 安装完 IK 中文分词器后（当然不止这种中文分词器，还有其他的，可以参考我的文章 Elasticsearch 默认分词器和中分分词器之间的比较及使用方法），测试区别如下： ik_max_wordcurl -XGET ‘http://192.168.153.134:9200/_analyze?pretty&amp;analyzer=ik_max_word‘ -d ‘联想是全球最大的笔记本厂商’ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;联想&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;是&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;全球&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;最大&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;的&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;笔记本&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 5 &#125;, &#123; &quot;token&quot; : &quot;笔记&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 10, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 6 &#125;, &#123; &quot;token&quot; : &quot;本厂&quot;, &quot;start_offset&quot; : 10, &quot;end_offset&quot; : 12, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 7 &#125;, &#123; &quot;token&quot; : &quot;厂商&quot;, &quot;start_offset&quot; : 11, &quot;end_offset&quot; : 13, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 8 &#125; ]&#125; ik_smartcurl -XGET ‘http://localhost:9200/_analyze?pretty&amp;analyzer=ik_smart‘ -d ‘联想是全球最大的笔记本厂商’ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;联想&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;是&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;全球&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;最大&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;的&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;笔记本&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 5 &#125;, &#123; &quot;token&quot; : &quot;厂商&quot;, &quot;start_offset&quot; : 11, &quot;end_offset&quot; : 13, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 6 &#125; ]&#125; 安装 head 插件elasticsearch-head 是一个 elasticsearch 的集群管理工具，它是完全由 html5 编写的独立网页程序，你可以通过插件把它集成到 es。 效果如下图：（图片来自网络） 安装 git123yum remove gityum install gitgit clone git://github.com/mobz/elasticsearch-head.git 拉取 head 插件到本地，或者直接在 GitHub 下载 压缩包下来 安装nodejs先去官网下载 node-v8.4.0-linux-x64.tar.xz 12tar -Jxv -f node-v8.4.0-linux-x64.tar.xzmv node-v8.4.0-linux-x64 node 环境变量设置： 1vim /etc/profile 新增： 123export NODE_HOME=/opt/nodeexport PATH=$PATH:$NODE_HOME/binexport NODE_PATH=$NODE_HOME/lib/node_modules 使配置文件生效（这步很重要，自己要多注意这步） 1source /etc/profile 测试是否全局可用了： 1node -v 然后 12345mv elasticsearch-head headcd head/npm install -g grunt-clinpm installgrunt server 再 es 的配置文件中加： 12http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; 在浏览器打开 http://192.168.153.133:9100/ 就可以看到效果了， 遇到问题把坑都走了一遍，防止以后再次入坑，特此记录下来 1、ERROR Could not register mbeans java.security.AccessControlException: access denied (“javax.management.MBeanTrustPermission” “register”) 改变 elasticsearch 文件夹所有者到当前用户 sudo chown -R noroot:noroot elasticsearch 这是因为 elasticsearch 需要读写配置文件，我们需要给予 config 文件夹权限，上面新建了 elsearch 用户，elsearch 用户不具备读写权限，因此还是会报错，解决方法是切换到管理员账户，赋予权限即可： sudo -i chmod -R 775 config 2、[WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [] uncaught exception in thread [main]org.elasticsearch.bootstrap.StartupException: java.lang.RuntimeException: can not run elasticsearch as root 原因是elasticsearch默认是不支持用root用户来启动的。 解决方案一：Des.insecure.allow.root=true 修改/usr/local/elasticsearch-2.4.0/bin/elasticsearch， 添加 ES_JAVA_OPTS=”-Des.insecure.allow.root=true” 或执行时添加： sh /usr/local/elasticsearch-2.4.0/bin/elasticsearch -d -Des.insecure.allow.root=true 注意：正式环境用root运行可能会有安全风险，不建议用root来跑。 解决方案二：添加专门的用户 1234useradd elasticchown -R elastic:elastic elasticsearch-2.4.0su elasticsh /usr/local/elasticsearch-2.4.0/bin/elasticsearch -d 3、UnsupportedOperationException: seccomp unavailable: requires kernel 3.5+ with CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER compiled in 只是警告，使用新的linux版本，就不会出现此类问题了。 4、ERROR: [4] bootstrap checks failed[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536] 原因：无法创建本地文件问题,用户最大可创建文件数太小 解决方案：切换到 root 用户，编辑 limits.conf 配置文件， 添加类似如下内容： vim /etc/security/limits.conf 添加如下内容: 1234* soft nofile 65536* hard nofile 131072* soft nproc 2048* hard nproc 4096 [2]: max number of threads [1024] for user [tzs] is too low, increase to at least [2048] 原因：无法创建本地线程问题,用户最大可创建线程数太小 解决方案：切换到root用户，进入limits.d目录下，修改90-nproc.conf 配置文件。 vim /etc/security/limits.d/90-nproc.conf 找到如下内容： soft nproc 1024 修改为 soft nproc 2048 [3]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 原因：最大虚拟内存太小 root用户执行命令： sysctl -w vm.max_map_count=262144 或者修改 /etc/sysctl.conf 文件，添加 “vm.max_map_count”设置设置后，可以使用$ sysctl -p [4]: system call filters failed to install; check the logs and fix your configuration or disable system call filters at your own risk 原因：Centos6不支持SecComp，而ES5.4.1默认bootstrap.system_call_filter为true进行检测，所以导致检测失败，失败后直接导致ES不能启动。详见 ：https://github.com/elastic/elasticsearch/issues/22899 解决方法：在elasticsearch.yml中新增配置bootstrap.system_call_filter，设为false，注意要在Memory下面:bootstrap.memory_lock: falsebootstrap.system_call_filter: false 5、 java.lang.IllegalArgumentException: property [elasticsearch.version] is missing for plugin [head] 再 es 的配置文件中加： 12http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; 最后整个搭建的过程全程自己手动安装，不易，如果安装很多台机器，是否可以写个脚本之类的自动搭建呢？可以去想想的。首发于：http://www.54tianzhisheng.cn/2017/09/09/Elasticsearch-install/ ，转载请注明出处，谢谢配合！","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://yoursite.com/tags/ElasticSearch/"}]},{"title":"Elasticsearch 系列文章（一）：Elasticsearch 默认分词器和中分分词器之间的比较及使用方法","date":"2017-09-07T16:00:00.000Z","path":"2017/09/08/Elasticsearch-analyzers/","text":"介绍：ElasticSearch 是一个基于 Lucene 的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于 RESTful web 接口。Elasticsearch 是用 Java 开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。 Elasticsearch中，内置了很多分词器（analyzers）。下面来进行比较下系统默认分词器和常用的中文分词器之间的区别。 系统默认分词器：1、standard 分词器https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-standard-analyzer.html 如何使用：http://www.yiibai.com/lucene/lucene_standardanalyzer.html 英文的处理能力同于StopAnalyzer.支持中文采用的方法为单字切分。他会将词汇单元转换成小写形式，并去除停用词和标点符号。 12345/**StandardAnalyzer分析器*/public void standardAnalyzer(String msg)&#123; StandardAnalyzer analyzer = new StandardAnalyzer(Version.LUCENE_36); this.getTokens(analyzer, msg);&#125; 2、simple 分词器https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-simple-analyzer.html 如何使用: http://www.yiibai.com/lucene/lucene_simpleanalyzer.html 功能强于WhitespaceAnalyzer, 首先会通过非字母字符来分割文本信息，然后将词汇单元统一为小写形式。该分析器会去掉数字类型的字符。 12345/**SimpleAnalyzer分析器*/ public void simpleAnalyzer(String msg)&#123; SimpleAnalyzer analyzer = new SimpleAnalyzer(Version.LUCENE_36); this.getTokens(analyzer, msg); &#125; 3、Whitespace 分词器https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-whitespace-analyzer.html 如何使用：http://www.yiibai.com/lucene/lucene_whitespaceanalyzer.html 仅仅是去除空格，对字符没有lowcase化,不支持中文；并且不对生成的词汇单元进行其他的规范化处理。 12345/**WhitespaceAnalyzer分析器*/ public void whitespaceAnalyzer(String msg)&#123; WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_36); this.getTokens(analyzer, msg); &#125; 4、Stop 分词器https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stop-analyzer.html 如何使用：http://www.yiibai.com/lucene/lucene_stopanalyzer.html StopAnalyzer的功能超越了SimpleAnalyzer，在SimpleAnalyzer的基础上增加了去除英文中的常用单词（如the，a等），也可以更加自己的需要设置常用单词；不支持中文 12345/**StopAnalyzer分析器*/ public void stopAnalyzer(String msg)&#123; StopAnalyzer analyzer = new StopAnalyzer(Version.LUCENE_36); this.getTokens(analyzer, msg); &#125; 5、keyword 分词器KeywordAnalyzer把整个输入作为一个单独词汇单元，方便特殊类型的文本进行索引和检索。针对邮政编码，地址等文本信息使用关键词分词器进行索引项建立非常方便。 6、pattern 分词器https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pattern-analyzer.html 一个pattern类型的analyzer可以通过正则表达式将文本分成”terms”(经过token Filter 后得到的东西 )。接受如下设置: 一个 pattern analyzer 可以做如下的属性设置: lowercase terms是否是小写. 默认为 true 小写. pattern 正则表达式的pattern, 默认是 \\W+. flags 正则表达式的flags stopwords 一个用于初始化stop filter的需要stop 单词的列表.默认单词是空的列表 7、language 分词器https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html 一个用于解析特殊语言文本的analyzer集合。（ arabic,armenian, basque, brazilian, bulgarian, catalan, cjk, czech, danish, dutch, english, finnish, french,galician, german, greek, hindi, hungarian, indonesian, irish, italian, latvian, lithuanian, norwegian,persian, portuguese, romanian, russian, sorani, spanish, swedish, turkish, thai.）可惜没有中文。不予考虑 8、snowball 分词器一个snowball类型的analyzer是由standard tokenizer和standard filter、lowercase filter、stop filter、snowball filter这四个filter构成的。 snowball analyzer 在Lucene中通常是不推荐使用的。 9、Custom 分词器是自定义的analyzer。允许多个零到多个tokenizer，零到多个 Char Filters. custom analyzer 的名字不能以 “_”开头. The following are settings that can be set for a custom analyzer type: Setting Description tokenizer 通用的或者注册的tokenizer. filter 通用的或者注册的token filters char_filter 通用的或者注册的 character filters position_increment_gap 距离查询时，最大允许查询的距离，默认是100 自定义的模板： 1234567891011121314151617181920212223242526index : analysis : analyzer : myAnalyzer2 : type : custom tokenizer : myTokenizer1 filter : [myTokenFilter1, myTokenFilter2] char_filter : [my_html] position_increment_gap: 256 tokenizer : myTokenizer1 : type : standard max_token_length : 900 filter : myTokenFilter1 : type : stop stopwords : [stop1, stop2, stop3, stop4] myTokenFilter2 : type : length min : 0 max : 2000 char_filter : my_html : type : html_strip escaped_tags : [xxx, yyy] read_ahead : 1024 10、fingerprint 分词器https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-fingerprint-analyzer.html 中文分词器：1、ik-analyzerhttps://github.com/wks/ik-analyzer IKAnalyzer是一个开源的，基于java语言开发的轻量级的中文分词工具包。 采用了特有的“正向迭代最细粒度切分算法“，支持细粒度和最大词长两种切分模式；具有83万字/秒（1600KB/S）的高速处理能力。 采用了多子处理器分析模式，支持：英文字母、数字、中文词汇等分词处理，兼容韩文、日文字符 优化的词典存储，更小的内存占用。支持用户词典扩展定义 针对Lucene全文检索优化的查询分析器IKQueryParser(作者吐血推荐)；引入简单搜索表达式，采用歧义分析算法优化查询关键字的搜索排列组合，能极大的提高Lucene检索的命中率。 Maven用法： 12345&lt;dependency&gt; &lt;groupId&gt;org.wltea.ik-analyzer&lt;/groupId&gt; &lt;artifactId&gt;ik-analyzer&lt;/artifactId&gt; &lt;version&gt;3.2.8&lt;/version&gt;&lt;/dependency&gt; 在IK Analyzer加入Maven Central Repository之前，你需要手动安装，安装到本地的repository，或者上传到自己的Maven repository服务器上。 要安装到本地Maven repository，使用如下命令，将自动编译，打包并安装：mvn install -Dmaven.test.skip=true Elasticsearch添加中文分词安装IK分词插件https://github.com/medcl/elasticsearch-analysis-ik 进入elasticsearch-analysis-ik-master 更多安装请参考博客： 1、为elastic添加中文分词 ： http://blog.csdn.net/dingzfang/article/details/42776693 2、如何在Elasticsearch中安装中文分词器(IK+pinyin) ：http://www.cnblogs.com/xing901022/p/5910139.html 3、Elasticsearch 中文分词器 IK 配置和使用 ： http://blog.csdn.net/jam00/article/details/52983056 ik 带有两个分词器ik_max_word ：会将文本做最细粒度的拆分；尽可能多的拆分出词语 ik_smart：会做最粗粒度的拆分；已被分出的词语将不会再次被其它词语占有 区别： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133# ik_max_wordcurl -XGET &apos;http://localhost:9200/_analyze?pretty&amp;analyzer=ik_max_word&apos; -d &apos;联想是全球最大的笔记本厂商&apos;#返回&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;联想&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;是&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;全球&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;最大&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;的&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;笔记本&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 5 &#125;, &#123; &quot;token&quot; : &quot;笔记&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 10, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 6 &#125;, &#123; &quot;token&quot; : &quot;本厂&quot;, &quot;start_offset&quot; : 10, &quot;end_offset&quot; : 12, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 7 &#125;, &#123; &quot;token&quot; : &quot;厂商&quot;, &quot;start_offset&quot; : 11, &quot;end_offset&quot; : 13, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 8 &#125; ]&#125;# ik_smartcurl -XGET &apos;http://localhost:9200/_analyze?pretty&amp;analyzer=ik_smart&apos; -d &apos;联想是全球最大的笔记本厂商&apos;# 返回&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;联想&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;是&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;全球&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;最大&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;的&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;笔记本&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 5 &#125;, &#123; &quot;token&quot; : &quot;厂商&quot;, &quot;start_offset&quot; : 11, &quot;end_offset&quot; : 13, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 6 &#125; ]&#125; 下面我们来创建一个索引，使用 ik创建一个名叫 iktest 的索引，设置它的分析器用 ik ，分词器用 ik_max_word，并创建一个 article 的类型，里面有一个 subject 的字段，指定其使用 ik_max_word 分词器 12345678910111213141516171819202122curl -XPUT &apos;http://localhost:9200/iktest?pretty&apos; -d &apos;&#123; &quot;settings&quot; : &#123; &quot;analysis&quot; : &#123; &quot;analyzer&quot; : &#123; &quot;ik&quot; : &#123; &quot;tokenizer&quot; : &quot;ik_max_word&quot; &#125; &#125; &#125; &#125;, &quot;mappings&quot; : &#123; &quot;article&quot; : &#123; &quot;dynamic&quot; : true, &quot;properties&quot; : &#123; &quot;subject&quot; : &#123; &quot;type&quot; : &quot;string&quot;, &quot;analyzer&quot; : &quot;ik_max_word&quot; &#125; &#125; &#125; &#125;&#125;&apos; 批量添加几条数据，这里我指定元数据 _id 方便查看，subject 内容为我随便找的几条新闻的标题 123456789101112curl -XPOST http://localhost:9200/iktest/article/_bulk?pretty -d &apos;&#123; &quot;index&quot; : &#123; &quot;_id&quot; : &quot;1&quot; &#125; &#125;&#123;&quot;subject&quot; : &quot;＂闺蜜＂崔顺实被韩检方传唤 韩总统府促彻查真相&quot; &#125;&#123; &quot;index&quot; : &#123; &quot;_id&quot; : &quot;2&quot; &#125; &#125;&#123;&quot;subject&quot; : &quot;韩举行＂护国训练＂ 青瓦台:决不许国家安全出问题&quot; &#125;&#123; &quot;index&quot; : &#123; &quot;_id&quot; : &quot;3&quot; &#125; &#125;&#123;&quot;subject&quot; : &quot;媒体称FBI已经取得搜查令 检视希拉里电邮&quot; &#125;&#123; &quot;index&quot; : &#123; &quot;_id&quot; : &quot;4&quot; &#125; &#125;&#123;&quot;subject&quot; : &quot;村上春树获安徒生奖 演讲中谈及欧洲排外问题&quot; &#125;&#123; &quot;index&quot; : &#123; &quot;_id&quot; : &quot;5&quot; &#125; &#125;&#123;&quot;subject&quot; : &quot;希拉里团队炮轰FBI 参院民主党领袖批其“违法”&quot; &#125;&apos; 查询 “希拉里和韩国” 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071curl -XPOST http://localhost:9200/iktest/article/_search?pretty -d&apos;&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;subject&quot; : &quot;希拉里和韩国&quot; &#125;&#125;, &quot;highlight&quot; : &#123; &quot;pre_tags&quot; : [&quot;&lt;font color=&apos;red&apos;&gt;&quot;], &quot;post_tags&quot; : [&quot;&lt;/font&gt;&quot;], &quot;fields&quot; : &#123; &quot;subject&quot; : &#123;&#125; &#125; &#125;&#125;&apos;#返回&#123; &quot;took&quot; : 113, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 5, &quot;successful&quot; : 5, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : 4, &quot;max_score&quot; : 0.034062363, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;iktest&quot;, &quot;_type&quot; : &quot;article&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 0.034062363, &quot;_source&quot; : &#123; &quot;subject&quot; : &quot;韩举行＂护国训练＂ 青瓦台:决不许国家安全出问题&quot; &#125;, &quot;highlight&quot; : &#123; &quot;subject&quot; : [ &quot;&lt;font color=red&gt;韩&lt;/font&gt;举行＂护&lt;font color=red&gt;国&lt;/font&gt;训练＂ 青瓦台:决不许国家安全出问题&quot; ] &#125; &#125;, &#123; &quot;_index&quot; : &quot;iktest&quot;, &quot;_type&quot; : &quot;article&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 0.0076681254, &quot;_source&quot; : &#123; &quot;subject&quot; : &quot;媒体称FBI已经取得搜查令 检视希拉里电邮&quot; &#125;, &quot;highlight&quot; : &#123; &quot;subject&quot; : [ &quot;媒体称FBI已经取得搜查令 检视&lt;font color=red&gt;希拉里&lt;/font&gt;电邮&quot; ] &#125; &#125;, &#123; &quot;_index&quot; : &quot;iktest&quot;, &quot;_type&quot; : &quot;article&quot;, &quot;_id&quot; : &quot;5&quot;, &quot;_score&quot; : 0.006709609, &quot;_source&quot; : &#123; &quot;subject&quot; : &quot;希拉里团队炮轰FBI 参院民主党领袖批其“违法”&quot; &#125;, &quot;highlight&quot; : &#123; &quot;subject&quot; : [ &quot;&lt;font color=red&gt;希拉里&lt;/font&gt;团队炮轰FBI 参院民主党领袖批其“违法”&quot; ] &#125; &#125;, &#123; &quot;_index&quot; : &quot;iktest&quot;, &quot;_type&quot; : &quot;article&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.0021509775, &quot;_source&quot; : &#123; &quot;subject&quot; : &quot;＂闺蜜＂崔顺实被韩检方传唤 韩总统府促彻查真相&quot; &#125;, &quot;highlight&quot; : &#123; &quot;subject&quot; : [ &quot;＂闺蜜＂崔顺实被&lt;font color=red&gt;韩&lt;/font&gt;检方传唤 &lt;font color=red&gt;韩&lt;/font&gt;总统府促彻查真相&quot; ] &#125; &#125; ] &#125;&#125; 这里用了高亮属性 highlight，直接显示到 html 中，被匹配到的字或词将以红色突出显示。若要用过滤搜索，直接将 match 改为 term 即可 热词更新配置网络词语日新月异，如何让新出的网络热词（或特定的词语）实时的更新到我们的搜索当中呢 先用 ik 测试一下 12345678910111213141516171819202122232425262728293031323334353637curl -XGET &apos;http://localhost:9200/_analyze?pretty&amp;analyzer=ik_max_word&apos; -d &apos;成龙原名陈港生&apos;#返回&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;成龙&quot;, &quot;start_offset&quot; : 1, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;原名&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;陈&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 6, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;港&quot;, &quot;start_offset&quot; : 6, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;生&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 4 &#125; ]&#125; ik 的主词典中没有”陈港生” 这个词，所以被拆分了。现在我们来配置一下 修改 IK 的配置文件 ：ES 目录/plugins/ik/config/ik/IKAnalyzer.cfg.xml 修改如下： 12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE properties SYSTEM &quot;http://java.sun.com/dtd/properties.dtd&quot;&gt;&lt;properties&gt; &lt;comment&gt;IK Analyzer 扩展配置&lt;/comment&gt; &lt;!--用户可以在这里配置自己的扩展字典 --&gt; &lt;entry key=&quot;ext_dict&quot;&gt;custom/mydict.dic;custom/single_word_low_freq.dic&lt;/entry&gt; &lt;!--用户可以在这里配置自己的扩展停止词字典--&gt; &lt;entry key=&quot;ext_stopwords&quot;&gt;custom/ext_stopword.dic&lt;/entry&gt; &lt;!--用户可以在这里配置远程扩展字典 --&gt; &lt;entry key=&quot;remote_ext_dict&quot;&gt;http://192.168.1.136/hotWords.php&lt;/entry&gt; &lt;!--用户可以在这里配置远程扩展停止词字典--&gt; &lt;!-- &lt;entry key=&quot;remote_ext_stopwords&quot;&gt;words_location&lt;/entry&gt; --&gt;&lt;/properties&gt; 这里我是用的是远程扩展字典，因为可以使用其他程序调用更新，且不用重启 ES，很方便；当然使用自定义的 mydict.dic 字典也是很方便的，一行一个词，自己加就可以了 既然是远程词典，那么就要是一个可访问的链接，可以是一个页面，也可以是一个txt的文档，但要保证输出的内容是 utf-8 的格式 hotWords.php 的内容 12345678$s = &lt;&lt;&lt;'EOF'陈港生元楼蓝瘦EOF;header('Last-Modified: '.gmdate('D, d M Y H:i:s', time()).' GMT', true, 200);header('ETag: \"5816f349-19\"');echo $s; ik 接收两个返回的头部属性 Last-Modified 和 ETag，只要其中一个有变化，就会触发更新，ik 会每分钟获取一次重启 Elasticsearch ，查看启动记录，看到了三个词已被加载进来 再次执行上面的请求，返回, 就可以看到 ik 分词器已经匹配到了 “陈港生” 这个词，同理一些关于我们公司的专有名字（例如：永辉、永辉超市、永辉云创、云创 …. ）也可以自己手动添加到字典中去。 2、结巴中文分词特点：1、支持三种分词模式： 精确模式，试图将句子最精确地切开，适合文本分析； 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义； 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。 2、支持繁体分词 3、支持自定义词典 3、THULACTHULAC（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。THULAC具有如下几个特点： 能力强。利用我们集成的目前世界上规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。 准确率高。该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达97.3％，词性标注的F1值可达到92.9％，与该数据集上最好方法效果相当。 速度较快。同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到1.3MB/s。 中文分词工具thulac4j发布 1、规范化分词词典，并去掉一些无用词； 2、重写DAT（双数组Trie树）的构造算法，生成的DAT size减少了8%左右，从而节省了内存； 3、优化分词算法，提高了分词速率。 12345&lt;dependency&gt; &lt;groupId&gt;io.github.yizhiru&lt;/groupId&gt; &lt;artifactId&gt;thulac4j&lt;/artifactId&gt; &lt;version&gt;$&#123;thulac4j.version&#125;&lt;/version&gt;&lt;/dependency&gt; http://www.cnblogs.com/en-heng/p/6526598.html thulac4j支持两种分词模式： SegOnly模式，只分词没有词性标注； SegPos模式，分词兼有词性标注。 12345678910// SegOnly modeString sentence = \"滔滔的流水，向着波士顿湾无声逝去\";SegOnly seg = new SegOnly(\"models/seg_only.bin\");System.out.println(seg.segment(sentence));// [滔滔, 的, 流水, ，, 向着, 波士顿湾, 无声, 逝去]// SegPos modeSegPos pos = new SegPos(\"models/seg_pos.bin\");System.out.println(pos.segment(sentence));//[滔滔/a, 的/u, 流水/n, ，/w, 向着/p, 波士顿湾/ns, 无声/v, 逝去/v] 4、NLPIR中科院计算所 NLPIR：http://ictclas.nlpir.org/nlpir/ (可直接在线分析中文) 下载地址：https://github.com/NLPIR-team/NLPIR 中科院分词系统(NLPIR)JAVA简易教程: http://www.cnblogs.com/wukongjiuwo/p/4092480.html 5、ansj分词器https://github.com/NLPchina/ansj_seg 这是一个基于n-Gram+CRF+HMM的中文分词的java实现. 分词速度达到每秒钟大约200万字左右（mac air下测试），准确率能达到96%以上 目前实现了.中文分词. 中文姓名识别 . 用户自定义词典,关键字提取，自动摘要，关键字标记等功能可以应用到自然语言处理等方面,适用于对分词效果要求高的各种项目. maven 引入： 12345&lt;dependency&gt; &lt;groupId&gt;org.ansj&lt;/groupId&gt; &lt;artifactId&gt;ansj_seg&lt;/artifactId&gt; &lt;version&gt;5.1.1&lt;/version&gt;&lt;/dependency&gt; 调用demo 1234String str = \"欢迎使用ansj_seg,(ansj中文分词)在这里如果你遇到什么问题都可以联系我.我一定尽我所能.帮助大家.ansj_seg更快,更准,更自由!\" ; System.out.println(ToAnalysis.parse(str)); 欢迎/v,使用/v,ansj/en,_,seg/en,,,(,ansj/en,中文/nz,分词/n,),在/p,这里/r,如果/c,你/r,遇到/v,什么/r,问题/n,都/d,可以/v,联系/v,我/r,./m,我/r,一定/d,尽我所能/l,./m,帮助/v,大家/r,./m,ansj/en,_,seg/en,更快/d,,,更/d,准/a,,,更/d,自由/a,! 6、哈工大的LTPhttps://link.zhihu.com/?target=https%3A//github.com/HIT-SCIR/ltp LTP制定了基于XML的语言处理结果表示，并在此基础上提供了一整套自底向上的丰富而且高效的中文语言处理模块（包括词法、句法、语义等6项中文处理核心技术），以及基于动态链接库（Dynamic Link Library, DLL）的应用程序接口、可视化工具，并且能够以网络服务（Web Service）的形式进行使用。 关于LTP的使用，请参考: http://ltp.readthedocs.io/zh_CN/latest/ 7、庖丁解牛下载地址：http://pan.baidu.com/s/1eQ88SZS 使用分为如下几步： 配置dic文件：修改paoding-analysis.jar中的paoding-dic-home.properties文件，将“#paoding.dic.home=dic”的注释去掉，并配置成自己dic文件的本地存放路径。eg：/home/hadoop/work/paoding-analysis-2.0.4-beta/dic 把Jar包导入到项目中：将paoding-analysis.jar、commons-logging.jar、lucene-analyzers-2.2.0.jar和lucene-core-2.2.0.jar四个包导入到项目中，这时就可以在代码片段中使用庖丁解牛工具提供的中文分词技术，例如： 123456789101112Analyzer analyzer = new PaodingAnalyzer(); //定义一个解析器String text = \"庖丁系统是个完全基于lucene的中文分词系统，它就是重新建了一个analyzer，叫做PaodingAnalyzer，这个analyer的核心任务就是生成一个可以切词TokenStream。\"; &lt;span style=\"font-family: Arial, Helvetica, sans-serif;\"&gt;//待分词的内容&lt;/span&gt;TokenStream tokenStream = analyzer.tokenStream(text, new StringReader(text)); //得到token序列的输出流try &#123; Token t; while ((t = tokenStream.next()) != null) &#123; System.out.println(t); //输出每个token &#125;&#125; catch (IOException e) &#123; e.printStackTrace();&#125; 8、sogo在线分词sogo在线分词采用了基于汉字标注的分词方法，主要使用了线性链链CRF（Linear-chain CRF）模型。词性标注模块主要基于结构化线性模型（Structured Linear Model） 在线使用地址为：http://www.sogou.com/labs/webservice/ 9、word分词地址： https://github.com/ysc/word word分词是一个Java实现的分布式的中文分词组件，提供了多种基于词典的分词算法，并利用ngram模型来消除歧义。能准确识别英文、数字，以及日期、时间等数量词，能识别人名、地名、组织机构名等未登录词。能通过自定义配置文件来改变组件行为，能自定义用户词库、自动检测词库变化、支持大规模分布式环境，能灵活指定多种分词算法，能使用refine功能灵活控制分词结果，还能使用词频统计、词性标注、同义标注、反义标注、拼音标注等功能。提供了10种分词算法，还提供了10种文本相似度算法，同时还无缝和Lucene、Solr、ElasticSearch、Luke集成。注意：word1.3需要JDK1.8 maven 中引入依赖： 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apdplat&lt;/groupId&gt; &lt;artifactId&gt;word&lt;/artifactId&gt; &lt;version&gt;1.3&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; ElasticSearch插件： 1234567891011121314151617181920212223242526272829303132333435363738394041424344451、打开命令行并切换到elasticsearch的bin目录cd elasticsearch-2.1.1/bin2、运行plugin脚本安装word分词插件：./plugin install http://apdplat.org/word/archive/v1.4.zip安装的时候注意： 如果提示： ERROR: failed to download 或者 Failed to install word, reason: failed to download 或者 ERROR: incorrect hash (SHA1) 则重新再次运行命令，如果还是不行，多试两次如果是elasticsearch1.x系列版本，则使用如下命令：./plugin -u http://apdplat.org/word/archive/v1.3.1.zip -i word3、修改文件elasticsearch-2.1.1/config/elasticsearch.yml，新增如下配置：index.analysis.analyzer.default.type : &quot;word&quot;index.analysis.tokenizer.default.type : &quot;word&quot;4、启动ElasticSearch测试效果，在Chrome浏览器中访问：http://localhost:9200/_analyze?analyzer=word&amp;text=杨尚川是APDPlat应用级产品开发平台的作者5、自定义配置修改配置文件elasticsearch-2.1.1/plugins/word/word.local.conf6、指定分词算法修改文件elasticsearch-2.1.1/config/elasticsearch.yml，新增如下配置：index.analysis.analyzer.default.segAlgorithm : &quot;ReverseMinimumMatching&quot;index.analysis.tokenizer.default.segAlgorithm : &quot;ReverseMinimumMatching&quot;这里segAlgorithm可指定的值有：正向最大匹配算法：MaximumMatching逆向最大匹配算法：ReverseMaximumMatching正向最小匹配算法：MinimumMatching逆向最小匹配算法：ReverseMinimumMatching双向最大匹配算法：BidirectionalMaximumMatching双向最小匹配算法：BidirectionalMinimumMatching双向最大最小匹配算法：BidirectionalMaximumMinimumMatching全切分算法：FullSegmentation最少词数算法：MinimalWordCount最大Ngram分值算法：MaxNgramScore如不指定，默认使用双向最大匹配算法：BidirectionalMaximumMatching 10、jcseg分词器https://code.google.com/archive/p/jcseg/ 11、stanford分词器Stanford大学的一个开源分词工具，目前已支持汉语。 首先，去【1】下载Download Stanford Word Segmenter version 3.5.2，取得里面的 data 文件夹，放在maven project的 src/main/resources 里。 然后，maven依赖添加： 123456789101112131415161718192021222324&lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;corenlp.version&gt;3.6.0&lt;/corenlp.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt; &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt; &lt;version&gt;$&#123;corenlp.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt; &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt; &lt;version&gt;$&#123;corenlp.version&#125;&lt;/version&gt; &lt;classifier&gt;models&lt;/classifier&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt; &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt; &lt;version&gt;$&#123;corenlp.version&#125;&lt;/version&gt; &lt;classifier&gt;models-chinese&lt;/classifier&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 测试： 12345678910111213141516171819202122232425262728293031323334353637383940414243import java.util.Properties;import edu.stanford.nlp.ie.crf.CRFClassifier;public class CoreNLPSegment &#123; private static CoreNLPSegment instance; private CRFClassifier classifier; private CoreNLPSegment()&#123; Properties props = new Properties(); props.setProperty(\"sighanCorporaDict\", \"data\"); props.setProperty(\"serDictionary\", \"data/dict-chris6.ser.gz\"); props.setProperty(\"inputEncoding\", \"UTF-8\"); props.setProperty(\"sighanPostProcessing\", \"true\"); classifier = new CRFClassifier(props); classifier.loadClassifierNoExceptions(\"data/ctb.gz\", props); classifier.flags.setProperties(props); &#125; public static CoreNLPSegment getInstance() &#123; if (instance == null) &#123; instance = new CoreNLPSegment(); &#125; return instance; &#125; public String[] doSegment(String data) &#123; return (String[]) classifier.segmentString(data).toArray(); &#125; public static void main(String[] args) &#123; String sentence = \"他和我在学校里常打桌球。\"; String ret[] = CoreNLPSegment.getInstance().doSegment(sentence); for (String str : ret) &#123; System.out.println(str); &#125; &#125;&#125; 博客： https://blog.sectong.com/blog/corenlp_segment.html http://blog.csdn.net/lightty/article/details/51766602 12、SmartcnSmartcn为Apache2.0协议的开源中文分词系统，Java语言编写，修改的中科院计算所ICTCLAS分词系统。很早以前看到Lucene上多了一个中文分词的contribution，当时只是简单的扫了一下.class文件的文件名，通过文件名可以看得出又是一个改的ICTCLAS的分词系统。 http://lucene.apache.org/core/5_1_0/analyzers-smartcn/org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer.html 13、pinyin 分词器pinyin分词器可以让用户输入拼音，就能查找到相关的关键词。比如在某个商城搜索中，输入 yonghui，就能匹配到 永辉。这样的体验还是非常好的。 pinyin分词器的安装与IK是一样的。下载地址：https://github.com/medcl/elasticsearch-analysis-pinyin 一些参数请参考 GitHub 的 readme 文档。 这个分词器在1.8版本中，提供了两种分词规则： pinyin,就是普通的把汉字转换成拼音； pinyin_first_letter，提取汉字的拼音首字母 使用： 1.Create a index with custom pinyin analyzer 1234567891011121314151617181920212223curl -XPUT http://localhost:9200/medcl/ -d&apos;&#123; &quot;index&quot; : &#123; &quot;analysis&quot; : &#123; &quot;analyzer&quot; : &#123; &quot;pinyin_analyzer&quot; : &#123; &quot;tokenizer&quot; : &quot;my_pinyin&quot; &#125; &#125;, &quot;tokenizer&quot; : &#123; &quot;my_pinyin&quot; : &#123; &quot;type&quot; : &quot;pinyin&quot;, &quot;keep_separate_first_letter&quot; : false, &quot;keep_full_pinyin&quot; : true, &quot;keep_original&quot; : true, &quot;limit_first_letter_length&quot; : 16, &quot;lowercase&quot; : true, &quot;remove_duplicated_term&quot; : true &#125; &#125; &#125; &#125;&#125;&apos; 2.Test Analyzer, analyzing a chinese name, such as 刘德华 1http://localhost:9200/medcl/_analyze?text=%e5%88%98%e5%be%b7%e5%8d%8e&amp;analyzer=pinyin_analyzer 123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;liu&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 1, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;de&quot;, &quot;start_offset&quot; : 1, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;hua&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;刘德华&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;ldh&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 4 &#125; ]&#125; 3.Create mapping 12345678910111213141516171819curl -XPOST http://localhost:9200/medcl/folks/_mapping -d&apos;&#123; &quot;folks&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;fields&quot;: &#123; &quot;pinyin&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;store&quot;: &quot;no&quot;, &quot;term_vector&quot;: &quot;with_offsets&quot;, &quot;analyzer&quot;: &quot;pinyin_analyzer&quot;, &quot;boost&quot;: 10 &#125; &#125; &#125; &#125; &#125;&#125;&apos; 4.Indexing 1curl -XPOST http://localhost:9200/medcl/folks/andy -d&apos;&#123;&quot;name&quot;:&quot;刘德华&quot;&#125;&apos; 5.Let’s search 12345http://localhost:9200/medcl/folks/_search?q=name:%E5%88%98%E5%BE%B7%E5%8D%8Ecurl http://localhost:9200/medcl/folks/_search?q=name.pinyin:%e5%88%98%e5%be%b7curl http://localhost:9200/medcl/folks/_search?q=name.pinyin:liucurl http://localhost:9200/medcl/folks/_search?q=name.pinyin:ldhcurl http://localhost:9200/medcl/folks/_search?q=name.pinyin:de+hua 6.Using Pinyin-TokenFilter 123456789101112131415161718192021222324252627curl -XPUT http://localhost:9200/medcl1/ -d&apos;&#123; &quot;index&quot; : &#123; &quot;analysis&quot; : &#123; &quot;analyzer&quot; : &#123; &quot;user_name_analyzer&quot; : &#123; &quot;tokenizer&quot; : &quot;whitespace&quot;, &quot;filter&quot; : &quot;pinyin_first_letter_and_full_pinyin_filter&quot; &#125; &#125;, &quot;filter&quot; : &#123; &quot;pinyin_first_letter_and_full_pinyin_filter&quot; : &#123; &quot;type&quot; : &quot;pinyin&quot;, &quot;keep_first_letter&quot; : true, &quot;keep_full_pinyin&quot; : false, &quot;keep_none_chinese&quot; : true, &quot;keep_original&quot; : false, &quot;limit_first_letter_length&quot; : 16, &quot;lowercase&quot; : true, &quot;trim_whitespace&quot; : true, &quot;keep_none_chinese_in_first_letter&quot; : true &#125; &#125; &#125; &#125;&#125;&apos; Token Test:刘德华 张学友 郭富城 黎明 四大天王 1curl -XGET http://localhost:9200/medcl1/_analyze?text=%e5%88%98%e5%be%b7%e5%8d%8e+%e5%bc%a0%e5%ad%a6%e5%8f%8b+%e9%83%ad%e5%af%8c%e5%9f%8e+%e9%bb%8e%e6%98%8e+%e5%9b%9b%e5%a4%a7%e5%a4%a9%e7%8e%8b&amp;analyzer=user_name_analyzer 12345678910111213141516171819202122232425262728293031323334353637383940&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;ldh&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;zxy&quot;, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;gfc&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;lm&quot;, &quot;start_offset&quot; : 12, &quot;end_offset&quot; : 14, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;sdtw&quot;, &quot;start_offset&quot; : 15, &quot;end_offset&quot; : 19, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 4 &#125; ]&#125; 7.Used in phrase query (1)、 123456789101112131415161718192021222324252627282930PUT /medcl/ &#123; &quot;index&quot; : &#123; &quot;analysis&quot; : &#123; &quot;analyzer&quot; : &#123; &quot;pinyin_analyzer&quot; : &#123; &quot;tokenizer&quot; : &quot;my_pinyin&quot; &#125; &#125;, &quot;tokenizer&quot; : &#123; &quot;my_pinyin&quot; : &#123; &quot;type&quot; : &quot;pinyin&quot;, &quot;keep_first_letter&quot;:false, &quot;keep_separate_first_letter&quot; : false, &quot;keep_full_pinyin&quot; : true, &quot;keep_original&quot; : false, &quot;limit_first_letter_length&quot; : 16, &quot;lowercase&quot; : true &#125; &#125; &#125; &#125; &#125; GET /medcl/folks/_search &#123; &quot;query&quot;: &#123;&quot;match_phrase&quot;: &#123; &quot;name.pinyin&quot;: &quot;刘德华&quot; &#125;&#125; &#125; (2)、 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647PUT /medcl/ &#123; &quot;index&quot; : &#123; &quot;analysis&quot; : &#123; &quot;analyzer&quot; : &#123; &quot;pinyin_analyzer&quot; : &#123; &quot;tokenizer&quot; : &quot;my_pinyin&quot; &#125; &#125;, &quot;tokenizer&quot; : &#123; &quot;my_pinyin&quot; : &#123; &quot;type&quot; : &quot;pinyin&quot;, &quot;keep_first_letter&quot;:false, &quot;keep_separate_first_letter&quot; : true, &quot;keep_full_pinyin&quot; : false, &quot;keep_original&quot; : false, &quot;limit_first_letter_length&quot; : 16, &quot;lowercase&quot; : true &#125; &#125; &#125; &#125; &#125; POST /medcl/folks/andy &#123;&quot;name&quot;:&quot;刘德华&quot;&#125; GET /medcl/folks/_search &#123; &quot;query&quot;: &#123;&quot;match_phrase&quot;: &#123; &quot;name.pinyin&quot;: &quot;刘德h&quot; &#125;&#125; &#125; GET /medcl/folks/_search &#123; &quot;query&quot;: &#123;&quot;match_phrase&quot;: &#123; &quot;name.pinyin&quot;: &quot;刘dh&quot; &#125;&#125; &#125; GET /medcl/folks/_search &#123; &quot;query&quot;: &#123;&quot;match_phrase&quot;: &#123; &quot;name.pinyin&quot;: &quot;dh&quot; &#125;&#125; &#125; 14、Mmseg 分词器也支持 Elasticsearch 下载地址：https://github.com/medcl/elasticsearch-analysis-mmseg/releases 根据对应的版本进行下载 如何使用： 1、创建索引： 1curl -XPUT http://localhost:9200/index 2、创建 mapping 123456789101112curl -XPOST http://localhost:9200/index/fulltext/_mapping -d&apos;&#123; &quot;properties&quot;: &#123; &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;term_vector&quot;: &quot;with_positions_offsets&quot;, &quot;analyzer&quot;: &quot;mmseg_maxword&quot;, &quot;search_analyzer&quot;: &quot;mmseg_maxword&quot; &#125; &#125;&#125;&apos; 3.Indexing some docs 123456789101112131415curl -XPOST http://localhost:9200/index/fulltext/1 -d&apos;&#123;&quot;content&quot;:&quot;美国留给伊拉克的是个烂摊子吗&quot;&#125;&apos;curl -XPOST http://localhost:9200/index/fulltext/2 -d&apos;&#123;&quot;content&quot;:&quot;公安部：各地校车将享最高路权&quot;&#125;&apos;curl -XPOST http://localhost:9200/index/fulltext/3 -d&apos;&#123;&quot;content&quot;:&quot;中韩渔警冲突调查：韩警平均每天扣1艘中国渔船&quot;&#125;&apos;curl -XPOST http://localhost:9200/index/fulltext/4 -d&apos;&#123;&quot;content&quot;:&quot;中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首&quot;&#125;&apos; 4.Query with highlighting(查询高亮) 123456789101112curl -XPOST http://localhost:9200/index/fulltext/_search -d&apos;&#123; &quot;query&quot; : &#123; &quot;term&quot; : &#123; &quot;content&quot; : &quot;中国&quot; &#125;&#125;, &quot;highlight&quot; : &#123; &quot;pre_tags&quot; : [&quot;&lt;tag1&gt;&quot;, &quot;&lt;tag2&gt;&quot;], &quot;post_tags&quot; : [&quot;&lt;/tag1&gt;&quot;, &quot;&lt;/tag2&gt;&quot;], &quot;fields&quot; : &#123; &quot;content&quot; : &#123;&#125; &#125; &#125;&#125;&apos; 5、结果： 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot;: 14, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 2, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;index&quot;, &quot;_type&quot;: &quot;fulltext&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 2, &quot;_source&quot;: &#123; &quot;content&quot;: &quot;中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首&quot; &#125;, &quot;highlight&quot;: &#123; &quot;content&quot;: [ &quot;&lt;tag1&gt;中国&lt;/tag1&gt;驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首 &quot; ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;index&quot;, &quot;_type&quot;: &quot;fulltext&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 2, &quot;_source&quot;: &#123; &quot;content&quot;: &quot;中韩渔警冲突调查：韩警平均每天扣1艘中国渔船&quot; &#125;, &quot;highlight&quot;: &#123; &quot;content&quot;: [ &quot;均每天扣1艘&lt;tag1&gt;中国&lt;/tag1&gt;渔船 &quot; ] &#125; &#125; ] &#125;&#125; 参考博客： 为elastic添加中文分词: http://blog.csdn.net/dingzfang/article/details/42776693 15、bosonnlp （玻森数据中文分析器）下载地址：https://github.com/bosondata/elasticsearch-analysis-bosonnlp 如何使用： 运行 ElasticSearch 之前需要在 config 文件夹中修改 elasticsearch.yml 来定义使用玻森中文分析器，并填写玻森 API_TOKEN 以及玻森分词 API 的地址，即在该文件结尾处添加： 12345678910111213141516171819202122index: analysis: analyzer: bosonnlp: type: bosonnlp API_URL: http://api.bosonnlp.com/tag/analysis # You MUST give the API_TOKEN value, otherwise it doesn&apos;t work API_TOKEN: *PUT YOUR API TOKEN HERE* # Please uncomment if you want to specify ANY ONE of the following # areguments, otherwise the DEFAULT value will be used, i.e., # space_mode is 0, # oov_level is 3, # t2s is 0, # special_char_conv is 0. # More detials can be found in bosonnlp docs: # http://docs.bosonnlp.com/tag.html # # # space_mode: put your value here(range from 0-3) # oov_level: put your value here(range from 0-4) # t2s: put your value here(range from 0-1) # special_char_conv: put your value here(range from 0-1) 需要注意的是 必须在 API_URL 填写给定的分词地址以及在API_TOKEN：PUT YOUR API TOKEN HERE 中填写给定的玻森数据API_TOKEN，否则无法使用玻森中文分析器。该 API_TOKEN 是注册玻森数据账号所获得。 如果配置文件中已经有配置过其他的 analyzer，请直接在 analyzer 下如上添加 bosonnlp analyzer。 如果有多个 node 并且都需要 BosonNLP 的分词插件，则每个 node 下的 yaml 文件都需要如上安装和设置。 另外，玻森中文分词还提供了4个参数（space_mode，oov_level，t2s，special_char_conv）可满足不同的分词需求。如果取默认值，则无需任何修改；否则，可取消对应参数的注释并赋值。 测试： 建立 index 1curl -XPUT &apos;localhost:9200/test&apos; 测试分析器是否配置成功 1curl -XGET &apos;localhost:9200/test/_analyze?analyzer=bosonnlp&amp;pretty&apos; -d &apos;这是玻森数据分词的测试&apos; 结果 123456789101112131415161718192021222324252627282930313233343536373839404142434445&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;这&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 1, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;是&quot;, &quot;start_offset&quot; : 1, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;玻森&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 4, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;数据&quot;, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 6, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;分词&quot;, &quot;start_offset&quot; : 6, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;的&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 9, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 5 &#125;, &#123; &quot;token&quot; : &quot;测试&quot;, &quot;start_offset&quot; : 9, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 6 &#125; ]&#125; 配置 Token Filter 现有的 BosonNLP 分析器没有内置 token filter，如果有过滤 Token 的需求，可以利用 BosonNLP Tokenizer 和 ES 提供的 token filter 搭建定制分析器。 步骤 配置定制的 analyzer 有以下三个步骤： 添加 BosonNLP tokenizer在 elasticsearch.yml 文件中 analysis 下添加 tokenizer， 并在 tokenizer 中添加 BosonNLP tokenizer 的配置： 123456789101112131415161718192021222324index: analysis: analyzer: ... tokenizer: bosonnlp: type: bosonnlp API_URL: http://api.bosonnlp.com/tag/analysis # You MUST give the API_TOKEN value, otherwise it doesn&apos;t work API_TOKEN: *PUT YOUR API TOKEN HERE* # Please uncomment if you want to specify ANY ONE of the following # areguments, otherwise the DEFAULT value will be used, i.e., # space_mode is 0, # oov_level is 3, # t2s is 0, # special_char_conv is 0. # More detials can be found in bosonnlp docs: # http://docs.bosonnlp.com/tag.html # # # space_mode: put your value here(range from 0-3) # oov_level: put your value here(range from 0-4) # t2s: put your value here(range from 0-1) # special_char_conv: put your value here(range from 0-1) 添加 token filter 在 elasticsearch.yml 文件中 analysis 下添加 filter， 并在 filter 中添加所需 filter 的配置（下面例子中，我们以 lowercase filter 为例）： 123456789index: analysis: analyzer: ... tokenizer: ... filter: lowercase: type: lowercase 添加定制的 analyzer 在 elasticsearch.yml 文件中 analysis 下添加 analyzer， 并在 analyzer 中添加定制的 analyzer 的配置（下面例子中，我们把定制的 analyzer 命名为 filter_bosonnlp）： 12345678index: analysis: analyzer: ... filter_bosonnlp: type: custom tokenizer: bosonnlp filter: [lowercase] 自定义分词器虽然Elasticsearch带有一些现成的分析器，然而在分析器上Elasticsearch真正的强大之处在于，你可以通过在一个适合你的特定数据的设置之中组合字符过滤器、分词器、词汇单元过滤器来创建自定义的分析器。 字符过滤器： 字符过滤器 用来 整理 一个尚未被分词的字符串。例如，如果我们的文本是HTML格式的，它会包含像 &lt;p&gt; 或者 &lt;div&gt; 这样的HTML标签，这些标签是我们不想索引的。我们可以使用 html清除 字符过滤器 来移除掉所有的HTML标签，并且像把 &amp;Aacute; 转换为相对应的Unicode字符 Á 这样，转换HTML实体。 一个分析器可能有0个或者多个字符过滤器。 分词器: 一个分析器 必须 有一个唯一的分词器。 分词器把字符串分解成单个词条或者词汇单元。 标准 分析器里使用的 标准 分词器 把一个字符串根据单词边界分解成单个词条，并且移除掉大部分的标点符号，然而还有其他不同行为的分词器存在。 词单元过滤器: 经过分词，作为结果的 词单元流 会按照指定的顺序通过指定的词单元过滤器 。 词单元过滤器可以修改、添加或者移除词单元。我们已经提到过 lowercase 和 stop 词过滤器 ，但是在 Elasticsearch 里面还有很多可供选择的词单元过滤器。 词干过滤器 把单词 遏制 为 词干。 ascii_folding 过滤器移除变音符，把一个像 “très” 这样的词转换为 “tres” 。 ngram 和 edge_ngram 词单元过滤器 可以产生 适合用于部分匹配或者自动补全的词单元。 创建一个自定义分析器我们可以在 analysis 下的相应位置设置字符过滤器、分词器和词单元过滤器: 1234567891011PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;char_filter&quot;: &#123; ... custom character filters ... &#125;, &quot;tokenizer&quot;: &#123; ... custom tokenizers ... &#125;, &quot;filter&quot;: &#123; ... custom token filters ... &#125;, &quot;analyzer&quot;: &#123; ... custom analyzers ... &#125; &#125; &#125;&#125; 这个分析器可以做到下面的这些事: 1、使用 html清除 字符过滤器移除HTML部分。 2、使用一个自定义的 映射 字符过滤器把 &amp; 替换为 “和” ： 123456&quot;char_filter&quot;: &#123; &quot;&amp;_to_and&quot;: &#123; &quot;type&quot;: &quot;mapping&quot;, &quot;mappings&quot;: [ &quot;&amp;=&gt; and &quot;] &#125;&#125; 3、使用 标准 分词器分词。 4、小写词条，使用 小写 词过滤器处理。 5、使用自定义 停止 词过滤器移除自定义的停止词列表中包含的词： 123456&quot;filter&quot;: &#123; &quot;my_stopwords&quot;: &#123; &quot;type&quot;: &quot;stop&quot;, &quot;stopwords&quot;: [ &quot;the&quot;, &quot;a&quot; ] &#125;&#125; 我们的分析器定义用我们之前已经设置好的自定义过滤器组合了已经定义好的分词器和过滤器： 12345678&quot;analyzer&quot;: &#123; &quot;my_analyzer&quot;: &#123; &quot;type&quot;: &quot;custom&quot;, &quot;char_filter&quot;: [ &quot;html_strip&quot;, &quot;&amp;_to_and&quot; ], &quot;tokenizer&quot;: &quot;standard&quot;, &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;my_stopwords&quot; ] &#125;&#125; 汇总起来，完整的 创建索引 请求 看起来应该像这样： 1234567891011121314151617181920212223PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;char_filter&quot;: &#123; &quot;&amp;_to_and&quot;: &#123; &quot;type&quot;: &quot;mapping&quot;, &quot;mappings&quot;: [ &quot;&amp;=&gt; and &quot;] &#125;&#125;, &quot;filter&quot;: &#123; &quot;my_stopwords&quot;: &#123; &quot;type&quot;: &quot;stop&quot;, &quot;stopwords&quot;: [ &quot;the&quot;, &quot;a&quot; ] &#125;&#125;, &quot;analyzer&quot;: &#123; &quot;my_analyzer&quot;: &#123; &quot;type&quot;: &quot;custom&quot;, &quot;char_filter&quot;: [ &quot;html_strip&quot;, &quot;&amp;_to_and&quot; ], &quot;tokenizer&quot;: &quot;standard&quot;, &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;my_stopwords&quot; ] &#125;&#125;&#125;&#125;&#125; 索引被创建以后，使用 analyze API 来 测试这个新的分析器： 12GET /my_index/_analyze?analyzer=my_analyzerThe quick &amp; brown fox 下面的缩略结果展示出我们的分析器正在正确地运行： 12345678&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;quick&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;and&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;brown&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;fox&quot;, &quot;position&quot; : 5 &#125; ]&#125; 这个分析器现在是没有多大用处的，除非我们告诉 Elasticsearch在哪里用上它。我们可以像下面这样把这个分析器应用在一个 string 字段上： 123456789PUT /my_index/_mapping/my_type&#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;my_analyzer&quot; &#125; &#125;&#125; 最后整理参考网上资料，如有不正确的地方还请多多指教！","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://yoursite.com/tags/ElasticSearch/"}]},{"title":"那些年我看过的书 —— 致敬我的大学生活 —— Say Good Bye ！","date":"2017-08-27T16:00:00.000Z","path":"2017/08/28/recommend-books/","text":"开头2017.08.21 正式开启我入职的里程，现在已是工作了一个星期了，这个星期算是我入职的过渡期，算是知道了学校生活和工作的差距了，总之，尽快习惯这种生活吧。下面讲下自己的找工作经历和大学阅读的书籍，算是一种书籍推荐，为还在迷茫的你指引方向，同时为我三年的大学生活致敬！也激励我大四在公司实习能更上一层楼！ 找工作经历这段经历，算是自己很难忘记的经历吧。既辛酸既充实的日子！也很感谢自己在这段时间的系统复习，感觉把自己的基础知识再次聚集在一起了，自己的能力在这一段时间提升的也很快。后面有机会的话我也想写一系列的相关文章，为后来准备工作（面试）的同学提供一些自己的帮助。自己在找工作的这段时间面过的公司也有几家大厂，但是结果都不是很好，对我自己有很大的压力，当时心里真的感觉 ：“自己真的有这么差”，为什么一直被拒，当时很怀疑自己的能力，自己也有总结原因。一是面试的时候自己准备的还不够充分，虽说自己脑子里对这些基础有点印象，但是面试的时候自己稍紧张下就描述不怎么清楚了，导致面试官觉得你可能广度够了，深度还不够（这是阿里面试官电话面试说的）；二是自己的表达能力还是有所欠缺，不能够将自己所要表达的东西说出来，这可能我要在后面加强的地方；三是我的学校问题。在面了几家公司失败后，终于面了家公司要我了，我也确定在这家公司了。很幸运，刚出来，就有一个很好（很负责）的架构师带我，这周就给了我一个很牛逼的项目给我看（虽然自己目前还没有思路改里面的代码），里面新东西很多，说吃透了这个项目，以后绝对可以拿出去吹逼（一脸正经.jpg）。目前我的找工作经历就简短的介绍到这里了，如果感兴趣的话，可以加群：528776268 进来和我讨论交流。 书籍推荐大学，我不怎么喜欢玩游戏，自己也还算不怎么堕落吧，看了以下的一些书籍，算是对我后面写博客、找工作也有很大的帮助。如果你是大神，请忽略，如果你还是还在大学，和我一样不想把时间浪费在游戏上，可以看看我推荐的一些书籍，有想讨论的请在评论下留下你的评论或者加上面给的群号。 Java1、《Java 核心技术》卷一 、卷二 两本书，算是入门比较好的书籍了 2、《疯狂 Java 讲义》 很厚的一本书，里面的内容也是很注重基础了 3、《Java 并发编程的艺术》—— 方腾飞 、魏鹏、程晓明著 方腾飞 是并发编程网的创始人，里面的文章确实还不错，可以多看看里面的文章，收获绝对很大。 4、《 Java多线程编程核心技术》—— 高洪岩著 这本书也算是入门多线程编程的不错书籍，我之前还写了一篇读书笔记呢，《Java 多线程编程核心技术》学习笔记及总结 , 大家如果不想看书的可以去看我的笔记。 5、《Java 并发编程实战》 这本书讲的有点难懂啊，不过确实也是一本很好的书，以上三本书籍如果都弄懂了，我觉得你并发编程这块可能大概就 OK 了，然后再去看看线程池的源码，了解下线程池，我觉得那就更棒了。不想看的话，请看我的博客：Java 线程池艺术探索 我个人觉得还是写的很不错，那些大厂面试也几乎都会问线程池的东西，然后大概内容也就是我这博客写的 6、《Effective Java》中文版 第二版 算是 Java 的进阶书籍了，面试好多问题也是从这出来的 7、《深入理解 Java 虚拟机——JVM高级特性与最佳实践》第二版 这算是国内讲 JVM 最清楚的书了吧，目前还是只看了一遍，后面继续啃，大厂面试几乎也是都会考 JVM 的，阿里面 JVM 特别多，想进阿里的同学请一定要买这本书去看。 8、《深入分析Java Web技术内幕 修订版》许令波著 里面知识很广，每一章都是一个不同的知识，可见作者的优秀，不愧是阿里大神。 9、《大型网站系统与 Java 中间件实践》—— 曽宪杰 著 作者是前淘宝技术总监，见证了淘宝网的发展，里面的讲的内容也是很好，看完能让自己也站在高处去思考问题。 10、《大型网站技术架构 —— 核心原理与案例分析》 —— 李智慧 著 最好和上面那本书籍一起看，效果更好，两本看完了，提升思想的高度！ 11、《疯狂Java.突破程序员基本功的16课》 李刚 著 书中很注重 Java 的一些细节，讲的很深入，但是书中的错别字特多，可以看看我的读书笔记：《疯狂 Java 突破程序员基本功的 16 课》读书笔记 12、《Spring 实战》 Spring 入门书籍 13、《Spring 揭秘》—— 王福强 著 这本书别提多牛了，出版时期为 2009 年，豆瓣评分为 9.0 分，写的是真棒！把 Spring 的 IOC 和 AOP 特性写的很清楚，把 Spring 的来龙去脉讲的很全。墙裂推荐这本书籍，如果你想看 Spring，作者很牛，资深架构师，很有幸和作者有过一次交流，当时因为自己的一篇博客 Pyspider框架 —— Python爬虫实战之爬取 V2EX 网站帖子，竟然找到我想叫我去实习，可惜了，当时差点就跟着他混了。作者还有一本书 《Spring Boot 揭秘》。 14、《Spring 技术内幕》—— 深入解析 Spring 架构与设计原理 讲解 Spring 源码，深入了内部机制，个人觉得还是不错的。 15、Spring 官方的英文文档 这个别提了，很好，能看英文尽量看英文 16、《跟开涛学 Spring 3》 《跟开涛学 Spring MVC》 京东大神，膜 17、《看透springMvc源代码分析与实践》 算是把 Spring MVC 源码讲的很好的了 见我的笔记： 1、通过源码详解 Servlet 2 、看透 Spring MVC 源代码分析与实践 —— 网站基础知识 3 、看透 Spring MVC 源代码分析与实践 —— 俯视 Spring MVC 4 、看透 Spring MVC 源代码分析与实践 —— Spring MVC 组件分析 18、《Spring Boot 实战》 19、Spring Boot 官方 Reference Guide 网上好多写 SpringBoot 的博客，几乎和这个差不多。 20、《JavaEE开发的颠覆者: Spring Boot实战》 21、MyBatis 当然是官方的文档最好了，而且还是中文的。 自己也写过几篇文章，帮助过很多人入门，传送门： 1、通过项目逐步深入了解Mybatis（一）/) 2、通过项目逐步深入了解Mybatis（二）/) 3、通过项目逐步深入了解Mybatis（三）/) 4、通过项目逐步深入了解Mybatis（四）/) 22、《深入理解 Java 内存模型》—— 程晓明 著 我觉得每个 Java 程序员都应该了解下 Java 的内存模型，该书籍我看的是电子版的，不多，但是讲的却很清楚，把重排序、顺序一致性、Volatile、锁、final等写的很清楚。 Linux《鸟哥的Linux私房菜 基础学习篇(第三版) 》 鸟哥的Linux私房菜：服务器架设篇(第3版) 鸟哥的书 计算机网络《计算机网络第六版——谢希仁 编》 《计算机网络自顶向下方法》 计算机系统《代码揭秘：从C／C.的角度探秘计算机系统 —— 左飞》 《深入理解计算机系统》 《计算机科学导论_佛罗赞》 数据库《高性能MySQL》 《Mysql技术内幕InnoDB存储引擎》 Python这门语言语法很简单，上手快，不过我目前好久没用了，都忘得差不多了。当时是看的廖雪峰的 Python 博客 自己也用 Python 做爬虫写过几篇博客，不过有些是在前人的基础上写的。感谢那些栽树的人！ 工具Git ： 廖雪峰的 Git 教程 IDEA：IntelliJ IDEA 简体中文专题教程 Maven：《Maven实战》 其他《如何高效学习-斯科特杨》 教你怎样高效学习的 《软技能：代码之外的生存指南》 程序员除了写代码，还得懂点其他的软技能。 《提问的智慧“中文版”》 《How-To-Ask-Questions-The-Smart-Way》 作为程序员的你，一定要学会咋提问，不然别人都不想鸟你。 优秀网站推荐1、GitHub 别和我说不知道 2、InfoQ 文章很不错 3、CSDN 经常看博客专家的博客，里面大牛很多，传送门：zhisheng 4、知乎 多关注些大牛，看他们吹逼 5、掘金 自己也在上面写专栏，粉丝已经超过一万了，传送门 ：zhisheng 6、并发编程网 前面已经介绍 7、developerworks 上面的博客也很好 8、博客园 里面应该大牛也很多，不过自己没在上面写过博客 9、微信公众号 关注了很多人，有些人的文章确实很好。 10、牛客网 刷笔试题不错的地方，里面大牛超多，怀念叶神和左神讲课的时候，还有很有爱的牛妹。 11、优秀博主的博客地址了 优秀博客推荐廖雪峰 Git 和 Python 入门文章就是从他博客看的 阮一峰的网络日志 酷壳-陈皓 RednaxelaFX R大，牛逼的不得了 江南白衣 老司机 stormzhang 人称帅逼张，微信公众号写的不错 你假笨 阿里搞 JVM 的，很厉害 占小狼 泥瓦匠BYSocket 崔庆才 写了好多 Python 爬虫相关的文章 纯洁的微笑 SpringBoot 系列不错，其他的文章自己看了感觉是自己喜欢的那种文笔 程序猿DD 周立 芋艿V的博客 好多系列的源码分析 zhisheng 这个是我不要脸，竟然把自己博客地址的写上去了 最后送一句话，越努力，越幸运，祝早日成为大神！ 这些地方可以找到我： blog: http://www.54tianzhisheng.cn/ GitHub: https://github.com/zhisheng17 QQ 群：528776268","tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]},{"title":"Ubuntu16.10 安装 Nginx","date":"2017-08-17T16:00:00.000Z","path":"2017/08/18/Ubuntu-install-Nginx/","text":"安装 Nginx 依赖库安装 gcc g++ 的依赖库Ubuntu 平台使用： 12apt-get install build-essentialapt-get install libtool CentOS 平台使用： 12345centos平台编译环境使用如下指令安装make：yum -y install gcc automake autoconf libtool make安装g++:yum install gcc gcc-c++ 安装 pcre 依赖库12sudo apt-get updatesudo apt-get install libpcre3 libpcre3-dev 安装 zlib 依赖库1apt-get install zlib1g-dev 安装 ssl 依赖库1apt-get install openssl 安装 Nginx在网上下载了 nginx-1.8.1.tar.gz 版本。 1234567891011121314151617#解压：tar -zxvf nginx-1.8.1.tar.gz#进入解压目录：cd nginx-1.8.1#配置：./configure --prefix=/usr/local/nginx#编辑nginx：make注意：这里可能会报错，提示“pcre.h No such file or directory”,具体详见：http://stackoverflow.com/questions/22555561/error-building-fatal-error-pcre-h-no-such-file-or-directory需要安装 libpcre3-dev,命令为：sudo apt-get install libpcre3-dev#安装nginx：sudo make install#启动nginx：sudo /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf注意：-c 指定配置文件的路径，不加的话，nginx会自动加载默认路径的配置文件，可以通过 -h查看帮助命令。#查看nginx进程：ps -ef|grep nginx Nginx 常用命令启动 Nginx切换到 /usr/local/nginx/sbin/ 目录下，执行命令 1./nginx 查看效果： 停止 Nginx12./nginx -s stop./nginx -s quit -s 都是采用向 Nginx 发送信号的方式。 Nginx 重新加载配置文件1./nginx -s reload 指定配置文件1./nginx -c /usr/local/nginx/conf/nginx.conf -c 表示 configuration，指定配置文件 查看 Nginx 版本12./nginx -v //查看 Nginx 版本信息的参数./nginx -V //查看 Nginx 详细的版本信息 检查配置文件是否正确1./nginx -t 如果出现测试失败，表示没有访问错误日志文件和进程，可以 sudo 一下。配置正确的话会有相关的提示。 显示帮助信息123./nginx -h或者./nginx -? Nginx 的特点和应用场合见文章：Nginx 基本知识快速入门 最后文章首发地址：zhisheng的博客 ，转载请注明地址 http://www.54tianzhisheng.cn/2017/08/18/Ubuntu-install-Nginx/","tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://yoursite.com/tags/Nginx/"}]},{"title":"马云热血励志演讲《最伟大的成功》","date":"2017-08-10T16:00:00.000Z","path":"2017/08/11/most-success/","text":"当你乐观的时候，总是会有机会的。那么你可能要问了“机会在哪呢？”你可能没有特别想实现的事，没有迫切要成功的欲望，没有勇攀高峰的决心，没有水滴石穿的毅力，也没有一颗所向披靡的强大心脏。那么，让来自马云的这段演讲来告诉你，你有什么。你有年轻的身体，你有奇妙的想法，你有乐观的心态，你有无限的可能性。 我想，马云的这段励志演讲为我们提供了一面镜子可以用于自照。上面“你有什么”说完了，接下来让我讨论一下“你可能没有什么”。 你总爱抱怨，机会却总是躲在人们抱怨的地方。你没有仔细想想怎么能把事情做得不一样。你没有行动力，你缺少坚持下去的长劲儿。你抗压能力差，你动不动就玻璃心。你不相信自己也不相信别人，你怕犯错。现在是不是觉得这碗鸡汤有点难以下咽了？如果认识到差距，不如从今天开始改变。明天的你只要比今天的你多迈出0.1步，也是进步。 为自己而工作。停止抱怨，用抱怨的时间多做事。把那些夜里冒出来的好点子在白天付诸行动，既然有了设想，那就行动起来。行动是你迈出的第一步，后面可能会更难，历经无数次动摇，面临无数次诱惑，感受无数次失败的苦味和难以为继的辛酸。顶住这一切，比常人更勇敢地去面对，并且坚持下去。排除万难，别被来自世人的非议和质疑影响。相信你自己，相信你的团队。服务好你的客户，之后再想怎么回馈社会。犯足够多的错，年轻时走过的弯路是最棒的收获。 马云还曾经说过“今天很艰难，明天比今天更难，后天可能是美好的，但更多的人死在了明天”。是不是感到膝盖中箭了？不妨干了这碗“毒鸡汤”。成功的法则本就并非千篇一律，你会有你自己向上的学问。那不如从明天开始，去摸索，去践行，哪怕只比昨天的你多迈出0.01步。 送给正在找实习工作的自己！加油！！！","tags":[{"name":"励志","slug":"励志","permalink":"http://yoursite.com/tags/励志/"}]},{"title":"源码大招：不服来战！撸这些完整项目，你不牛逼都难！","date":"2017-08-07T16:00:00.000Z","path":"2017/08/08/android-projects/","text":"经常有人问我有没有什么项目代码，我回复说去 Github 找，但是还是好多人不知道如何找到那些比较好的项目。 今天花了点时间找了些安卓的项目，觉得还是不错的，几乎就是自己生活常用的一些 app ，如果你是一个 Android 开发者，我觉得撸完这些项目，你想不牛逼都难。 关注我 菜鸟新闻菜鸟新闻 客户端是一个仿照36Kr官方,实 时抓取36Kr官网数据的资讯类新闻客户端。 包括首页新闻,详情,发现,活动,实时数据抓取,侧滑效果,第三方登录以及分享,消息推送等相关功能客户端。 课程地址： http://www.cniao5.com/clazz/view/10076.html视频下载链接： http://pan.baidu.com/s/1eQLyQxc 密码：3ts1 项目源码下载地址：https://github.com/yxs666/cniao5-news 运行截图: .gif) KuaiChuan仿茄子快传的一款文件传输应用， 涉及到Socket通信，包括TCP，UDP通信 项目源码：https://github.com/mayubao/KuaiChuan 运行截图： CoolShopping一个仿拉手团购的购物App，采用Bmob后台实现短信验证码注册、登录、收藏、订单管理、自动更新等功能，数据抓取自拉手团购 项目地址：https://github.com/myxh/CoolShopping 运行截图： RNPolymerPoRNPolymerPo 是一个基于 React Native 的生活类聚合实战项目，目前由于没有 MAC 设备，所以没有适配 iOS，感兴趣的可以自行适配 app 目录下相关 JS 代码即可。 项目地址：https://github.com/yanbober/RNPolymerPo 运行截图： bilibili仿 bilibili 的客户端 项目地址：https://github.com/HotBitmapGG/bilibili-android-client 运行截图： StockChart采用主流rxjava+retrofit+dagger2框架，StockChart看股票的分时图，k线图。 项目地址：https://github.com/AndroidJiang/StockChart Android精准计步器项目地址：https://github.com/linglongxin24/DylanStepCount 运行截图： 菜鸟微博菜鸟微博《通过对新浪微博开发案例的详细解析，讲解了一个完整的 Android 实际项目的开发过程。 有新浪微博的主要功能，有Toolbar,RecyclerView等最新控件的用法；各种快速开发框架的使用，比如 Glide,PhotoView ，EventBus ，OKHttp，pullToRefresh等。 学习视频+源码 视频中还会讲到MVP设计模式以及一些架构师的入门知识。 课程地址： http://www.cniao5.com/clazz/view/10075.html视频下载链接： http://pan.baidu.com/s/1gexq3VP 密码：f0t9 项目地址：https://github.com/yxs666/cniao5-weibo 运行截图： 在线云打印平台一个在线云打印平台（android部分）含订单管理、百度地图、二维码等等 项目地址：https://github.com/LehmanHe/A4print 运行截图： 铜板街项目地址：https://github.com/robotlife/TongBanJie 运行截图： 礼物说项目地址：https://github.com/Orangelittle/Liwusuo IotXmpp本项目是基于XMPP的物联网客户端软件的实现，其实现的主要功能是一款能和物联网节点交互的即时通讯软件。目前支持九类传感器节点交互，主要有：温湿度、风扇、直流电机、LED灯、步进电机、门磁、光电接近、烟雾和光照。本软件不仅能和这些传感器节点交互，还实现了类似微信的订阅和取消订阅功能。当订阅一个节点后节点就会按照设定好的周期向客户端汇报数据，客户端也能设置周期、设置报警上下限等。这些功能的实现极大的方便了我们和物联网节点的交互。 项目地址：https://github.com/tiandawu/IotXmpp 项目截图： Lives生活娱乐结合的APP, 现有主要功能： 图书 翻译 音乐 视频 项目地址：https://github.com/Allyns/Lives 项目截图： CoCoin一款多视图记账APP 项目地址：https://github.com/Nightonke/CoCoin 运行截图： AppLockAppLock应用锁，保护你的隐私 项目地址：https://github.com/lizixian18/AppLock 运行截图： jianshi 简诗一款优雅的中国风Android App，包括Android端和Server端，支持登录注册，数据云端同步，离线数据存储和截屏分享等功能。 项目地址： 运行截图： storage-chooser一款文件管理器app 项目地址：https://github.com/codekidX/storage-chooser 运行截图： LQRWeChat仿最新版微信6.5.7（除图片选择器外）。本项目基于融云SDK，使用目前较火的 Rxjava+Retrofit+MVP+Glide 技术开发。相比上个版本，加入发送位置消息，红包消息等功能。 项目地址：https://github.com/GitLqr/LQRWeChat 运行截图： PonyExpress 小马快递小马快递，您的好帮手。查询并跟踪快递，快递信息及时掌握。支持全国100多家快递公司，支持扫码查询，智能识别快递公司。附带生成二维码小工具，方便实用。体积小巧，无广告，无多余权限。 项目地址：https://github.com/wangchenyan/PonyExpress 运行截图： CloudReader 云阅一款基于网易云音乐UI，使用Gank.Io及豆瓣api开发的符合Google Material Design的Android客户端。项目采取的是MVVM-DataBinding架构开发，现主要包括：干货区、电影区和书籍区三个子模块。DIY网易云音乐原来是如此Cool 项目地址：https://github.com/youlookwhat/CloudReader 运行截图： 硅谷商城是一款按照企业级标准研发的项目。本套代码是目前国内市场第一套详细讲解商城类项目的免费代码。该代码中的内容包括但不仅限于，框架的搭建 、主页模块、分类模块、发现模块、购物车模块和个人中心模块。项目中讲解的主流技术包括且不限于RadioGroup + Fragment、OKHttp、FastJson、RecyclerView、 ScrollViewContainer、Banner、倒计时秒杀、自定义购物车、支付宝等技术。该项目中讲解的技术可应用在电商、新闻、旅游、医疗、在线教育等领域。 项目地址：https://github.com/atguigu01/Shopping 运行截图： 觉得棒的，欢迎点赞和转发分享，谢谢大家！转载的话注明来源地址为 www.54tianzhisheng.cn/2017/05/13/android-projects/ 即可。","tags":[{"name":"Android","slug":"Android","permalink":"http://yoursite.com/tags/Android/"}]},{"title":"Nginx 基本知识快速入门","date":"2017-08-04T16:00:00.000Z","path":"2017/08/05/Nginx/","text":"什么是 Nginx？Nginx 是一个高性能的 HTTP 和反向代理服务器，以高稳定性、丰富的功能集、示例配置文件和低系统资源的消耗而闻名。 Nginx 特点 处理静态文件，索引文件以及自动索引；打开文件描述符缓冲． 无缓存的反向代理加速，简单的负载均衡和容错． FastCGI，简单的负载均衡和容错． 模块化的结构。包括 gzipping, byte ranges, chunked responses,以及 SSI-filter 等 filter。如果由 FastCGI 或其它代理服务器处理单页中存在的多个 SSI，则这项处理可以并行运行，而不需要相互等待。 支持 SSL 和 TLSSNI． 主要应用场合1、静态 HTTP 服务器首先，Nginx是一个 HTTP 服务器，可以将服务器上的静态文件（如 HTML、图片）通过 HTTP 协议展现给客户端。 配置： 123456server &#123; listen 80; # 端口号 location / &#123; root /usr/share/nginx/html; # 静态文件路径 &#125;&#125; 2、反向代理服务器什么是反向代理？ 客户端本来可以直接通过 HTTP 协议访问某网站应用服务器，如果网站管理员在中间加上一个 Nginx，客户端请求Nginx，Nginx 请求应用服务器，然后将结果返回给客户端，此时 Nginx 就是反向代理服务器。 配置： 123456server &#123; listen 80; location / &#123; proxy_pass http://192.168.20.1:8080; # 应用服务器HTTP地址 &#125;&#125; 既然服务器可以直接 HTTP 访问，为什么要在中间加上一个反向代理，不是多此一举吗？反向代理有什么作用？继续往下看，下面的负载均衡、虚拟主机，都基于反向代理实现，当然反向代理的功能也不仅仅是这些。 3、负载均衡当网站访问量非常大，网站站长开心赚钱的同时，也摊上事儿了。因为网站越来越慢，一台服务器已经不够用了。于是将相同的应用部署在多台服务器上，将大量用户的请求分配给多台机器处理。同时带来的好处是，其中一台服务器万一挂了，只要还有其他服务器正常运行，就不会影响用户使用。 当我们网站进行大的升级更新时，我们不可能直接将所有的服务器都关掉，然后再升级的。通常我们都是批量的关掉一些服务器，去升级网站，当有用户的请求时则分配给其他还在运作的机器处理。当之前关掉的机器更新完成后，再次开启，然后又批量关掉部分机器，如上循环，直到最后全部机器都更新完成。这样就不会影响用户使用。 Nginx 可以通过反向代理来实现负载均衡。 配置： 12345678910upstream myapp &#123; server 192.168.20.1:8080; # 应用服务器1 server 192.168.20.2:8080; # 应用服务器2&#125;server &#123; listen 80; location / &#123; proxy_pass http://myapp; &#125;&#125; 4、虚拟主机网站访问量大，需要负载均衡。然而并不是所有网站都如此出色，有的网站，由于访问量太小，需要节省成本，将多个网站部署在同一台服务器上。 例如将 www.aaa.com 和 www.bbb.com 两个网站部署在同一台服务器上，两个域名解析到同一个 IP 地址，但是用户通过两个域名却可以打开两个完全不同的网站，互相不影响，就像访问两个服务器一样，所以叫两个虚拟主机。 配置： 12345678910111213141516171819server &#123; listen 80 default_server; server_name _; return 444; # 过滤其他域名的请求，返回444状态码&#125;server &#123; listen 80; server_name www.aaa.com; # www.aaa.com域名 location / &#123; proxy_pass http://localhost:8080; # 对应端口号8080 &#125;&#125;server &#123; listen 80; server_name www.bbb.com; # www.bbb.com域名 location / &#123; proxy_pass http://localhost:8081; # 对应端口号8081 &#125;&#125; 在服务器 8080 和 8081 两个端口分别开了一个应用，客户端通过不同的域名访问，根据 server_name 可以反向代理到对应的应用服务器。 虚拟主机的原理是通过 HTTP 请求头中的 Host 是否匹配 server_name 来实现的，有兴趣的同学可以研究一下 HTTP 协议。 另外，server_name 配置还可以过滤有人恶意将某些域名指向你的主机服务器。 5、FastCGINginx 本身不支持 PHP 等语言，但是它可以通过 FastCGI 来将请求扔给某些语言或框架处理（例如 PHP、Python、Perl）。 123456789server &#123; listen 80; location ~ \\.php$ &#123; include fastcgi_params; fastcgi_param SCRIPT_FILENAME /PHP文件路径$fastcgi_script_name; # PHP文件路径 fastcgi_pass 127.0.0.1:9000; # PHP-FPM地址和端口号 # 另一种方式：fastcgi_pass unix:/var/run/php5-fpm.sock; &#125;&#125; 配置中将 .php 结尾的请求通过 FashCGI 交给 PHP-FPM 处理，PHP-FPM 是 PHP 的一个 FastCGI 管理器。有关FashCGI 可以查阅其他资料，本文不再介绍。 fastcgi_pass 和 proxy_pass 有什么区别？下面一张图带你看明白： 参考资料1、Nginx基本功能极速入门 2、Nginx 学习笔记","tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://yoursite.com/tags/Nginx/"}]},{"title":"秋招第一站 —— 亚信科技","date":"2017-08-03T16:00:00.000Z","path":"2017/08/04/yaxin/","text":"第 1 站、亚信科技 Java 开发1）自我介绍（说到一个亮点：长期坚持写博客，面试官觉得这个习惯很好，算加分项吧） 2）看到简历项目中用到 Solr，详细的问了下 Solr（自己介绍了下 Solr 的使用场景和建立索引等东西）3）项目里面写了一个 “ 敏感词和 JS 标签过滤防 XSS 攻击”，面试官让我讲了下这个 XSS 攻击，并且是怎样实现的 4）项目里写了支持 Markdown，问是不是自己写的解析代码，（回答不是，自己引用的是 GitHub上的一个开源项目解析的） 5）想问我前端的知识，我回复到：自己偏后端开发，前端只是了解，然后面试官就不问了 6）问我考不考研？ 7）觉得杭州怎么样？是打算就呆在杭州还是把杭州作为一个跳板？ 8）有啥小目标？以后是打算继续技术方向，还是先技术后管理（还开玩笑的说：是不是赚他几个亿，当时我笑了笑） 9）有啥兴趣爱好？ 大概就记得这么多了，目前已经拿到 Offer 了。 总结：面试问的问题不算多，主要是通过简历上项目所涉及的东西提问的，如果自己不太会的切记不要写上去。面试主要考察你回答问题来判断你的逻辑是否很清楚。","tags":[{"name":"面经","slug":"面经","permalink":"http://yoursite.com/tags/面经/"}]},{"title":"秋招第二站 —— 内推爱奇艺（一面二面）","date":"2017-08-03T16:00:00.000Z","path":"2017/08/04/iqiyi/","text":"第 2 站 、爱奇艺 后端 Java 开发实习生笔试（半个小时）题目：（记得一些） 1、重载重写的区别？ 2、转发和重定向的区别？3、画下 HashMap 的结构图？HashMap 、 HashTable 和 ConcurrentHashMap 的区别？ 4、statement 和 preparedstatement 区别？ 5、JSP 中一个 中取值与直接取值的区别？会有什么安全问题？ 6、实现一个线程安全的单例模式 7、一个写 sql 语句的题目 8、自己实现一个 List，（主要实现 add等常用方法） 9、Spring 中 IOC 和 AOP 的理解？ 10、两个对象的 hashcode 相同，是否对象相同？equal() 相同呢？ 11、@RequestBody 和 @ResponseBody 区别？ 12、JVM 一个错误，什么情况下会发生？ 13、常用的 Linux 命令？ 第一轮面试（80 分钟）1、自我介绍 2、介绍你最熟悉的一个项目 3、讲下这个 XSS 攻击 4、HashMap 的结构？HashMap 、 HashTable 和 ConcurrentHashMap 的区别？ 5、HashMap 中怎么解决冲突的？（要我详细讲下） 6、ConcurrentHashMap 和 HashTable 中线程安全的区别？为啥建议用 ConcurrentHashMap ？能把 ConcurrentHashMap 里面的实现详细的讲下吗？ 7、Session 和 Cookie 的区别？ 8、你项目中登录是怎样做的，用的 Cookie 和 Session？ 9、讲讲你对 Spring 中的 IOC 和 AOP 的理解？ 10、问了好几个注解的作用？ 11、statement 和 preparedstatement 区别？ 12、$ 和 # 的区别？以及这两个在哪些地方用？ 13、前面项目介绍了数据是爬虫爬取过来的，那你讲讲你的爬虫是多线程的吧？ 14、讲讲 Python 中的多线程和 Java 中的多线程区别？ 15、自己刚好前几天在看线程池，立马就把面试官带到我熟悉的线程池，和面试官讲了下 JDK 自带的四种线程池、ThreadPoolExecutor 类中的最重要的构造器里面的七个参数，然后再讲了下线程任务进入线程池和核心线程数、缓冲队列、最大线程数量比较。 16、线程同步，你了解哪几种方式？ 17、讲下 Synchronized？ 18、讲下 RecentLock 可重入锁？ 什么是可重入锁？为什么要设计可重入锁？ 19、讲下 Volatile 吧？他是怎样做到同步的？ 20、Volatile 为什么不支持原子性？举个例子 21、Atomic 怎么设计的？（没看过源码，当时回答错了，后来才发现里面全部用 final 修饰的属性和方法） 22、问几个前端的标签吧？（问了一个不会，直接说明我偏后端，前端只是了解，后面就不问了） 23、SpringBoot 的了解？ 24、Linux 常用命令？ 25、JVM 里的几个问题？ 26、事务的特性？ 27、隔离级别？ 28、网络状态码？以 2、3、4、5 开头的代表什么意思。 29、并发和并行的区别？ 30、你有什么问题想问我的？ 一面面完后面试官和说这份试卷是用来考 1~3 年开发工作经验的，让我准备一下，接下来的二面。 第二轮面试（半个小时）1、一上来就问怎么简历名字都没有，我指了简历第一行的我的名字，还特意大写了，然后就问学校是不是在上海，我回答在南昌（感觉被鄙视了一波，后面我在回答问题的时候面试官就一直在玩手机，估计后面对我的印象就不是很好了） 2、自我介绍 3、说一说数据库建表吧（从范式讲） 4、讲讲多态？（这个我答出来了，可是面试官竟然说不是这样吧，可能面试官没听请，后面还说我是不是平时写多态比较少，感觉这个也让面试官对我印象减分） 5、将两个数转换（不借助第三个参数） 6、手写个插入排序吧（写完了和面试官讲了下执行流程） 7、讲讲你对 Spring 中的 IOC 和 AOP 的理解？ 8、问了几个常用的 Linux 命令？ 9、也问到多线程？和一面一样把自己最近看的线程池也讲了一遍 10、学 Java 多久了？ 11、你有什么想问的？ 总结：面试题目大概就是这么多了，有些问题自己也忘记了，面试题目顺序不一定是按照上面所写的。再次感谢爱奇艺的第一面面试官了，要不是他帮忙内推的，我可能还没有机会收到面试机会。自己接到爱奇艺面试邀请电话是星期一晚上快7点中的，之后加了面试官微信约好了星期四面试的（时间准备较短，之前没系统的复习过）。星期四一大早（5点就起床了），然后就收拾了下，去等公交车，转了两次车，然后再做地铁去爱奇艺公司的，总共路上花费时间四个多小时。总的来说，这次面试准备的时间不是很充裕，所以准备的个人觉得不是很好，通过这次的面试，发现面试还是比较注重基础和深度的，我也知道了自己的一些弱处，还需要在哪里加强，面试技巧上也要掌握些。为后面的其他公司继续做好充足的准备。加油！！！","tags":[{"name":"面经","slug":"面经","permalink":"http://yoursite.com/tags/面经/"}]},{"title":"秋招第三站 —— 内推阿里（一面）","date":"2017-08-03T16:00:00.000Z","path":"2017/08/04/alibaba/","text":"3、阿里巴巴（菜鸟网络部门）（一面 49 分钟）2017.08.02 晚上9点21打电话过来，预约明天什么时候有空面试，约好第二天下午两点。 2017.08.03 下午两点10分打过来了。 说看了我的博客和 GitHub，觉得我学的还行，知识广度都还不错，但是还是要问问具体情况，为什么没看到你春招的记录，什么原因没投阿里？非得说一个原因，那就是：我自己太菜了，不敢投。1、先自我介绍 2、什么是多态？哪里体现了多态的概念？ 3、HashMap 源码分析，把里面的东西问了个遍？最后问是不是线程安全？引出 ConcurrentHashMap 4、ConcurrentHashMap 源码分析 5、类加载，双亲委托机制 6、Java内存模型（一开始说的不是他想要的，主要想问我堆和栈的细节） 7、垃圾回收算法 8、线程池，自己之前看过，所以说的比较多，最后面试官说了句：看你对线程池了解还是很深了 9、事务的四种特性 10、什么是死锁？ 11、乐观锁和悲观锁的策略 12、高可用网站的设计（有什么技术实现） 13、低耦合高内聚 14、设计模式了解不？你用过哪几种，为什么用，单例模式帮我们做什么东西？有什么好处？ 15、你参与什么项目中成长比较快？学到了什么东西，以前是没有学过的？ 16、项目中遇到的最大困难是怎样的？是怎么解决的？ 17、智力题（两根不均匀的香，点一头烧完要一个小时，怎么确定15分钟） 18、你有什么问题想要问我的？ 19、问了菜鸟网络他们部门主要做什么？ 20、对我这次面试做个评价：看了你博客和 GitHub，知道你对学习的热情还是很高的，花了不少功夫，但是有些东西还是需要加强深度，阿里需要那种对技术有深度，有自己独到见解的人才。意思就是 GG 了。 总结：面试总的来说，第一次电话面试，感觉好紧张，好多问题自己会点，但是其中的细节没弄清楚，自己准备的也不够充分。面试官很友好，看到我紧张，也安慰我说不要紧，不管以后出去面试啥的，不需要紧张，公司问的问题可能很广，你只需要把你知道的说出来就行，不会的直接说不会就行。之前一直不敢投阿里，因为自己准备的完全不够充分，但是在朋友磊哥的帮助下，还是试了下，不管结果怎么样，经历过总比没有的好。","tags":[{"name":"面经","slug":"面经","permalink":"http://yoursite.com/tags/面经/"}]},{"title":"Java 线程池艺术探索","date":"2017-07-28T16:00:00.000Z","path":"2017/07/29/ThreadPool/","text":"线程池Wiki 上是这样解释的：Thread Pool 作用：利用线程池可以大大减少在创建和销毁线程上所花的时间以及系统资源的开销！ 下面主要讲下线程池中最重要的一个类 ThreadPoolExecutor 。 ThreadPoolExecutor ThreadPoolExecutor 构造器： 有四个构造器的，挑了参数最长的一个进行讲解。 七个参数： corePoolSize：核心池的大小，在创建了线程池后，默认情况下，线程池中并没有任何线程，而是等待有任务到来才创建线程去执行任务，当有任务来之后，就会创建一个线程去执行任务，当线程池中的线程数目达到corePoolSize后，就会把到达的任务放到缓存队列当中； maximumPoolSize：线程池最大线程数； keepAliveTime：表示线程没有任务执行时最多保持多久时间会终止； unit：参数keepAliveTime的时间单位（DAYS、HOURS、MINUTES、SECONDS 等）； workQueue：阻塞队列，用来存储等待执行的任务； ArrayBlockingQueue （有界队列） LinkedBlockingQueue （无界队列） SynchronousQueue threadFactory：线程工厂，主要用来创建线程 handler：拒绝处理任务的策略 AbortPolicy：丢弃任务并抛出 RejectedExecutionException 异常。（默认这种） DiscardPolicy：也是丢弃任务，但是不抛出异常 DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程） CallerRunsPolicy：由调用线程处理该任务 重要方法： execute()：通过这个方法可以向线程池提交一个任务，交由线程池去执行； shutdown()：关闭线程池； execute() 方法： 注：JDK 1.7 和 1.8 这个方法有点区别，下面代码是 1.8 中的。 1234567891011121314151617181920212223public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); int c = ctl.get(); //1、如果当前的线程数小于核心线程池的大小，根据现有的线程作为第一个 Worker 运行的线程，新建一个 Worker，addWorker 自动的检查当前线程池的状态和 Worker 的数量，防止线程池在不能添加线程的状态下添加线程 if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; //2、如果线程入队成功，然后还是要进行 double-check 的，因为线程在入队之后状态是可能会发生变化的 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); // recheck 防止线程池状态的突变，如果突变，那么将 reject 线程，防止 workQueue 中增加新线程 if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0)//上下两个操作都有 addWorker 的操作，但是如果在workQueue.offer 的时候 Worker 变为 0，那么将没有 Worker 执行新的 task，所以增加一个 Worker. addWorker(null, false); &#125; //3、如果 task 不能入队(队列满了)，这时候尝试增加一个新线程，如果增加失败那么当前的线程池状态变化了或者线程池已经满了然后拒绝task else if (!addWorker(command, false)) reject(command); &#125; 其中调用了 addWorker() 方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768private boolean addWorker(Runnable firstTask, boolean core) &#123;// firstTask: 新增一个线程并执行这个任务，可空，增加的线程从队列获取任务；core：是否使用 corePoolSize 作为上限，否则使用 maxmunPoolSize retry: for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. /** * rs!=Shutdown || fistTask！=null || workQueue.isEmpty * 如果当前的线程池的状态 &gt; SHUTDOWN 那么拒绝 Worker 的 add 如果 =SHUTDOWN * 那么此时不能新加入不为 null 的 Task，如果在 workQueue 为 empty 的时候不能加入任何类型的 Worker， * 如果不为 empty 可以加入 task 为 null 的 Worker, 增加消费的 Worker */ if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp;! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); //如果当前的数量超过了 CAPACITY，或者超过了 corePoolSize 和 maximumPoolSize（试 core 而定） if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; //CAS 尝试增加线程数，如果失败，证明有竞争，那么重新到 retry。 if (compareAndIncrementWorkerCount(c))// AtomicInteger 的 CAS 操作; break retry; c = ctl.get(); // Re-read ctl //判断当前线程池的运行状态,状态发生改变，重试 retry; if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; w = new Worker(firstTask);// Worker 为内部类，封装了线程和任务，通过 ThreadFactory 创建线程，可能失败抛异常或者返回 null final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable // SHUTDOWN 以后的状态和 SHUTDOWN 状态下 firstTask 为 null，不可新增线程 throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s;//记录最大线程数 workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w);//失败回退,从 wokers 移除 w, 线程数减一，尝试结束线程池(调用tryTerminate 方法) &#125; return workerStarted; &#125; 示意图： 执行流程： 1、当有任务进入时，线程池创建线程去执行任务，直到核心线程数满为止 2、核心线程数量满了之后，任务就会进入一个缓冲的任务队列中 当任务队列为无界队列时，任务就会一直放入缓冲的任务队列中，不会和最大线程数量进行比较 当任务队列为有界队列时，任务先放入缓冲的任务队列中，当任务队列满了之后，才会将任务放入线程池，此时会与线程池中最大的线程数量进行比较，如果超出了，则默认会抛出异常。然后线程池才会执行任务，当任务执行完，又会将缓冲队列中的任务放入线程池中，然后重复此操作。 shutdown() 方法： 1234567891011121314151617public void shutdown() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; //判断是否可以操作目标线程 checkShutdownAccess(); //设置线程池状态为 SHUTDOWN, 此处之后，线程池中不会增加新 Task advanceRunState(SHUTDOWN); //中断所有的空闲线程 interruptIdleWorkers(); onShutdown(); // hook for ScheduledThreadPoolExecutor &#125; finally &#123; mainLock.unlock(); &#125; //转到 Terminate tryTerminate(); &#125; 参考资料：深入理解java线程池—ThreadPoolExecutor JDK 自带四种线程池分析与比较 1、newFixedThreadPool创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 2、newSingleThreadExecutor创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 3、newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 4、newScheduledThreadPool创建一个定长线程池，支持定时及周期性任务执行。 四种线程池其实内部方法都是调用的 ThreadPoolExecutor 类，只不过利用了其不同的构造器方法而已（传入自己需要传入的参数），那么利用这个特性，我们自己也是可以实现自己定义的线程池的。 自定义线程池1、创建任务类 12345678910111213141516171819202122232425262728293031323334353637package com.zhisheng.thread.threadpool.demo;/** * Created by 10412 on 2017/7/24. * 任务 */public class MyTask implements Runnable&#123; private int taskId; //任务 id private String taskName; //任务名字 public int getTaskId() &#123; return taskId; &#125; public void setTaskId(int taskId) &#123; this.taskId = taskId; &#125; public String getTaskName() &#123; return taskName; &#125; public void setTaskName(String taskName) &#123; this.taskName = taskName; &#125; public MyTask(int taskId, String taskName) &#123; this.taskId = taskId; this.taskName = taskName; &#125; @Override public void run() &#123; System.out.println(\"当前正在执行 ****** 线程Id--&gt;\" + taskId + \",任务名称--&gt;\" + taskName); try &#123; Thread.currentThread().sleep(5 * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"线程Id--&gt;\" + taskId + \",任务名称--&gt;\" + taskName + \" ----------- 执行完毕！\"); &#125;&#125; 2、自定义拒绝策略，实现 RejectedExecutionHandler 接口，重写 rejectedExecution 方法 12345678910111213141516package com.zhisheng.thread.threadpool.demo;import java.util.concurrent.RejectedExecutionHandler;import java.util.concurrent.ThreadPoolExecutor;/** * Created by 10412 on 2017/7/24. * 自定义拒绝策略，实现 RejectedExecutionHandler 接口 */public class RejectedThreadPoolHandler implements RejectedExecutionHandler&#123; public RejectedThreadPoolHandler() &#123; &#125; @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) &#123; System.out.println(\"WARNING 自定义拒绝策略: Task \" + r.toString() + \" rejected from \" + executor.toString()); &#125;&#125; 3、创建线程池 1234567891011121314151617181920212223242526272829303132package com.zhisheng.thread.threadpool.demo;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;/** * Created by 10412 on 2017/7/24. */public class ThreadPool&#123; public static void main(String[] args) &#123; //核心线程数量为 2，最大线程数量 4，空闲线程存活的时间 60s，有界队列长度为 3, //ThreadPoolExecutor pool = new ThreadPoolExecutor(2, 4, 60, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(3)); //核心线程数量为 2，最大线程数量 4，空闲线程存活的时间 60s， 无界队列, //ThreadPoolExecutor pool = new ThreadPoolExecutor(2, 4, 60L, TimeUnit.SECONDS, new LinkedBlockingDeque&lt;&gt;()); //核心线程数量为 2，最大线程数量 4，空闲线程存活的时间 60s，有界队列长度为 3, 使用自定义拒绝策略 ThreadPoolExecutor pool = new ThreadPoolExecutor(2, 4, 60, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(3), new RejectedThreadPoolHandler()); for (int i = 1; i &lt;= 10; i++) &#123; //创建 10 个任务 MyTask task = new MyTask(i, \"任务\" + i); //运行 pool.execute(task); System.out.println(\"活跃的线程数：\"+pool.getActiveCount() + \",核心线程数：\" + pool.getCorePoolSize() + \",线程池大小：\" + pool.getPoolSize() + \",队列的大小\" + pool.getQueue().size()); &#125; //关闭线程池 pool.shutdown(); &#125;&#125; 这里运行结果就不截图了，我在本地测试了代码是没问题的，感兴趣的建议还是自己跑一下，然后分析下结果是不是和前面分析的一样，如有问题，请在我博客下面评论！ 总结本文一开始讲了线程池的介绍和好处，然后分析了线程池中最核心的 ThreadPoolExecutor 类中构造器的七个参数的作用、类中两个重要的方法，然后在对比研究了下 JDK 中自带的四种线程池的用法和内部代码细节，最后写了一个自定义的线程池。","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"多线程","slug":"多线程","permalink":"http://yoursite.com/tags/多线程/"},{"name":"线程池","slug":"线程池","permalink":"http://yoursite.com/tags/线程池/"}]},{"title":"Java 性能调优需要格外注意的细节","date":"2017-07-24T16:00:00.000Z","path":"2017/07/25/Java-performance-tuning/","text":"昨天写了篇文章 《MySQL 处理海量数据时的一些优化查询速度方法》 ，其实开发中不止数据库要优化，还有我们本身的开发代码也需要优化，这样我们开发的产品才能够得到极致的体验。也许有些人认为这些细小的地方没有啥好修改的，改与不改对运行效率没啥大的影响？首先在我们本地一个人测试下效率是不怎么明显，但是如果到发布上线后，你的用户有几百万，甚至上千万，这些用户同时访问你的网站，那么你的网站是否经得住考验呢，效率那高不高呢，如果效率不高，那么需要多出很多买服务器的经费呢，所以想想还是很有必要注意这些小的细节。今天就讲讲一些 Java 性能调优需要格外注意的一些细节。 代码优化的目标： 减少代码的体积 提高代码的运行效率 代码优化细节1、尽量指定类、方法的final修饰符 带有final修饰符的类是不可派生的。在Java核心API中，有许多应用final的例子，例如java.lang.String，整个类都是final的。为类指定final修饰符可以让类不可以被继承，为方法指定final修饰符可以让方法不可以被重写。如果指定了一个类为final。Java编译器会寻找机会内联所有的final方法，内联对于提升Java运行效率作用重大，具体参见Java运行期优化。 此举能够使性能平均提高50% 。 2、尽量重用对象 特别是String对象的使用，出现字符串连接时应该使用StringBuilder/StringBuffer代替。由于Java虚拟机不仅要花时间生成对象，以后可能还需要花时间对这些对象进行垃圾回收和处理，因此，生成过多的对象将会给程序的性能带来很大的影响。 3、尽可能使用局部变量 调用方法时传递的参数以及在调用中创建的临时变量都保存在栈中速度较快，其他变量，如静态变量、实例变量等，都在堆中创建，速度较慢。另外，栈中创建的变量，随着方法的运行结束，这些内容就没了，不需要额外的垃圾回收。 4、及时关闭流 Java编程过程中，进行数据库连接、I/O流操作时务必小心，在使用完毕后，及时关闭以释放资源。因为对这些大对象的操作会造成系统大的开销，稍有不慎，将会导致严重的后果。 5、尽量减少对变量的重复计算 明确一个概念，对方法的调用，即使方法中只有一句语句，也是有消耗的，包括创建栈帧、调用方法时保护现场、调用方法完毕时恢复现场等。所以例如下面的操作： 12for (int i = 0; i &lt; list.size(); i++)&#123;...&#125; 建议替换为： 12for (int i = 0, int length = list.size(); i &lt; length; i++)&#123;...&#125; 这样，在list.size()很大的时候，就减少了很多的消耗 6、尽量采用懒加载的策略，即在需要的时候才创建 例如： 1234String str = \"aaa\";if (i == 1)&#123; list.add(str);&#125; 建议替换为： 12345if (i == 1)&#123; String str = \"aaa\"; list.add(str);&#125; 7、慎用异常 异常对性能不利。抛出异常首先要创建一个新的对象，Throwable接口的构造函数调用名为 fillInStackTrace() 的本地同步方法，fillInStackTrace() 方法检查堆栈，收集调用跟踪信息。只要有异常被抛出，Java虚拟机就必须调整调用堆栈，因为在处理过程中创建了一个新的对象。异常只能用于错误处理，不应该用来控制程序流程。 8、不要在循环中使用 try…catch…，应该把其放在最外层 除非不得已。如果毫无理由地这么写了，只要你的领导资深一点、有强迫症一点，八成就要骂你为什么写出这种垃圾代码来了。 9、如果能估计到待添加的内容长度，为底层以数组方式实现的集合、工具类指定初始长度 比如 ArrayList、LinkedLlist、StringBuilder、StringBuffer、HashMap、HashSet 等等，以 StringBuilder 为例： （1）StringBuilder() // 默认分配16个字符的空间 （2）StringBuilder(int size) // 默认分配size个字符的空间 （3）StringBuilder(String str) // 默认分配16个字符+str.length()个字符空间 可以通过类（这里指的不仅仅是上面的 StringBuilder ）的来设定它的初始化容量，这样可以明显地提升性能。比如 StringBuilder 吧，length 表示当前的 StringBuilder 能保持的字符数量。因为当 StringBuilder 达到最大容量的时候，它会将自身容量增加到当前的2倍再加2，无论何时只要 StringBuilder 达到它的最大容量，它就不得不创建一个新的字符数组然后将旧的字符数组内容拷贝到新字符数组中—-这是十分耗费性能的一个操作。试想，如果能预估到字符数组中大概要存放5000个字符而不指定长度，最接近5000的2次幂是4096，每次扩容加的2不管，那么： （1）在4096 的基础上，再申请8194个大小的字符数组，加起来相当于一次申请了12290个大小的字符数组，如果一开始能指定5000个大小的字符数组，就节省了一倍以上的空间； （2）把原来的4096个字符拷贝到新的的字符数组中去。 这样，既浪费内存空间又降低代码运行效率。所以，给底层以数组实现的集合、工具类设置一个合理的初始化容量是错不了的，这会带来立竿见影的效果。但是，注意，像HashMap这种是以数组+链表实现的集合，别把初始大小和你估计的大小设置得一样，因为一个table上只连接一个对象的可能性几乎为0。初始大小建议设置为2的N次幂，如果能估计到有2000个元素，设置成 new HashMap(128)、new HashMap(256) 都可以。 10、当复制大量数据时，使用 System.arraycopy() 命令 11、乘法和除法使用移位操作 例如： 12345for (val = 0; val &lt; 100000; val += 5)&#123; a = val * 8; b = val / 2;&#125; 用移位操作可以极大地提高性能，因为在计算机底层，对位的操作是最方便、最快的，因此建议修改为： 12345for (val = 0; val &lt; 100000; val += 5)&#123; a = val &lt;&lt; 3; b = val &gt;&gt; 1;&#125; 移位操作虽然快，但是可能会使代码不太好理解，因此最好加上相应的注释。 12、循环内不要不断创建对象引用 例如： 1234for (int i = 1; i &lt;= count; i++)&#123; Object obj = new Object();&#125; 这种做法会导致内存中有count份Object对象引用存在，count很大的话，就耗费内存了，建议为改为： 12345Object obj = null;for (int i = 0; i &lt;= count; i++) &#123; obj = new Object(); &#125; 这样的话，内存中只有一份 Object 对象引用，每次 new Object() 的时候，Object 对象引用指向不同的Object罢了，但是内存中只有一份，这样就大大节省了内存空间了。 13、基于效率和类型检查的考虑，应该尽可能使用array，无法确定数组大小时才使用 ArrayList 14、尽量使用 HashMap、ArrayList、StringBuilder，除非线程安全需要，否则不推荐使用 Hashtable、Vector、StringBuffer，后三者由于使用同步机制而导致了性能开销 15、不要将数组声明为 public static final 因为这毫无意义，这样只是定义了引用为 static final，数组的内容还是可以随意改变的，将数组声明为 public 更是一个安全漏洞，这意味着这个数组可以被外部类所改变。 16、尽量在合适的场合使用单例 使用单例可以减轻加载的负担、缩短加载的时间、提高加载的效率，但并不是所有地方都适用于单例，简单来说，单例主要适用于以下三个方面： （1）控制资源的使用，通过线程同步来控制资源的并发访问 （2）控制实例的产生，以达到节约资源的目的 （3）控制数据的共享，在不建立直接关联的条件下，让多个不相关的进程或线程之间实现通信 17、尽量避免随意使用静态变量 要知道，当某个对象被定义为static的变量所引用，那么gc通常是不会回收这个对象所占有的堆内存的，如： 1234public class A&#123; private static B b = new B();&#125; 此时静态变量b的生命周期与A类相同，如果A类不被卸载，那么引用B指向的B对象会常驻内存，直到程序终止 18、及时清除不再需要的会话 为了清除不再活动的会话，许多应用服务器都有默认的会话超时时间，一般为30分钟。当应用服务器需要保存更多的会话时，如果内存不足，那么操作系统会把部分数据转移到磁盘，应用服务器也可能根据MRU（最近最频繁使用）算法把部分不活跃的会话转储到磁盘，甚至可能抛出内存不足的异常。如果会话要被转储到磁盘，那么必须要先被序列化，在大规模集群中，对对象进行序列化的代价是很昂贵的。因此，当会话不再需要时，应当及时调用 HttpSession 的 invalidate() 方法清除会话。 19、实现 RandomAccess 接口的集合比如 ArrayList，应当使用最普通的 for 循环而不是 foreach 循环来遍历 这是JDK推荐给用户的。JDK API对于 RandomAccess 接口的解释是：实现 RandomAccess 接口用来表明其支持快速随机访问，此接口的主要目的是允许一般的算法更改其行为，从而将其应用到随机或连续访问列表时能提供良好的性能。实际经验表明，实现 RandomAccess 接口的类实例，假如是随机访问的，使用普通 for 循环效率将高于使用 foreach 循环；反过来，如果是顺序访问的，则使用 Iterator 会效率更高。可以使用类似如下的代码作判断： 12345678910if (list instanceof RandomAccess)&#123; for (int i = 0; i &lt; list.size(); i++)&#123;&#125;&#125;else&#123; Iterator&lt;?&gt; iterator = list.iterable(); while (iterator.hasNext()) &#123; iterator.next(); &#125;&#125; foreach 循环的底层实现原理就是迭代器 Iterator，参见Java语法糖1：可变长度参数以及 foreach 循环原理。所以后半句”反过来，如果是顺序访问的，则使用 Iterator 会效率更高”的意思就是顺序访问的那些类实例，使用 foreach 循环去遍历。 20、使用同步代码块替代同步方法 这点在多线程模块中的 synchronized 锁方法块一文中已经讲得很清楚了，除非能确定一整个方法都是需要进行同步的，否则尽量使用同步代码块，避免对那些不需要进行同步的代码也进行了同步，影响了代码执行效率。 21、将常量声明为 static final，并以大写命名 这样在编译期间就可以把这些内容放入常量池中，避免运行期间计算生成常量的值。另外，将常量的名字以大写命名也可以方便区分出常量与变量 22、不要创建一些不使用的对象，不要导入一些不使用的类 这毫无意义，如果代码中出现”The value of the local variable i is not used”、”The import java.util is never used”，那么请删除这些无用的内容 23、程序运行过程中避免使用反射 关于，请参见反射。反射是Java提供给用户一个很强大的功能，功能强大往往意味着效率不高。不建议在程序运行过程中使用尤其是频繁使用反射机制，特别是Method的invoke方法，如果确实有必要，一种建议性的做法是将那些需要通过反射加载的类在项目启动的时候通过反射实例化出一个对象并放入内存—-用户只关心和对端交互的时候获取最快的响应速度，并不关心对端的项目启动花多久时间。 24、使用数据库连接池和线程池 这两个池都是用于重用对象的，前者可以避免频繁地打开和关闭连接，后者可以避免频繁地创建和销毁线程 25、使用带缓冲的输入输出流进行IO操作 带缓冲的输入输出流，即BufferedReader、BufferedWriter、BufferedInputStream、BufferedOutputStream，这可以极大地提升IO效率 26、顺序插入和随机访问比较多的场景使用ArrayList，元素删除和中间插入比较多的场景使用LinkedList这个，理解ArrayList和LinkedList的原理就知道了 27、不要让public方法中有太多的形参 public方法即对外提供的方法，如果给这些方法太多形参的话主要有两点坏处： 1、违反了面向对象的编程思想，Java讲求一切都是对象，太多的形参，和面向对象的编程思想并不契合 2、参数太多势必导致方法调用的出错概率增加 至于这个”太多”指的是多少个，3、4个吧。比如我们用JDBC写一个insertStudentInfo方法，有10个学生信息字段要插如Student表中，可以把这10个参数封装在一个实体类中，作为insert方法的形参。 28、字符串变量和字符串常量equals的时候将字符串常量写在前面 这是一个比较常见的小技巧了，如果有以下代码： 12String str = \"123\";if (str.equals(\"123\")) &#123;...&#125; 建议修改为： 12345String str = \"123\";if (\"123\".equals(str))&#123;...&#125; 这么做主要是可以避免空指针异常 29、请知道，在java中if (i == 1)和if (1 == i)是没有区别的，但从阅读习惯上讲，建议使用前者 平时有人问，”if (i == 1)”和”if (1== i)”有没有区别，这就要从C/C++讲起。 在C/C++中，”if (i == 1)”判断条件成立，是以0与非0为基准的，0表示false，非0表示true，如果有这么一段代码： 123456int i = 2;if (i == 1)&#123;...&#125;else&#123;...&#125; C/C++判断”i==1″不成立，所以以0表示，即false。但是如果： 1234567int i = 2;if (i = 1)&#123; ... &#125;else&#123;... &#125; 万一程序员一个不小心，把”if (i == 1)”写成”if (i = 1)”，这样就有问题了。在if之内将i赋值为1，if判断里面的内容非0，返回的就是true了，但是明明i为2，比较的值是1，应该返回的false。这种情况在C/C++的开发中是很可能发生的并且会导致一些难以理解的错误产生，所以，为了避免开发者在if语句中不正确的赋值操作，建议将if语句写为： 123456int i = 2;if (1 == i) &#123;... &#125;else&#123;... &#125; 这样，即使开发者不小心写成了”1 = i”，C/C++编译器也可以第一时间检查出来，因为我们可以对一个变量赋值i为1，但是不能对一个常量赋值1为i。 但是，在Java中，C/C++这种”if (i = 1)”的语法是不可能出现的，因为一旦写了这种语法，Java就会编译报错”Type mismatch: cannot convert from int to boolean”。但是，尽管Java的”if (i == 1)”和”if (1 == i)”在语义上没有任何区别，但是从阅读习惯上讲，建议使用前者会更好些。 30、不要对数组使用toString()方法 看一下对数组使用toString()打印出来的是什么： 12345public static void main(String[] args)&#123; int[] is = new int[]&#123;1, 2, 3&#125;; System.out.println(is.toString());&#125; 结果是： 1[I@18a992f 本意是想打印出数组内容，却有可能因为数组引用is为空而导致空指针异常。不过虽然对数组toString()没有意义，但是对集合toString()是可以打印出集合里面的内容的，因为集合的父类AbstractCollections重写了Object的toString()方法。 31、不要对超出范围的基本数据类型做向下强制转型 这绝不会得到想要的结果： 12345public static void main(String[] args)&#123; long l = 12345678901234L;int i = (int)l; System.out.println(i);&#125; 我们可能期望得到其中的某几位，但是结果却是： 1942892530 解释一下。Java中long是8个字节64位的，所以12345678901234在计算机中的表示应该是： 0000 0000 0000 0000 0000 1011 0011 1010 0111 0011 1100 1110 0010 1111 1111 0010 一个int型数据是4个字节32位的，从低位取出上面这串二进制数据的前32位是： 0111 0011 1100 1110 0010 1111 1111 0010 这串二进制表示为十进制1942892530，所以就是我们上面的控制台上输出的内容。从这个例子上还能顺便得到两个结论： 1、整型默认的数据类型是int，long l = 12345678901234L，这个数字已经超出了int的范围了，所以最后有一个L，表示这是一个long型数。顺便，浮点型的默认类型是double，所以定义float的时候要写成””float f = 3.5f” 2、接下来再写一句”int ii = l + i;”会报错，因为long + int是一个long，不能赋值给int 32、公用的集合类中不使用的数据一定要及时remove掉 如果一个集合类是公用的（也就是说不是方法里面的属性），那么这个集合里面的元素是不会自动释放的，因为始终有引用指向它们。所以，如果公用集合里面的某些数据不使用而不去remove掉它们，那么将会造成这个公用集合不断增大，使得系统有内存泄露的隐患。 33、把一个基本数据类型转为字符串，基本数据类型.toString()是最快的方式、String.valueOf(数据)次之、数据+””最慢 把一个基本数据类型转为一般有三种方式，我有一个Integer型数据i，可以使用i.toString()、String.valueOf(i)、i+””三种方式，三种方式的效率如何，看一个测试： 1234567891011121314151617181920212223public static void main(String[] args)&#123; int loopTime = 50000; Integer i = 0; long startTime = System.currentTimeMillis(); for (int j = 0; j &lt; loopTime; j++) &#123; String str = String.valueOf(i); &#125;System.out.println(\"String.valueOf()：\" + (System.currentTimeMillis() - startTime) + \"ms\"); startTime = System.currentTimeMillis(); for (int j = 0; j &lt; loopTime; j++) &#123; String str = i.toString(); &#125;System.out.println(\"Integer.toString()：\" + (System.currentTimeMillis() - startTime) + \"ms\");startTime = System.currentTimeMillis(); for (int j = 0; j &lt; loopTime; j++)&#123; String str = i + \"\";&#125;System.out.println(\"i + \\\"\\\"：\" + (System.currentTimeMillis() - startTime) + \"ms\");&#125; 运行结果为： 1String.valueOf()：11ms Integer.toString()：5ms i + &quot;&quot;：25ms 所以以后遇到把一个基本数据类型转为String的时候，优先考虑使用toString()方法。至于为什么，很简单： 1、String.valueOf()方法底层调用了Integer.toString()方法，但是会在调用前做空判断 2、Integer.toString()方法就不说了，直接调用了 3、i + “”底层使用了StringBuilder实现，先用append方法拼接，再用toString()方法获取字符串 三者对比下来，明显是2最快、1次之、3最慢 34、使用最有效率的方式去遍历Map 遍历Map的方式有很多，通常场景下我们需要的是遍历Map中的Key和Value，那么推荐使用的、效率最高的方式是： 123456789101112public static void main(String[] args)&#123;HashMap&lt;String, String&gt; hm = new HashMap&lt;String, String&gt;();hm.put(\"111\", \"222\");Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = hm.entrySet();Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iter = entrySet.iterator(); while (iter.hasNext())&#123;Map.Entry&lt;String, String&gt; entry = iter.next();System.out.println(entry.getKey() + \"\\t\" + entry.getValue());&#125;&#125; 如果你只是想遍历一下这个Map的key值，那用”Set keySet = hm.keySet();”会比较合适一些 35、对资源的close()建议分开操作 意思是，比如我有这么一段代码： 12345try&#123;XXX.close();YYY.close();&#125;catch (Exception e)&#123;...&#125; 建议修改为： 123456789try&#123; XXX.close(); &#125;catch (Exception e) &#123; ... &#125;try&#123; YYY.close(); &#125;catch (Exception e) &#123; ... &#125; 虽然有些麻烦，却能避免资源泄露。我想，如果没有修改过的代码，万一XXX.close()抛异常了，那么就进入了cath块中了，YYY.close()不会执行，YYY这块资源就不会回收了，一直占用着，这样的代码一多，是可能引起资源句柄泄露的。而改为上面的写法之后，就保证了无论如何XXX和YYY都会被close掉。 以上就是 Java 开发编程注意细节的全部内容了，感谢大家的阅读！","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"Spring MVC系列文章（五）：看透 Spring MVC 源代码分析与实践 ——  Spring MVC 组件分析","date":"2017-07-20T16:00:00.000Z","path":"2017/07/21/Spring-MVC03/","text":"由于星期一接到面试通知，和面试官约好了星期四面试，所以这几天没更新完这系列的文章，面完试后立马就把这个解决掉。通过这次面试，也让我懂得了很多，知道了自己的一些不足之处，后面还要继续下功夫好好的深入复习下去。这几篇文章写的我觉得还是不够仔细，感兴趣的还是建议自己去看看源码。 第 11 章 —— 组件概览HandlerMapping 根据 request 找到对应的处理器 Handler 和 Interceptors。内部只有一个方法1HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception; HandlerAdapter Handler 适配器，内部方法如下： 123boolean supports(Object handler);//判断是否可以使用某个 HandlerModelAndView handle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception; //具体使用long getLastModified(HttpServletRequest request, Object handler);//获取资源上一次修改的时间 HandlerExceptionResolver 根据异常设置 ModelAndView ，再交给 render 方法进行渲染。 12ModelAndView resolveException( HttpServletRequest request, HttpServletResponse response, @Nullable Object handler, Exception ex) ViewResolver 用来将 String 类型的视图名和 Locale 解析为 View 类型的视图。 1View resolveViewName(String viewName, Locale locale) throws Exception; 它的一个实现类 BeanNameViewResolver，它重写 resolveViewName 方法如下: 1234567891011121314151617181920212223public View resolveViewName(String viewName, Locale locale) throws BeansException &#123; ApplicationContext context = getApplicationContext(); //如果应用上下文没有找到视图，返回 null if (!context.containsBean(viewName)) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(\"No matching bean found for view name '\" + viewName + \"'\"); &#125; // Allow for ViewResolver chaining... return null; &#125; //如果找到的视图类型不匹配，也返回 null if (!context.isTypeMatch(viewName, View.class)) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(\"Found matching bean for view name '\" + viewName + \"' - to be ignored since it does not implement View\"); &#125; // Since we're looking into the general ApplicationContext here, // let's accept this as a non-match and allow for chaining as well... return null; &#125; //根据视图名称从 Spring 容器中查找 Bean，返回找到的 bean return context.getBean(viewName, View.class); &#125; RequestToViewNameTranslator 获取 request 中的视图名。接口里面也是只有一个方法： 1String getViewName(HttpServletRequest request) throws Exception; //根据 request 查找视图名 LocaleResolver 用于从 request 解析出 Locale。 123456public interface LocaleResolver &#123; //从 request 解析出 Locale Locale resolveLocale(HttpServletRequest request); //根据 request 设置 locale void setLocale(HttpServletRequest request, HttpServletResponse response, @Nullable Locale locale);&#125; ThemeResolver 解析主题 123456public interface ThemeResolver &#123; //通过给定的 request 查找主题名 String resolveThemeName(HttpServletRequest request); //根据给定的 request 设置主题名 void setThemeName(HttpServletRequest request, HttpServletResponse response, String themeName);&#125; 在 RequestContext.java 文件中可以获取主题： 1234567891011121314151617public String getThemeMessage(String code, String defaultMessage) &#123; //获取主题的信息 return getTheme().getMessageSource().getMessage(code, null, defaultMessage, this.locale); &#125;public Theme getTheme() &#123; //判断主题是否为空 if (this.theme == null) &#123; // 通过 RequestContextUtils 获取 request 中的主题名 this.theme = RequestContextUtils.getTheme(this.request); if (this.theme == null) &#123; //如果还是为空的话 //那就是没有有效的主题解析器和主题 this.theme = getFallbackTheme(); &#125; &#125; return this.theme; &#125; RequestContextUtils.getTheme() 方法： 1234567891011public static Theme getTheme(HttpServletRequest request) &#123; ThemeResolver themeResolver = getThemeResolver(request); ThemeSource themeSource = getThemeSource(request); if (themeResolver != null &amp;&amp; themeSource != null) &#123; String themeName = themeResolver.resolveThemeName(request); return themeSource.getTheme(themeName); &#125; else &#123; return null; &#125; &#125; MultipartResolver 用于处理上传请求，处理方法：将普通的 request 包装成 MultipartHttpServletRequest 12345678public interface MultipartResolver &#123; //根据 request 判断是否是上传请求 boolean isMultipart(HttpServletRequest request); //将 request 包装成 MultipartHttpServletRequest MultipartHttpServletRequest resolveMultipart(HttpServletRequest request) throws MultipartException; //清理上传过程中产生的临时资源 void cleanupMultipart(MultipartHttpServletRequest request);&#125; FlashMapManager FlashMap 主要在 redirect 中传递参数，FlashMapManager 用来管理 FlashMap 的。 1234567public interface FlashMapManager &#123; //恢复参数，并将恢复过的和超时的参数从保存介质中删除 @Nullable FlashMap retrieveAndUpdate(HttpServletRequest request, HttpServletResponse response); //将参数保存起来 void saveOutputFlashMap(FlashMap flashMap, HttpServletRequest request, HttpServletResponse response);&#125; 小结介绍 Spring MVC 中九大组件的接口、作用、内部方法实现及作用进行了简单的介绍，详细的还需大家自己去看源码。 总结Spring MVC 原理总结本质是一个 Servlet，这个 Servlet 继承自 HttpServlet。Spring MVC 中提供了三个层次的 Servlet：HttpServletBean、FrameworkServlet 和 DispatcherServlet。他们相互继承， HttpServletBean 直接继承自 Java 的 HttpServlet。HttpServletBean 用于将 Servlet 中的 Servlet 中配置的参数设置到相应的属性中，FrameworkServlet 初始化了 Spring MVC 中所使用的 WebApplicationContext，具体处理请求的 9 大组件是在 DispatcherServlet 中初始化的，整个继承图如下： 最后文章可转发，但请注明原创地址，谢谢支持。","tags":[{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://yoursite.com/tags/Spring-MVC/"}]},{"title":"Spring MVC系列文章（四）：看透 Spring MVC 源代码分析与实践 ——  俯视 Spring MVC","date":"2017-07-13T16:00:00.000Z","path":"2017/07/14/Spring-MVC02/","text":"Spring MVC Spring MVC 之初体验环境搭建在 IDEA 中新建一个 web 项目，用 Maven 管理项目的话，在 pom.xml 中加入 Spring MVC 和 Servlet 依赖即可。 12345678910111213&lt;!-- https://mvnrepository.com/artifact/org.springframework/spring-webmvc --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.3.9.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/javax.servlet/javax.servlet-api --&gt;&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; Spring MVC 简单配置 在 web.xml 中配置 Servlet 创建 Spring MVC 的 xml 配置文件 创建 Controller 和 View 1、web.xml 12345678910111213141516171819202122232425262728293031323334&lt;!-- Spring MVC配置 --&gt;&lt;!-- ====================================== --&gt;&lt;servlet&gt; &lt;servlet-name&gt;spring&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 可以自定义servlet.xml配置文件的位置和名称，默认为WEB-INF目录下，名称为[&lt;servlet-name&gt;]-servlet.xml，如spring-servlet.xml &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;/WEB-INF/spring-servlet.xml&lt;/param-value&gt;&amp;nbsp; 默认 &lt;/init-param&gt; --&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;spring&lt;/servlet-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;&lt;!-- Spring配置 --&gt;&lt;!-- ====================================== --&gt;&lt;listener&gt; &lt;listenerclass&gt; org.springframework.web.context.ContextLoaderListener &lt;/listener-class&gt;&lt;/listener&gt;&lt;!-- 指定Spring Bean的配置文件所在目录。默认配置在WEB-INF目录下 --&gt;&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:config/applicationContext.xml&lt;/param-value&gt;&lt;/context-param&gt; 2、spring-servlet.xml 123456789101112131415161718192021&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:p=\"http://www.springframework.org/schema/p\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-3.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-3.0.xsd http://www.springframework.org/schema/context &lt;a href=\"http://www.springframework.org/schema/context/spring-context-3.0.xsd\"&gt;http://www.springframework.org/schema/context/spring-context-3.0.xsd&lt;/a&gt;\"&gt; &lt;!-- 启用spring mvc 注解 --&gt; &lt;context:annotation-config /&gt; &lt;!-- 设置使用注解的类所在的jar包 --&gt; &lt;context:component-scan base-package=\"controller\"&gt;&lt;/context:component-scan&gt; &lt;!-- 完成请求和注解POJO的映射 --&gt; &lt;bean class=\"org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter\" /&gt; &lt;!-- 对转向页面的路径解析。prefix：前缀， suffix：后缀 --&gt; &lt;bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\" p:prefix=\"/jsp/\" p:suffix=\".jsp\" /&gt;&lt;/beans&gt; 3、Controller 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package controller;import javax.servlet.http.HttpServletRequest;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import entity.User;@Controller //类似Struts的Actionpublic class TestController &#123; @RequestMapping(\"/test/login.do\") // 请求url地址映射，类似Struts的action-mapping public String testLogin(@RequestParam(value=\"username\")String username, String password, HttpServletRequest request) &#123; // @RequestParam是指请求url地址映射中必须含有的参数(除非属性 required=false, 默认为 true) // @RequestParam可简写为：@RequestParam(\"username\") if (!\"admin\".equals(username) || !\"admin\".equals(password)) &#123; return \"loginError\"; // 跳转页面路径（默认为转发），该路径不需要包含spring-servlet配置文件中配置的前缀和后缀 &#125; return \"loginSuccess\"; &#125; @RequestMapping(\"/test/login2.do\") public ModelAndView testLogin2(String username, String password, int age)&#123; // request和response不必非要出现在方法中，如果用不上的话可以去掉 // 参数的名称是与页面控件的name相匹配，参数类型会自动被转换 if (!\"admin\".equals(username) || !\"admin\".equals(password) || age &lt; 5) &#123; return new ModelAndView(\"loginError\"); // 手动实例化ModelAndView完成跳转页面（转发），效果等同于上面的方法返回字符串 &#125; return new ModelAndView(new RedirectView(\"../index.jsp\")); // 采用重定向方式跳转页面 // 重定向还有一种简单写法 // return new ModelAndView(\"redirect:../index.jsp\"); &#125; @RequestMapping(\"/test/login3.do\") public ModelAndView testLogin3(User user) &#123; // 同样支持参数为表单对象，类似于Struts的ActionForm，User不需要任何配置，直接写即可 String username = user.getUsername(); String password = user.getPassword(); int age = user.getAge(); if (!\"admin\".equals(username) || !\"admin\".equals(password) || age &lt; 5) &#123; return new ModelAndView(\"loginError\"); &#125; return new ModelAndView(\"loginSuccess\"); &#125; @Resource(name = \"loginService\") // 获取applicationContext.xml中bean的id为loginService的，并注入 private LoginService loginService; //等价于spring传统注入方式写get和set方法，这样的好处是简洁工整，省去了不必要得代码 @RequestMapping(\"/test/login4.do\") public String testLogin4(User user) &#123; if (loginService.login(user) == false) &#123; return \"loginError\"; &#125; return \"loginSuccess\"; &#125;&#125; @RequestMapping 可以写在方法上，也可以写在类上，上面代码方法上的 RequestMapping 都含有 /test ， 那么我们就可以将其抽出直接写在类上，那么方法里面就不需要写 /test 了。 如下即可： 12345678910111213141516@Controller@RequestMapping(\"/test\")public class TestController &#123; @RequestMapping(\"/login.do\") // 请求url地址映射，类似Struts的action-mapping public String testLogin(@RequestParam(value=\"username\")String username, String password, HttpServletRequest request) &#123; // @RequestParam是指请求url地址映射中必须含有的参数(除非属性 required=false, 默认为 true) // @RequestParam可简写为：@RequestParam(\"username\") if (!\"admin\".equals(username) || !\"admin\".equals(password)) &#123; return \"loginError\"; // 跳转页面路径（默认为转发），该路径不需要包含spring-servlet配置文件中配置的前缀和后缀 &#125; return \"loginSuccess\"; &#125; //省略其他的&#125; 上面的代码方法的参数中可以看到有一个 @RequestParam 注解，其实还有 @PathVariable 。这两个的区别是啥呢？ @PathVariable 标记在方法的参数上，利用它标记的参数可以利用请求路径传值。 @RequestParam是指请求url地址映射中必须含有的参数(除非属性 required=false, 默认为 true) 看如下例子： 123456789101112@RequestMapping(\"/user/&#123;userId&#125;\") // 请求url地址映射public String userinfo(Model model, @PathVariable(\"userId\") int userId, HttpSession session) &#123; System.out.println(\"进入 userinfo 页面\"); //判断是否有用户登录 User user1 = (User) session.getAttribute(\"user\"); if (user1 == null) &#123; return \"login\"; &#125; User user = userService.selectUserById(userId); model.addAttribute(\"user\", user); return \"userinfo\"; &#125; 上面例子中如果浏览器请求的是 /user/1 的时候，就表示此时的用户 id 为 1，此时就会先从 session 中查找是否有 “user” 属性，如果有的话，就代表用户此时处于登录的状态，如果没有的话，就会让用户返回到登录页面，这种机制在各种网站经常会使用的，然后根据这个 id = 1 ，去查找用户的信息，然后把查找的 “user” 放在 model 中，然后返回用户详情页面，最后在页面中用 $!{user.name} 获取用户的名字，同样的方式可以获取用户的其他信息，把所有的用户详情信息展示出来。 创建 Spring MVC 之器Spring MVC 核心 Servlet 架构图如下： Java 中常用的 Servlet 我在另外一篇文章写的很清楚了，有兴趣的请看：通过源码详解 Servlet ，这里我就不再解释了。 这里主要讲 Spring 中的 HttpServletBean、FrameworkServlet、DispatcherServlet 这三个类的创建过程。 通过上面的图，可以看到这三个类直接实现三个接口：EnvironmentCapable、EnvironmentAware、ApplicationContextAware。下面我们直接看下这三个接口的内部是怎样写的。 EnvironmentCapable.java 12345public interface EnvironmentCapable &#123; //返回组件的环境，可能返回 null 或者默认环境 @Nullable Environment getEnvironment();&#125; EnvironmentAware.java 1234public interface EnvironmentAware extends Aware &#123; //设置组件的运行环境 void setEnvironment(Environment environment);&#125; ApplicationContextAware.java 12345public interface ApplicationContextAware extends Aware &#123; //设置运行对象的应用上下文 //当类实现这个接口后，这个类可以获取ApplicationContext中所有的bean，也就是说这个类可以直接获取Spring配置文件中所有有引用到的bean对象 void setApplicationContext(ApplicationContext applicationContext) throws BeansException;&#125; 怎么使用这个这个接口呢？ 参考文章：org.springframework.context.ApplicationContextAware使用理解 HttpServletBean 这里就直接看其中最重要的 init() 方法的代码了： 12345678910111213141516171819202122232425262728293031323334/** * 将配置参数映射到此servlet的bean属性，并调用子类初始化。 * 如果 bean 配置不合法（或者需要的参数丢失）或者子类初始化发生错误，那么就会抛出 ServletException 异常 */@Overridepublic final void init() throws ServletException &#123; //日志代码删除了 // 从init参数设置bean属性。 //获得web.xml中的contextConfigLocation配置属性，就是spring MVC的配置文件 PropertyValues pvs = new ServletConfigPropertyValues(getServletConfig(), this.requiredProperties); if (!pvs.isEmpty()) &#123; try &#123; BeanWrapper bw = PropertyAccessorFactory.forBeanPropertyAccess(this); //获取服务器的各种信息 ResourceLoader resourceLoader = new ServletContextResourceLoader(getServletContext()); bw.registerCustomEditor(Resource.class, new ResourceEditor(resourceLoader, getEnvironment())); //模板方法，可以在子类中调用，做一些初始化工作，bw代表DispatcherServelt initBeanWrapper(bw); //将配置的初始化值设置到DispatcherServlet中 bw.setPropertyValues(pvs, true); &#125; catch (BeansException ex) &#123; //日志代码 throw ex; &#125; &#125; // Let subclasses do whatever initialization they like. //模板方法，子类初始化的入口方法 initServletBean(); //日志代码删除了&#125; FrameworkServlet 其中重要方法如下：里面也就两句关键代码，日志代码我直接删掉了 12345678910111213141516171819202122protected final void initServletBean() throws ServletException &#123; //日志代码删除了 long startTime = System.currentTimeMillis(); //就是 try 语句里面有两句关键代码 try &#123; //初始化 webApplicationContext this.webApplicationContext = initWebApplicationContext(); //模板方法， initFrameworkServlet(); &#125; catch (ServletException ex) &#123; this.logger.error(\"Context initialization failed\", ex); throw ex; &#125; catch (RuntimeException ex) &#123; this.logger.error(\"Context initialization failed\", ex); throw ex; &#125; //日志代码删除了 &#125; 再来看看上面代码中调用的 initWebApplicationContext() 方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344protected WebApplicationContext initWebApplicationContext() &#123; //获取 rootContext WebApplicationContext rootContext = WebApplicationContextUtils.getWebApplicationContext(getServletContext()); WebApplicationContext wac = null; if (this.webApplicationContext != null) &#123; // 上下文实例在构造时注入 - &gt;使用它 wac = this.webApplicationContext; if (wac instanceof ConfigurableWebApplicationContext) &#123; ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) wac; if (!cwac.isActive()) &#123; // 如果上下文尚未刷新 -&gt; 提供诸如设置父上下文，设置应用程序上下文ID等服务 if (cwac.getParent() == null) &#123; // 上下文实例被注入没有显式的父类 -&gt; 将根应用程序上下文（如果有的话可能为null）设置为父级 cwac.setParent(rootContext); &#125; configureAndRefreshWebApplicationContext(cwac); &#125; &#125; &#125; if (wac == null) &#123; // 当 WebApplicationContext 已经存在 ServletContext 中时，通过配置在 servlet 中的 ContextAttribute 参数获取 wac = findWebApplicationContext(); &#125; if (wac == null) &#123; // 如果 WebApplicationContext 还没有创建，则创建一个 wac = createWebApplicationContext(rootContext); &#125; if (!this.refreshEventReceived) &#123; // 当 ContextRefreshedEvent 事件没有触发时调用此方法，模板方法，可以在子类重写 onRefresh(wac); &#125; if (this.publishContext) &#123; // 将 ApplicationContext 保存到 ServletContext 中去 String attrName = getServletContextAttributeName(); getServletContext().setAttribute(attrName, wac); if (this.logger.isDebugEnabled()) &#123; this.logger.debug(\"Published WebApplicationContext of servlet '\" + getServletName() + \"' as ServletContext attribute with name [\" + attrName + \"]\"); &#125; &#125; return wac; &#125; initWebApplicationContext 方法做了三件事： 获取 Spring 的根容器 rootContext 设置 webApplicationContext 并根据情况调用 onRefresh 方法 将 webApplicationContext 设置到 ServletContext 中 这里在讲讲上面代码中的 wac == null 的几种情况： 1）、当 WebApplicationContext 已经存在 ServletContext 中时，通过配置在 servlet 中的 ContextAttribute 参数获取，调用的是 findWebApplicationContext() 方法 123456789101112protected WebApplicationContext findWebApplicationContext() &#123; String attrName = getContextAttribute(); if (attrName == null) &#123; return null; &#125; WebApplicationContext wac = WebApplicationContextUtils.getWebApplicationContext(getServletContext(), attrName); if (wac == null) &#123; throw new IllegalStateException(\"No WebApplicationContext found: initializer not registered?\"); &#125; return wac; &#125; 2)、如果 WebApplicationContext 还没有创建，调用的是 createWebApplicationContext 方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758protected WebApplicationContext createWebApplicationContext(@Nullable ApplicationContext parent) &#123; //获取创建类型 Class&lt;?&gt; contextClass = getContextClass(); //删除了打印日志代码 //检查创建类型 if (!ConfigurableWebApplicationContext.class.isAssignableFrom(contextClass)) &#123; throw new ApplicationContextException( \"Fatal initialization error in servlet with name '\" + getServletName() + \"': custom WebApplicationContext class [\" + contextClass.getName() + \"] is not of type ConfigurableWebApplicationContext\"); &#125; //具体创建 ConfigurableWebApplicationContext wac = (ConfigurableWebApplicationContext) BeanUtils.instantiateClass(contextClass); wac.setEnvironment(getEnvironment()); wac.setParent(parent); //并设置的 contextConfigLocation 参数传给 wac，默认是 WEB-INFO/[ServletName]-Servlet.xml wac.setConfigLocation(getContextConfigLocation()); //调用的是下面的方法 configureAndRefreshWebApplicationContext(wac); return wac; &#125;protected void configureAndRefreshWebApplicationContext(ConfigurableWebApplicationContext wac) &#123; if (ObjectUtils.identityToString(wac).equals(wac.getId())) &#123; // The application context id is still set to its original default value // -&gt; assign a more useful id based on available information if (this.contextId != null) &#123; wac.setId(this.contextId); &#125; else &#123; // Generate default id... wac.setId(ConfigurableWebApplicationContext.APPLICATION_CONTEXT_ID_PREFIX + ObjectUtils.getDisplayString(getServletContext().getContextPath()) + '/' + getServletName()); &#125; &#125; wac.setServletContext(getServletContext()); wac.setServletConfig(getServletConfig()); wac.setNamespace(getNamespace()); wac.addApplicationListener(new SourceFilteringListener(wac, new ContextRefreshListener())); // The wac environment's #initPropertySources will be called in any case when the context // is refreshed; do it eagerly here to ensure servlet property sources are in place for // use in any post-processing or initialization that occurs below prior to #refresh ConfigurableEnvironment env = wac.getEnvironment(); if (env instanceof ConfigurableWebEnvironment) &#123; ((ConfigurableWebEnvironment) env).initPropertySources(getServletContext(), getServletConfig()); &#125; postProcessWebApplicationContext(wac); applyInitializers(wac); wac.refresh(); &#125; 里面还有 doXXX() 方法，大家感兴趣的可以去看看。 DispatcherServlet DispatcherServlet 继承自 FrameworkServlet，onRefresh 方法是 DispatcherServlet 的入口方法，在 initStrategies 方法中调用了 9 个初始化的方法。 这里分析其中一个初始化方法：initLocaleResolver() 方法 123456789101112private void initLocaleResolver(ApplicationContext context) &#123; try &#123; //在 context 中获取 this.localeResolver = context.getBean(LOCALE_RESOLVER_BEAN_NAME, LocaleResolver.class); //删除了打印日志的代码 &#125; catch (NoSuchBeanDefinitionException ex) &#123; //使用默认的策略 this.localeResolver = getDefaultStrategy(context, LocaleResolver.class); //删除了打印日志的代码 &#125; &#125; 查看默认策略代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647protected &lt;T&gt; T getDefaultStrategy(ApplicationContext context, Class&lt;T&gt; strategyInterface) &#123; //调用 getDefaultStrategies 方法 List&lt;T&gt; strategies = getDefaultStrategies(context, strategyInterface); if (strategies.size() != 1) &#123; throw new BeanInitializationException( \"DispatcherServlet needs exactly 1 strategy for interface [\" + strategyInterface.getName() + \"]\"); &#125; return strategies.get(0); &#125; /** * Create a List of default strategy objects for the given strategy interface. * &lt;p&gt;The default implementation uses the \"DispatcherServlet.properties\" file (in the same * package as the DispatcherServlet class) to determine the class names. It instantiates * the strategy objects through the context's BeanFactory. */ @SuppressWarnings(\"unchecked\") protected &lt;T&gt; List&lt;T&gt; getDefaultStrategies(ApplicationContext context, Class&lt;T&gt; strategyInterface) &#123; String key = strategyInterface.getName(); //根据策略接口的名字从 defaultStrategies 获取所需策略的类型 String value = defaultStrategies.getProperty(key); if (value != null) &#123; //如果有多个默认值的话，就以逗号分隔为数组 String[] classNames = StringUtils.commaDelimitedListToStringArray(value); List&lt;T&gt; strategies = new ArrayList&lt;&gt;(classNames.length); //按获取到的类型初始化策略 for (String className : classNames) &#123; try &#123; Class&lt;?&gt; clazz = ClassUtils.forName(className, DispatcherServlet.class.getClassLoader()); Object strategy = createDefaultStrategy(context, clazz); strategies.add((T) strategy); &#125; catch (ClassNotFoundException ex) &#123; throw new BeanInitializationException( \"Could not find DispatcherServlet's default strategy class [\" + className + \"] for interface [\" + key + \"]\", ex); &#125; catch (LinkageError err) &#123; throw new BeanInitializationException( \"Error loading DispatcherServlet's default strategy class [\" + className + \"] for interface [\" + key + \"]: problem with class file or dependent class\", err); &#125; &#125; return strategies; &#125; else &#123; return new LinkedList&lt;&gt;(); &#125; &#125; 其他几个方法大概也类似，我就不再写了。 小结主要讲了 Spring MVC 自身创建过程，分析了 Spring MVC 中 Servlet 的三个层次：HttpServletBean、FrameworkServlet 和 DispatcherServlet。HttpServletBean 继承自 Java 的 HttpServlet，其作用是将配置的参数设置到相应的属性上；FrameworkServlet 初始化了 WebApplicationContext；DispatcherServlet 初始化了自身的 9 个组件。 Spring MVC 之用分析 Spring MVC 是怎么处理请求的。首先分析 HttpServletBean、FrameworkServlet 和 DispatcherServlet 这三个 Servlet 的处理过程，最后分析 doDispatcher 的结构。 HttpServletBean 参与了创建工作，并没有涉及请求的处理。 FrameworkServlet 在类中的 service() 、doGet()、doPost()、doPut()、doDelete()、doOptions()、doTrace() 这些方法中可以看到都调用了一个共同的方法 processRequest() ，它是类在处理请求中最核心的方法。 123456789101112131415161718192021222324252627282930313233343536373839404142protected final void processRequest(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; long startTime = System.currentTimeMillis(); Throwable failureCause = null; //获取 LocaleContextHolder 中原来保存的 LocaleContext LocaleContext previousLocaleContext = LocaleContextHolder.getLocaleContext(); //获取当前请求的 LocaleContext LocaleContext localeContext = buildLocaleContext(request); //获取 RequestContextHolder 中原来保存的 RequestAttributes RequestAttributes previousAttributes = RequestContextHolder.getRequestAttributes(); //获取当前请求的 ServletRequestAttributes ServletRequestAttributes requestAttributes = buildRequestAttributes(request, response, previousAttributes); WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); asyncManager.registerCallableInterceptor(FrameworkServlet.class.getName(), new RequestBindingInterceptor());//将当前请求的 LocaleContext 和 ServletRequestAttributes 设置到 LocaleContextHolder 和 RequestContextHolder initContextHolders(request, localeContext, requestAttributes); try &#123; //实际处理请求的入口，这是一个模板方法，在 Dispatcher 类中才有具体实现 doService(request, response); &#125;catch (ServletException ex) &#123; failureCause = ex; throw ex; &#125;catch (IOException ex) &#123; failureCause = ex; throw ex; &#125;catch (Throwable ex) &#123; failureCause = ex; throw new NestedServletException(\"Request processing failed\", ex); &#125;finally &#123; //将 previousLocaleContext，previousAttributes 恢复到 LocaleContextHolder 和 RequestContextHolder 中 resetContextHolders(request, previousLocaleContext, previousAttributes); if (requestAttributes != null) &#123; requestAttributes.requestCompleted(); &#125; //删除了日志打印代码 //发布了一个 ServletRequestHandledEvent 类型的消息 publishRequestHandledEvent(request, response, startTime, failureCause); &#125; &#125; DispatcherServlet 上一章中其实还没把该类讲清楚，在这个类中，里面的智行处理的入口方法应该是 doService 方法，方法里面调用了 doDispatch 进行具体的处理，在调用 doDispatch 方法之前 doService 做了一些事情：首先判断是不是 include 请求，如果是则对 request 的 Attribute 做个快照备份，等 doDispatcher 处理完之后（如果不是异步调用且未完成）进行还原 ，在做完快照后又对 request 设置了一些属性。 123456789101112131415161718192021222324252627282930313233343536373839protected void doService(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; // Keep a snapshot of the request attributes in case of an include, // to be able to restore the original attributes after the include. Map&lt;String, Object&gt; attributesSnapshot = null; if (WebUtils.isIncludeRequest(request)) &#123; attributesSnapshot = new HashMap&lt;&gt;(); Enumeration&lt;?&gt; attrNames = request.getAttributeNames(); while (attrNames.hasMoreElements()) &#123; String attrName = (String) attrNames.nextElement(); if (this.cleanupAfterInclude || attrName.startsWith(DEFAULT_STRATEGIES_PREFIX))&#123; attributesSnapshot.put(attrName, request.getAttribute(attrName)); &#125; &#125; &#125; // Make framework objects available to handlers and view objects. request.setAttribute(WEB_APPLICATION_CONTEXT_ATTRIBUTE, getWebApplicationContext()); request.setAttribute(LOCALE_RESOLVER_ATTRIBUTE, this.localeResolver); request.setAttribute(THEME_RESOLVER_ATTRIBUTE, this.themeResolver); request.setAttribute(THEME_SOURCE_ATTRIBUTE, getThemeSource()); FlashMap inputFlashMap = this.flashMapManager.retrieveAndUpdate(request, response); if (inputFlashMap != null) &#123; request.setAttribute(INPUT_FLASH_MAP_ATTRIBUTE, Collections.unmodifiableMap(inputFlashMap)); &#125; request.setAttribute(OUTPUT_FLASH_MAP_ATTRIBUTE, new FlashMap()); request.setAttribute(FLASH_MAP_MANAGER_ATTRIBUTE, this.flashMapManager); try &#123; //调用 doDispatch 方法 doDispatch(request, response); &#125;finally &#123; if (!WebAsyncUtils.getAsyncManager(request).isConcurrentHandlingStarted()) &#123; // Restore the original attribute snapshot, in case of an include. if (attributesSnapshot != null) &#123; restoreAttributesAfterInclude(request, attributesSnapshot); &#125; &#125; &#125; &#125; doDispatch() 方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false; WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try &#123; ModelAndView mv = null; Exception dispatchException = null; try &#123; //检查是不是上传请求 processedRequest = checkMultipart(request); multipartRequestParsed = (processedRequest != request); // Determine handler for the current request. 根据 request 找到 Handler mappedHandler = getHandler(processedRequest); if (mappedHandler == null || mappedHandler.getHandler() == null) &#123; noHandlerFound(processedRequest, response); return; &#125; // Determine handler adapter for the current request.根据 Handler 找到对应的 HandlerAdapter HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // Process last-modified header, if supported by the handler. //处理 GET 、 HEAD 请求的 LastModified String method = request.getMethod(); boolean isGet = \"GET\".equals(method); if (isGet || \"HEAD\".equals(method)) &#123; long lastModified = ha.getLastModified(request, mappedHandler.getHandler()); if (logger.isDebugEnabled()) &#123; logger.debug(\"Last-Modified value for [\" + getRequestUri(request) + \"] is: \" + lastModified); &#125; if (new ServletWebRequest(request, response).checkNotModified(lastModified) &amp;&amp; isGet) &#123; return; &#125; &#125; //执行相应的 Interceptor 的 preHandle if (!mappedHandler.applyPreHandle(processedRequest, response)) &#123; return; &#125; // Actually invoke the handler. HandlerAdapter 使用 Handler 处理请求 mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); //如果需要异步处理，直接返回 if (asyncManager.isConcurrentHandlingStarted()) &#123; return; &#125; //当 view 为空时，根据 request 设置默认 view applyDefaultViewName(processedRequest, mv); //执行相应 Interceptor 的 postHandler mappedHandler.applyPostHandle(processedRequest, response, mv); &#125;catch (Exception ex) &#123; dispatchException = ex; &#125;catch (Throwable err) &#123; // As of 4.3, we're processing Errors thrown from handler methods as well, // making them available for @ExceptionHandler methods and other scenarios. dispatchException = new NestedServletException(\"Handler dispatch failed\", err); &#125; //调用 processDispatchResult 方法处理上面处理之后的结果（包括处理异常，渲染页面，发出完成通知触发 Interceptor 的 afterCompletion） processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); &#125;catch (Exception ex) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, ex); &#125;catch (Throwable err) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException(\"Handler processing failed\", err)); &#125;finally &#123; //判断是否执行异步请求 if (asyncManager.isConcurrentHandlingStarted()) &#123; // Instead of postHandle and afterCompletion if (mappedHandler != null) &#123; mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); &#125; &#125;else &#123; // Clean up any resources used by a multipart request. 删除上传请求的资源 if (multipartRequestParsed) &#123; cleanupMultipart(processedRequest); &#125; &#125; &#125; &#125; Handler，HandlerMapping，HandlerAdapter 三个区别： Handler：处理器，对应 MVC 的 C层，也就是 Controller 层，具体表现形式有很多种，可以是类，方法，它的类型是 Object，只要可以处理实际请求就可以是 Handler。 HandlerMapping：用来查找 Handler 的。 HandlerAdapter ：Handler 适配器， 另外 View 和 ViewResolver 的原理与 Handler 和 HandlerMapping 的原理类似。 小结本章分析了 Spring MVC 的请求处理的过程。","tags":[{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://yoursite.com/tags/Spring-MVC/"}]},{"title":"Spring MVC系列文章（三）：看透 Spring MVC 源代码分析与实践 ——  网站基础知识","date":"2017-07-13T16:00:00.000Z","path":"2017/07/14/Spring-MVC01/","text":"网站架构及其演变过程基础结构网络传输分解方式： 标准的 OSI 参考模型 TCP/IP 参考模型 海量数据的解决方案 缓存和页面静态化 缓存 通过程序直接保存在内存中 使用缓存框架 （Encache、Redis、Memcache） 页面静态化 使用模板技术生成（Velocity、FreeMaker等） 数据库优化 表结构优化 SQL 语句优化 分区 分表 索引优化 使用存储过程代替直接操作过程 分离活跃数据 批量读取和延迟修改 读写分离 分布式数据库 NoSQL 和 Hadoop 高并发的解决方案 应用和静态资源的分离：静态文件（图片、视频、JS、CSS等）放在专门的服务器上 页面缓存（Nginx 服务器、Squid 服务器） 集群与分布式 反向代理 CDN 底层优化：网络传输协议 常见协议和标准TCP/IP 协议IP：查找地址，对应着国际互联网 TCP：规范传输规则，对应着传输层 TCP 在传输之前会进行三次沟通，称 “三次握手”，传完数据断开的时候要进行四次沟通，称 “四次挥手”。 TCP 两个序号，三个标志位含义： seq：表示所传数据的序号。TCP 传输时每一个字节都有一个序号，发送数据的时候会将数据的第一个序号发送给对方，接收方会按序号检查是否接收完整了，如果没接收完整就需要重新传送，这样就可以保证数据的完整性。 ack：表示确认号。接收端用它来给发送端反馈已经成功接收到的数据信息，它的值为希望接收的下一个数据包起始序号。 ACK：确认位，只有 ACK = 1 的时候 ack 才起作用。正常通信时 ACK 为 1，第一次发起请求时因为没有需要确认接收的数据所以 ACK 为 0。 SYN：同步位，用于在建立连接时同步序号。刚开始建立连接时并没有历史接收的数据，所以 ack 也就没有办法设置，这是按照正常的机制就无法运行了，SYN 的作用就是解决这个问题的，当接收端接收到 SYN = 1 的报文时就会直接将 ack 设置为接收到的 seq + 1 的值，注意这里的值并不是检验后设置的，而是根据 SYN 直接设置的，这样正常的机制就可以运行了，所以 SYN 叫同步位。SYN 会在前两次握手时都为 1，这是因为通信的双方的 ack 都需要设置一个初始值。 FIN：终止位，用来在数据传输完毕后释放连接。 DNS 的设置DNS 解析参考域名设置，如下是我在腾讯云域名的设置 记录类型： A记录： 将域名指向一个IPv4地址（例如：8.8.8.8）CNAME：将域名指向另一个域名（例如 www.54tianzhisheng.cn）MX： 将域名指向邮件服务器地址TXT： 可任意填写，长度限制255，通常做SPF记录（反垃圾邮件）NS： 域名服务器记录，将子域名指定其他DNS服务器解析AAAA：将域名指向一个iPv6地址（例如：ff06:0:0:0:0:0:0:c3）SRV：记录提供特定服务的服务器（例如_xmpp-server._tcp）显性URL：将域名301重定向到另一个地址隐性URL：类似显性URL，但是会隐藏真实目标地址 主机记录： 要解析 www.54tianzhisheng.cn，请填写 www。主机记录就是域名前缀，常见用法有： www: 解析后的域名为 www.54tianzhisheng.cn。@: 直接解析主域名 54tianzhisheng.cn。*: 泛解析，匹配其他所有域名 .54tianzhisheng.cn。mail: 将域名解析为 mail.54tianzhisheng.cn，通常用于解析邮箱服务器。二级域名: 如：abc.54tianzhisheng.cn，填写abc。*手机网站: 如：m.54tianzhisheng.cn，填写m。 Java 中 Socket 的用法普通 Soket 的用法Socket 分为 ServerSocket 和 Socket 两大类。 ServerSocket 用于服务器端，可以通过 accept 方法监听请求，监听到请求后返回 Socket； Socket 用户具体完成数据传输，客户端直接使用 Socket 发送请求并传输数据。 随便写了个单方面发送消息的 demo： 客户端： 12345678910111213141516171819202122232425import java.io.IOException;import java.io.OutputStream;import java.net.Socket;/** * Created by 10412 on 2017/5/2. * TCP客户端： ①：建立tcp的socket服务，最好明确具体的地址和端口。这个对象在创建时，就已经可以对指定ip和端口进行连接(三次握手)。 ②：如果连接成功，就意味着通道建立了，socket流就已经产生了。只要获取到socket流中的读取流和写入流即可，只要通过getInputStream和getOutputStream就可以获取两个流对象。 ③：关闭资源。 *///单方面的输入！public class TcpClient&#123; public static void main(String[] args) &#123; try &#123; Socket s = new Socket(\"127.0.0.1\", 9999); OutputStream o = s.getOutputStream(); o.write(\"tcp sssss\".getBytes()); s.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 服务器端： 1234567891011121314151617181920212223242526272829303132import java.io.IOException;import java.io.InputStream;import java.net.ServerSocket;import java.net.Socket;/** * Created by 10412 on 2017/5/2. */public class TcpServer&#123; public static void main(String[] args) &#123; try &#123; ServerSocket ss = new ServerSocket(9999);//建立服务端的socket服务 Socket s = ss.accept();//获取客户端对象 String ip = s.getInetAddress().getHostAddress(); int port = s.getPort(); System.out.println(ip + \" : \" + port + \" connected\"); // 可以通过获取到的socket对象中的socket流和具体的客户端进行通讯。 InputStream ins = s.getInputStream();//读取客户端的数据，使用客户端对象的socket读取流 byte[] bytes = new byte[1024]; int len = ins.read(bytes); String text = new String(bytes, 0, len); System.out.println(text); //关闭资源 s.close(); ss.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; NioSocket 的用法见以前的一篇文章：Java NIO 系列教程 书中第五章简单的讲了下实现 HTTP 协议。第六章主要讲 Servlet，写了 Servlet 接口和其实现类。第七章把 Tomcat 分析的很不错，如果有读者感兴趣的话，可以去看看。","tags":[{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://yoursite.com/tags/Spring-MVC/"}]},{"title":"通过源码详解 Servlet","date":"2017-07-08T16:00:00.000Z","path":"2017/07/09/servlet/","text":"Servlet 结构 1、ServletServlet 该接口定义了5个方法。 init()，初始化 servlet 对象，完成一些初始化工作。它是由 servlet 容器控制的，该方法只能被调用一次 service()，接受客户端请求对象，执行业务操作，利用响应对象响应客户端请求。 destroy()，当容器监测到一个servlet从服务中被移除时，容器调用该方法，释放资源，该方法只能被调用一次。 getServletConfig()，ServletConfig 是容器向 servlet 传递参数的载体。 getServletInfo()，获取 servlet 相关信息。 Servlet 的生命周期： 1，初始化阶段 调用 init() 方法 2，响应客户请求阶段 调用 service() 方法 3，终止阶段 调用 destroy() 方法 在 Servlet 接口中的五个方法中涉及的接口有三个：ServletConfig 、 ServletRequest、 ServletResponse 这里先讲讲 ServletRequest 和 ServletResponse。 1）ServletRequest 由 Servlet 容器来管理，当客户请求到来时，容器创建一个 ServletRequest 对象，封装请求数据，同时创建一个 ServletResponse 对象，封装响应数据。这两个对象将被容器作为 service（）方法的参数传递给 Servlet，Serlvet 利用 ServletRequest 对象获取客户端发来的请求数据，利用 ServletResponse 对象发送响应数据。 下面是 ServletRequest 中所有的方法，根据方法名大概就可以猜到这些方法到底是干啥用的。 2）ServletResponse 发送响应数据 2、ServletConfigServletConfig 是容器向 servlet 传递参数的载体。 ServletConfig的4个常用方法： 1）public String getInitParameter（String name）：返回指定名称的初始化参数值； 2）public Enumeration getInitParameterNames（）：返回一个包含所有初始化参数名的 Enumeration 对象； 3）public String getServletName()：返回在 DD 文件中&lt;servlet-name&gt;元素指定的 Servlet 名称； 4）public ServletContext getServletContext（）：返回该 Servlet 所在的上下文对象； 这里详细讲下 ServletContext ： Servlet 上下文对象（ServletContext）：每个Web应用程序在被启动时都会创建一个唯一的上下文对象，Servlet 可通过其获得 Web 应用程序的初始化参数或 Servlet 容器的版本等信息，也可被 Servlet 用来与其他 Servlet 共享数据。 1、获得 ServletContext 应用： （1）、直接调用 getServletContext（）方法 ServletContext context = getServletContext（）; （2）、使用 ServletConfig 应用，再调用它的 getServletContext（）方法 ServletContext context = getServletConfig.getServletContext(); 2、获得应用程序的初始化参数： （1）、public String getInitParameter（String name）：返回指定参数名的字符串参数值，没有则返回 null； （2）、public Enumeration getInitParameterNames()：返回一个包含多有初始化参数名的 Enumeration 对象； 3、通过 ServletContext 对象获得资源 （1）、public URl getResource（String path）:返回由给定路径的资源的 URL 对象，以 “/” 开头，为相对路径，相对于Web 应用程序的文档根目录； （2）、public InputStream getResourceAsStream（String path）：从资源上获得一个 InputStream 对象，等价于getResource（path）.oprenStream(); （3）、public String getRealPath(String path)：返回给定的虚拟路径的真实路径； 4、登陆日志：使用 log（）方法可以将指定的消息写到服务器的日志文件中 （1）、public void log（String msg）：参数 msg 为写入日志文件消息 （2）、public void log（String msg，Throwable throwable）：将 msg 指定的消息和异常的栈跟踪信息写入日志文件 5、使用 RequestDispatcher 实现请求转发 （1）、RequestDispatcher getRequestDiapatcher(String path)：必须以 “/“ 开头相对于应用程序根目录，而ServletRequest 可以传递一个相对路径 （2）、RequestDipatcher getNamedDiapatcher（String name）：参数 name 为一个命名的 Servlet 对象 6、使用 ServletContext 对象存储数据 （1）、public void serAttribute（String name，Object object）：将给定名称的属性值对象绑定到上下文对象上； （2）、public Object getAttribute（String name）：返回绑定到上下文对象的给定名称的属性值； （3）、public Enumeration getAttributeNames()：返回绑定到上下文对象上的所有属性名的 Enumeration 对象； （4）、public void removeAttribute（String name）：删除绑定到上下文对象指定名称的属性； ServletRequest 共享的对象仅在请求的生存周期中可以被访问； HttpSession 共享的对象仅在会话的生存周期中可以被访问； ServletContext 共享的对象在整个 Web 应用程序启动的生存周期中可以被访问； 7、检索 Servlet 容器的信息 （1）、public String getServletInfo()：返回 Servlet 所运行容器的名称和版本； （2）、public int getMajorVersion（）：返回容器所支持的 Servlet API 的主版本号； （3）、public int getMinorVersion（）：返回容器所支持的 Servlet API 的次版本号； （4）、public String getServletContext（）：返回 ServletContext 对应的 web 应用程序名称 &lt;display-name&gt;元素定义的名称； 3、GenericServlet 抽象类GenericServlet 定义了一个通用的，不依赖具体协议的 Servlet，它实现了 Servlet 接口和 ServletConfig 接口，它的方法在文章的第一张图就给出了。 4、HttpServlet 抽象类4.1、HTTP 请求方式 GET : 获取由请求 URL 标识的资源 POST : 向 Web 服务器发送无限制长度的数据 PUT : 存储一个资源到请求的 URL DELETE : 删除由 URL 标识的资源 HEAD : 返回 URL 标识的头信息 OPTIONS : 返回服务器支持的 HTTP 方法 TRACE : 返回 TRACE 请求附带的头字段 4.2、对应的服务方法： doGet() : 调用服务器的资源, 并将其作为响应返回给客户端. doGet() 调用在 URL 里显示正在传送给 Servlet 的数据,这在系统的安全方面可能带来一些问题, 比如说, 用户登录时, 表单里的用户名和密码需要发送到服务器端, doGet() 调用会在浏览器的 URL 里显示用户名和密码. doPost() : 它用于把客户端的数据传给服务端, 使用它可以以隐藏方式给服务器端发送数据. Post 适合发送大量数据. doPut() : 调用和 doPost() 相似, 并且它允许客户端把真正的文件存放在服务器上, 而不仅仅是传送数据. doDelete() : 它允许客户端删除服务器端的文件或者 Web 页面．它的使用非常少． doHead() : 它用于处理客户端的 Head 调用,并且返回一个 response. 当客户端只需要响应的 Header 时,它就发出一个Header 请求.这种情况下客户端往往关心响应的长度和响应的 MIME 类型. doOptions(): 它用于处理客户端的 Options 调用,通过这个调用, 客户端可以获得此 Servlet 支持的方法.如果 Servlet 覆盖了 doPost() 方法, 那么将返回: Allow: POST, TRACE, OPTIONS, HEAD doTrace：处理 TRACE 请求 4.3、Servlet Service 方法详解12345678910111213141516public void service(ServletRequest req, ServletResponse res) throws ServletException, IOException &#123; HttpServletRequest request; HttpServletResponse response; // 如果传入的 HTTP 请求和 HTTP 响应不是 HTTP 的领域模型，则抛出 Servlet 异常，这个异常会被 Servlet 容器所处理 if (!(req instanceof HttpServletRequest &amp;&amp; res instanceof HttpServletResponse)) &#123; throw new ServletException(\"non-HTTP request or response\"); &#125; // 既然是 HTTP 协议绑定的 Serlvet, 强制转换到 HTTP 的领域模型 request = (HttpServletRequest) req; response = (HttpServletResponse) res; // 如果传入的请求和响应是预期的 HTTP 请求和 HTTP 响应，则调用 HttpServlet 的 service() 方法。 service(request, response); &#125; 4.4、HttpServlet service 方法详解1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; // 从 HTTP 请求中取得这次请求所使用的 HTTT 方法 String method = req.getMethod(); // 如果这次请求使用 GET 方法 if (method.equals(METHOD_GET)) &#123; // 取得这个 Servlet 的最后修改的时间 long lastModified = getLastModified(req); if (lastModified == -1) &#123; // servlet doesn't support if-modified-since, no reason // to go through further expensive logic //-1 代表这个 Servlet 不支持最后修改操作，直接调用 doGet() 进行处理 HTTP GET 请求 doGet(req, resp); &#125; else &#123; // 如果这个 Servlet 支持最后修改操作，取得请求头中包含的请求的最后修改时间 long ifModifiedSince = req.getDateHeader(HEADER_IFMODSINCE); if (ifModifiedSince &lt; lastModified) &#123; // If the servlet mod time is later, call doGet() // Round down to the nearest second for a proper compare // A ifModifiedSince of -1 will always be less // 如果请求头中包含的修改时间早于这个 Servlet 的最后修改时间，说明这个 Servlet 自从客户上一次 HTTP 请求已经被修改了 , 设置最新修改时间到响应头中 maybeSetLastModified(resp, lastModified); // 调用 doGet 进行进行处理 HTTP GET 请求 doGet(req, resp); &#125; else &#123; // 如果请求头中包含修改时间晚于这个 Servlet 的最后修改时间，说明这个 Servlet 自从请求的最后修改时间后没有更改过，这种情况下，仅仅返回一个 HTTP 响应状态 SC_NOT_MODIFIED resp.setStatus(HttpServletResponse.SC_NOT_MODIFIED); &#125; &#125; &#125; else if (method.equals(METHOD_HEAD)) &#123; // 如果这次请求使用 HEAD 方法 // 如果这个 Servlet 支持最后修改操作，则设置这个 Servlet 的最后修改时间到响应头中 long lastModified = getLastModified(req); maybeSetLastModified(resp, lastModified); // 和对 HTTP GET 方法处理不同的是，无论请求头中的修改时间是不是早于这个 Sevlet 的最后修改时间，都会发 HEAD 响应给客户，因为 HTTP HEAD 响应是用来查询 Servlet 头信息的操作 doHead(req, resp); &#125; else if (method.equals(METHOD_POST)) &#123; // 如果这次请求使用 POST 方法 doPost(req, resp); &#125; else if (method.equals(METHOD_PUT)) &#123; // 如果这次请求使用 PUT 方法 doPut(req, resp); &#125; else if (method.equals(METHOD_DELETE)) &#123; // 如果这次请求使用 DELETE 方法 doDelete(req, resp); &#125; else if (method.equals(METHOD_OPTIONS)) &#123; // 如果这次请求使用 OPTIONS 方法 doOptions(req,resp); &#125; else if (method.equals(METHOD_TRACE)) &#123; // 如果这次请求使用 TRACE 方法 doTrace(req,resp); &#125; else &#123; // Note that this means NO servlet supports whatever // method was requested, anywhere on this server. // 如果这次请求是其他未知方法，返回错误代码 SC_NOT_IMPLEMENTED 给 HTTP 响应，并且显示一个错误消息，说明这个操作是没有实现的 String errMsg = lStrings.getString(\"http.method_not_implemented\"); Object[] errArgs = new Object[1]; errArgs[0] = method; errMsg = MessageFormat.format(errMsg, errArgs); resp.sendError(HttpServletResponse.SC_NOT_IMPLEMENTED, errMsg); &#125; &#125; 5、Servlet 的多线程问题1、当涉及到 Servlet 需要共享资源是，需保证 Servlet 是线程安全的 2、注意事项： （1）、用方法的局部变量保持请求中的专有数据； （2）、只用 Servlet 的成员变量来存放那些不会改变的数据； （3）、对可能被请求修改的成员变量同步（用 Synchronized 关键字修饰）； （4）、如果 Servlet 访问外部资源，那么需要同步访问这些资源； 3、实现 SingleThreadModel 接口的 Servlet 在被多个客户请求时一个时刻只能有一个线程运行，不推荐使用。 4、如果必须在 servlet 使用同步代码，应尽量在最小的范围上（代码块）进行同步，同步代码越少，Servlet 执行才能越好，避免对 doGet() 或 doPost() 方法同步。 总结全文首先通过一张 Servlet 中的核心 Servlet 类图关系，了解了几种 Servlet 之间的关系及其内部方法。然后在分别介绍这几种 Servlet，通过分析部分重要方法的源码来了解，还介绍了 Servlet 中多线程的问题的解决方法。 注：文章原创，首发于：zhisheng 的博客，文章可转载但请注明地址为：http://www.54tianzhisheng.cn/2017/07/09/servlet/","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Servlet","slug":"Servlet","permalink":"http://yoursite.com/tags/Servlet/"}]},{"title":"Velocity 循环指令一种好的解决方法","date":"2017-06-27T16:00:00.000Z","path":"2017/06/28/Velocity-foreach/","text":"前提前台的数据经常是由需要通过 foreach 循环获取。 好的解决方案：（拿我最近做的一个项目做例子）购物商城左边的导航栏，商品大分类和小分类（Category） 1、在 model 包下创建一个 ViewObject 类 1234567891011public class ViewObject&#123; private Map&lt;String, Object&gt; objs = new HashMap&lt;&gt;(); public void set(String key, Object value) &#123; objs.put(key, value); &#125; public Object get(String key) &#123; return objs.get(key); &#125;&#125; 2、在 controller 包下创建个 BaseController 类 12345678910111213141516171819202122232425262728293031323334/** * 在每个页面显示图书大分类，抽离出来 * @return */ public List&lt;ViewObject&gt; selectAllCategory() &#123; List&lt;Category&gt; categories = categoryService.selectAllCategory(); List&lt;ViewObject&gt; vos = new ArrayList&lt;&gt;(); for (Category category : categories) &#123; ViewObject vo = new ViewObject(); vo.set(\"category\", category); vo.set(\"id\", category.getId()); //System.out.println(\"category 中的 id 是 \"+category.getId()); vos.add(vo); &#125; return vos; &#125; /** * 获取图书的小分类，在这里将小分类中的大分类id查找出来，保存在 cds.id 中， * 然后在模板引擎中通过将 vos.id 和 cds.id 相比较。然后如果相同的话，就取出来放在对应的大分类下 * @return */ public List&lt;ViewObject&gt; selectAllCategoryDetail() &#123; List&lt;CategoryDetail&gt; categoryDetails = categoryDetailService.selectAllCategoryDetail(); List&lt;ViewObject&gt; cds = new ArrayList&lt;&gt;(); for (CategoryDetail categoryDetail : categoryDetails) &#123; ViewObject vo = new ViewObject(); vo.set(\"categoryDetail\", categoryDetail); //System.out.println(\"categoryDetail 中的 categoryDetail id =\" + categoryDetail.getId() + \"category id = \" + categoryDetail.getCategory_id() + \" name = \" + categoryDetail.getName()); vo.set(\"id\", categoryDetail.getCategory_id()); cds.add(vo); &#125; return cds; &#125; 3、在 IndexController 类下，需要继承 BaseController.java 类 12345678910111213/** * 返回首页 * @param model * @return */ @RequestMapping(path = &#123;\"/\", \"/index\"&#125;) public String index(Model model) &#123; //模板引擎设置图书分类左边导航栏 model.addAttribute(\"vos\", selectAllCategory()); model.addAttribute(\"cds\", selectAllCategoryDetail()); //返回主页 return \"index\"; &#125; 4、抽离导航部分的代码 left.html 1234567891011121314151617&lt;!--左边图书分类导航栏--&gt;&lt;div class=\"c3_b1_left\"&gt; &lt;dl&gt; #foreach($vo in $vos) &lt;dd&gt; &lt;h1&gt;$!&#123;vo.category.name&#125;&lt;/h1&gt; &lt;p&gt; #foreach($cd in $cds) #if($vo.id == $cd.id) &lt;a href=\"/list\"&gt;$!&#123;cd.categoryDetail.name&#125;&lt;/a&gt; #end #end &lt;/p&gt; &lt;/dd&gt; #end &lt;/dl&gt;&lt;/div&gt; 5、首页中相应的位置引入 left.html 1#parse(\"left.html\") 这样就可以解决问题了，可是有时候我们需要控制循环的个数，因为我们网页端可能只需要特定的数据量 那么就需要中断 foreach，可以使用 #break 指令终止循环 123456#foreach( $vo in $vos ) #if( $foreach.count &gt; 5 ) #break #end $!&#123;vo.customer.Name&#125;#end 参考Velocity入门指南——第七章 循环指令","tags":[{"name":"Velocity","slug":"Velocity","permalink":"http://yoursite.com/tags/Velocity/"}]},{"title":"Java IO流学习超详细总结（图文并茂）","date":"2017-06-22T16:00:00.000Z","path":"2017/06/23/java-io/","text":"Java流操作有关的类或接口：Java流类图结构： 流的概念和作用流是一组有顺序的，有起点和终点的字节集合，是对数据传输的总称或抽象。即数据在两设备间的传输称为流，流的本质是数据传输，根据数据传输特性将流抽象为各种类，方便更直观的进行数据操作。 IO流的分类 根据处理数据类型的不同分为：字符流和字节流 根据数据流向不同分为：输入流和输出流 字符流和字节流字符流的由来： 因为数据编码的不同，而有了对字符进行高效操作的流对象。本质其实就是基于字节流读取时，去查了指定的码表。 字节流和字符流的区别： 读写单位不同：字节流以字节（8bit）为单位，字符流以字符为单位，根据码表映射字符，一次可能读多个字节。 处理对象不同：字节流能处理所有类型的数据（如图片、avi等），而字符流只能处理字符类型的数据。 结论：只要是处理纯文本数据，就优先考虑使用字符流。 除此之外都使用字节流。 输入流和输出流对输入流只能进行读操作，对输出流只能进行写操作，程序中需要根据待传输数据的不同特性而使用不同的流。 Java IO流对象1.输入字节流InputStreamIO 中输入字节流的继承图可见上图，可以看出： InputStream 是所有的输入字节流的父类，它是一个抽象类。 ByteArrayInputStream、StringBufferInputStream、FileInputStream 是三种基本的介质流，它们分别从Byte 数组、StringBuffer、和本地文件中读取数据。PipedInputStream 是从与其它线程共用的管道中读取数据，与Piped 相关的知识后续单独介绍。 ObjectInputStream 和所有FilterInputStream 的子类都是装饰流（装饰器模式的主角）。 2.输出字节流OutputStreamIO 中输出字节流的继承图可见上图，可以看出： OutputStream 是所有的输出字节流的父类，它是一个抽象类。 ByteArrayOutputStream、FileOutputStream 是两种基本的介质流，它们分别向Byte 数组、和本地文件中写入数据。PipedOutputStream 是向与其它线程共用的管道中写入数据， ObjectOutputStream 和所有FilterOutputStream 的子类都是装饰流。 3.字节流的输入与输出的对应 图中蓝色的为主要的对应部分，红色的部分就是不对应部分。紫色的虚线部分代表这些流一般要搭配使用。从上面的图中可以看出Java IO 中的字节流是极其对称的。“存在及合理”我们看看这些字节流中不太对称的几个类吧！ LineNumberInputStream 主要完成从流中读取数据时，会得到相应的行号，至于什么时候分行、在哪里分行是由改类主动确定的，并不是在原始中有这样一个行号。在输出部分没有对应的部分，我们完全可以自己建立一个LineNumberOutputStream，在最初写入时会有一个基准的行号，以后每次遇到换行时会在下一行添加一个行号，看起来也是可以的。好像更不入流了。 PushbackInputStream 的功能是查看最后一个字节，不满意就放入缓冲区。主要用在编译器的语法、词法分析部分。输出部分的BufferedOutputStream 几乎实现相近的功能。 StringBufferInputStream 已经被Deprecated，本身就不应该出现在InputStream 部分，主要因为String 应该属于字符流的范围。已经被废弃了，当然输出部分也没有必要需要它了！还允许它存在只是为了保持版本的向下兼容而已。 SequenceInputStream 可以认为是一个工具类，将两个或者多个输入流当成一个输入流依次读取。完全可以从IO 包中去除，还完全不影响IO 包的结构，却让其更“纯洁”――纯洁的Decorator 模式。 PrintStream 也可以认为是一个辅助工具。主要可以向其他输出流，或者FileInputStream 写入数据，本身内部实现还是带缓冲的。本质上是对其它流的综合运用的一个工具而已。一样可以踢出IO 包！System.out 和System.out 就是PrintStream 的实例！ 4.字符输入流Reader在上面的继承关系图中可以看出： Reader 是所有的输入字符流的父类，它是一个抽象类。 CharReader、StringReader 是两种基本的介质流，它们分别将Char 数组、String中读取数据。PipedReader 是从与其它线程共用的管道中读取数据。 BufferedReader 很明显就是一个装饰器，它和其子类负责装饰其它Reader 对象。 FilterReader 是所有自定义具体装饰流的父类，其子类PushbackReader 对Reader 对象进行装饰，会增加一个行号。 InputStreamReader 是一个连接字节流和字符流的桥梁，它将字节流转变为字符流。FileReader 可以说是一个达到此功能、常用的工具类，在其源代码中明显使用了将FileInputStream 转变为Reader 的方法。我们可以从这个类中得到一定的技巧。Reader 中各个类的用途和使用方法基本和InputStream 中的类使用一致。后面会有Reader 与InputStream 的对应关系。 5.字符输出流Writer在上面的关系图中可以看出： Writer 是所有的输出字符流的父类，它是一个抽象类。 CharArrayWriter、StringWriter 是两种基本的介质流，它们分别向Char 数组、String 中写入数据。PipedWriter 是向与其它线程共用的管道中写入数据， BufferedWriter 是一个装饰器为Writer 提供缓冲功能。 PrintWriter 和PrintStream 极其类似，功能和使用也非常相似。 OutputStreamWriter 是OutputStream 到Writer 转换的桥梁，它的子类FileWriter 其实就是一个实现此功能的具体类（具体可以研究一SourceCode）。功能和使用和OutputStream 极其类似，后面会有它们的对应图。 6.字符流的输入与输出的对应 7.字符流与字节流转换转换流的特点： 其是字符流和字节流之间的桥梁 可对读取到的字节数据经过指定编码转换成字符 可对读取到的字符数据经过指定编码转换成字节 何时使用转换流？ 当字节和字符之间有转换动作时； 流操作的数据需要编码或解码时。 具体的对象体现： InputStreamReader:字节到字符的桥梁 OutputStreamWriter:字符到字节的桥梁 这两个流对象是字符体系中的成员，它们有转换作用，本身又是字符流，所以在构造的时候需要传入字节流对象进来。 8.File类File类是对文件系统中文件以及文件夹进行封装的对象，可以通过对象的思想来操作文件和文件夹。 File类保存文件或目录的各种元数据信息，包括文件名、文件长度、最后修改时间、是否可读、获取当前文件的路径名，判断指定文件是否存在、获得当前目录中的文件列表，创建、删除文件和目录等方法。 9.RandomAccessFile类该对象并不是流体系中的一员，其封装了字节流，同时还封装了一个缓冲区（字符数组），通过内部的指针来操作字符数组中的数据。 该对象特点： 该对象只能操作文件，所以构造函数接收两种类型的参数：a.字符串文件路径；b.File对象。 该对象既可以对文件进行读操作，也能进行写操作，在进行对象实例化时可指定操作模式(r,rw) 注意：该对象在实例化时，如果要操作的文件不存在，会自动创建；如果文件存在，写数据未指定位置，会从头开始写，即覆盖原有的内容。 可以用于多线程下载或多个线程同时写数据到文件。 更多精彩文章请见我的个人博客地址：http://www.54tianzhisheng.cn","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://yoursite.com/tags/IO/"}]},{"title":"AJAX 学习","date":"2017-06-22T16:00:00.000Z","path":"2017/06/23/AJAX/","text":"背景最近的项目中大量地方需要使用 AJAX，无奈，谁叫我既要写前台又要写后台呢，只好学习下这个技术点，主要参考 W3school 文档，下面记录下这些知识点，便于日后自己查阅，下面的一些测试代码建议在 W3school 中测试。 AJAX 基础： AJAX = Asynchronous JavaScript and XML（异步的 JavaScript 和 XML）。 AJAX 是一种在无需重新加载整个网页的情况下，能够更新部分网页的技术。 在很多网站可以见到使用这种技术。 AJAX - XMLHttpRequest 创建 XMLHttpRequest 对象 XMLHttpRequest 是 AJAX 的基础。XMLHttpRequest 用于在后台与服务器交换数据。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。 创建 XMLHttpRequest 对象的语法： 1variable = new XMLHttpRequest(); 但是对于老版本的 Internet Explorer （IE5 和 IE6）却是使用 ActiveX 对象，所以在开发中为了适应大多数的浏览器，常使用如下： 123456789var xmlhttp;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp = new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp = new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125; 向服务器发送请求 使用 XMLHttpRequest 对象的 open() 和 send() 方法： 12xmlhttp.open(\"GET\",\"test1.txt\",true);xmlhttp.send(); method description open(method, url, async) 规定请求的类型、URL 以及是否异步处理请求。method：请求的类型；GET 或 POST url：文件在服务器上的位置 async：true（异步）或 false（同步） send(string) 将请求发送到服务器。string：仅用于 POST 请求 GET 还是 POST？ 与 POST 相比，GET 更简单也更快，并且在大部分情况下都能用。 然而，在以下情况中，请使用 POST 请求： 无法使用缓存文件（更新服务器上的文件或数据库） 向服务器发送大量数据（POST 没有数据量限制） 发送包含未知字符的用户输入时，POST 比 GET 更稳定也更可靠 示例：GET 请求 1、简单的 GET 请求 12345678910111213141516171819202122232425262728293031323334&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function loadXMLDoc()&#123;var xmlhttp;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125;xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; &#125; &#125;xmlhttp.open(\"GET\",\"/ajax/demo_get.asp?t=\" + Math.random(),true);xmlhttp.send();&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;h2&gt;AJAX&lt;/h2&gt;&lt;button type=\"button\" onclick=\"loadXMLDoc()\"&gt;请求数据&lt;/button&gt;&lt;div id=\"myDiv\"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 2、通过 GET 方法发送信息 12345678910111213141516171819202122232425262728293031323334&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function loadXMLDoc()&#123;var xmlhttp;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125;xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; &#125; &#125;xmlhttp.open(\"GET\",\"/ajax/demo_get2.asp?fname=Bill&amp;lname=Gates\",true);xmlhttp.send();&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;h2&gt;AJAX&lt;/h2&gt;&lt;button type=\"button\" onclick=\"loadXMLDoc()\"&gt;请求数据&lt;/button&gt;&lt;div id=\"myDiv\"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 示例：POST 请求 1、简单 POST 请求 12345678910111213141516171819202122232425262728293031323334&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function loadXMLDoc()&#123;var xmlhttp;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125;xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; &#125; &#125;xmlhttp.open(\"POST\",\"/ajax/demo_post.asp\",true);xmlhttp.send();&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;h2&gt;AJAX&lt;/h2&gt;&lt;button type=\"button\" onclick=\"loadXMLDoc()\"&gt;请求数据&lt;/button&gt;&lt;div id=\"myDiv\"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 2、像 HTML 表单那样 POST 数据，请使用 setRequestHeader() 来添加 HTTP 头。然后在 send() 方法中规定您希望发送的数据 123456789101112131415161718192021222324252627282930313233343536&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function loadXMLDoc()&#123;var xmlhttp;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125;xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; &#125; &#125;xmlhttp.open(\"POST\",\"/ajax/demo_post2.asp\",true);xmlhttp.setRequestHeader(\"Content-type\",\"application/x-www-form-urlencoded\");xmlhttp.send(\"fname=Bill&amp;lname=Gates\");&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;h2&gt;AJAX&lt;/h2&gt;&lt;button type=\"button\" onclick=\"loadXMLDoc()\"&gt;请求数据&lt;/button&gt;&lt;div id=\"myDiv\"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 注意：setRequestHeader(header, value) 向请求添加 HTTP 头，header: 规定头的名称, value: 规定头的值。 url - 服务器上的文件 open() 方法的 url 参数是服务器上文件的地址： 1xmlhttp.open(\"GET\",\"ajax_test.asp\",true); 该文件可以是任何类型的文件，比如 .txt 和 .xml，或者服务器脚本文件，比如 .asp 和 .php （在传回响应之前，能够在服务器上执行任务）。 异步 - True or False ？ XMLHttpRequest 对象如果要用于 AJAX 的话，其 open() 方法的 async 参数必须设置为 true，对于 web 开发人员来说，发送异步请求是一个巨大的进步。很多在服务器执行的任务都相当费时。AJAX 出现之前，这可能会引起应用程序挂起或停止。 通过 AJAX，JavaScript 无需等待服务器的响应，而是： 在等待服务器响应时执行其他脚本 当响应就绪后对响应进行处理 Async = true 当使用 async = true 时，请规定在响应处于 onreadystatechange 事件中的就绪状态时执行的函数 123456789101112131415161718192021222324252627282930313233&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function loadXMLDoc()&#123;var xmlhttp;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125;xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; &#125; &#125;xmlhttp.open(\"GET\",\"/ajax/test1.txt\",true);xmlhttp.send();&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=\"myDiv\"&gt;&lt;h2&gt;Let AJAX change this text&lt;/h2&gt;&lt;/div&gt;&lt;button type=\"button\" onclick=\"loadXMLDoc()\"&gt;通过 AJAX 改变内容&lt;/button&gt;&lt;/body&gt;&lt;/html&gt; Async = false 如需使用 async = false，请将 open() 方法中的第三个参数改为 false 不推荐使用 async = false，但是对于一些小型的请求，也是可以的。 请记住，JavaScript 会等到服务器响应就绪才继续执行。如果服务器繁忙或缓慢，应用程序会挂起或停止。 注释：当您使用 async=false 时，请不要编写 onreadystatechange 函数 - 把代码放到 send() 语句后面即可： 123456789101112131415161718192021222324252627&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function loadXMLDoc()&#123;var xmlhttp;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125;xmlhttp.open(\"GET\",\"/ajax/test1.txt\",false);xmlhttp.send();document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText;&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=\"myDiv\"&gt;&lt;h2&gt;Let AJAX change this text&lt;/h2&gt;&lt;/div&gt;&lt;button type=\"button\" onclick=\"loadXMLDoc()\"&gt;通过 AJAX 改变内容&lt;/button&gt;&lt;/body&gt;&lt;/html&gt; 服务器响应 使用 XMLHttpRequest 对象的 responseText 或 responseXML 属性。 responseText 获得字符串形式的响应数据。 responseXML 获得 XML 形式的响应数据。 1、responseText 属性 如果来自服务器的响应并非 XML，请使用 responseText 属性。 responseText 属性返回字符串形式的响应，因此您可以这样使用： 1document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; 2、responseXML 属性 如果来自服务器的响应是 XML，而且需要作为 XML 对象进行解析，请使用 responseXML 属性： 请求 books.xml 文件，并解析响应： 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function loadXMLDoc()&#123;var xmlhttp;var txt,x,i;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125;xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; xmlDoc=xmlhttp.responseXML; txt=\"\"; x=xmlDoc.getElementsByTagName(\"title\"); for (i=0;i&lt;x.length;i++) &#123; txt=txt + x[i].childNodes[0].nodeValue + \"&lt;br /&gt;\"; &#125; document.getElementById(\"myDiv\").innerHTML=txt; &#125; &#125;xmlhttp.open(\"GET\",\"/example/xmle/books.xml\",true);xmlhttp.send();&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;h2&gt;My Book Collection:&lt;/h2&gt;&lt;div id=\"myDiv\"&gt;&lt;/div&gt;&lt;button type=\"button\" onclick=\"loadXMLDoc()\"&gt;获得我的图书收藏列表&lt;/button&gt;&lt;/body&gt;&lt;/html&gt; onreadystatechange 事件 当请求被发送到服务器时，我们需要执行一些基于响应的任务。每当 readyState 改变时，就会触发 onreadystatechange 事件。readyState 属性存有 XMLHttpRequest 的状态信息。 下面是 XMLHttpRequest 对象的三个重要的属性： onreadystatechange 存储函数（或函数名），每当 readyState 属性改变时，就会调用该函数 readyState 存有 XMLHttpRequest 的状态。从 0 到 4 发生变化。 0: 请求未初始化 1: 服务器连接已建立 2: 请求已接收 3: 请求处理中 4: 请求已完成，且响应已就绪 status 200: “OK” 404: 未找到页面 在 onreadystatechange 事件中，我们规定当服务器响应已做好被处理的准备时所执行的任务。 当 readyState 等于 4 且状态为 200 时，表示响应已就绪： 1234567xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; &#125; &#125; 使用 Callback 函数 callback 函数是一种以参数形式传递给另一个函数的函数。 如果您的网站上存在多个 AJAX 任务，那么您应该为创建 XMLHttpRequest 对象编写一个标准 的函数，并为每个 AJAX 任务调用该函数。 该函数调用应该包含 URL 以及发生 onreadystatechange 事件时执行的任务（每次调用可能不尽相同）： 12345678910function myFunction()&#123;loadXMLDoc(\"ajax_info.txt\",function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; &#125; &#125;);&#125; AJAX - 高级ASP/PHP 请求实例 - AJAX 用于创造动态性更强的应用程序。 AJAX 可用来与数据库进行动态通信。 AJAX 可用来与 XML 文件进行交互式通信。 AJAX 实例使用 XMLHttpRequest 对象的实例","tags":[{"name":"AJAX","slug":"AJAX","permalink":"http://yoursite.com/tags/AJAX/"}]},{"title":"java.sql.SQLException Field 'id' doesn't have a default value","date":"2017-06-19T16:00:00.000Z","path":"2017/06/20/Java-error1/","text":"1、错误描述 在做一个电商网站项目时，使用 Mybatis + MySQL 时出现问题 Caused by: java.sql.SQLException: Field &#39;id&#39; doesn&#39;t have a default value ，网上很多人说是 MyBatis 插入数据行 ID 没生成自增。但是我尝试好久，没解决该问题。 2、错误原因 后来才发现是因为创建数据库时的建表语句中的 id 是主键的，但是在插入的过程中，没有给予数值，并且没有让 id 自增。 3、解决办法 修改数据库表中的id，让其自增（在插入的过程中，不插入id数据时）。 （我是直接将整个数据库都导出来，然后在每个表的 id 后面加上一个 auto_increment）, 如下 ：12345678910CREATE TABLE `d_user` ( `id` int(11) NOT NULL auto_increment, `name` varchar(45) DEFAULT NULL, `password` varchar(45) DEFAULT NULL, `zip` varchar(45) DEFAULT NULL, `address` varchar(45) DEFAULT NULL, `phone` varchar(45) DEFAULT NULL, `email` varchar(45) DEFAULT NULL, PRIMARY KEY (`id`))","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"中缀表达式转换成前缀和后缀表达式这类题目的超实用解题技巧","date":"2017-06-18T16:00:00.000Z","path":"2017/06/19/中缀表达式转换成前缀和后缀表达式这类题目的超实用解题技巧/","text":"看到了标题如果还不了解的这几个概念的请先看看博客：详解前缀、中缀、后缀表达式 先给几个中缀表达式转换成后缀表达式题目做做吧，最后我们在总结超实用的技巧！！ 1. 表达式“X=A+B*（C–D）/E”的后缀表示形式可以为 A. XAB+CDE/-*= B. XA+BC-DE/*= C. XABCD-*E/+= D. XABCDE+*/= 先把答案说出来吧，不过你可以自己先好好的想想可以怎么做才能更快的把答案选出来呢？ 答案：C 先看看下面图片中的这种方法如何？ 2. 已知-算术表达式的中缀表达式为a-(b+c/d)*e,其后缀形式为() A. -a+b*c/d B. -a+b*cd/e C. -+*abc/de D. abcd/+e*- 答案：D 3. 算术表达式a+b*(c+d/e)转为后缀表达式后为() A. ab+cde/* B. abcde/+*+ C. abcde/*++ D. abcde*/++ 答案：B 4. 表达式a*(b+c)-d的后缀表达式是() A. abcd*+- B. abc+*d- C. abc*+d- D. -+*abcd 答案：B 好，题目我们也看了这么多了，那我们该如何解决这一类的题目呢？如果你看了文章首部的那篇 博客的话，那你肯定会觉得那个解法很复杂，如果真的是在笔试中出现这样的题目，那得耗费不 少的时间啊。有人要问了，说了那么一堆，那究竟有没有什么快速的方法呢或者说有没有什么简 单的方法可以直接口算的把答案写出来呢，答案是：有的！而且还真的是特别的简单！！！ 解题重点： 这里我给出一个中缀表达式~ a+b*c-(d+e) 第一步：按照运算符的优先级对所有的运算单位加括号 则式子变成拉：((a+(b*c))-(d+e)) 第二步：转换前缀与后缀表达式 1. 前缀表达式：把运算符号移动到对应的括号前面 则变成拉：-( +(a *(bc)) +(de)) 把括号去掉：-+a*bc+de 前缀表达式出现 2. 后缀表达式：把运算符号移动到对应的括号后面 则变成拉：((a(bc)* )+ (de)+ )- 把括号去掉：abc*+de+- 后缀表达式出现 发现没有，前缀表达式，后缀表达式是不需要用括号来进行优先级的确定的。 如果你习惯拉他的运算方法。计算的时候也就是从两个操作数的前面 或者后面找运算符。而不是中间找，那么也就直接可以口算啦！ 你说这种方法是不是很简单啊！！！ 现在你再去看看刚才的那四道题目，是不是很简单的答案就口算出来了啊！！！ 6不6？","tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/数据结构/"},{"name":"表达式","slug":"表达式","permalink":"http://yoursite.com/tags/表达式/"}]},{"title":"循环队列的相关条件和公式","date":"2017-06-17T16:00:00.000Z","path":"2017/06/18/循环队列的相关条件和公式/","text":"循环队列的相关条件和公式：队尾指针是rear,队头是front，其中QueueSize为循环队列的最大长度 队空条件：rear==front 队满条件：(rear+1) %QueueSIze==front 计算队列长度：（rear-front+QueueSize）%QueueSize 入队：（rear+1）%QueueSize 出队：（front+1）%QueueSize 队列中元素的个数： (rear-front+QueueSize)%QueueSize","tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/数据结构/"},{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/算法/"},{"name":"循环队列","slug":"循环队列","permalink":"http://yoursite.com/tags/循环队列/"}]},{"title":"Bootstrap入门需掌握的知识点（一）","date":"2017-06-17T16:00:00.000Z","path":"2017/06/18/Bootstrap入门需掌握的知识点（一）/","text":"BootstrapBootstrap中文网：http://www.bootcss.com/ 1.什么是 Bootstrap？ 官方介绍：简洁、直观、强悍的前端开发框架，让web开发更迅速、简单。 Bootstrap 下载 Bootstrap3下载地址：http://v3.bootcss.com/getting-started/#download Bootstrap 文件目录结构 1234567891011121314151617dist -css -bootstrap.css -bootstrap.css.map -bootstrap.min.css（常用） -bootstrap-theme.css -bootstrap-theme.css.map -bootstrap-theme.min.css -fonts -glyphicons-halflings-regular.eot -glyphicons-halflings-regular.svg -glyphicons-halflings-regular.ttf -glyphicons-halflings-regular.woff -js -bootstrap.js -bootstrap.min.js（常用） -npm.js Bootstrap 依赖 要想使用 Bootstrap ，那么必须先引入 jQuery（jquery.min.js）文件。 5.使用 Bootstrap 压缩版本适于实际应用，未压缩版本适于开发调试过程 直接引用官网下载下来的文件。 使用 Bootstrap 中文网提供的免费 CDN 服务。 1234567891011&lt;!-- 新 Bootstrap 核心 CSS 文件 --&gt;&lt;link rel=\"stylesheet\" href=\"http://cdn.bootcss.com/bootstrap/3.3.0/css/bootstrap.min.css\"&gt;&lt;!-- 可选的Bootstrap主题文件（一般不用引入） --&gt;&lt;link rel=\"stylesheet\" href=\"http://cdn.bootcss.com/bootstrap/3.3.0/css/bootstrap-theme.min.css\"&gt;&lt;!-- jQuery文件。务必在bootstrap.min.js 之前引入 --&gt;&lt;script src=\"http://cdn.bootcss.com/jquery/1.11.1/jquery.min.js\"&gt;&lt;/script&gt;&lt;!-- 最新的 Bootstrap 核心 JavaScript 文件 --&gt;&lt;script src=\"http://cdn.bootcss.com/bootstrap/3.3.0/js/bootstrap.min.js\"&gt;&lt;/script&gt; Bootstrap 基本模板 123456789101112131415161718192021222324252627&lt;!DOCTYPE html&gt;&lt;html lang=\"zh-cn\"&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"&gt; &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt; &lt;title&gt;Bootstrap 基本模板&lt;/title&gt; &lt;!-- Bootstrap --&gt; &lt;link href=\"css/bootstrap.min.css\" rel=\"stylesheet\"&gt; &lt;!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries --&gt; &lt;!-- WARNING: Respond.js doesn't work if you view the page via file:// --&gt; &lt;!--[if lt IE 9]&gt; &lt;script src=\"http://cdn.bootcss.com/html5shiv/3.7.2/html5shiv.min.js\"&gt;&lt;/script&gt; &lt;script src=\"http://cdn.bootcss.com/respond.js/1.4.2/respond.min.js\"&gt;&lt;/script&gt; &lt;![endif]--&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;你好，世界！&lt;/h1&gt; &lt;!-- jQuery (necessary for Bootstrap's JavaScript plugins) --&gt; &lt;script src=\"http://cdn.bootcss.com/jquery/1.11.1/jquery.min.js\"&gt;&lt;/script&gt; &lt;!-- Include all compiled plugins (below), or include individual files as needed --&gt; &lt;script src=\"js/bootstrap.min.js\"&gt;&lt;/script&gt; &lt;/body&gt;&lt;/html&gt; Bootstrap 实例精选：http://v3.bootcss.com/getting-started/#examples 全局 CSS 样式HTML5 文档类型Bootstrap 使用到的某些 HTML 元素和 CSS 属性需要将页面设置为 HTML5 文档类型。 1234&lt;!DOCTYPE html&gt;&lt;html lang=\"zh-CN\"&gt; ...&lt;/html&gt; 移动设备优先在 bootstrap3 中移动设备优先考虑的。为了保证适当的绘制和触屏缩放，需要在&lt;head&gt;之中添加 viewport 元数据标签。 1&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt; 在移动设备浏览器上，可以通过视口 viewport 设置meta属性为user-scalable=no可以禁用其缩放（zooming）功能，这样后用户只能滚动屏幕。（看情况而定） 1&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1, maximum-scala=1, user-scalable=no\"&gt; 排版与链接Bootstrap 排版、链接样式设置了基本的全局样式。分别是： 为 body 元素设置 background-color: #fff; 使用 @font-family-base、@font-size-base 和 @line-height-base a变量作为排版的基本参数 为所有链接设置了基本颜色 @link-color ，并且当链接处于 :hover 状态时才添加下划线 这些样式都能在 scaffolding.less 文件中找到对应的源码。 Normalize.css为了增强跨浏览器表现的一致性，bootstrap使用了 Normalize.css，这是由 Nicolas Gallagher 和 Jonathan Neal 维护的一个CSS 重置样式库。 布局容器Bootstrap 需要为页面内容和栅格系统包裹一个 .container 容器。Bootstrap提供了两个作此用处的类。注意，由于 padding等属性的原因，这两种容器类不能互相嵌套。 .container 类用于固定宽度并支持响应式布局的容器。 123&lt;div class=\"container\"&gt; ...&lt;/div&gt; .container-fluid 类用于 100% 宽度，占据全部视口（viewport）的容器。 123&lt;div class=\"container-fluid\"&gt; ...&lt;/div&gt; 栅格系统Bootstrap 提供了一套响应式、移动设备优先的流式栅格系统，随着屏幕或视口（viewport）尺寸的增加，系统会自动分为最多12列。它包含了易于使用的预定义类，还有强大的mixin 用于生成更具语义的布局。 简介栅格系统用于通过一系列的行（row）与列（column）的组合来创建页面布局，你的内容就可以放入这些创建好的布局中。下面就介绍一下 Bootstrap 栅格系统的工作原理： “行（row）”必须包含在 .container （固定宽度）或 .container-fluid （100% 宽度）中，以便为其赋予合适的排列（aligment）和内补（padding）。 通过“行（row）”在水平方向创建一组“列（column）”。 你的内容应当放置于“列（column）”内，并且，只有“列（column）”可以作为行（row）”的直接子元素。 类似 .row 和 .col-xs-4 这种预定义的类，可以用来快速创建栅格布局。Bootstrap 源码中定义的 mixin 也可以用来创建语义化的布局。 通过为“列（column）”设置 padding 属性，从而创建列与列之间的间隔（gutter）。通过为 .row 元素设置负值margin 从而抵消掉为 .container 元素设置的 padding，也就间接为“行（row）”所包含的“列（column）”抵消掉了padding。 The negative margin is why the examples below are outdented. It’s so that content within grid columns is lined up with non-grid content. Grid columns are created by specifying the number of twelve available columns you wish to span. For example, three equal columns would use three .col-xs-4. 如果一“行（row）”中包含了的“列（column）”大于 12，多余的“列（column）”所在的元素将被作为一个整体另起一行排列。 Grid classes apply to devices with screen widths greater than or equal to the breakpoint sizes, and override grid classes targeted at smaller devices. Therefore, applying any .col-md- class to an element will not only affect its styling on medium devices but also on large devices if a .col-lg- class is not present. 通过研究后面的实例，可以将这些原理应用到你的代码中。 媒体查询在栅格系统中，我们在 Less 文件中使用以下媒体查询（media query）来创建关键的分界点阈值。 1234567891011/* 超小屏幕（手机，小于 768px） *//* 没有任何媒体查询相关的代码，因为这在 Bootstrap 中是默认的（还记得 Bootstrap 是移动设备优先的吗？） *//* 小屏幕（平板，大于等于 768px） */@media (min-width: @screen-sm-min) &#123; ... &#125;/* 中等屏幕（桌面显示器，大于等于 992px） */@media (min-width: @screen-md-min) &#123; ... &#125;/* 大屏幕（大桌面显示器，大于等于 1200px） */@media (min-width: @screen-lg-min) &#123; ... &#125; 偶尔也会在媒体查询代码中包含 max-width 从而将 CSS 的影响限制在更小范围的屏幕大小之内 1234@media (max-width: @screen-xs-max) &#123; ... &#125;@media (min-width: @screen-sm-min) and (max-width: @screen-sm-max) &#123; ... &#125;@media (min-width: @screen-md-min) and (max-width: @screen-md-max) &#123; ... &#125;@media (min-width: @screen-lg-min) &#123; ... &#125; 栅格参数通过下表可以详细查看 Bootstrap 的栅格系统是如何在多种屏幕设备上工作的。 超小屏幕 手机 (&lt;768px) 小屏幕 平板 (≥768px) 中等屏幕 桌面显示器 (≥992px) 大屏幕 大桌面显示器 (≥1200px) 栅格系统行为 总是水平排列 开始是堆叠在一起的，当大于这些阈值时将变为水平排列C 同左 同左 .container 最大宽度 None （自动） 750px 970px 1170px 类前缀 .col-xs- .col-sm- .col-md- .col-lg- 列（column）数 12 12 12 12 最大列（column）宽 自动 ~62px ~81px ~97px 槽（gutter）宽 30px （每列左右均有 15px） 同左 同左 同左 可嵌套 是 是 是 是 偏移（Offsets） 是 是 是 是 列排序 是 是 是 是 实例：从堆叠到水平排列使用单一的一组 .col-md-* 栅格类，就可以创建一个基本的栅格系统，在手机和平板设备上一开始是堆叠在一起的（超小屏幕到小屏幕这一范围），在桌面（中等）屏幕设备上变为水平排列。所有“列（column）必须放在 ” .row 内。 123456789101112131415161718192021222324252627&lt;div class=\"row\"&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt;&lt;/div&gt;&lt;div class=\"row\"&gt; &lt;div class=\"col-md-8\"&gt;.col-md-8&lt;/div&gt; &lt;div class=\"col-md-4\"&gt;.col-md-4&lt;/div&gt;&lt;/div&gt;&lt;div class=\"row\"&gt; &lt;div class=\"col-md-4\"&gt;.col-md-4&lt;/div&gt; &lt;div class=\"col-md-4\"&gt;.col-md-4&lt;/div&gt; &lt;div class=\"col-md-4\"&gt;.col-md-4&lt;/div&gt;&lt;/div&gt;&lt;div class=\"row\"&gt; &lt;div class=\"col-md-6\"&gt;.col-md-6&lt;/div&gt; &lt;div class=\"col-md-6\"&gt;.col-md-6&lt;/div&gt;&lt;/div&gt; 实例：移动设备和桌面屏幕是否不希望在小屏幕设备上所有列都堆叠在一起？那就使用针对超小屏幕和中等屏幕设备所定义的类吧，即 .col-xs-*和 .col-md-*。请看下面的实例，研究一下这些是如何工作的。 123456789101112131415161718&lt;!-- Stack the columns on mobile by making one full-width and the other half-width --&gt;&lt;div class=\"row\"&gt; &lt;div class=\"col-xs-12 col-md-8\"&gt;.col-xs-12 .col-md-8&lt;/div&gt; &lt;div class=\"col-xs-6 col-md-4\"&gt;.col-xs-6 .col-md-4&lt;/div&gt;&lt;/div&gt;&lt;!-- Columns start at 50% wide on mobile and bump up to 33.3% wide on desktop --&gt;&lt;div class=\"row\"&gt; &lt;div class=\"col-xs-6 col-md-4\"&gt;.col-xs-6 .col-md-4&lt;/div&gt; &lt;div class=\"col-xs-6 col-md-4\"&gt;.col-xs-6 .col-md-4&lt;/div&gt; &lt;div class=\"col-xs-6 col-md-4\"&gt;.col-xs-6 .col-md-4&lt;/div&gt;&lt;/div&gt;&lt;!-- Columns are always 50% wide, on mobile and desktop --&gt;&lt;div class=\"row\"&gt; &lt;div class=\"col-xs-6\"&gt;.col-xs-6&lt;/div&gt; &lt;div class=\"col-xs-6\"&gt;.col-xs-6&lt;/div&gt;&lt;/div&gt; 排版标题HTML 中的所有标题标签，到 均可使用。另外，还提供了 .h1 到 .h6 类，为的是给内联（inline）属性的文本赋予标题的样式 123456&lt;h1&gt;h1. Bootstrap heading&lt;/h1&gt;&lt;h2&gt;h2. Bootstrap heading&lt;/h2&gt;&lt;h3&gt;h3. Bootstrap heading&lt;/h3&gt;&lt;h4&gt;h4. Bootstrap heading&lt;/h4&gt;&lt;h5&gt;h5. Bootstrap heading&lt;/h5&gt;&lt;h6&gt;h6. Bootstrap heading&lt;/h6&gt; 在标题内还可以包含 &lt;small&gt; 标签或赋予 .small 类的元素，可以用来标记副标题。 123456&lt;h1&gt;h1. Bootstrap heading &lt;small&gt;Secondary text&lt;/small&gt;&lt;/h1&gt;&lt;h2&gt;h2. Bootstrap heading &lt;small&gt;Secondary text&lt;/small&gt;&lt;/h2&gt;&lt;h3&gt;h3. Bootstrap heading &lt;small&gt;Secondary text&lt;/small&gt;&lt;/h3&gt;&lt;h4&gt;h4. Bootstrap heading &lt;small&gt;Secondary text&lt;/small&gt;&lt;/h4&gt;&lt;h5&gt;h5. Bootstrap heading &lt;small&gt;Secondary text&lt;/small&gt;&lt;/h5&gt;&lt;h6&gt;h6. Bootstrap heading &lt;small&gt;Secondary text&lt;/small&gt;&lt;/h6&gt; 页面主体Bootstrap 将全局 font-size 设置为 14px，line-height 设置为 1.428。这些属性直接赋予 元素和所有段落元素。另外， （段落）元素还被设置了等于 1/2 行高（即 10px）的底部外边距（margin）。 中心内容通过添加 .lead 类可以让段落突出显示。 1&lt;p class=\"lead\"&gt;...&lt;/p&gt; 使用 Less 工具构建variables.less 文件中定义的两个 Less 变量决定了排版尺寸：@font-size-base 和 @line-height-base。第一个变量定义了全局 font-size 基准，第二个变量是 line-height 基准。我们使用这些变量和一些简单的公式计算出其它所有页面元素的 margin、 padding 和 line-height。自定义这些变量即可改变 Bootstrap 的默认样式 内联文本元素标记文本为了高亮文本，可以使用 &lt;mark&gt; 标签 1You can use the mark tag to &lt;mark&gt;highlight&lt;/mark&gt; text. 被删除的文本对于被删除的文本，可以使用 &lt;del&gt; 标签。 1&lt;del&gt;This line of text is meant to be treated as deleted text.&lt;/del&gt; 无用文本对于无用文本可以使用 &lt;s&gt; 标签。 1&lt;s&gt;This line of text is meant to be treated as no longer accurate.&lt;/s&gt; 插入文本而外插入文本使用 &lt;ins&gt; 标签 1&lt;ins&gt;This line of text is meant to be treated as an addition to the document.&lt;/ins&gt; 带下划线的文本为文本添加下划线，使用 &lt;u&gt; 标签。 1&lt;u&gt;This line of text will render as underlined&lt;/u&gt; 小号文本使用标签 &lt;small&gt; 着重强调使用标签 &lt;strong&gt; 标签 斜体使用 &lt;em&gt; 标签 文本对齐 12345&lt;p class=\"text-left\"&gt;Left aligned text.&lt;/p&gt;&lt;p class=\"text-center\"&gt;Center aligned text.&lt;/p&gt;&lt;p class=\"text-right\"&gt;Right aligned text.&lt;/p&gt;&lt;p class=\"text-justify\"&gt;Justified text.&lt;/p&gt;&lt;p class=\"text-nowrap\"&gt;No wrap text.&lt;/p&gt; 改变大小写 123&lt;p class=\"text-lowercase\"&gt;Lowercased text.&lt;/p&gt;&lt;p class=\"text-uppercase\"&gt;Uppercased text.&lt;/p&gt;&lt;p class=\"text-capitalize\"&gt;Capitalized text.&lt;/p&gt; 引用在你的文档中引用其他的来源，可以使用 &lt;blockquote&gt; 来表示引用样式。对于直接引用，建议使用 &lt;p&gt; 标签。 123&lt;blockquote&gt; &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer posuere erat a ante.&lt;/p&gt;&lt;/blockquote&gt; 列表无序列表排列顺序无关紧要的一列元素。 123&lt;ul&gt; &lt;li&gt;...&lt;/li&gt;&lt;/ul&gt; 有序列表顺序至关重要的一组元素 123&lt;ol&gt; &lt;li&gt;...&lt;/li&gt;&lt;/ol&gt; 代码内联代码1For example, &lt;code&gt;&amp;lt;section&amp;gt;&lt;/code&gt; should be wrapped as inline. 用户输入通过 kbd 标签标记用户通过键盘输入的内容。 12To switch directories, type &lt;kbd&gt;cd&lt;/kbd&gt; followed by the name of the directory.&lt;br&gt;To edit settings, press &lt;kbd&gt;&lt;kbd&gt;ctrl&lt;/kbd&gt; + &lt;kbd&gt;,&lt;/kbd&gt;&lt;/kbd&gt; 代码块多行代码可以使用 &lt;pre&gt; 标签。为了正确的展示代码，注意将尖括号做转义处理。 变量通过 &lt;var&gt; 标签标记变量 程序输出通过 &lt;samp&gt; 标签来标记程序输出的内容 期待后面的文章！","tags":[{"name":"前端","slug":"前端","permalink":"http://yoursite.com/tags/前端/"},{"name":"Bootstrap","slug":"Bootstrap","permalink":"http://yoursite.com/tags/Bootstrap/"}]},{"title":"Bootstrap入门需掌握的知识点（二）","date":"2017-06-17T16:00:00.000Z","path":"2017/06/18/Bootstrap入门需掌握的知识点（二）/","text":"相关阅读：Bootstrap入门需掌握的知识点（一）表格基本实例为任意 &lt;table&gt; 标签添加 .table 类可以为其赋予基本的样式 — 少量的内补（padding）和水平方向的分隔线。 123&lt;table class=&quot;table&quot;&gt; ...&lt;/table&gt; 条纹状表格通过 .table-striped 类可以给 &lt;tbody&gt; 之内的每一行增加斑马条纹样式。 123&lt;table class=\"table table-striped\"&gt; ...&lt;/table&gt; 带边框的表格添加 .table-bordered 类为表格和其中的每个单元格增加边框。 123&lt;table class=\"table table-bordered\"&gt; ...&lt;/table&gt; 鼠标悬停通过添加 .table-hover 类可以让 中的每一行对鼠标悬停状态作出响应。 123&lt;table class=\"table table-hover\"&gt; ...&lt;/table&gt; 状态类通过这些状态类可以为行或单元格设置颜色。 Class 描述 .active 鼠标悬停在行或单元格上时所设置的颜色 .success 标识成功或积极的动作 .info 标识普通的提示信息或动作 .warning 标识警告或需要用户注意 .danger 标识危险或潜在的带来负面影响的动作 123456789101112131415&lt;!-- On rows --&gt;&lt;tr class=\"active\"&gt;...&lt;/tr&gt;&lt;tr class=\"success\"&gt;...&lt;/tr&gt;&lt;tr class=\"warning\"&gt;...&lt;/tr&gt;&lt;tr class=\"danger\"&gt;...&lt;/tr&gt;&lt;tr class=\"info\"&gt;...&lt;/tr&gt;&lt;!-- On cells (`td` or `th`) --&gt;&lt;tr&gt; &lt;td class=\"active\"&gt;...&lt;/td&gt; &lt;td class=\"success\"&gt;...&lt;/td&gt; &lt;td class=\"warning\"&gt;...&lt;/td&gt; &lt;td class=\"danger\"&gt;...&lt;/td&gt; &lt;td class=\"info\"&gt;...&lt;/td&gt;&lt;/tr&gt; 响应式表格将任何 .table 元素包裹在 .table-responsive 元素内，即可创建响应式表格，其会在小屏幕设备上（小于768px）水平滚动。当屏幕大于 768px 宽度时，水平滚动条消失。 12345&lt;div class=\"table-responsive\"&gt; &lt;table class=\"table\"&gt; ... &lt;/table&gt;&lt;/div&gt; 表单基本实例单独的表单控件会被自动赋予一些全局样式。所有设置了 .form-control 类的 &lt;input&gt;、&lt;textarea&gt; 和 &lt;select&gt; 元素都将被默认设置宽度属性为 width: 100%;。 将 label 元素和前面提到的控件包裹在 .form-group 中可以获得最好的排列。 123456789101112131415161718192021&lt;form role=\"form\"&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"exampleInputEmail1\"&gt;Email address&lt;/label&gt; &lt;input type=\"email\" class=\"form-control\" id=\"exampleInputEmail1\" placeholder=\"Enter email\"&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"exampleInputPassword1\"&gt;Password&lt;/label&gt; &lt;input type=\"password\" class=\"form-control\" id=\"exampleInputPassword1\" placeholder=\"Password\"&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"exampleInputFile\"&gt;File input&lt;/label&gt; &lt;input type=\"file\" id=\"exampleInputFile\"&gt; &lt;p class=\"help-block\"&gt;Example block-level help text here.&lt;/p&gt; &lt;/div&gt; &lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\"&gt; Check me out &lt;/label&gt; &lt;/div&gt; &lt;button type=\"submit\" class=\"btn btn-default\"&gt;Submit&lt;/button&gt;&lt;/form&gt; 注意：不要将表单组直接和输入框组混合使用，建议将输入框组嵌套到表单组中使用。 内联表单为 &lt;form&gt; 元素添加 .form-inline 类可使其内容左对齐并且表现为 inline-block 级别的控件。只适用于视口（viewport）至少在 768px 宽度时（视口宽度再小的话就会使表单折叠）。 注意： 需要手动设置宽度在 Bootstrap 中，输入框和单选/多选框控件默认被设置为 width: 100%; 宽度。在内联表单，我们将这些元素的宽度设置为 width: auto;，因此，多个控件可以排列在同一行。根据你的布局需求，可能需要一些额外的定制化组件。 一定要添加 label 标签如果你没有为每个输入控件设置 label 标签，屏幕阅读器将无法正确识别。对于这些内联表单，你可以通过为label 设置 .sr-only 类将其隐藏。 12345678910111213141516171819202122&lt;form class=\"form-inline\" role=\"form\"&gt; &lt;div class=\"form-group\"&gt; &lt;label class=\"sr-only\" for=\"exampleInputEmail2\"&gt;Email address&lt;/label&gt; &lt;input type=\"email\" class=\"form-control\" id=\"exampleInputEmail2\" placeholder=\"Enter email\"&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;div class=\"input-group\"&gt; &lt;div class=\"input-group-addon\"&gt;@&lt;/div&gt; &lt;input class=\"form-control\" type=\"email\" placeholder=\"Enter email\"&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label class=\"sr-only\" for=\"exampleInputPassword2\"&gt;Password&lt;/label&gt; &lt;input type=\"password\" class=\"form-control\" id=\"exampleInputPassword2\" placeholder=\"Password\"&gt; &lt;/div&gt; &lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\"&gt; Remember me &lt;/label&gt; &lt;/div&gt; &lt;button type=\"submit\" class=\"btn btn-default\"&gt;Sign in&lt;/button&gt;&lt;/form&gt; 水平排列的表单通过为表单添加 .form-horizontal 类，并联合使用 Bootstrap 预置的栅格类，可以将 label 标签和控件组水平并排布局。这样做将改变 .form-group 的行为，使其表现为栅格系统中的行（row），因此就无需再额外添加 .row 了。 12345678910111213141516171819202122232425262728&lt;form class=\"form-horizontal\" role=\"form\"&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"inputEmail3\" class=\"col-sm-2 control-label\"&gt;Email&lt;/label&gt; &lt;div class=\"col-sm-10\"&gt; &lt;input type=\"email\" class=\"form-control\" id=\"inputEmail3\" placeholder=\"Email\"&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"inputPassword3\" class=\"col-sm-2 control-label\"&gt;Password&lt;/label&gt; &lt;div class=\"col-sm-10\"&gt; &lt;input type=\"password\" class=\"form-control\" id=\"inputPassword3\" placeholder=\"Password\"&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;div class=\"col-sm-offset-2 col-sm-10\"&gt; &lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\"&gt; Remember me &lt;/label&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;div class=\"col-sm-offset-2 col-sm-10\"&gt; &lt;button type=\"submit\" class=\"btn btn-default\"&gt;Sign in&lt;/button&gt; &lt;/div&gt; &lt;/div&gt;&lt;/form&gt; 被支持的控件表单布局实例中展示了其所支持的标准表单控件。 1、输入框包括大部分表单控件、文本输入域控件，还支持所有 HTML5 类型的输入控件：text、password、datetime、datetime-local、date、month、time、week、number、email、url、search、tel 和 color。 1&lt;input type=\"text\" class=\"form-control\" placeholder=\"Text input\"&gt; 如需在文本输入域 &lt;input&gt; 前面或后面添加文本内容或按钮控件，请参考输入控件组。 2、文本域支持多行文本的表单控件。可根据需要改变 rows 属性。 1&lt;textarea class=\"form-control\" rows=\"3\"&gt;&lt;/textarea&gt; 3、多选和单选框多选框（checkbox）用于选择列表中的一个或多个选项，而单选框（radio）用于从多个选项中只选择一个。 设置了 disabled 属性的单选或多选框都能被赋予合适的样式。对于和多选或单选框联合使用的 &lt;label&gt; 标签，如果也希望将悬停于上方的鼠标设置为“禁止点击”的样式，请将 .disabled 类赋予 .radio、.radio-inline、.checkbox 、.checkbox-inline 或 &lt;fieldset&gt;。 12345678910111213141516171819202122232425262728293031&lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\" value=\"\"&gt; Option one is this and that&amp;mdash;be sure to include why it's great &lt;/label&gt;&lt;/div&gt;&lt;div class=\"checkbox disabled\"&gt; &lt;label&gt; &lt;input type=\"checkbox\" value=\"\" disabled&gt; Option two is disabled &lt;/label&gt;&lt;/div&gt;&lt;div class=\"radio\"&gt; &lt;label&gt; &lt;input type=\"radio\" name=\"optionsRadios\" id=\"optionsRadios1\" value=\"option1\" checked&gt; Option one is this and that&amp;mdash;be sure to include why it's great &lt;/label&gt;&lt;/div&gt;&lt;div class=\"radio\"&gt; &lt;label&gt; &lt;input type=\"radio\" name=\"optionsRadios\" id=\"optionsRadios2\" value=\"option2\"&gt; Option two can be something else and selecting it will deselect option one &lt;/label&gt;&lt;/div&gt;&lt;div class=\"radio disabled\"&gt; &lt;label&gt; &lt;input type=\"radio\" name=\"optionsRadios\" id=\"optionsRadios3\" value=\"option3\" disabled&gt; Option three is disabled &lt;/label&gt;&lt;/div&gt; 内联单选和多选框通过将 .checkbox-inline 或 .radio-inline 类应用到一系列的多选框（checkbox）或单选框（radio）控件上，可以使这些控件排列在一行。 12345678910111213141516171819&lt;label class=\"checkbox-inline\"&gt; &lt;input type=\"checkbox\" id=\"inlineCheckbox1\" value=\"option1\"&gt; 1&lt;/label&gt;&lt;label class=\"checkbox-inline\"&gt; &lt;input type=\"checkbox\" id=\"inlineCheckbox2\" value=\"option2\"&gt; 2&lt;/label&gt;&lt;label class=\"checkbox-inline\"&gt; &lt;input type=\"checkbox\" id=\"inlineCheckbox3\" value=\"option3\"&gt; 3&lt;/label&gt;&lt;label class=\"radio-inline\"&gt; &lt;input type=\"radio\" name=\"inlineRadioOptions\" id=\"inlineRadio1\" value=\"option1\"&gt; 1&lt;/label&gt;&lt;label class=\"radio-inline\"&gt; &lt;input type=\"radio\" name=\"inlineRadioOptions\" id=\"inlineRadio2\" value=\"option2\"&gt; 2&lt;/label&gt;&lt;label class=\"radio-inline\"&gt; &lt;input type=\"radio\" name=\"inlineRadioOptions\" id=\"inlineRadio3\" value=\"option3\"&gt; 3&lt;/label&gt; 复选框 12345678910&lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\" id=\"blankCheckbox\" value=\"option1\"&gt; &lt;/label&gt;&lt;/div&gt;&lt;div class=\"radio\"&gt; &lt;label&gt; &lt;input type=\"radio\" name=\"blankRadio\" id=\"blankRadio1\" value=\"option1\"&gt; &lt;/label&gt;&lt;/div&gt; 下拉列表（select）使用默认选项或添加 multiple 属性可以同时显示多个选项。 123456789101112131415&lt;select class=\"form-control\"&gt; &lt;option&gt;1&lt;/option&gt; &lt;option&gt;2&lt;/option&gt; &lt;option&gt;3&lt;/option&gt; &lt;option&gt;4&lt;/option&gt; &lt;option&gt;5&lt;/option&gt;&lt;/select&gt;&lt;select multiple class=\"form-control\"&gt; &lt;option&gt;1&lt;/option&gt; &lt;option&gt;2&lt;/option&gt; &lt;option&gt;3&lt;/option&gt; &lt;option&gt;4&lt;/option&gt; &lt;option&gt;5&lt;/option&gt;&lt;/select&gt; 静态控件如果需要在表单中将一行纯文本和 label 元素放置于同一行，为 &lt;p&gt; 元素添加 .form-control-static 类即可。 1234567891011121314&lt;form class=\"form-horizontal\" role=\"form\"&gt; &lt;div class=\"form-group\"&gt; &lt;label class=\"col-sm-2 control-label\"&gt;Email&lt;/label&gt; &lt;div class=\"col-sm-10\"&gt; &lt;p class=\"form-control-static\"&gt;email@example.com&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"inputPassword\" class=\"col-sm-2 control-label\"&gt;Password&lt;/label&gt; &lt;div class=\"col-sm-10\"&gt; &lt;input type=\"password\" class=\"form-control\" id=\"inputPassword\" placeholder=\"Password\"&gt; &lt;/div&gt; &lt;/div&gt;&lt;/form&gt; 1234567891011&lt;form class=\"form-inline\" role=\"form\"&gt; &lt;div class=\"form-group\"&gt; &lt;label class=\"sr-only\"&gt;Email&lt;/label&gt; &lt;p class=\"form-control-static\"&gt;email@example.com&lt;/p&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"inputPassword2\" class=\"sr-only\"&gt;Password&lt;/label&gt; &lt;input type=\"password\" class=\"form-control\" id=\"inputPassword2\" placeholder=\"Password\"&gt; &lt;/div&gt; &lt;button type=\"submit\" class=\"btn btn-default\"&gt;Confirm identity&lt;/button&gt;&lt;/form&gt; 输入框焦点我们将某些表单控件的默认 outline 样式移除，然后对 :focus 状态赋予 box-shadow 属性。 被禁用的输入框为输入框设置 disabled 属性可以防止用户输入，并能对外观做一些修改，使其更直观。 1&lt;input class=\"form-control\" id=\"disabledInput\" type=\"text\" placeholder=\"Disabled input here...\" disabled&gt; 1234567891011121314151617181920&lt;form role=\"form\"&gt; &lt;fieldset disabled&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"disabledTextInput\"&gt;Disabled input&lt;/label&gt; &lt;input type=\"text\" id=\"disabledTextInput\" class=\"form-control\" placeholder=\"Disabled input\"&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"disabledSelect\"&gt;Disabled select menu&lt;/label&gt; &lt;select id=\"disabledSelect\" class=\"form-control\"&gt; &lt;option&gt;Disabled select&lt;/option&gt; &lt;/select&gt; &lt;/div&gt; &lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\"&gt; Can't check this &lt;/label&gt; &lt;/div&gt; &lt;button type=\"submit\" class=\"btn btn-primary\"&gt;Submit&lt;/button&gt; &lt;/fieldset&gt;&lt;/form&gt; 只读输入框为输入框设置 readonly 属性可以禁止用户输入，并且输入框的样式也是禁用状态。 1&lt;input class=\"form-control\" type=\"text\" placeholder=\"Readonly input here…\" readonly&gt; 校验状态Bootstrap 对表单控件的校验状态，如 error、warning 和 success 状态，都定义了样式。使用时，添加 .has-warning、.has-error 或 .has-success 类到这些控件的父元素即可。任何包含在此元素之内的 .control-label、.form-control 和 .help-block 元素都将接受这些校验状态的样式。 123456789101112131415161718192021222324252627282930313233343536&lt;div class=\"form-group has-success\"&gt; &lt;label class=\"control-label\" for=\"inputSuccess1\"&gt;Input with success&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" id=\"inputSuccess1\"&gt;&lt;/div&gt;&lt;div class=\"form-group has-warning\"&gt; &lt;label class=\"control-label\" for=\"inputWarning1\"&gt;Input with warning&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" id=\"inputWarning1\"&gt;&lt;/div&gt;&lt;div class=\"form-group has-error\"&gt; &lt;label class=\"control-label\" for=\"inputError1\"&gt;Input with error&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" id=\"inputError1\"&gt;&lt;/div&gt;&lt;div class=\"has-success\"&gt; &lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\" id=\"checkboxSuccess\" value=\"option1\"&gt; Checkbox with success &lt;/label&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class=\"has-warning\"&gt; &lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\" id=\"checkboxWarning\" value=\"option1\"&gt; Checkbox with warning &lt;/label&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class=\"has-error\"&gt; &lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\" id=\"checkboxError\" value=\"option1\"&gt; Checkbox with error &lt;/label&gt; &lt;/div&gt;&lt;/div&gt; 添加额外的图标你还可以针对校验状态为输入框添加额外的图标。只需设置相应的 .has-feedback 类并添加正确的图标即可。 Feedback icons only work with textual &lt;input class=&quot;form-control&quot;&gt; elements. 图标、label 和输入控件组对于不带有 label 标签的输入框以及右侧带有附加组件的输入框组，需要手动为其图标定位。为了让所有用户都能访问你的网站，我们强烈建议为所有输入框添加 label 标签。如果你不希望将 label 标签展示出来，可以通过添加 sr-only 类来实现。如果的确不能添加 label 标签，请调整图标的 top 值。对于输入框组，请根据你的实际情况调整 right 值。 123456789101112131415&lt;div class=\"form-group has-success has-feedback\"&gt; &lt;label class=\"control-label\" for=\"inputSuccess2\"&gt;Input with success&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" id=\"inputSuccess2\"&gt; &lt;span class=\"glyphicon glyphicon-ok form-control-feedback\"&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=\"form-group has-warning has-feedback\"&gt; &lt;label class=\"control-label\" for=\"inputWarning2\"&gt;Input with warning&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" id=\"inputWarning2\"&gt; &lt;span class=\"glyphicon glyphicon-warning-sign form-control-feedback\"&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=\"form-group has-error has-feedback\"&gt; &lt;label class=\"control-label\" for=\"inputError2\"&gt;Input with error&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" id=\"inputError2\"&gt; &lt;span class=\"glyphicon glyphicon-remove form-control-feedback\"&gt;&lt;/span&gt;&lt;/div&gt; 控件尺寸通过 .input-lg 类似的类可以为控件设置高度，通过 .col-lg-* 类似的类可以为控件设置宽度。 高度尺寸创建大一些或小一些的表单控件以匹配按钮尺寸。 1234567&lt;input class=\"form-control input-lg\" type=\"text\" placeholder=\".input-lg\"&gt;&lt;input class=\"form-control\" type=\"text\" placeholder=\"Default input\"&gt;&lt;input class=\"form-control input-sm\" type=\"text\" placeholder=\".input-sm\"&gt;&lt;select class=\"form-control input-lg\"&gt;...&lt;/select&gt;&lt;select class=\"form-control\"&gt;...&lt;/select&gt;&lt;select class=\"form-control input-sm\"&gt;...&lt;/select&gt; 按钮预定义样式使用下面列出的类可以快速创建一个带有预定义样式的按钮。 1234567891011121314151617181920&lt;!-- Standard button --&gt;&lt;button type=\"button\" class=\"btn btn-default\"&gt;Default&lt;/button&gt;&lt;!-- Provides extra visual weight and identifies the primary action in a set of buttons --&gt;&lt;button type=\"button\" class=\"btn btn-primary\"&gt;Primary&lt;/button&gt;&lt;!-- Indicates a successful or positive action --&gt;&lt;button type=\"button\" class=\"btn btn-success\"&gt;Success&lt;/button&gt;&lt;!-- Contextual button for informational alert messages --&gt;&lt;button type=\"button\" class=\"btn btn-info\"&gt;Info&lt;/button&gt;&lt;!-- Indicates caution should be taken with this action --&gt;&lt;button type=\"button\" class=\"btn btn-warning\"&gt;Warning&lt;/button&gt;&lt;!-- Indicates a dangerous or potentially negative action --&gt;&lt;button type=\"button\" class=\"btn btn-danger\"&gt;Danger&lt;/button&gt;&lt;!-- Deemphasize a button by making it look like a link while maintaining button behavior --&gt;&lt;button type=\"button\" class=\"btn btn-link\"&gt;Link&lt;/button&gt; 尺寸需要让按钮具有不同尺寸吗？使用 .btn-lg、.btn-sm 或 .btn-xs 可以获得不同尺寸的按钮。 12345678910111213141516&lt;p&gt; &lt;button type=\"button\" class=\"btn btn-primary btn-lg\"&gt;Large button&lt;/button&gt; &lt;button type=\"button\" class=\"btn btn-default btn-lg\"&gt;Large button&lt;/button&gt;&lt;/p&gt;&lt;p&gt; &lt;button type=\"button\" class=\"btn btn-primary\"&gt;Default button&lt;/button&gt; &lt;button type=\"button\" class=\"btn btn-default\"&gt;Default button&lt;/button&gt;&lt;/p&gt;&lt;p&gt; &lt;button type=\"button\" class=\"btn btn-primary btn-sm\"&gt;Small button&lt;/button&gt; &lt;button type=\"button\" class=\"btn btn-default btn-sm\"&gt;Small button&lt;/button&gt;&lt;/p&gt;&lt;p&gt; &lt;button type=\"button\" class=\"btn btn-primary btn-xs\"&gt;Extra small button&lt;/button&gt; &lt;button type=\"button\" class=\"btn btn-default btn-xs\"&gt;Extra small button&lt;/button&gt;&lt;/p&gt; 通过给按钮添加 .btn-block 类可以将其拉伸至父元素100%的宽度，而且按钮也变为了块级（block）元素。 12&lt;button type=\"button\" class=\"btn btn-primary btn-lg btn-block\"&gt;Block level button&lt;/button&gt;&lt;button type=\"button\" class=\"btn btn-default btn-lg btn-block\"&gt;Block level button&lt;/button&gt; 激活状态当按钮处于激活状态时，其表现为被按压下去（底色更深、边框夜色更深、向内投射阴影）。对于 &lt;button&gt; 元素，是通过 :active 状态实现的。对于 &lt;a&gt;元素，是通过 .active 类实现的。然而，你还可以将 .active 应用到 &lt;button&gt;上，并通过编程的方式使其处于激活状态。 button 元素由于 :active 是伪状态，因此无需额外添加，但是在需要让其表现出同样外观的时候可以添加 .active 类。 12&lt;button type=\"button\" class=\"btn btn-primary btn-lg active\"&gt;Primary button&lt;/button&gt;&lt;button type=\"button\" class=\"btn btn-default btn-lg active\"&gt;Button&lt;/button&gt; 链接（&lt;a&gt;）元素可以为基于 &lt;a&gt;元素创建的按钮添加 .active 类。 12&lt;a href=\"#\" class=\"btn btn-primary btn-lg active\" role=\"button\"&gt;Primary link&lt;/a&gt;&lt;a href=\"#\" class=\"btn btn-default btn-lg active\" role=\"button\"&gt;Link&lt;/a&gt; 按钮类为 &lt;a&gt;、&lt;button&gt; 或 &lt;input&gt; 元素应用按钮类 1234&lt;a class=\"btn btn-default\" href=\"#\" role=\"button\"&gt;Link&lt;/a&gt;&lt;button class=\"btn btn-default\" type=\"submit\"&gt;Button&lt;/button&gt;&lt;input class=\"btn btn-default\" type=\"button\" value=\"Input\"&gt;&lt;input class=\"btn btn-default\" type=\"submit\" value=\"Submit\"&gt; 图片响应式图片在 Bootstrap 版本 3 中，通过为图片添加 .img-responsive 类可以让图片支持响应式布局。其实质是为图片设置了 max-width: 100%; 和 height: auto; 属性，从而让图片在其父元素中更好的缩放。 1&lt;img src=\"...\" class=\"img-responsive\" alt=\"Responsive image\"&gt; 图片形状通过为 &lt;img&gt; 元素添加以下相应的类，可以让图片呈现不同的形状。 123&lt;img src=&quot;...&quot; alt=&quot;...&quot; class=&quot;img-rounded&quot;&gt;&lt;img src=&quot;...&quot; alt=&quot;...&quot; class=&quot;img-circle&quot;&gt;&lt;img src=&quot;...&quot; alt=&quot;...&quot; class=&quot;img-thumbnail&quot;&gt;","tags":[{"name":"前端","slug":"前端","permalink":"http://yoursite.com/tags/前端/"},{"name":"Bootstrap","slug":"Bootstrap","permalink":"http://yoursite.com/tags/Bootstrap/"}]},{"title":"搭建一个博客项目后的碎碎念","date":"2017-06-16T16:00:00.000Z","path":"2017/06/17/blog-talk/","text":"前言 以前大二的时候就想一个人独立做一个由 Java 开发的个人博客, 可耐当时还很弱鸡，一个人难以独挡一片，因为要会的东西太多，后来自己看到很多都是由 WordPress 搭建的博客，很多模板很漂亮，可是自己要稍微对 “拍黄片” 了解一点，并且里面的各种插件特特别的多。去年的时候就开始用上了 GitHub Page 搭建静态的博客，因为自己一直习惯用 Markdown 写作，写完后，软件可以直接生成 PDF 和 HTML 文件，这样就很方便了，直接将自己的 HTML、PDF 和 MD 文件一起 push 到 GitHub 上，然后自己在通过域名加上文章链接就可以直接访问我的博客了，这样就省了很多事了。还提供了 PDF 和 MD 版本，对有不同需求的人都可满足了。可是后来觉得这样的逼格还是不够高，就又开始折腾 Hexo 了，发现用 Hexo 也是很非常简单的（其实是看到 Hexo 的 yilia 主题非常漂亮）。于是就换上了 Hexo 了，自己在这上面写博客也很方便。每次用软件写完后，在 Git Bash 下敲一行命令 hexo d -g 就行了，很方便！前段时间看到了一款开源的博客（由 Java 搭建而成）—— Tale，主题比较简洁，符合程序员的范。也刚好符合自己最初的想法，但是我是没打算放弃现在的博客，就是有一个想法，自己也跟着在那个基础山修改下。（因为 Tale 使用的是轻量级 mvc 框架 Blade 开发，我好像不太了解这个框架呢），想着就 SpringBoot 开发比较快，上手也简单。当时就有这个想法，可怜没时间，不过前些天发现有人就是基于那个 Tale 博客重新修改了，用的就是 SpringBoot ，哇，果然是英雄所见略同。当时就和作者邮件联系了，于是蹭这些天的时间赶紧去看看，结果不只是看看，完全自己就全部敲了一遍，终于在今天搞定了，为了庆祝，才写下这篇文章，好好记录这些美好的时刻（博客可以完全发挥，不限题材）。通过自己深入这个项目，才能够很了解内部的实现方式，这点收获很大，这十天时间花的值，再此感谢两位原作者 ZHENFENG13 、otale 。 博客介绍Tale 使用了轻量级 mvc 框架 Blade 开发，默认主题使用了漂亮的 pinghsu 。 My-Blog 使用的是 Docker + SpringBoot + Mybatis + thymeleaf 打造的一个个人博客模板。 Blog 是自己花了十天的时间把整个项目的代码都敲了一遍，熟悉了整个项目，做了优化，去除了 Docker， 其中修改了原来的一些 bug，并在原作者的项目中提出了 issue ， 原作者已修复。 : 喜欢该项目的话，可以给项目点个 star，如果你想在这基础上修改，那么建议你 fork 该项目，然后再修改哦。 博客首页： 归档： 友链： 关于： 搜索： 后台管理 管理登录： 管理首页： 发布文章： 文章管理： 页面管理： 分类标签： 文件管理： 友链管理： 系统设置： 最后我什么我这么喜欢折腾博客呢，熟悉我的朋友都知道，我再很多平台都写过博客，有些是他们平台的运营人员邀请过去的。可是在这些平台上写博客终究是没有感觉，如今自己在自己的博客网站写文章，比较轻松，而且也符合我的写作风格。在其他的平台都有些大大小小的不适（对程序员来说应该是 bug），虽然目前还是会在这些平台继续发布我新写的文章，但是我保证最新的文章，首发肯定是我自己的博客网站，有些是不会在其他平台发的，有觉得不错的可以 RSS 订阅我的博客，或者是直接收藏网址下来。自从写博客下来遇到很多志同道合的人，这点正是让我觉得有写下去的必要了。自己将会坚持下去，时刻警醒自己：勿忘初心！最后的最后，还是想说一句：如果你想和我一样折腾博客，那么我建议你先在一家平台坚持写下去，等博客数量上来了，在自己折腾自己的博客网站。还有就是你想提高自己的话，还是需要很在意你的基础，然后就是要多练手几个项目，我自己在练手这个项目的时候就收获很多。","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"详解 Filter 过滤器","date":"2017-06-16T16:00:00.000Z","path":"2017/06/17/详解 Filter 过滤器/","text":"1、简介 Filter也称之为过滤器，它是Servlet技术中最实用的技术，WEB开发人员通过Filter技术，对web服务器管理的所有web资源：例如Jsp, Servlet, 静态图片文件或静态 html 文件等进行拦截，从而实现一些特殊的功能。例如实现URL级别的权限访问控制、过滤敏感词汇、压缩响应信息等一些高级功能。 它主要用于对用户请求进行预处理，也可以对HttpServletResponse 进行后处理。使用Filter 的完整流程：Filter 对用户请求进行预处理，接着将请求交给Servlet 进行处理并生成响应，最后Filter 再对服务器响应进行后处理。 Filter功能： 在HttpServletRequest 到达 Servlet 之前，拦截客户的 HttpServletRequest 。 根据需要检查 HttpServletRequest ，也可以修改HttpServletRequest 头和数据。 在HttpServletResponse 到达客户端之前，拦截HttpServletResponse 。 根据需要检查 HttpServletResponse ，也可以修改HttpServletResponse头和数据。 2、如何实现拦截 Filter接口中有一个doFilter方法，当开发人员编写好Filter，并配置对哪个web资源进行拦截后，WEB服务器每次在调用web资源的service方法之前，都会先调用一下filter的doFilter方法，因此，在该方法内编写代码可达到如下目的： 调用目标资源之前，让一段代码执行。 是否调用目标资源（即是否让用户访问web资源）。 web服务器在调用doFilter方法时，会传递一个filterChain对象进来，filterChain对象是filter接口中最重要的一个对象，它也提供了一个doFilter方法，开发人员可以根据需求决定是否调用此方法，调用该方法，则web服务器就会调用web资源的service方法，即web资源就会被访问，否则web资源不会被访问。 3、Filter开发两步走 编写java类实现Filter接口，并实现其doFilter方法。 在 web.xml 文件中使用和元素对编写的filter类进行注册，并设置它所能拦截的资源。 web.xml配置各节点介绍： 12345678910111213141516&lt;filter-name&gt;用于为过滤器指定一个名字，该元素的内容不能为空。&lt;filter-class&gt;元素用于指定过滤器的完整的限定类名。&lt;init-param&gt;元素用于为过滤器指定初始化参数，它的子元素&lt;param-name&gt;指定参数的名字，&lt;param-value&gt;指定参数的值。在过滤器中，可以使用FilterConfig接口对象来访问初始化参数。&lt;filter-mapping&gt;元素用于设置一个 Filter 所负责拦截的资源。一个Filter拦截的资源可通过两种方式来指定：Servlet 名称和资源访问的请求路径&lt;filter-name&gt;子元素用于设置filter的注册名称。该值必须是在&lt;filter&gt;元素中声明过的过滤器的名字&lt;url-pattern&gt;设置 filter 所拦截的请求路径(过滤器关联的URL样式)&lt;servlet-name&gt;指定过滤器所拦截的Servlet名称。&lt;dispatcher&gt;指定过滤器所拦截的资源被 Servlet 容器调用的方式，可以是REQUEST,INCLUDE,FORWARD和ERROR之一，默认REQUEST。用户可以设置多个&lt;dispatcher&gt; 子元素用来指定 Filter 对资源的多种调用方式进行拦截。&lt;dispatcher&gt; 子元素可以设置的值及其意义：REQUEST：当用户直接访问页面时，Web容器将会调用过滤器。如果目标资源是通过RequestDispatcher的include()或forward()方法访问时，那么该过滤器就不会被调用。INCLUDE：如果目标资源是通过RequestDispatcher的include()方法访问时，那么该过滤器将被调用。除此之外，该过滤器不会被调用。FORWARD：如果目标资源是通过RequestDispatcher的forward()方法访问时，那么该过滤器将被调用，除此之外，该过滤器不会被调用。ERROR：如果目标资源是通过声明式异常处理机制调用时，那么该过滤器将被调用。除此之外，过滤器不会被调用。 4、Filter链 在一个web应用中，可以开发编写多个Filter，这些Filter组合起来称之为一个Filter链。 web服务器根据Filter在web.xml文件中的注册顺序，决定先调用哪个Filter，当第一个Filter的doFilter方法被调用时，web服务器会创建一个代表Filter链的FilterChain对象传递给该方法。在doFilter方法中，开发人员如果调用了FilterChain对象的doFilter方法，则web服务器会检查FilterChain对象中是否还有filter，如果有，则调用第2个filter，如果没有，则调用目标资源。 多个过滤器执行顺序 一个目标资源可以指定多个过滤器，过滤器的执行顺序是在web.xml文件中的部署顺序： 12345678910111213141516&lt;filter&gt; &lt;filter-name&gt;myFilter1&lt;/filter-name&gt; &lt;filter-class&gt;cn.cloud.filter.MyFilter1&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;myFilter1&lt;/filter-name&gt; &lt;url-pattern&gt;/index.jsp&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;filter&gt; &lt;filter-name&gt;myFilter2&lt;/filter-name&gt; &lt;filter-class&gt;cn. cloud.filter.MyFilter2&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;myFilter2&lt;/filter-name&gt; &lt;url-pattern&gt;/index.jsp&lt;/url-pattern&gt; &lt;/filter-mapping&gt; MyFilter1 12345678public class MyFilter1 extends HttpFilter &#123; public void doFilter(HttpServletRequest request, HttpServletResponse response, FilterChain chain) throws IOException, ServletException &#123; System.out.println(\"filter1 start...\"); chain.doFilter(request, response);//放行，执行MyFilter2的doFilter()方法 System.out.println(\"filter1 end...\"); &#125;&#125; MyFilter2 12345678public class MyFilter2 extends HttpFilter &#123; public void doFilter(HttpServletRequest request, HttpServletResponse response, FilterChain chain) throws IOException, ServletException &#123; System.out.println(\"filter2 start...\"); chain.doFilter(request, response);//放行，执行目标资源 System.out.println(\"filter2 end...\"); &#125;&#125; 12345&lt;body&gt; This is my JSP page. &lt;br&gt; &lt;h1&gt;index.jsp&lt;/h1&gt; &lt;%System.out.println(\"index.jsp\"); %&gt; &lt;/body&gt; 当有用户访问index.jsp页面时，输出结果如下： 12345filter1 start...filter2 start...index.jspfilter2 end...filter1 end... 5、Filter的生命周期 1public void init(FilterConfig filterConfig) throws ServletException;//初始化 和我们编写的Servlet程序一样，Filter的创建和销毁由WEB服务器负责。 web 应用程序启动时，web 服务器将创建Filter 的实例对象，并调用其init方法，读取web.xml配置，完成对象的初始化功能，从而为后续的用户请求作好拦截的准备工作（filter对象只会创建一次，init方法也只会执行一次）。开发人员通过init方法的参数，可获得代表当前filter配置信息的FilterConfig对象。 1public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException;//拦截请求 这个方法完成实际的过滤操作。当客户请求访问与过滤器关联的URL（目标资源）的时候，Servlet过滤器将先执行doFilter方法。FilterChain参数用于访问后续过滤器。 1public void destroy();//销毁 服务器在创建Filter对象之后，把Filter放到缓存中一直使用（会驻留在内存），通常不会销毁它，当web应用移除或服务器停止时才销毁Filter对象。在Web容器卸载 Filter 对象之前被调用。该方法在Filter的生命周期中仅执行一次。在这个方法中，可以释放过滤器使用的资源。 6、FilterConfig接口 用户在配置filter时，可以使用为filter配置一些初始化参数，当web容器实例化Filter对象，调用其init方法时，会把封装了filter初始化参数的filterConfig对象传递进来。因此开发人员在编写filter时，通过filterConfig对象的方法，就可获得以下内容： 1234String getFilterName();//得到filter的名称；与&lt;filter-name&gt;元素对应。String getInitParameter(String name);//返回在部署描述中指定名称的初始化参数的值。如果不存在返回null，与&lt;init-param&gt;元素对应.Enumeration getInitParameterNames();//返回过滤器的所有初始化参数的名字的枚举集合。public ServletContext getServletContext();//返回Servlet上下文对象的引用。 7、FilterChain doFilter()方法的参数中有一个类型为FilterChain的参数，它只有一个方法：doFilter(ServletRequest,ServletResponse) doFilter() 方法的放行，让请求流访问目标资源！其实调用该方法的意思是，当前 Filter 放行了，但不代表其他过滤器也放行。一个目标资源上，可能部署了多个过滤器，所以调用 FilterChain 类的 doFilter() 方法表示的是执行下一个过滤器的 doFilter() 方法，或者是执行目标资源！ 如果当前过滤器是最后一个过滤器，那么调用 chain.doFilter() 方法表示执行目标资源，而不是最后一个过滤器，那么 chain.doFilter() 表示执行下一个过滤器的 doFilter() 方法。 8、过滤器的应用场景 执行目标资源之前做预处理工作，例如设置编码，这种试通常都会放行，只是在目标资源执行之前做一些准备工作； 通过条件判断是否放行，例如校验当前用户是否已经登录，或者用户IP是否已经被禁用； 在目标资源执行后，做一些后续的特殊处理工作，例如把目标资源输出的数据进行处理 设置目标资源 在web.xml文件中部署Filter时，可以通过“*”来执行目标资源： 1234&lt;filter-mapping&gt; &lt;filter-name&gt;myfilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; 特性与Servlet完全相同！通过这一特性，可以在用户访问敏感资源时，执行过滤器，例如：/admin/*，可以把所有管理员才能访问的资源放到/admin路径下，这时可以通过过滤器来校验用户身份。 还可以为指定目标资源为某个Servlet，例如： 12345678910111213141516&lt;servlet&gt; &lt;servlet-name&gt;myservlet&lt;/servlet-name&gt; &lt;servlet-class&gt;cn.cloud.servlet.MyServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;myservlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/abc&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;filter&gt; &lt;filter-name&gt;myfilter&lt;/filter-name&gt; &lt;filter-class&gt;cn.cloud.filter.MyFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;myfilter&lt;/filter-name&gt; &lt;servlet-name&gt;myservlet&lt;/servlet-name&gt; &lt;/filter-mapping&gt; 当用户访问http://localhost:8080/filtertest/abc时，会执行名字为myservlet的Servlet，这时会执行过滤器。 9、四种拦截方式 写一个过滤器，指定过滤的资源为b.jsp，然后在浏览器中直接访问b.jsp，会发现过滤器执行了.但是，当在a.jsp中request.getRequestDispathcer(“/b.jsp”).forward(request,response)时，就不会再执行过滤器了！也就是说，默认情况下，只能直接访问目标资源才会执行过滤器，而forward执行目标资源，不会执行过滤器！ 12345678public class MyFilter extends HttpFilter &#123; public void doFilter(HttpServletRequest request, HttpServletResponse response, FilterChain chain) throws IOException, ServletException &#123; System.out.println(\"myfilter...\"); chain.doFilter(request, response); &#125;&#125; 12345678&lt;filter&gt; &lt;filter-name&gt;myfilter&lt;/filter-name&gt; &lt;filter-class&gt;cn.itcast.filter.MyFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;myfilter&lt;/filter-name&gt; &lt;url-pattern&gt;/b.jsp&lt;/url-pattern&gt; &lt;/filter-mapping&gt; 123&lt;body&gt; &lt;h1&gt;b.jsp&lt;/h1&gt; &lt;/body&gt; 12345&lt;h1&gt;a.jsp&lt;/h1&gt; &lt;% request.getRequestDispatcher(\"/b.jsp\").forward(request, response); %&gt; &lt;/body&gt; 在浏览器输入： http://localhost:8080/filtertest/b.jsp 直接访问b.jsp时，会执行过滤器内容； http://localhost:8080/filtertest/a.jsp 访问a.jsp，但a.jsp会forward到b.jsp，这时就不会执行过滤器！ 过滤器有四种拦截方式！分别是：REQUEST、FORWARD、INCLUDE、ERROR。 REQUEST：直接访问目标资源时执行过滤器。包括：在地址栏中直接访问、表单提交、超链接、重定向，只要在地址栏中可以看到目标资源的路径，就是REQUEST FORWARD：转发访问执行过滤器。包括RequestDispatcher#forward()方法、标签都是转发访问 INCLUDE：包含访问执行过滤器。包括RequestDispatcher#include()方法、标签都是包含访问 ERROR：当目标资源在web.xml中配置为中时，并且真的出现了异常，转发到目标资源时，会执行过滤器。 可以在中添加0~n个子元素，来说明当前访问的拦截方式。 如： 123456&lt;filter-mapping&gt; &lt;filter-name&gt;myfilter&lt;/filter-name&gt; &lt;url-pattern&gt;/b.jsp&lt;/url-pattern&gt; &lt;dispatcher&gt;REQUEST&lt;/dispatcher&gt; &lt;dispatcher&gt;FORWARD&lt;/dispatcher&gt; &lt;/filter-mapping&gt; 最为常用的就是REQUEST和FORWARD两种拦截方式，而INCLUDE和ERROR都比较少用！其中INCLUDE比较好理解，ERROR方式不易理解，下面给出ERROR拦截方式的例子： 123456789&lt;filter-mapping&gt; &lt;filter-name&gt;myfilter&lt;/filter-name&gt; &lt;url-pattern&gt;/b.jsp&lt;/url-pattern&gt; &lt;dispatcher&gt;ERROR&lt;/dispatcher&gt; &lt;/filter-mapping&gt; &lt;error-page&gt; &lt;error-code&gt;500&lt;/error-code&gt; &lt;location&gt;/b.jsp&lt;/location&gt; &lt;/error-page&gt; 1234567&lt;body&gt; &lt;h1&gt;a.jsp&lt;/h1&gt; &lt;% if(true) throw new RuntimeException(\"嘻嘻~\"); %&gt; &lt;/body&gt; 10、Filter使用案例 1、使用Filter验证用户登录安全控制 前段时间参与维护一个项目，用户退出系统后，再去地址栏访问历史，根据url，仍然能够进入系统响应页面。我去检查一下发现对请求未进行过滤验证用户登录。添加一个filter搞定问题！ 先在web.xml配置 123456789101112131415161718192021222324&lt;filter&gt; &lt;filter-name&gt;SessionFilter&lt;/filter-name&gt; &lt;filter-class&gt;com.action.login.SessionFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;logonStrings&lt;/param-name&gt;&lt;!-- 对登录页面不进行过滤 --&gt; &lt;param-value&gt;/project/index.jsp;login.do&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;includeStrings&lt;/param-name&gt;&lt;!-- 只对指定过滤参数后缀进行过滤 --&gt; &lt;param-value&gt;.do;.jsp&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;redirectPath&lt;/param-name&gt;&lt;!-- 未通过跳转到登录界面 --&gt; &lt;param-value&gt;/index.jsp&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;disabletestfilter&lt;/param-name&gt;&lt;!-- Y:过滤无效 --&gt; &lt;param-value&gt;N&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;SessionFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 接着编写FilterServlet： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package com.action.login;import java.io.IOException;import javax.servlet.Filter;import javax.servlet.FilterChain;import javax.servlet.FilterConfig;import javax.servlet.ServletException;import javax.servlet.ServletRequest;import javax.servlet.ServletResponse;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import javax.servlet.http.HttpServletResponseWrapper;/** * 判断用户是否登录,未登录则退出系统 */public class SessionFilter implements Filter &#123; public FilterConfig config; public void destroy() &#123; this.config = null; &#125; public static boolean isContains(String container, String[] regx) &#123; boolean result = false; for (int i = 0; i &lt; regx.length; i++) &#123; if (container.indexOf(regx[i]) != -1) &#123; return true; &#125; &#125; return result; &#125; public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; HttpServletRequest hrequest = (HttpServletRequest)request; HttpServletResponseWrapper wrapper = new HttpServletResponseWrapper((HttpServletResponse) response); String logonStrings = config.getInitParameter(\"logonStrings\"); // 登录登陆页面 String includeStrings = config.getInitParameter(\"includeStrings\"); // 过滤资源后缀参数 String redirectPath = hrequest.getContextPath() + config.getInitParameter(\"redirectPath\");// 没有登陆转向页面 String disabletestfilter = config.getInitParameter(\"disabletestfilter\");// 过滤器是否有效 if (disabletestfilter.toUpperCase().equals(\"Y\")) &#123; // 过滤无效 chain.doFilter(request, response); return; &#125; String[] logonList = logonStrings.split(\";\"); String[] includeList = includeStrings.split(\";\"); if (!this.isContains(hrequest.getRequestURI(), includeList)) &#123;// 只对指定过滤参数后缀进行过滤 chain.doFilter(request, response); return; &#125; if (this.isContains(hrequest.getRequestURI(), logonList)) &#123;// 对登录页面不进行过滤 chain.doFilter(request, response); return; &#125; String user = ( String ) hrequest.getSession().getAttribute(\"useronly\");//判断用户是否登录 if (user == null) &#123; wrapper.sendRedirect(redirectPath); return; &#125;else &#123; chain.doFilter(request, response); return; &#125; &#125; public void init(FilterConfig filterConfig) throws ServletException &#123; config = filterConfig; &#125;&#125; 这样既可完成对用户所有请求，均要经过这个Filter进行验证用户登录。 2、防止中文乱码过滤器 项目使用spring框架时。当前台JSP页面和JAVA代码中使用了不同的字符集进行编码的时候就会出现表单提交的数据或者上传/下载中文名称文件出现乱码的问题，那就可以使用这个过滤器。 12345678910111213141516&lt;filter&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt;&lt;!--用来指定一个具体的字符集--&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt;&lt;!--true：无论request是否指定了字符集，都是用encoding；false：如果request已指定一个字符集，则不使用encoding--&gt; &lt;param-value&gt;false&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 3、Spring+Hibernate的OpenSessionInViewFilter控制session的开关 当 hibernate+spring 配合使用的时候，如果设置了lazy=true（延迟加载）,那么在读取数据的时候，当读取了父数据后，hibernate 会自动关闭 session，这样，当要使用与之关联数据、子数据的时候，系统会抛出lazyinit的错误，这时就需要使用 spring 提供的 OpenSessionInViewFilter 过滤器。 OpenSessionInViewFilter主要是保持 Session 状态直到 request 将全部页面发送到客户端，直到请求结束后才关闭 session，这样就可以解决延迟加载带来的问题。 注意：OpenSessionInViewFilter 配置要写在struts2的配置前面。因为 tomcat 容器在加载过滤器的时候是按照顺序加载的，如果配置文件先写的是 struts2 的过滤器配置，然后才是 OpenSessionInViewFilter 过滤器配置，所以加载的顺序导致，action 在获得数据的时候 session 并没有被 spring 管理。 1234567891011121314151617&lt;!-- lazy loading enabled in spring --&gt;&lt;filter&gt; &lt;filter-name&gt;OpenSessionInViewFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.orm.hibernate3.support.OpenSessionInViewFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;sessionFactoryBeanName&lt;/param-name&gt;&lt;!-- 可缺省。默认是从spring容器中找id为sessionFactory的bean，如果id不为sessionFactory，则需要配置如下，此处SessionFactory为spring容器中的bean。 --&gt; &lt;param-value&gt;sessionFactory&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;singleSession&lt;/param-name&gt;&lt;!-- singleSession默认为true,若设为false则等于没用OpenSessionInView --&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;OpenSessionInViewFilter&lt;/filter-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 4、Struts2的web.xml配置 项目中使用Struts2同样需要在web.xml配置过滤器，用来截取请求，转到Struts2的Action进行处理。 注意：如果在2.1.3以前的Struts2版本，过滤器使用org.apache.struts2.dispatcher.FilterDispatcher。否则使用org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter。从Struts2.1.3开始，将废弃ActionContextCleanUp过滤器，而在StrutsPrepareAndExecuteFilter过滤器中包含相应的功能。 三个初始化参数配置： config参数：指定要加载的配置文件。逗号分割。 actionPackages参数：指定Action类所在的包空间。逗号分割。 configProviders参数：自定义配置文件提供者，需要实现ConfigurationProvider接口类。逗号分割。 123456789&lt;!-- struts 2.x filter --&gt;&lt;filter&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;filter-class&gt;org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/filter-mapping&gt;","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Filter过滤器","slug":"Filter过滤器","permalink":"http://yoursite.com/tags/Filter过滤器/"}]},{"title":"详细深入分析 Java ClassLoader 工作机制","date":"2017-06-16T16:00:00.000Z","path":"2017/06/17/详细深入分析 Java ClassLoader 工作机制/","text":"什么是 ClassLoader ？大家都知道，当我们写好一个 Java 程序之后，不是管是 C/S 还是 B/S 应用，都是由若干个 .class 文件组织而成的一个完整的 Java 应用程序，当程序在运行时，即会调用该程序的一个入口函数来调用系统的相关功能，而这些功能都被封装在不同的 class 文件当中，所以经常要从这个 class 文件中要调用另外一个 class 文件中的方法，如果另外一个文件不存在的，则会引发系统异常。而程序在启动的时候，并不会一次性加载程序所要用的所有class文件，而是根据程序的需要，通过Java的类加载机制（ClassLoader）来动态加载某个 class 文件到内存当中的，从而只有 class 文件被载入到了内存之后，才能被其它 class 所引用。所以 ClassLoader 就是用来动态加载 class 文件到内存当中用的。 ClassLoader 作用： 负责将 Class 加载到 JVM 中 审查每个类由谁加载（父优先的等级加载机制） 将 Class 字节码重新解析成 JVM 统一要求的对象格式 1、ClassLoader 类结构分析为了更好的理解类的加载机制，我们来深入研究一下 ClassLoader 和他的方法。 public abstract class ClassLoader ClassLoader类是一个抽象类，sun公司是这么解释这个类的： 1234567/** * A class loader is an object that is responsible for loading classes. The * class ClassLoader is an abstract class. Given the binary name of a class, a class loader should attempt to * locate or generate data that constitutes a definition for the class. A * typical strategy is to transform the name into a file name and then read a * &quot;class file&quot; of that name from a file system.**/ 大致意思如下： class loader 是一个负责加载 classes 的对象，ClassLoader 类是一个抽象类，需要给出类的二进制名称，class loader 尝试定位或者产生一个 class 的数据，一个典型的策略是把二进制名字转换成文件名然后到文件系统中找到该文件。 以下是 ClassLoader 常用到的几个方法及其重载方法： ClassLoader defineClass(byte[], int, int) 把字节数组 b中的内容转换成 Java 类，返回的结果是java.lang.Class类的实例。这个方法被声明为final的 findClass(String name) 查找名称为 name的类，返回的结果是java.lang.Class类的实例 loadClass(String name) 加载名称为 name的类，返回的结果是java.lang.Class类的实例 resolveClass(Class&lt;?&gt;) 链接指定的 Java 类 其中 defineClass 方法用来将 byte 字节流解析成 JVM 能够识别的 Class 对象，有了这个方法意味着我们不仅仅可以通过 class 文件实例化对象，还可以通过其他方式实例化对象，如果我们通过网络接收到一个类的字节码，拿到这个字节码流直接创建类的 Class 对象形式实例化对象。如果直接调用这个方法生成类的 Class 对象，这个类的 Class 对象还没有 resolve ，这个 resolve 将会在这个对象真正实例化时才进行。 接下来我们看loadClass方法的实现方式： 123456789101112131415161718192021222324252627282930313233343536protected Class&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; // First, check if the class has already been loaded Class c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; if (parent != null) &#123; c = parent.loadClass(name, false); &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125; &#125; 该方法大概意思： 使用指定的二进制名称来加载类，这个方法的默认实现按照以下顺序查找类： 调用findLoadedClass(String) 方法检查这个类是否被加载过 使用父加载器调用 loadClass(String) 方法，如果父加载器为 Null，类加载器装载虚拟机内置的加载器调用 findClass(String) 方法装载类， 如果，按照以上的步骤成功的找到对应的类，并且该方法接收的 resolve 参数的值为 true,那么就调用resolveClass(Class) 方法来处理类。 ClassLoader 的子类最好覆盖 findClass(String) 而不是这个方法。 除非被重写，这个方法默认在整个装载过程中都是同步的（线程安全的）。 2、ClassLoader 的等级加载机制Java默认提供的三个ClassLoader BootStrap ClassLoader：称为启动类加载器，是Java类加载层次中最顶层的类加载器，负责加载JDK中的核心类库，如：rt.jar、resources.jar、charsets.jar等，可通过如下程序获得该类加载器从哪些地方加载了相关的jar或class文件： 12345678910public class BootStrapTest&#123; public static void main(String[] args) &#123; URL[] urls = sun.misc.Launcher.getBootstrapClassPath().getURLs(); for (int i = 0; i &lt; urls.length; i++) &#123; System.out.println(urls[i].toExternalForm()); &#125; &#125;&#125; 以下内容是上述程序从本机JDK环境所获得的结果： 其实上述结果也是通过查找 sun.boot.class.path 这个系统属性所得知的。 1System.out.println(System.getProperty(\"sun.boot.class.path\")); 1打印结果：C:\\Java\\jdk1.8.0_60\\jre\\lib\\resources.jar;C:\\Java\\jdk1.8.0_60\\jre\\lib\\rt.jar;C:\\Java\\jdk1.8.0_60\\jre\\lib\\sunrsasign.jar;C:\\Java\\jdk1.8.0_60\\jre\\lib\\jsse.jar;C:\\Java\\jdk1.8.0_60\\jre\\lib\\jce.jar;C:\\Java\\jdk1.8.0_60\\jre\\lib\\charsets.jar;C:\\Java\\jdk1.8.0_60\\jre\\lib\\jfr.jar;C:\\Java\\jdk1.8.0_60\\jre\\classes Extension ClassLoader：称为扩展类加载器，负责加载Java的扩展类库，Java 虚拟机的实现会提供一个扩展库目录。该类加载器在此目录里面查找并加载 Java 类。默认加载JAVA_HOME/jre/lib/ext/目下的所有jar。 App ClassLoader：称为系统类加载器，负责加载应用程序classpath目录下的所有jar和class文件。一般来说，Java 应用的类都是由它来完成加载的。可以通过 ClassLoader.getSystemClassLoader()来获取它。 ​ 除了系统提供的类加载器以外，开发人员可以通过继承java.lang.ClassLoader类的方式实现自己的类加载器，以满足一些特殊的需求。 除了引导类加载器之外，所有的类加载器都有一个父类加载器。 给出的 getParent()方法可以得到。对于系统提供的类加载器来说，系统类加载器的父类加载器是扩展类加载器，而扩展类加载器的父类加载器是引导类加载器；对于开发人员编写的类加载器来说，其父类加载器是加载此类加载器 Java 类的类加载器。因为类加载器 Java 类如同其它的 Java 类一样，也是要由类加载器来加载的。一般来说，开发人员编写的类加载器的父类加载器是系统类加载器。类加载器通过这种方式组织起来，形成树状结构。树的根节点就是引导类加载器。 ​ ClassLoader加载类的原理1. 原理介绍ClassLoader使用的是双亲委托模型来搜索类的，每个ClassLoader实例都有一个父类加载器的引用（不是继承的关系，是一个包含的关系），虚拟机内置的类加载器（Bootstrap ClassLoader）本身没有父类加载器，但可以用作其它ClassLoader实例的的父类加载器。当一个ClassLoader实例需要加载某个类时，它会试图亲自搜索某个类之前，先把这个任务委托给它的父类加载器，这个过程是由上至下依次检查的，首先由最顶层的类加载器Bootstrap ClassLoader试图加载，如果没加载到，则把任务转交给Extension ClassLoader试图加载，如果也没加载到，则转交给App ClassLoader进行加载，如果它也没有加载得到的话，则返回给委托的发起者，由它到指定的文件系统或网络等URL中加载该类。如果它们都没有加载到这个类时，则抛出ClassNotFoundException异常。否则将这个找到的类生成一个类的定义，并将它加载到内存当中，最后返回这个类在内存中的Class实例对象。 2、为什么要使用双亲委托这种模型呢？因为这样可以避免重复加载，当父亲已经加载了该类的时候，就没有必要 ClassLoader再加载一次。考虑到安全因素，我们试想一下，如果不使用这种委托模式，那我们就可以随时使用自定义的String来动态替代java核心api中定义的类型，这样会存在非常大的安全隐患，而双亲委托的方式，就可以避免这种情况，因为String已经在启动时就被引导类加载器（Bootstrcp ClassLoader）加载，所以用户自定义的ClassLoader永远也无法加载一个自己写的String，除非你改变JDK中ClassLoader搜索类的默认算法。 3、 但是JVM在搜索类的时候，又是如何判定两个class是相同的呢？JVM在判定两个class是否相同时，不仅要判断两个类名是否相同，而且要判断是否由同一个类加载器实例加载的。只有两者同时满足的情况下，JVM才认为这两个class是相同的。就算两个class是同一份class字节码，如果被两个不同的ClassLoader实例所加载，JVM也会认为它们是两个不同class。比如网络上的一个Java类org.classloader.simple.NetClassLoaderSimple，javac编译之后生成字节码文件NetClassLoaderSimple.class，ClassLoaderA和ClassLoaderB这两个类加载器并读取了NetClassLoaderSimple.class文件，并分别定义出了java.lang.Class实例来表示这个类，对于JVM来说，它们是两个不同的实例对象，但它们确实是同一份字节码文件，如果试图将这个Class实例生成具体的对象进行转换时，就会抛运行时异常java.lang.ClassCaseException，提示这是两个不同的类型。现在通过实例来验证上述所描述的是否正确：1）、在web服务器上建一个org.classloader.simple.NetClassLoaderSimple.java类 1234567public class NetClassLoaderSimple&#123; private NetClassLoaderSimple instance; public void setNetClassLoaderSimple(Object object)&#123; this.instance = (NetClassLoaderSimple)object; &#125;&#125; org.classloader.simple.NetClassLoaderSimple类的setNetClassLoaderSimple方法接收一个Object类型参数，并将它强制转换成org.classloader.simple.NetClassLoaderSimple类型。 2）、测试两个class是否相同 NetWorkClassLoader.java 12345678910111213141516171819202122package classloader;public class NewworkClassLoaderTest &#123; public static void main(String[] args) &#123; try &#123; //测试加载网络中的class文件 String rootUrl = &quot;http://localhost:8080/httpweb/classes&quot;; String className = &quot;org.classloader.simple.NetClassLoaderSimple&quot;; NetworkClassLoader ncl1 = new NetworkClassLoader(rootUrl); NetworkClassLoader ncl2 = new NetworkClassLoader(rootUrl); Class&lt;?&gt; clazz1 = ncl1.loadClass(className); Class&lt;?&gt; clazz2 = ncl2.loadClass(className); Object obj1 = clazz1.newInstance(); Object obj2 = clazz2.newInstance(); clazz1.getMethod(&quot;setNetClassLoaderSimple&quot;, Object.class).invoke(obj1, obj2); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 首先获得网络上一个class文件的二进制名称，然后通过自定义的类加载器NetworkClassLoader创建两个实例，并根据网络地址分别加载这份class，并得到这两个ClassLoader实例加载后生成的Class实例clazz1和clazz2，最后将这两个Class实例分别生成具体的实例对象obj1和obj2，再通过反射调用clazz1中的setNetClassLoaderSimple方法。 3）、查看测试结果 结论：从结果中可以看出，运行时抛出了java.lang.ClassCastException异常。虽然两个对象obj1和 obj2的类的名字相同，但是这两个类是由不同的类加载器实例来加载的，所以JVM认为它们就是两个不同的类。 了解了这一点之后，就可以理解代理模式的设计动机了。代理模式是为了保证 Java 核心库的类型安全。所有 Java 应用都至少需要引用 java.lang.Object类，也就是说在运行的时候，java.lang.Object这个类需要被加载到 Java 虚拟机中。如果这个加载过程由 Java 应用自己的类加载器来完成的话，很可能就存在多个版本的 java.lang.Object类，而且这些类之间是不兼容的。通过代理模式，对于 Java 核心库的类的加载工作由引导类加载器来统一完成，保证了Java 应用所使用的都是同一个版本的 Java 核心库的类，是互相兼容的。 不同的类加载器为相同名称的类创建了额外的名称空间。相同名称的类可以并存在 Java 虚拟机中，只需要用不同的类加载器来加载它们即可。不同类加载器加载的类之间是不兼容的，这就相当于在 Java 虚拟机内部创建了一个个相互隔离的 Java 类空间。 ClassLoader的体系架构： 类加载器的树状组织结构测试一： 1234567891011public class ClassLoaderTree&#123; public static void main(String[] args) &#123; ClassLoader loader = ClassLoaderTree.class.getClassLoader(); while (loader!=null)&#123; System.out.println(loader.toString()); loader = loader.getParent(); &#125; System.out.println(loader); &#125;&#125; 每个 Java 类都维护着一个指向定义它的类加载器的引用，通过 getClassLoader()方法就可以获取到此引用。代码中通过递归调用 getParent()方法来输出全部的父类加载器。 结果是： 第一个输出的是ClassLoaderTree类的类加载器，即系统类加载器。它是sun.misc.Launcher$AppClassLoader类的实例；第二个输出的是扩展类加载器，是sun.misc.Launcher$ExtClassLoader类的实例。需要注意的是这里并没有输出引导类加载器，这是由于有些 JDK 的实现对于父类加载器是引导类加载器的情况，getParent()方法返回 null。第三行结果说明：ExtClassLoader的类加器是Bootstrap ClassLoader，因为Bootstrap ClassLoader不是一个普通的Java类，所以ExtClassLoader的parent=null，所以第三行的打印结果为null就是这个原因。 测试二： 将ClassLoaderTree.class打包成ClassLoaderTree.jar，放到Extension ClassLoader的加载目录下（JAVA_HOME/jre/lib/ext），然后重新运行这个程序，得到的结果会是什么样呢？ 此处我在 IDEA 中的运行结果还和上面的一样，与文章 深入分析Java ClassLoader原理 中的有差距，具体原因未弄清楚，还希望读者能够亲自测试。 那文章中的结果是： 打印结果分析：为什么第一行的结果是ExtClassLoader呢？ 因为 ClassLoader 的委托模型机制，当我们要用 ClassLoaderTest.class 这个类的时候，AppClassLoader 在试图加载之前，先委托给 Bootstrcp ClassLoader，Bootstracp ClassLoader 发现自己没找到，它就告诉 ExtClassLoader，兄弟，我这里没有这个类，你去加载看看，然后 Extension ClassLoader 拿着这个类去它指定的类路径（JAVA_HOME/jre/lib/ext）试图加载，唉，它发现在ClassLoaderTest.jar 这样一个文件中包含 ClassLoaderTest.class 这样的一个文件，然后它把找到的这个类加载到内存当中，并生成这个类的 Class 实例对象，最后把这个实例返回。所以 ClassLoaderTest.class 的类加载器是 ExtClassLoader。 第二行的结果为null，是因为ExtClassLoader的父类加载器是Bootstrap ClassLoader。 JVM加载class文件的两种方法； 隐式加载， 程序在运行过程中当碰到通过new 等方式生成对象时，隐式调用类装载器加载对应的类到jvm中。 显式加载， 通过class.forname()、this.getClass.getClassLoader().loadClass()等方法显式加载需要的类，或者我们自己实现的 ClassLoader 的 findlass() 方法。 下面介绍下 class.forName的加载类方法： Class.forName是一个静态方法，同样可以用来加载类。该方法有两种形式：Class.forName(String name,boolean initialize, ClassLoader loader)和Class.forName(String className)。第一种形式的参数 name表示的是类的全名；initialize表示是否初始化类；loader表示加载时使用的类加载器。第二种形式则相当于设置了参数 initialize的值为 true，loader的值为当前类的类加载器。Class.forName的一个很常见的用法是在加载数据库驱动的时候。如Class.forName(&quot;org.apache.derby.jdbc.EmbeddedDriver&quot;)用来加载 Apache Derby 数据库的驱动。 类加载的动态性体现：一个应用程序总是由n多个类组成，Java程序启动时，并不是一次把所有的类全部加载后再运行，它总是先把保证程序运行的基础类一次性加载到jvm中，其它类等到jvm用到的时候再加载，这样的好处是节省了内存的开销，因为java最早就是为嵌入式系统而设计的，内存宝贵，这是一种可以理解的机制，而用到时再加载这也是java动态性的一种体现。 3、如何加载 class 文件 第一阶段找到 .class 文件并把这个文件包含的字节码加载到内存中。 第二阶段中分三步，字节码验证；class 类数据结构分析及相应的内存分配；最后的符号表的链接。 第三阶段是类中静态属性和初始化赋值，以及静态块的执行等。 3.1 、加载字节码到内存。。 3.2 、验证与分析 字节码验证，类装入器对于类的字节码要做很多检测，以确保格式正确，行为正确。 类装备，准备代表每个类中定义的字段、方法和实现接口所必须的数据结构。 解析，装入器装入类所引用的其他所有类。 4、常见加载类错误分析4.1 、 ClassNotFoundExecptionClassNotFoundExecption 异常是平常碰到的最多的。这个异常通常发生在显示加载类的时候。 12345678910public class ClassNotFoundExceptionTest&#123; public static void main(String[] args) &#123; try &#123; Class.forName(&quot;NotFoundClass&quot;); &#125;catch (ClassNotFoundException e)&#123; e.printStackTrace(); &#125; &#125;&#125; 显示加载一个类通常有： 通过类 Class 中的 forName() 方法 通过类 ClassLoader 中的 loadClass() 方法 通过类 ClassLoader 中的 findSystemClass() 方法 出现这种错误其实就是当 JVM 要加载指定文件的字节码到内存时，并没有找到这个文件对应的字节码，也就是这个文件并不存在。解决方法就是检查在当前的 classpath 目录下有没有指定的文件。 4.2 、 NoClassDefFoundError在JavaDoc中对NoClassDefFoundError的产生可能的情况就是使用new关键字、属性引用某个类、继承了某个接口或者类，以及方法的某个参数中引用了某个类，这时就会触发JVM或者类加载器实例尝试加载类型的定义，但是该定义却没有找到，影响了执行路径。换句话说，在编译时这个类是能够被找到的，但是在执行时却没有找到。 解决这个错误的方法就是确保每个类引用的类都在当前的classpath下面。 4.3 、 UnsatisfiedLinkError该错误通常是在 JVM 启动的时候，如果 JVM 中的某个 lib 删除了，就有可能报这个错误。 12345678910public class UnsatisfiedLinkErrorTest&#123; public native void nativeMethod(); static &#123; System.loadLibrary(\"NoLib\"); &#125; public static void main(String[] args) &#123; new UnsatisfiedLinkErrorTest().nativeMethod(); //解析native标识的方法时JVM找不到对应的库文件 &#125;&#125; 4.4 、 ClassCastException该错误通常出现强制类型转换时出现这个错误。 123456789101112public class ClassCastExceptionTest&#123; public static Map m = new HashMap()&#123; &#123; put(\"a\", \"2\"); &#125; &#125;; public static void main(String[] args) &#123; Integer integer = (Integer) m.get(\"a\"); //将m强制转换成Integer类型 System.out.println(integer); &#125;&#125; 注意：JVM 在做类型转换时的规则： 对于普通对象，对象必须是目标类的实例或目标类的子类的实例。如果目标类是接口，那么会把它当作实现了该接口的一个子类。 对于数组类型，目标类必须是数组类型或 java.lang.Object、java.lang.Cloneable、java.io.Serializable。 如果不满足上面的规则，JVM 就会报错，有两种方式可避免错误： 在容器类型中显式的指明这个容器所包含的对象类型。 先通过 instanceof 检查是不是目标类型，然后再进行强制类型的转换。 上面代码中改成如下就可以避免错误了： 4.5 、 ExceptionInInitializerError12345678910public class ExceptionInInitializerErrorTest&#123; public static Map m = new HashMap()&#123;&#123; m.put(\"a\", \"2\"); &#125;&#125;; public static void main(String[] args) &#123; Integer integer = (Integer) m.get(\"a\"); System.out.println(integer); &#125;&#125; 在初始化这个类时，给静态属性 m 赋值时出现了异常导致抛出错误 ExceptionInInitializerError。 4.6 NoSuchMethodErrorNoSuchMethodError代表这个类型确实存在，但是一个不正确的版本被加载了。为了解决这个问题我们可以使用 ‘­verbose:class’ 来判断该JVM加载的到底是哪个版本。 4.7 LinkageError有时候事情会变得更糟，和 ClassCastException 本质一样，加载自不同位置的相同类在同一段逻辑（比如：方法）中交互时，会出现 LinkageError 。 LinkageError 需要观察哪个类被不同的类加载器加载了，在哪个方法或者调用处发生（交汇）的，然后才能想解决方法，解决方法无外乎两种。第一，还是不同的类加载器加载，但是相互不再交汇影响，这里需要针对发生问题的地方做一些改动，比如更换实现方式，避免出现上述问题；第二，冲突的类需要由一个Parent类加载器进行加载。LinkageError 和ClassCastException 本质是一样的，加载自不同类加载器的类型，在同一个类的方法或者调用中出现，如果有转型操作那么就会抛 ClassCastException ，如果是直接的方法调用处的参数或者返回值解析，那么就会产生 LinkageError 。 5、常用的 ClassLoader 分析。。参见书籍《深入分析Java Web技术内幕》 6、如何实现自己的 ClassLoaderClassLoader 能够完成的事情有以下情况： 在自定义路径下查找自定义的class类文件。 对我们自己要加载的类做特殊处理。 可以定义类的实现机制。 虽然在绝大多数情况下，系统默认提供的类加载器实现已经可以满足需求。但是在某些情况下，您还是需要为应用开发出自己的类加载器。比如您的应用通过网络来传输 Java 类的字节代码，为了保证安全性，这些字节代码经过了加密处理。这个时候您就需要自己的类加载器来从某个网络地址上读取加密后的字节代码，接着进行解密和验证，最后定义出要在 Java 虚拟机中运行的类来。 定义自已的类加载器分为两步：1、继承java.lang.ClassLoader2、重写父类的findClass方法 6.1 、文件系统类加载器加载存储在文件系统上的 Java 字节代码。 123456789101112131415161718192021222324252627282930313233343536373839404142public class FileSystemClassLoader extends ClassLoader&#123; private String rootDir; public FileSystemClassLoader(String rootDir)&#123; this.rootDir = rootDir; &#125; protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; byte[] classData = getClassData(name); if (classData == null)&#123; throw new ClassNotFoundException(); &#125; else &#123; return defineClass(name, classData, 0, classData.length); &#125; &#125; private byte[] getClassData(String className) &#123; String path = classNameToPath(className); try &#123; InputStream ins = new FileInputStream(path); ByteArrayOutputStream baos = new ByteArrayOutputStream(); int bufferSize = 4096; byte[] buffer = new byte[bufferSize]; int bytesNumRead = 0; while ((bytesNumRead = ins.read(buffer)) != -1)&#123; baos.write(buffer, 0, bytesNumRead); &#125; return baos.toByteArray(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; private String classNameToPath(String className) &#123; return rootDir + File.separatorChar + className.replace('.', File.separatorChar) + \".class\"; &#125;&#125; 类 FileSystemClassLoader继承自类java.lang.ClassLoader。java.lang.ClassLoader类的方法loadClass()封装了前面提到的代理模式的实现。该方法会首先调用 findLoadedClass()方法来检查该类是否已经被加载过；如果没有加载过的话，会调用父类加载器的loadClass()方法来尝试加载该类；如果父类加载器无法加载该类的话，就调用 findClass()方法来查找该类。因此，为了保证类加载器都正确实现代理模式，在开发自己的类加载器时，最好不要覆写 loadClass()方法，而是覆写findClass()方法。 类 FileSystemClassLoader的 findClass()方法首先根据类的全名在硬盘上查找类的字节代码文件（.class 文件），然后读取该文件内容，最后通过 defineClass()方法来把这些字节代码转换成 java.lang.Class类的实例。 6.2 、 网络类加载器一个网络类加载器来说明如何通过类加载器来实现组件的动态更新。即基本的场景是：Java 字节代码（.class）文件存放在服务器上，客户端通过网络的方式获取字节代码并执行。当有版本更新的时候，只需要替换掉服务器上保存的文件即可。通过类加载器可以比较简单的实现这种需求。 类 NetworkClassLoader 负责通过网络下载 Java 类字节代码并定义出 Java 类。它的实现与FileSystemClassLoader 类似。在通过 NetworkClassLoader 加载了某个版本的类之后，一般有两种做法来使用它。第一种做法是使用 Java 反射 API。另外一种做法是使用接口。需要注意的是，并不能直接在客户端代码中引用从服务器上下载的类，因为客户端代码的类加载器找不到这些类。使用 Java 反射 API 可以直接调用 Java 类的方法。而使用接口的做法则是把接口的类放在客户端中，从服务器上加载实现此接口的不同版本的类。在客户端通过相同的接口来使用这些实现类。 网络类加载器的代码：ClassLoader 7、类加载器与Web容器对于运行在 Java EE™容器中的 Web 应用来说，类加载器的实现方式与一般的 Java 应用有所不同。不同的 Web 容器的实现方式也会有所不同。以 Apache Tomcat 来说，每个Web 应用都有一个对应的类加载器实例。该类加载器也使用代理模式，所不同的是它是首先尝试去加载某个类，如果找不到再代理给父类加载器。这与一般类加载器的顺序是相反的。这是 Java Servlet 规范中的推荐做法，其目的是使得Web 应用自己的类的优先级高于 Web 容器提供的类。这种代理模式的一个例外是：Java 核心库的类是不在查找范围之内的。这也是为了保证 Java 核心库的类型安全。 绝大多数情况下，Web 应用的开发人员不需要考虑与类加载器相关的细节。下面给出几条简单的原则： 每个 Web 应用自己的 Java 类文件和使用的库的 jar 包，分别放在 WEB-INF/classes和 WEB-INF/lib目录下面。 多个应用共享的 Java 类文件和 jar 包，分别放在 Web 容器指定的由所有 Web 应用共享的目录下面。 当出现找不到类的错误时，检查当前类的类加载器和当前线程的上下文类加载器是否正确 8、总结本篇文章详细深入的介绍了 ClassLoader 的工作机制，还写了如何自己实现所需的 ClassLoader 。 参考资料1、深度分析 Java 的 ClassLoader 机制（源码级别） 2、深入浅出ClassLoader 3、深入探讨 Java 类加载器 4、深入分析Java ClassLoader原理 5、《深入分析 Java Web 技术内幕》修订版 —— 深入分析 ClassLoader 工作机制","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"},{"name":"类加载机制","slug":"类加载机制","permalink":"http://yoursite.com/tags/类加载机制/"}]},{"title":"通过项目逐步深入了解Spring MVC（一）","date":"2017-06-15T16:00:00.000Z","path":"2017/06/16/通过项目逐步深入了解Spring MVC（一）/","text":"相关阅读：本文档和项目代码地址：https://github.com/zhisheng17/springmvc 了解 Spring： Spring 官网：http://spring.io/ 一个好的东西一般都会有一个好的文档解释说明，如果你英语还行，建议还是看官方文档。 Spring MVC基础知识什么是Spring MVC？ Spring MVC框架原理（掌握） ​ 前端控制器、处理器映射器、处理器适配器、试图解析器Spring MVC 入门程序 ​ 目的：对前端控制器、处理器映射器、处理器适配器、试图解析器学习 ​ 非注解的处理器映射器、处理器适配器 ​ 注解的处理器映射器、处理器适配器（掌握） Spring MVC 和 Mybatis 整合（掌握） Spring MVC 注解开发：（掌握） ​ 常用的注解学习 ​ 参数绑定（简单类型，pojo类型、集合类型） ​ 自定义的参数绑定（掌握） Spring MVC 和 Struts2区别 Spring MVC高级应用参数绑定（集合类型） 数据回显 上传图片 json 数据交互 RESTful 支持 拦截器 Spring MVC 框架什么是Spring MVC？springmvc是spring框架的一个模块，springmvc和spring无需通过中间整合层进行整合。springmvc是一个基于mvc的web框架。 Web MVCMVC 设计模式在 B/S 系统下应用： 1、 用户发起request请求至控制器(Controller) 控制接收用户请求的数据，委托给模型进行处理 2、 控制器通过模型(Model)处理数据并得到处理结果 模型通常是指业务逻辑 3、 模型处理结果返回给控制器 4、 控制器将模型数据在视图(View)中展示 web中模型无法将数据直接在视图上显示，需要通过控制器完成。如果在C/S应用中模型是可以将数据在视图中展示的。 5、 控制器将视图response响应给用户 通过视图展示给用户要的数据或处理结果。 Spring MVC 框架 第一步：发起请求到前端控制器(DispatcherServlet) 第二步：前端控制器请求HandlerMapping查找 Handler 可以根据xml配置、注解进行查找 第三步：处理器映射器HandlerMapping向前端控制器返回Handler 第四步：前端控制器调用处理器适配器去执行Handler 第五步：处理器适配器去执行Handler 第六步：Handler执行完成给适配器返回ModelAndView 第七步：处理器适配器向前端控制器返回ModelAndView ModelAndView是springmvc框架的一个底层对象，包括Model和view 第八步：前端控制器请求视图解析器去进行视图解析 根据逻辑视图名解析成真正的视图(jsp) 第九步：视图解析器向前端控制器返回View 第十步：前端控制器进行视图渲染 视图渲染将模型数据(在ModelAndView对象中)填充到request域 第十一步：前端控制器向用户响应结果 组件： 1、前端控制器DispatcherServlet（不需要程序员开发） 作用接收请求，响应结果，相当于转发器，中央处理器。 有了DispatcherServlet减少了其它组件之间的耦合度。 2、处理器映射器HandlerMapping(不需要程序员开发) 作用：根据请求的url查找Handler 3、处理器适配器HandlerAdapter 作用：按照特定规则（HandlerAdapter要求的规则）去执行Handler 4、处理器Handler(需要程序员开发) 注意：编写Handler时按照HandlerAdapter的要求去做，这样适配器才可以去正确执行Handler 5、视图解析器View resolver(不需要程序员开发) 作用：进行视图解析，根据逻辑视图名解析成真正的视图（view） 6、视图View(需要程序员开发jsp) View是一个接口，实现类支持不同的View类型（jsp、freemarker、pdf…）","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://yoursite.com/tags/Spring-MVC/"}]},{"title":"通过项目逐步深入了解Mybatis（四）","date":"2017-06-14T16:00:00.000Z","path":"2017/06/15/通过项目逐步深入了解Mybatis(四)/","text":"相关阅读： 1、通过项目逐步深入了解Mybatis&lt;一&gt; 2、通过项目逐步深入了解Mybatis&lt;二&gt; 3、通过项目逐步深入了解Mybatis&lt;三&gt; 本项目所有代码及文档都托管在 Github地址：https://github.com/zhisheng17/mybatis 延迟加载什么是延迟加载？resultMap可以实现高级映射（使用association、collection实现一对一及一对多映射），association、collection具备延迟加载功能。需求：如果查询订单并且关联查询用户信息。如果先查询订单信息即可满足要求，当我们需要查询用户信息时再查询用户信息。把对用户信息的按需去查询就是延迟加载。 延迟加载：先从单表查询、需要时再从关联表去关联查询，大大提高 数据库性能，因为查询单表要比关联查询多张表速度要快。 打开延迟加载开关在mybatis核心配置文件中配置： lazyLoadingEnabled、aggressiveLazyLoading 设置项 描述 允许值 默认值 lazyLoadingEnabled 全局性设置懒加载。如果设为‘false’，则所有相关联的都会被初始化加载。 true \\ false false aggressiveLazyLoading 当设置为‘true’的时候，懒加载的对象可能被任何懒属性全部加载。否则，每个属性都按需加载。 true \\ false true 1234&lt;settings&gt; &lt;setting name=\"lazyLoadingEnabled\" value=\"true\"/&gt; &lt;setting name=\"aggressiveLazyLoading\" value=\"false\"/&gt;&lt;/settings&gt; 使用 association 实现延迟加载需求：查询订单并且关联查询用户信息 Mapper.xml需要定义两个 mapper 的方法对应的 statement。 1、只查询订单信息 SQL 语句： select * from orders 在查询订单的 statement 中使用 association 去延迟加载（执行）下边的 statement (关联查询用户信息) 1234&lt;!--查询订单并且关联查询用户信息，关联用户信息需要通过 association 延迟加载--&gt; &lt;select id=\"findOrdersUserLazyLoading\" resultMap=\"OrdersUserLazyLoadingResultMap\"&gt; select * from orders &lt;/select&gt; 2、关联查询用户信息 通过上面查询订单信息中的 user_id 来关联查询用户信息。使用 UserMapper.xml 中的 findUserById SQL语句：select * from user where id = user_id 123&lt;select id=\"findUserById\" parameterType=\"int\" resultType=\"user\"&gt; select * from user where id = #&#123;value&#125; &lt;/select&gt; 上边先去执行 findOrdersUserLazyLoading，当需要去查询用户的时候再去执行 findUserById ，通过 resultMap的定义将延迟加载执行配置起来。也就是通过 resultMap 去加载 UserMapper.xml 文件中的 select = findUserById 延迟加载的 resultMap1234567891011121314151617181920&lt;!--定义 关联用户信息（通过 association 延迟加载）的resultMap--&gt; &lt;resultMap id=\"OrdersUserLazyLoadingResultMap\" type=\"cn.zhisheng.mybatis.po.Orders\"&gt; &lt;!--对订单信息映射--&gt; &lt;id column=\"id\" property=\"id\"/&gt; &lt;result column=\"user_id\" property=\"userId\"/&gt; &lt;result column=\"number\" property=\"number\"/&gt; &lt;result column=\"createtime\" property=\"createtime\"/&gt; &lt;result column=\"note\" property=\"note\"/&gt; &lt;!-- 实现对用户信息进行延迟加载 select：指定延迟加载需要执行的statement的id（是根据user_id查询用户信息的statement） 要使用userMapper.xml中findUserById完成根据用户id(user_id)用户信息的查询，如果findUserById不在本mapper中需要前边加namespace column：订单信息中关联用户信息查询的列，是user_id 关联查询的sql理解为： SELECT orders.*, (SELECT username FROM USER WHERE orders.user_id = user.id)username, (SELECT sex FROM USER WHERE orders.user_id = user.id)sex FROM orders--&gt; &lt;association property=\"user\" javaType=\"cn.zhisheng.mybatis.po.User\" select=\"cn.zhisheng.mybatis.mapper.UserMapper.findUserById\" column=\"user_id\"&gt; &lt;/association&gt; &lt;/resultMap&gt; OrderMapperCustom.java1public List&lt;Orders&gt; findOrdersUserLazyLoading() throws Exception; 测试代码：1234567891011121314151617@Test public void testFindOrdersUserLazyLoading() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建OrdersMapperCustom对象,mybatis自动生成代理对象 OrdersMapperCustom ordersMapperCustom = sqlSession.getMapper(OrdersMapperCustom.class); //查询订单信息 List&lt;Orders&gt; list = ordersMapperCustom.findOrdersUserLazyLoading(); //遍历所查询的的订单信息 for (Orders orders : list) &#123; //查询用户信息 User user = orders.getUser(); System.out.println(user); &#125; sqlSession.close(); &#125; 测试结果： 整个延迟加载的思路： 1、执行上边mapper方法（findOrdersUserLazyLoading），内部去调用cn.zhisheng.mybatis.mapper.OrdersMapperCustom 中的 findOrdersUserLazyLoading 只查询 orders 信息（单表）。 2、在程序中去遍历上一步骤查询出的 List，当我们调用 Orders 中的 getUser 方法时，开始进行延迟加载。 3、延迟加载，去调用 UserMapper.xml 中 findUserbyId 这个方法获取用户信息。 思考：不使用 mybatis 提供的 association 及 collection 中的延迟加载功能，如何实现延迟加载？？ 实现方法如下： 定义两个mapper方法： 1、查询订单列表 2、根据用户id查询用户信息 实现思路： 先去查询第一个mapper方法，获取订单信息列表 在程序中（service），按需去调用第二个mapper方法去查询用户信息。 总之： 使用延迟加载方法，先去查询 简单的 sql（最好单表，也可以关联查询），再去按需要加载关联查询的其它信息。 一对多延迟加载上面的那个案例是一对一延迟加载，那么如果我们想一对多进行延迟加载呢，其实也是很简单的。 一对多延迟加载的方法同一对一延迟加载，在collection标签中配置select内容。 延迟加载总结：作用： 当需要查询关联信息时再去数据库查询，默认不去关联查询，提高数据库性能。只有使用resultMap支持延迟加载设置。 场合： 当只有部分记录需要关联查询其它信息时，此时可按需延迟加载，需要关联查询时再向数据库发出sql，以提高数据库性能。 当全部需要关联查询信息时，此时不用延迟加载，直接将关联查询信息全部返回即可，可使用resultType或resultMap完成映射。 查询缓存什么是查询缓存？mybatis提供查询缓存，用于减轻数据压力，提高数据库性能。 mybaits提供一级缓存，和二级缓存。 一级缓存是SqlSession级别的缓存。在操作数据库时需要构造 sqlSession对象，在对象中有一个数据结构（HashMap）用于存储缓存数据。不同的sqlSession之间的缓存数据区域（HashMap）是互相不影响的。 二级缓存是mapper级别的缓存，多个SqlSession去操作同一个Mapper的sql语句，多个SqlSession可以共用二级缓存，二级缓存是跨SqlSession的。 为什么要用缓存？ 如果缓存中有数据就不用从数据库中获取，大大提高系统性能。 一级缓存工作原理： 第一次发起查询用户id为1的用户信息，先去找缓存中是否有id为1的用户信息，如果没有，从数据库查询用户信息。 得到用户信息，将用户信息存储到一级缓存中。 如果sqlSession去执行commit操作（执行插入、更新、删除），清空SqlSession中的一级缓存，这样做的目的为了让缓存中存储的是最新的信息，避免脏读。 第二次发起查询用户id为1的用户信息，先去找缓存中是否有id为1的用户信息，缓存中有，直接从缓存中获取用户信息。 一级缓存测试 Mybatis 默认支持一级缓存，不需要在配置文件中配置。 所以我们直接按照上面的步骤进行测试： 123456789101112131415//一级缓存测试 @Test public void testCache1() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建UserMapper对象,mybatis自动生成代理对象 UserMapper userMapper = sqlSession.getMapper(UserMapper.class); //查询使用的是同一个session //第一次发起请求，查询Id 为1的用户信息 User user1 = userMapper.findUserById(1); System.out.println(user1); //第二次发起请求，查询Id 为1的用户信息 User user2 = userMapper.findUserById(1); System.out.println(user2); sqlSession.close(); &#125; 通过结果可以看出第二次没有发出sql查询请求， 所以我们需要在中间执行 commit 操作 123456789//如果sqlSession去执行commit操作（执行插入、更新、删除），// 清空SqlSession中的一级缓存，这样做的目的为了让缓存中存储的是最新的信息，避免脏读。//更新user1的信息，user1.setUsername(\"李飞\");//user1.setSex(\"男\");//user1.setAddress(\"北京\");userMapper.updateUserById(user1);//提交事务,才会去清空缓存sqlSession.commit(); 测试 一级缓存应用 正式开发，是将 mybatis 和 spring 进行整合开发，事务控制在 service 中。 一个 service 方法中包括很多 mapper 方法调用。 service{ //开始执行时，开启事务，创建SqlSession对象 //第一次调用mapper的方法findUserById(1) //第二次调用mapper的方法findUserById(1)，从一级缓存中取数据 //方法结束，sqlSession关闭 } 如果是执行两次service调用查询相同的用户信息，不走一级缓存，因为session方法结束，sqlSession就关闭，一级缓存就清空。 二级缓存原理 首先开启mybatis的二级缓存。 sqlSession1去查询用户id为1的用户信息，查询到用户信息会将查询数据存储到二级缓存中。 如果SqlSession3去执行相同 mapper下sql，执行commit提交，清空该 mapper下的二级缓存区域的数据。 sqlSession2去查询用户id为1的用户信息，去缓存中找是否存在数据，如果存在直接从缓存中取出数据。 二级缓存与一级缓存区别，二级缓存的范围更大，多个sqlSession可以共享一个UserMapper的二级缓存区域。 UserMapper有一个二级缓存区域（按namespace分） ，其它mapper也有自己的二级缓存区域（按namespace分）。 每一个namespace的mapper都有一个二缓存区域，两个mapper的namespace如果相同，这两个mapper执行sql查询到数据将存在相同的二级缓存区域中。 开启二级缓存： mybaits的二级缓存是mapper范围级别，除了在SqlMapConfig.xml设置二级缓存的总开关，还要在具体的mapper.xml中开启二级缓存 在 SqlMapConfig.xml 开启二级开关 12&lt;!-- 开启二级缓存 --&gt;&lt;setting name=\"cacheEnabled\" value=\"true\"/&gt; 然后在你的 Mapper 映射文件中添加一行： ，表示此 mapper 开启二级缓存。 调用 pojo 类实现序列化接口： 二级缓存需要查询结果映射的pojo对象实现java.io.Serializable接口实现序列化和反序列化操作（因为二级缓存数据存储介质多种多样，在内存不一样），注意如果存在父类、成员pojo都需要实现序列化接口。 12public class Orders implements Serializablepublic class User implements Serializable 测试 12345678910111213141516171819202122232425262728293031323334//二级缓存测试 @Test public void testCache2() throws Exception &#123; SqlSession sqlSession1 = sqlSessionFactory.openSession(); SqlSession sqlSession2 = sqlSessionFactory.openSession(); SqlSession sqlSession3 = sqlSessionFactory.openSession(); //创建UserMapper对象,mybatis自动生成代理对象 UserMapper userMapper1 = sqlSession1.getMapper(UserMapper.class); //sqlSession1 执行查询 写入缓存(第一次查询请求) User user1 = userMapper1.findUserById(1); System.out.println(user1); sqlSession1.close(); //sqlSession3 执行提交 清空缓存 UserMapper userMapper3 = sqlSession3.getMapper(UserMapper.class); User user3 = userMapper3.findUserById(1); user3.setSex(\"女\"); user3.setAddress(\"山东济南\"); user3.setUsername(\"崔建\"); userMapper3.updateUserById(user3); //提交事务，清空缓存 sqlSession3.commit(); sqlSession3.close(); //sqlSession2 执行查询(第二次查询请求) UserMapper userMapper2 = sqlSession2.getMapper(UserMapper.class); User user2 = userMapper2.findUserById(1); System.out.println(user2); sqlSession2.close(); &#125; 结果： useCache 配置 在 statement 中设置 useCache=false 可以禁用当前 select 语句的二级缓存，即每次查询都会发出sql去查询，默认情况是true，即该sql使用二级缓存。 1&lt;select id=\"findUserById\" parameterType=\"int\" resultType=\"user\" useCache=\"false\"&gt; 总结：针对每次查询都需要最新的数据sql，要设置成useCache=false，禁用二级缓存。 刷新缓存（清空缓存） 在mapper的同一个namespace中，如果有其它insert、update、delete操作数据后需要刷新缓存，如果不执行刷新缓存会出现脏读。 设置statement配置中的flushCache=”true” 属性，默认情况下为true即刷新缓存，如果改成false则不会刷新。使用缓存时如果手动修改数据库表中的查询数据会出现脏读。 如下： 1&lt;insert id=\"insetrUser\" parameterType=\"cn.zhisheng.mybatis.po.User\" flushCache=\"true\"&gt; 一般下执行完commit操作都需要刷新缓存，flushCache=true表示刷新缓存，这样可以避免数据库脏读。 Mybatis Cache参数flushInterval（刷新间隔）可以被设置为任意的正整数，而且它们代表一个合理的毫秒形式的时间段。默认情况是不设置，也就是没有刷新间隔，缓存仅仅调用语句时刷新。 size（引用数目）可以被设置为任意正整数，要记住你缓存的对象数目和你运行环境的可用内存资源数目。默认值是1024。 readOnly（只读）属性可以被设置为true或false。只读的缓存会给所有调用者返回缓存对象的相同实例。因此这些对象不能被修改。这提供了很重要的性能优势。可读写的缓存会返回缓存对象的拷贝（通过序列化）。这会慢一些，但是安全，因此默认是false。 如下例子： 1&lt;cache eviction=\"FIFO\" flushInterval=\"60000\" size=\"512\" readOnly=\"true\"/&gt; 这个更高级的配置创建了一个 FIFO 缓存,并每隔 60 秒刷新,存数结果对象或列表的 512 个引用,而且返回的对象被认为是只读的,因此在不同线程中的调用者之间修改它们会导致冲突。可用的收回策略有, 默认的是 LRU: LRU – 最近最少使用的:移除最长时间不被使用的对象。 FIFO – 先进先出:按对象进入缓存的顺序来移除它们。 SOFT – 软引用:移除基于垃圾回收器状态和软引用规则的对象。 WEAK – 弱引用:更积极地移除基于垃圾收集器状态和弱引用规则的对象。 Mybatis 整合 ehcacheehcache 是一个分布式缓存框架。 分布缓存 我们系统为了提高系统并发，性能、一般对系统进行分布式部署（集群部署方式） 不使用分布缓存，缓存的数据在各各服务单独存储，不方便系统 开发。所以要使用分布式缓存对缓存数据进行集中管理。 mybatis无法实现分布式缓存，需要和其它分布式缓存框架进行整合。 整合方法 mybatis 提供了一个二级缓存 cache 接口（org.apache.ibatis.cache 下的 Cache），如果要实现自己的缓存逻辑，实现cache接口开发即可。 12345678910import java.util.concurrent.locks.ReadWriteLock;public interface Cache &#123; String getId(); void putObject(Object var1, Object var2); Object getObject(Object var1); Object removeObject(Object var1); void clear(); int getSize(); ReadWriteLock getReadWriteLock();&#125; mybatis和ehcache整合，mybatis 和 ehcache 整合包中提供了一个 cache 接口的实现类(org.apache.ibatis.cache.impl 下的 PerpetualCache)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package org.apache.ibatis.cache.impl;import java.util.HashMap;import java.util.Map;import java.util.concurrent.locks.ReadWriteLock;import org.apache.ibatis.cache.Cache;import org.apache.ibatis.cache.CacheException;public class PerpetualCache implements Cache &#123; private String id; private Map&lt;Object, Object&gt; cache = new HashMap(); public PerpetualCache(String id) &#123; this.id = id; &#125; public String getId() &#123; return this.id; &#125; public int getSize() &#123; return this.cache.size(); &#125; public void putObject(Object key, Object value) &#123; this.cache.put(key, value); &#125; public Object getObject(Object key) &#123; return this.cache.get(key); &#125; public Object removeObject(Object key) &#123; return this.cache.remove(key); &#125; public void clear() &#123; this.cache.clear(); &#125; public ReadWriteLock getReadWriteLock() &#123; return null; &#125; public boolean equals(Object o) &#123; if(this.getId() == null) &#123; throw new CacheException(\"Cache instances require an ID.\"); &#125; else if(this == o) &#123; return true; &#125; else if(!(o instanceof Cache)) &#123; return false; &#125; else &#123; Cache otherCache = (Cache)o; return this.getId().equals(otherCache.getId()); &#125; &#125; public int hashCode() &#123; if(this.getId() == null) &#123; throw new CacheException(\"Cache instances require an ID.\"); &#125; else &#123; return this.getId().hashCode(); &#125; &#125;&#125; 通过实现 Cache 接口可以实现 mybatis 缓存数据通过其它缓存数据库整合，mybatis 的特长是sql操作，缓存数据的管理不是 mybatis 的特长，为了提高缓存的性能将 mybatis 和第三方的缓存数据库整合，比如 ehcache、memcache、redis等。 引入依赖包 ehcache-core-2.6.5.jar 和 mybatis-ehcache-1.0.2.jar 引入缓存配置文件 classpath下添加：ehcache.xml 内容如下： 1234567891011121314&lt;ehcache xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"http://ehcache.org/ehcache.xsd\"&gt; &lt;diskStore path=\"C:\\JetBrains\\IDEAProject\\ehcache\" /&gt; &lt;defaultCache maxElementsInMemory=\"1000\" maxElementsOnDisk=\"10000000\" eternal=\"false\" overflowToDisk=\"false\" timeToIdleSeconds=\"120\" timeToLiveSeconds=\"120\" diskExpiryThreadIntervalSeconds=\"120\" memoryStoreEvictionPolicy=\"LRU\"&gt; &lt;/defaultCache&gt;&lt;/ehcache&gt; 属性说明： diskStore：指定数据在磁盘中的存储位置。 defaultCache：当借助 CacheManager.add(“demoCache”) 创建Cache时，EhCache 便会采用指定的的管理策略 以下属性是必须的： maxElementsInMemory - 在内存中缓存的element的最大数目 maxElementsOnDisk - 在磁盘上缓存的element的最大数目，若是0表示无穷大 eternal - 设定缓存的elements是否永远不过期。如果为true，则缓存的数据始终有效，如果为false那么还要根据timeToIdleSeconds，timeToLiveSeconds判断 overflowToDisk- 设定当内存缓存溢出的时候是否将过期的element缓存到磁盘上 以下属性是可选的： timeToIdleSeconds - 当缓存在EhCache中的数据前后两次访问的时间超过timeToIdleSeconds的属性取值时，这些数据便会删除，默认值是0,也就是可闲置时间无穷大 timeToLiveSeconds - 缓存element的有效生命期，默认是0.,也就是element存活时间无穷大 diskSpoolBufferSizeMB 这个参数设置DiskStore(磁盘缓存)的缓存区大小.默认是30MB.每个Cache都应该有自己的一个缓冲区. diskPersistent- 在VM重启的时候是否启用磁盘保存EhCache中的数据，默认是false。 diskExpiryThreadIntervalSeconds - 磁盘缓存的清理线程运行间隔，默认是120秒。每个120s，相应的线程会进行一次EhCache中数据的清理工作 memoryStoreEvictionPolicy - 当内存缓存达到最大，有新的element加入的时候， 移除缓存中element的策略。默认是LRU（最近最少使用），可选的有LFU（最不常使用）和FIFO（先进先出） 开启ehcache缓存 EhcacheCache 是ehcache对Cache接口的实现；修改mapper.xml文件，在cache中指定EhcacheCache。 根据需求调整缓存参数： 123456789&lt;cache type=\"org.mybatis.caches.ehcache.EhcacheCache\" &gt; &lt;property name=\"timeToIdleSeconds\" value=\"3600\"/&gt; &lt;property name=\"timeToLiveSeconds\" value=\"3600\"/&gt; &lt;!-- 同ehcache参数maxElementsInMemory --&gt; &lt;property name=\"maxEntriesLocalHeap\" value=\"1000\"/&gt; &lt;!-- 同ehcache参数maxElementsOnDisk --&gt; &lt;property name=\"maxEntriesLocalDisk\" value=\"10000000\"/&gt; &lt;property name=\"memoryStoreEvictionPolicy\" value=\"LRU\"/&gt; &lt;/cache&gt; 测试 ：(这命中率就代表成功将ehcache 与 mybatis 整合了) 应用场景对于访问多的查询请求且用户对查询结果实时性要求不高，此时可采用 mybatis 二级缓存技术降低数据库访问量，提高访问速度，业务场景比如：耗时较高的统计分析sql、电话账单查询sql等。 实现方法如下：通过设置刷新间隔时间，由 mybatis 每隔一段时间自动清空缓存，根据数据变化频率设置缓存刷新间隔 flushInterval，比如设置为30分钟、60分钟、24小时等，根据需求而定。 局限性mybatis 二级缓存对细粒度的数据级别的缓存实现不好，比如如下需求：对商品信息进行缓存，由于商品信息查询访问量大，但是要求用户每次都能查询最新的商品信息，此时如果使用 mybatis 的二级缓存就无法实现当一个商品变化时只刷新该商品的缓存信息而不刷新其它商品的信息，因为 mybaits 的二级缓存区域以 mapper 为单位划分，当一个商品信息变化会将所有商品信息的缓存数据全部清空。解决此类问题需要在业务层根据需求对数据有针对性缓存。","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://yoursite.com/tags/Mybatis/"},{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://yoursite.com/tags/SpringMVC/"}]},{"title":"通过项目逐步深入了解Mybatis（三）","date":"2017-06-13T16:00:00.000Z","path":"2017/06/14/通过项目逐步深入了解Mybatis(三)/","text":"相关阅读：1、通过项目逐步深入了解Mybatis&lt;一&gt; 2、 通过项目逐步深入了解Mybatis&lt;二&gt; 本项目所有代码及文档都托管在 Github地址：https://github.com/zhisheng17/mybatis Mybatis 高级知识安排：对订单商品数据模型进行分析 订单商品数据模型 数据模型分析思路：1、每张表记录的数据内容（分模块对每张表记录的内容进行熟悉，相当于学习系统需求的过程） 2、每张表重要的的字段设置（非空字段、外键字段） 3、数据库级别表与表之间的关系（外键关系） 4、表与表业务之间的关系（要建立在每个业务意义的基础上去分析） 数据模型分析模型 用户表 user：记录购买商品的用户信息 订单表 order：记录用户所创建的订单(购买商品的订单) 订单明细表 orderdetail：（记录了订单的详细信息即购买商品的信息） 商品表 items：记录了商品信息 表与表业务之间的关系： 在分析表与表之间的业务关系时需要建立在某个业务意义基础上去分析。 先分析数据级别之间有关系的表之间的业务关系： 1、usre和orders： user —&gt; orders：一个用户可以创建多个订单，一对多 orders —&gt; user：一个订单只由一个用户创建，一对一 2、 orders和orderdetail： orders —&gt; orderdetail：一个订单可以包括 多个订单明细，因为一个订单可以购买多个商品，每个商品的购买信息在orderdetail记录，一对多关系 orderdetail —&gt; orders：一个订单明细只能包括在一个订单中，一对一 3、 orderdetail 和 itesm： orderdetail —&gt; itesms：一个订单明细只对应一个商品信息，一对一 items —&gt; orderdetail:一个商品可以包括在多个订单明细 ，一对多 再分析数据库级别没有关系的表之间是否有业务关系： 4、 orders 和 items： orders 和 items 之间可以通过 orderdetail 表建立 关系。 一对一查询需求：查询订单信息，关联查询创建订单的用户信息 使用 resultType sql 语句 确定查询的主表：订单表 确定查询的关联表：用户表 关联查询使用内链接？还是外链接？ 由于orders表中有一个外键（user_id），通过外键关联查询用户表只能查询出一条记录，可以使用内链接。 1SELECT orders.*, USER.username, USER.sex, USER.address FROM orders, USER WHERE orders.user_id = user.id 创建 pojo Orders.java 123456789101112public class Orders &#123; private Integer id; private Integer userId; private String number; private Date createtime; private String note; //用户信息 private User user; //订单明细 private List&lt;Orderdetail&gt; orderdetails; //getter and setter&#125; OrderCustom.java 1234567891011//通过此类映射订单和用户查询的结果，让此类继承包括 字段较多的pojo类public class OrdersCustom extends Orders&#123; //添加用户属性 /*USER.username, USER.sex, USER.address */ private String username; private String sex; private String address; //getter and setter&#125; 映射文件 OrdersMapperCustom.xml 1234&lt;!--查询订单关联查询用户信息--&gt; &lt;select id=\"findOrdersUser\" resultType=\"cn.zhisheng.mybatis.po.OrdersCustom\"&gt; SELECT orders.*, USER.username, USER.sex, USER.address FROM orders, USER WHERE orders.user_id = user.id &lt;/select&gt; Mapper 文件 OrdersMapperCustom.java 1234public interface OrdersMapperCustom&#123; public OrdersCustom findOrdersUser() throws Exception;&#125; 测试代码（记得在 SqlConfig.xml中添加载 OrdersMapperCustom.xml 文件） 1234567891011@Test public void testFindOrdersUser() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建OrdersMapperCustom对象,mybatis自动生成代理对象 OrdersMapperCustom ordersMapperCustom = sqlSession.getMapper(OrdersMapperCustom.class); //调用OrdersMapperCustom的方法 List&lt;OrdersCustom&gt; list = ordersMapperCustom.findOrdersUser(); System.out.println(list); sqlSession.close(); &#125; 测试结果 ​ ​ 使用 resultMap sql 语句（和上面的一致） 使用 resultMap 映射思路 使用 resultMap 将查询结果中的订单信息映射到 Orders 对象中，在 orders 类中添加 User 属性，将关联查询出来的用户信息映射到 orders 对象中的 user 属性中。 12//用户信息private User user; 映射文件 OrdersMapperCustom.xml 先定义 resultMap 1234567891011121314151617181920212223242526272829303132&lt;!--定义查询订单关联查询用户信息的resultMap 将整个查询结果映射到cn.zhisheng.mybatis.po.Orders --&gt; &lt;resultMap id=\"OrdersUserResultMap\" type=\"cn.zhisheng.mybatis.po.Orders\"&gt; &lt;!--配置映射的订单信息--&gt; &lt;!--id表示查询结果中的唯一标识 在这里是订单的唯一标识 如果是由多列组成的唯一标识，那么就需要配置多个id column：id 是订单信息中的唯一标识列 property：id 是订单信息唯一标识列所映射到orders中的id属性 最终resultMap对column和property做一个映射关系（对应关系） --&gt; &lt;id column=\"id\" property=\"id\"/&gt; &lt;result column=\"user_id\" property=\"userId\"/&gt; &lt;result column=\"number\" property=\"number\"/&gt; &lt;result column=\"createtime\" property=\"createtime\"/&gt; &lt;result column=\"note\" property=\"note\"/&gt; &lt;!--配置映射的关联用户信息 association 用于映射关联查询单个对象的信息 property 将要关联查询的用户信息映射到 orders中的属性中去 --&gt; &lt;association property=\"user\" javaType=\"cn.zhisheng.mybatis.po.User\"&gt; &lt;!--id 关联用户信息的唯一标识 column: 指定唯一标识用户的信息 property：映射到user的那个属性 --&gt; &lt;id column=\"user_id\" property=\"id\"/&gt; &lt;result column=\"username\" property=\"username\"/&gt; &lt;result column=\"sex\" property=\"sex\"/&gt; &lt;result column=\"address\" property=\"address\"/&gt; &lt;result column=\"birthday\" property=\"birthday\"/&gt; &lt;/association&gt; &lt;/resultMap&gt; 1234&lt;!--查询订单关联查询用户信息, 使用 resultMap--&gt; &lt;select id=\"findOrdersUserResultMap\" resultMap=\"OrdersUserResultMap\"&gt; SELECT orders.*, USER.username, USER.sex, USER.address FROM orders, USER WHERE orders.user_id = user.id &lt;/select&gt; Mapper 文件 1public List&lt;Orders&gt; findOrdersUserResultMap() throws Exception; 测试代码 1234567891011@Test public void testFindOrdersUserResultMap() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建OrdersMapperCustom对象,mybatis自动生成代理对象 OrdersMapperCustom ordersMapperCustom = sqlSession.getMapper(OrdersMapperCustom.class); //调用OrdersMapperCustom的方法 List&lt;Orders&gt; list = ordersMapperCustom.findOrdersUserResultMap(); System.out.println(list); sqlSession.close(); &#125; 测试结果 使用 resultType 和 resultMap 一对一查询小结 resultType：使用resultType实现较为简单，如果pojo中没有包括查询出来的列名，需要增加列名对应的属性，即可完成映射。如果没有查询结果的特殊要求建议使用resultType。 resultMap：需要单独定义resultMap，实现有点麻烦，如果对查询结果有特殊的要求，使用resultMap可以完成将关联查询映射pojo的属性中。resultMap可以实现延迟加载，resultType无法实现延迟加载。 一对多查询需求：查询订单及订单明细信息 SQL语句： 确定主查询表：订单表 确定关联查询表：订单明细表 在一对一查询基础上添加订单明细表关联即可。 12SELECT orders.*, USER.username, USER.sex, USER.address, orderdetail.id orderdetail_id, orderdetail.items_id, orderdetail.items_num, orderdetail.orders_id FROM orders, USER,orderdetail WHERE orders.user_id = user.id AND orderdetail.orders_id=orders.id 分析： 使用 resultType 将上边的查询结果映射到 pojo 中，订单信息的就是重复。 要求： 对 orders 映射不能出现重复记录。 在 orders.java 类中添加 List orderDetails 属性。 最终会将订单信息映射到 orders 中，订单所对应的订单明细映射到 orders 中的 orderDetails 属性中。 映射成的 orders 记录数为两条（orders信息不重复） 每个 orders 中的 orderDetails 属性存储了该订单所对应的订单明细。 映射文件： 首先定义 resultMap 1234567891011121314151617181920&lt;!--定义查询订单及订单明细信息的resultMap使用extends继承，不用在中配置订单信息和用户信息的映射--&gt; &lt;resultMap id=\"OrdersAndOrderDetailResultMap\" type=\"cn.zhisheng.mybatis.po.Orders\" extends=\"OrdersUserResultMap\"&gt; &lt;!-- 订单信息 --&gt; &lt;!-- 用户信息 --&gt; &lt;!-- 使用extends继承，不用在中配置订单信息和用户信息的映射 --&gt; &lt;!-- 订单明细信息 一个订单关联查询出了多条明细，要使用collection进行映射 collection：对关联查询到多条记录映射到集合对象中 property：将关联查询到多条记录映射到cn.zhisheng.mybatis.po.Orders哪个属性 ofType：指定映射到list集合属性中pojo的类型 --&gt; &lt;collection property=\"orderdetails\" ofType=\"cn.zhisheng.mybatis.po.Orderdetail\"&gt; &lt;!-- id：订单明细唯 一标识 property:要将订单明细的唯 一标识 映射到cn.zhisheng.mybatis.po.Orderdetail的哪个属性--&gt; &lt;id column=\"orderdetail_id\" property=\"id\"/&gt; &lt;result column=\"items_id\" property=\"itemsId\"/&gt; &lt;result column=\"items_num\" property=\"itemsNum\"/&gt; &lt;result column=\"orders_id\" property=\"ordersId\"/&gt; &lt;/collection&gt; &lt;/resultMap&gt; 12345&lt;!--查询订单及订单明细信息, 使用 resultMap--&gt; &lt;select id=\"findOrdersAndOrderDetailResultMap\" resultMap=\"OrdersAndOrderDetailResultMap\"&gt; SELECT orders.*, USER.username, USER.sex, USER.address, orderdetail.id orderdetail_id, orderdetail.items_id, orderdetail.items_num, orderdetail.orders_id FROM orders, USER,orderdetail WHERE orders.user_id = user.id AND orderdetail.orders_id=orders.id &lt;/select&gt; Mapper 文件 1public List&lt;Orders&gt; findOrdersAndOrderDetailResultMap() throws Exception; 测试文件 1234567891011@Test public void testFindOrdersAndOrderDetailResultMap() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建OrdersMapperCustom对象,mybatis自动生成代理对象 OrdersMapperCustom ordersMapperCustom = sqlSession.getMapper(OrdersMapperCustom.class); //调用OrdersMapperCustom的方法 List&lt;Orders&gt; list = ordersMapperCustom.findOrdersAndOrderDetailResultMap(); System.out.println(list); sqlSession.close(); &#125; 测试结果 总结：mybatis使用resultMap的collection对关联查询的多条记录映射到一个list集合属性中。 使用resultType实现：将订单明细映射到orders中的orderdetails中，需要自己处理，使用双重循环遍历，去掉重复记录，将订单明细放在orderdetails中。 多对多查询需求：查询用户及用户购买商品信息。 SQL语句： 查询主表是：用户表 关联表：由于用户和商品没有直接关联，通过订单和订单明细进行关联，所以关联表： orders、orderdetail、items 123SELECT orders.*, USER.username, USER.sex, USER.address, orderdetail.id orderdetail_id,orderdetail.items_id, orderdetail.items_num, orderdetail.orders_id, items.name items_name,items.detail items_detail, items.price items_price FROM orders, USER, orderdetail, items WHERE orders.user_id = user.id AND orderdetail.orders_id=orders.id AND orderdetail.items_id = items.id 映射思路： 将用户信息映射到 user 中。在 user 类中添加订单列表属性List orderslist，将用户创建的订单映射到orderslist在Orders中添加订单明细列表属性Listorderdetials，将订单的明细映射到orderdetials在OrderDetail中添加Items属性，将订单明细所对应的商品映射到Items 定义 resultMap：12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;!--定义查询用户及用户购买商品信息的 resultMap--&gt; &lt;resultMap id=\"UserAndItemsResultMap\" type=\"cn.zhisheng.mybatis.po.User\"&gt; &lt;!--用户信息--&gt; &lt;id column=\"user_id\" property=\"id\"/&gt; &lt;result column=\"username\" property=\"username\"/&gt; &lt;result column=\"sex\" property=\"sex\"/&gt; &lt;result column=\"birthday\" property=\"birthday\"/&gt; &lt;result column=\"address\" property=\"address\"/&gt; &lt;!--订单信息 一个用户对应多个订单，使用collection映射 --&gt; &lt;collection property=\"ordersList\" ofType=\"cn.zhisheng.mybatis.po.Orders\"&gt; &lt;id column=\"id\" property=\"id\"/&gt; &lt;result column=\"user_id\" property=\"userId\"/&gt; &lt;result column=\"number\" property=\"number\"/&gt; &lt;result column=\"createtime\" property=\"createtime\"/&gt; &lt;result column=\"note\" property=\"note\"/&gt; &lt;!-- 订单明细 一个订单包括 多个明细 --&gt; &lt;collection property=\"orderdetails\" ofType=\"cn.zhisheng.mybatis.po.Orderdetail\"&gt; &lt;id column=\"orderdetail_id\" property=\"id\"/&gt; &lt;result column=\"orders_id\" property=\"ordersId\"/&gt; &lt;result column=\"items_id\" property=\"itemsId\"/&gt; &lt;result column=\"items_num\" property=\"itemsNum\"/&gt; &lt;!-- 商品信息 一个订单明细对应一个商品 --&gt; &lt;association property=\"items\" javaType=\"cn.zhisheng.mybatis.po.Items\"&gt; &lt;id column=\"items_id\" property=\"id\"/&gt; &lt;result column=\"items_name\" property=\"name\"/&gt; &lt;result column=\"items_price\" property=\"price\"/&gt; &lt;result column=\"items_pic\" property=\"pic\"/&gt; &lt;result column=\"items_createtime\" property=\"createtime\"/&gt; &lt;result column=\"items_detail\" property=\"detail\"/&gt; &lt;/association&gt; &lt;/collection&gt; &lt;/collection&gt; &lt;/resultMap&gt; 映射文件12345&lt;!--查询用户及用户购买商品信息, 使用 resultMap--&gt; &lt;select id=\"findUserAndItemsResultMap\" resultMap=\"UserAndItemsResultMap\"&gt; SELECT orders.*, USER.username, USER.sex, USER.address, orderdetail.id orderdetail_id, orderdetail.items_id, orderdetail.items_num, orderdetail.orders_id FROM orders, USER,orderdetail WHERE orders.user_id = user.id AND orderdetail.orders_id=orders.id &lt;/select&gt; Mapper 文件1public List&lt;User&gt; findUserAndItemsResultMap() throws Exception; 测试文件1234567891011@Test public void testFindUserAndItemsResultMap() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建OrdersMapperCustom对象,mybatis自动生成代理对象 OrdersMapperCustom ordersMapperCustom = sqlSession.getMapper(OrdersMapperCustom.class); //调用OrdersMapperCustom的方法 List&lt;User&gt; list = ordersMapperCustom.findUserAndItemsResultMap(); System.out.println(list); sqlSession.close(); &#125; 测试： 我去，竟然报错了，但是不要怕，通过查看报错信息可以知道我忘记在 User.java 中加入 orderlist 属性了，接下来我加上去，并加上 getter 和 setter 方法。 12345678//用户创建的订单列表 private List&lt;Orders&gt; ordersList; public List&lt;Orders&gt; getOrdersList() &#123; return ordersList; &#125; public void setOrdersList(List&lt;Orders&gt; ordersList) &#123; this.ordersList = ordersList; &#125; 再次测试就能成功了。 多对多查询总结将查询用户购买的商品信息明细清单，（用户名、用户地址、购买商品名称、购买商品时间、购买商品数量） 针对上边的需求就使用resultType将查询到的记录映射到一个扩展的pojo中，很简单实现明细清单的功能。 一对多是多对多的特例，如下需求： 查询用户购买的商品信息，用户和商品的关系是多对多关系。 需求1： 查询字段：用户账号、用户名称、用户性别、商品名称、商品价格(最常见) 企业开发中常见明细列表，用户购买商品明细列表， 使用resultType将上边查询列映射到pojo输出。 需求2： 查询字段：用户账号、用户名称、购买商品数量、商品明细（鼠标移上显示明细） 使用resultMap将用户购买的商品明细列表映射到user对象中。 总结： 使用resultMap是针对那些对查询结果映射有特殊要求的功能，，比如特殊要求映射成list中包括多个list。 ResultMap 总结resultType：作用： 将查询结果按照sql列名pojo属性名一致性映射到pojo中。 场合： 常见一些明细记录的展示，比如用户购买商品明细，将关联查询信息全部展示在页面时，此时可直接使用resultType将每一条记录映射到pojo中，在前端页面遍历list（list中是pojo）即可。 resultMap： 使用association和collection完成一对一和一对多高级映射（对结果有特殊的映射要求）。 association：作用： 将关联查询信息映射到一个pojo对象中。 场合： 为了方便查询关联信息可以使用association将关联订单信息映射为用户对象的pojo属性中，比如：查询订单及关联用户信息。使用resultType无法将查询结果映射到pojo对象的pojo属性中，根据对结果集查询遍历的需要选择使用resultType还是resultMap。 collection：作用： 将关联查询信息映射到一个list集合中。 场合： 为了方便查询遍历关联信息可以使用collection将关联信息映射到list集合中，比如：查询用户权限范围模块及模块下的菜单，可使用collection将模块映射到模块list中，将菜单列表映射到模块对象的菜单list属性中，这样的作的目的也是方便对查询结果集进行遍历查询。如果使用resultType无法将查询结果映射到list集合中。","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://yoursite.com/tags/Mybatis/"},{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://yoursite.com/tags/SpringMVC/"}]},{"title":"Spring MVC系列文章（二）：Spring MVC+Hibernate JPA搭建的博客系统项目中所遇到的坑","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Spring MVC+Hibernate JPA搭建的博客系统项目中所遇到的坑/","text":"项目代码地址：https://github.com/zhisheng17/springmvc最近在学习 Spring MVC ，其中在做一个简单的博客系统demo，是使用 SpringMVC 集成 Spring Data JPA（由 Hibernate JPA 提供），来进行强大的数据库访问。结果其中遇到的坑不 是一点点啊，我差点崩溃了，其中最大的原因就是由于 Hibernate JPA 中的bug了，反正一开始 还不知道是这个问题，导致折腾了快一天的时间。想想都可怕啊。 mvc-dispatch-servlet.xml代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:mvc=\"http://www.springframework.org/schema/mvc\" xmlns:jpa=\"http://www.springframework.org/schema/data/jpa\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/data/jpa http://www.springframework.org/schema/data/jpa/spring-jpa.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd\"&gt; &lt;!--指明 controller 所在包，并扫描其中的注解--&gt; &lt;context:component-scan base-package=\"cn.zhisheng.controller\"/&gt; &lt;!-- 静态资源(js、image等)的访问 --&gt; &lt;mvc:default-servlet-handler/&gt; &lt;!-- 开启注解 --&gt; &lt;mvc:annotation-driven/&gt; &lt;!--ViewResolver 视图解析器--&gt; &lt;!--用于支持Servlet、JSP视图解析--&gt; &lt;bean id=\"jspViewResolver\" class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"&gt; &lt;property name=\"viewClass\" value=\"org.springframework.web.servlet.view.JstlView\"/&gt; &lt;property name=\"prefix\" value=\"/WEB-INF/pages/\"/&gt; &lt;property name=\"suffix\" value=\".jsp\"/&gt; &lt;/bean&gt; &lt;!-- 表示JPA Repository所在的包 --&gt; &lt;jpa:repositories base-package=\"cn.zhisheng.repository\"/&gt; &lt;bean id=\"entityManagerFactory\" class=\"org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean\"&gt; &lt;property name=\"persistenceUnitName\" value=\"defaultPersistenceUnit\"/&gt; &lt;property name=\"packagesToScan\" value=\"cn.zhisheng.model\" /&gt; &lt;property name=\"jpaVendorAdapter\"&gt; &lt;bean class=\"org.springframework.orm.jpa.vendor.HibernateJpaVendorAdapter\"/&gt; &lt;/property&gt; &lt;property name=\"jpaProperties\"&gt; &lt;props&gt; &lt;prop key=\"hibernate.connection.driver_class\"&gt;com.mysql.jdbc.Driver&lt;/prop&gt; &lt;prop key=\"hibernate.connection.url\"&gt;jdbc:mysql://localhost:3306/springdemo?useSSL=false&lt;/prop&gt; &lt;prop key=\"hibernate.connection.username\"&gt;root&lt;/prop&gt; &lt;prop key=\"hibernate.connection.password\"&gt;root&lt;/prop&gt; &lt;prop key=\"hibernate.show_sql\"&gt;false&lt;/prop&gt; &lt;prop key=\"hibernate.connection.useUnicode\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.connection.characterEncoding\"&gt;UTF-8&lt;/prop&gt; &lt;prop key=\"hibernate.format_sql\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.use_sql_comments\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.hbm2ddl.auto\"&gt;update&lt;/prop&gt; &lt;prop key=\"hibernate.connection.autoReconnect\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.dialect\"&gt;org.hibernate.dialect.MySQL5Dialect&lt;/prop&gt; &lt;prop key=\"connection.autoReconnectForPools\"&gt;true&lt;/prop&gt; &lt;prop key=\"connection.is-connection-validation-required\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.validate\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.connection.provider_class\"&gt;org.hibernate.service.jdbc.connections.internal.C3P0ConnectionProvider&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.min_size\"&gt;5&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.max_size\"&gt;600&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.timeout\"&gt;1800&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.max_statements\"&gt;50&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.preferredTestQuery\"&gt;SELECT 1;&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.testConnectionOnCheckout\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.idle_test_period\"&gt;3000&lt;/prop&gt; &lt;prop key=\"javax.persistence.validation.mode\"&gt;none&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- 事务管理 --&gt; &lt;bean id=\"transactionManager\" class=\"org.springframework.orm.jpa.JpaTransactionManager\"&gt; &lt;property name=\"entityManagerFactory\" ref=\"entityManagerFactory\"/&gt; &lt;/bean&gt; &lt;!-- 开启事务管理注解 --&gt; &lt;tx:annotation-driven transaction-manager=\"transactionManager\"/&gt;&lt;/beans&gt; pom.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.zhisheng&lt;/groupId&gt; &lt;artifactId&gt;springmvc&lt;/artifactId&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;name&gt;springmvc Maven Webapp&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;spring.version&gt;4.2.6.RELEASE&lt;/spring.version&gt; &lt;hibernate.version&gt;5.1.0.Final&lt;/hibernate.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-jpa&lt;/artifactId&gt; &lt;version&gt;1.10.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-entitymanager&lt;/artifactId&gt; &lt;version&gt;$&#123;hibernate.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-c3p0&lt;/artifactId&gt; &lt;version&gt;$&#123;hibernate.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.mchange&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.39&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;springmvc&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 一开始我是用默认的在resources文件里面生成了persistence.xml配置文件进行数据库配置的，后来由于用那种方法，碰到的问题有很多，自己搞了好几个个小时都没弄好，只好换种方法，没想到竟然还是这种效果（泪崩），看来是不治标也不治本。 无奈，只好硬刚了，碰到错误，百度+google，看了大量的的解决方法，都是没用，慢慢的我所加的jar包越来越多，用maven管理的依赖的也变得多起来了，但终究是不能够解决问题的。 其实这时我看了这么多的博客和解决方法，我已经知道了是 Hibernate JPA 的bug问题，途中自己也换了一些版本，还是没能解决办法。 最后在吃完完晚饭后，又折腾了快三小时，终于找到可靠有用的解决方案了。 运行成功后，我当时就激动起来了。马丹，老子终于将你解决了。 所以在这里立马就将自己这次的血崩历史纪录下来。 下面写下遇到的问题：（其中有些可能还不记得写了） java.lang.ClassNotFoundException: javax.persistence.EntityManager java.lang.NoSuchMethodError: javax.persistence.JoinColumn.foreignKey()Ljavax/persistence/ForeignKey; javax.persistence.PersistenceException: No Persistence provider for EntityManager named defaultPersistenceUnit javax.persistence.PersistenceException: No Persistence provider for EntityManager named defaultPersi java.lang.NoClassDefFoundError: org/hibernate/ejb/HibernatePersistence java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory java.lang.ClassNotFoundException: org.hibernate.MappingException NoSuchMethodError: javax.persistence.xxx 等，还有几个，忘记了。。 首先通过报错信息可以知道有些是因为jar包的问题，但是并不是光是缺少jar包的问题，很大的原 因就是因为jar包的版本不同，刚好那个jar包又是有问题的（自身有bug）。 就比如错误： java.lang.NoSuchMethodError: javax.persistence.JoinColumn.foreignKey()Ljavax/persistence/ForeignKey; 就是因为JAVAEE6.0中的 javax.persistence.jar与 hibernate4.3.8中的hibernate-jpa-2.1-api-1.0.0.Final.jar冲突 JoinColumn.foreignKey() was introduced with JPA 2.1, which was not implemented by Hibernate 4 until version 4.3. If you’re using an older version of Hibernate 4 then try upgrading to 4.3.x. If you’re already using Hibernate 4.3 then make sure you’re also using JPA 2.1 to make sure the API and implementation match up. 图片来自 : http://stackoverflow.com/questions/24588860/error-javax-persistence-joincolumn-foreignkeyljavax-persistence-foreignkey-wi I finally solved this similar problem, there was an old version(hibernate-jpa-2.0-api-1.0.0-Final.jar) in my lib folder which I guess has been preventing maven dependency from loading. So after I manually deleted it and added (hibernate-jpa-2.1-api-1.0.0-Final.jar) everything started to work. 意思大概就是： 因为JAVAEE6.0中的 javax.persistence.jar与 hibernate4.3.8中的hibernate-jpa-2.1-api-1.0.0.Final.jar冲突 ，我们在pom文件下添加依赖后，竟然没发现在 springmvc（项目名称）\\target\\springmvc（项目名称）\\WEB-INF\\lib 下看到 javax.persistence.jar 文件，结果竟然在 springmvc\\lib下找到他了。 解决办法就是在 pom文件和 mvc-dispatcher-servlet.xml 都配置好的情况下，将 springmvc\\lib下的 javax.persistence.jar 删除。 最后再说一句：Though the error drove almost crazy, hold on, you wil get smile ！ Fighting","tags":[{"name":"Bootstrap","slug":"Bootstrap","permalink":"http://yoursite.com/tags/Bootstrap/"},{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"},{"name":"Spring","slug":"Spring","permalink":"http://yoursite.com/tags/Spring/"},{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://yoursite.com/tags/Spring-MVC/"},{"name":"Hibernate JPA","slug":"Hibernate-JPA","permalink":"http://yoursite.com/tags/Hibernate-JPA/"}]},{"title":"Spring MVC系列文章（一）：Spring MVC + Hibernate JPA + Bootstrap 搭建的博客系统 Demo","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Spring MVC + Hibernate JPA + Bootstrap 搭建的博客系统/","text":"相关阅读：1、Spring MVC+Hibernate JPA+ Bootstrap 搭建的博客系统项目中所遇到的坑 由于整个系统不是很难，这里就不详细介绍了，我相信看源码的话，应该能够看得懂。 源码地址：https://github.com/zhisheng17/springmvc数据库：springdemo.sql 下面给出下整个系统的截图吧，觉得不错，可以给个 star ，哈哈！后续继续在这个项目中加入新的项目。 截图：首页 用户管理模块 用户列表 添加用户 用户信息详情 更新用户信息 删除用户 博客管理模块 博客列表 博客详情 添加博客 更新博客 删除博客","tags":[{"name":"Bootstrap","slug":"Bootstrap","permalink":"http://yoursite.com/tags/Bootstrap/"},{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"},{"name":"Spring","slug":"Spring","permalink":"http://yoursite.com/tags/Spring/"},{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://yoursite.com/tags/Spring-MVC/"},{"name":"Hibernate JPA","slug":"Hibernate-JPA","permalink":"http://yoursite.com/tags/Hibernate-JPA/"}]},{"title":"Python爬虫实战之爬取百度贴吧帖子","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Python爬虫实战之爬取百度贴吧帖子/","text":"大家好，上次我们实验了爬取了糗事百科的段子，那么这次我们来尝试一下爬取百度贴吧的帖子。与上一篇不同的是，这次我们需要用到文件的相关操作。 本篇目标 对百度贴吧的任意帖子进行抓取 指定是否只抓取楼主发帖内容 将抓取到的内容分析并保存到文件 1. URL格式的确定首先，我们先观察一下百度贴吧的任意一个帖子。 比如：http://tieba.baidu.com/p/3138733512?see_lz=1&amp;pn=1，这是一个关于NBA50大的盘点，分析一下这个地址。 http:// 代表资源传输使用http协议 tieba.baidu.com 是百度的二级域名，指向百度贴吧的服务器。 /p/3138733512 是服务器某个资源，即这个帖子的地址定位符 see_lz 和 pn 是该 URL 的两个参数，分别代表了只看楼主和帖子页码，等于1表示该条件为真 所以我们可以把URL分为两部分，一部分为基础部分，一部分为参数部分。 例如，上面的URL我们划分基础部分是 http://tieba.baidu.com/p/3138733512，参数部分是 ?see_lz=1&amp;pn=1 2. 页面的抓取熟悉了URL的格式，那就让我们用urllib2库来试着抓取页面内容吧。上一篇糗事百科我们最后改成了面向对象的编码方式，这次我们直接尝试一下，定义一个类名叫BDTB(百度贴吧)，一个初始化方法，一个获取页面的方法。 其中，有些帖子我们想指定给程序是否要只看楼主，所以我们把只看楼主的参数初始化放在类的初始化上，即init方法。另外，获取页面的方法我们需要知道一个参数就是帖子页码，所以这个参数的指定我们放在该方法中。 综上，我们初步构建出基础代码如下： 1234567891011121314151617181920212223242526#-*-coding:utf8-*-#created by 10412import urllibimport urllib2import re#百度贴吧爬虫类class BDTB: #初始化，传入基地址，是否只看楼主的参数 def __init__(self, baseUrl, seeLZ): self.baseURL = baseUrl self.seeLZ = &apos;?see_lz=&apos; + str(seeLZ) #传入页码，获取该页帖子的代码 def getPage(self, pageNum): try: url = self.baseURL + self.seeLZ + &apos;&amp;pn=&apos; + str(pageNum) request = urllib2.Request(url) response = urllib2.urlopen(request) print response.read() return response except urllib2.URLError, e: if hasattr(e, &quot;reason&quot;): print u&quot;连接百度贴吧失败,错误原因&quot;,e.reason return NonebaseURL = &apos;http://tieba.baidu.com/p/3138733512&apos;bdtb = BDTB(baseURL, 1)bdtb.getPage(1) 运行代码，我们可以看到屏幕上打印出了这个帖子第一页楼主发言的所有内容，形式为HTML代码。 3. 提取相关信息1)提取帖子标题在浏览器中审查元素，或者按F12，查看页面源代码，我们找到标题所在的代码段如下: 1&lt;h3 class=\"core_title_txt pull-left text-overflow \" title=\"纯原创我心中的NBA2014-2015赛季现役50大\" style=\"width: 416px\"&gt;纯原创我心中的NBA2014-2015赛季现役50大&lt;/h3&gt; 所以我们要提取 &lt;h3&gt; 中的内容，因为一开始可以查看整个界面的原代码，查看里面含有 &lt;h3&gt;标签的不止一个。所以需要写正则表达式来匹配，如下： 1&lt;h3 class=&quot;core_title_txt.*?&gt;(.*?)&lt;/h3&gt; 然后，我们可以写个获取标题的方法 12345678910# 获取帖子标题 def getTitle(self): page = self.getPage(1) pattern = re.compile(&apos;&lt;h3 class=&quot;core_title_txt.*?&gt;(.*?)&lt;/h3&gt;&apos;, re.S) result = re.search(pattern, page) if result: # print result.group(1) #测试输出 return result.group(1).strip() else: return None 2）提取帖子页数同样地，帖子总页数我们也可以通过分析页面中的共?页来获取。 1&lt;li class=\"l_reply_num\" style=\"margin-left:8px\"&gt;&lt;span class=\"red\" style=\"margin-right:3px\"&gt;4784&lt;/span&gt;回复贴，共&lt;span class=\"red\"&gt;36&lt;/span&gt;页&lt;/li&gt; 所以我们的获取总页数的方法如下 12345678910#获取帖子一共有多少页def getPageNum(self): page = self.getPage(1) pattern = re.compile('&lt;li class=\"l_reply_num.*?&lt;/span&gt;.*?&lt;span.*?&gt;(.*?)&lt;/span&gt;',re.S) result = re.search(pattern,page) if result: #print result.group(1) #测试输出 return result.group(1).strip() else: return None 3）提取正文内容审查元素，可以看到百度贴吧每一层楼的主要内容都在标签里面，所以我们可以写如下的正则表达式 1&lt;div id=\"post_content_.*?&gt;(.*?)&lt;/div&gt; 所以提取正文内容的方法： 123456#获取每一层楼的内容,传入页面内容def getContent(self,page): pattern = re.compile('&lt;div id=\"post_content_.*?&gt;(.*?)&lt;/div&gt;',re.S) items = re.findall(pattern,page) for item in items: print item 运行截图如下： 可以看到有很多的换行符和图片符，既然出现这样的情况，那肯定不是我们想要的结果。那我们就必须要将文本进行处理，将各种复杂的标签给剔除，还原帖子的原来面貌。可以使用一个方法或者类将这个处理文本的实现，不过为了更好的代码重用和架构，还是建议使用一个类。 我们将这个类命名为Too（工具类），里面定义一个replace方法，替换各种标签。然后在类中定义几个正则表达式，利用re.sub方法对文本进行匹配后然后替换。 123456789101112131415161718192021222324252627import re#处理页面标签类class Tool: #去除img标签,7位长空格 removeImg = re.compile('&lt;img.*?&gt;| &#123;7&#125;|') #删除超链接标签 removeAddr = re.compile('&lt;a.*?&gt;|&lt;/a&gt;') #把换行的标签换为\\n replaceLine = re.compile('&lt;tr&gt;|&lt;div&gt;|&lt;/div&gt;|&lt;/p&gt;') #将表格制表&lt;td&gt;替换为\\t replaceTD= re.compile('&lt;td&gt;') #把段落开头换为\\n加空两格 replacePara = re.compile('&lt;p.*?&gt;') #将换行符或双换行符替换为\\n replaceBR = re.compile('&lt;br&gt;&lt;br&gt;|&lt;br&gt;') #将其余标签剔除 removeExtraTag = re.compile('&lt;.*?&gt;') def replace(self,x): x = re.sub(self.removeImg,\"\",x) x = re.sub(self.removeAddr,\"\",x) x = re.sub(self.replaceLine,\"\\n\",x) x = re.sub(self.replaceTD,\"\\t\",x) x = re.sub(self.replacePara,\"\\n \",x) x = re.sub(self.replaceBR,\"\\n\",x) x = re.sub(self.removeExtraTag,\"\",x) #strip()将前后多余内容删除 return x.strip() 在使用时，我们只需要初始化一下这个类，然后调用replace方法即可。 现在整体代码是如下这样子的，现在我的代码是写到这样子的: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#-*-coding:utf8-*-#created by 10412import urllibimport urllib2import re# 处理页面标签类class Tool: # 去除img标签,7位长空格 removeImg = re.compile('&lt;img.*?&gt;| &#123;7&#125;|') # 删除超链接标签 removeAddr = re.compile('&lt;a.*?&gt;|&lt;/a&gt;') # 把换行的标签换为\\n replaceLine = re.compile('&lt;tr&gt;|&lt;div&gt;|&lt;/div&gt;|&lt;/p&gt;') # 将表格制表&lt;td&gt;替换为\\t replaceTD = re.compile('&lt;td&gt;') # 把段落开头换为\\n加空两格 replacePara = re.compile('&lt;p.*?&gt;') # 将换行符或双换行符替换为\\n replaceBR = re.compile('&lt;br&gt;&lt;br&gt;|&lt;br&gt;') # 将其余标签剔除 removeExtraTag = re.compile('&lt;.*?&gt;') def replace(self, x): x = re.sub(self.removeImg, \"\", x) x = re.sub(self.removeAddr, \"\", x) x = re.sub(self.replaceLine, \"\\n\", x) x = re.sub(self.replaceTD, \"\\t\", x) x = re.sub(self.replacePara, \"\\n \", x) x = re.sub(self.replaceBR, \"\\n\", x) x = re.sub(self.removeExtraTag, \"\", x) # strip()将前后多余内容删除 return x.strip()# 百度贴吧爬虫类class BDTB: # 初始化，传入基地址，是否只看楼主的参数 def __init__(self, baseUrl, seeLZ): self.baseURL = baseUrl self.seeLZ = '?see_lz=' + str(seeLZ) self.tool = Tool() # 传入页码，获取该页帖子的代码 def getPage(self, pageNum): try: url = self.baseURL + self.seeLZ + '&amp;pn=' + str(pageNum) request = urllib2.Request(url) response = urllib2.urlopen(request) return response.read().decode('utf-8') except urllib2.URLError, e: if hasattr(e, \"reason\"): print u\"连接百度贴吧失败,错误原因\", e.reason return None # 获取帖子标题 def getTitle(self): page = self.getPage(1) pattern = re.compile('&lt;h1 class=\"core_title_txt.*?&gt;(.*?)&lt;/h1&gt;', re.S) result = re.search(pattern, page) if result: # print result.group(1) #测试输出 return result.group(1).strip() else: return None # 获取帖子一共有多少页 def getPageNum(self): page = self.getPage(1) pattern = re.compile('&lt;li class=\"l_reply_num.*?&lt;/span&gt;.*?&lt;span.*?&gt;(.*?)&lt;/span&gt;', re.S) result = re.search(pattern, page) if result: # print result.group(1) #测试输出 return result.group(1).strip() else: return None # 获取每一层楼的内容,传入页面内容 def getContent(self, page): pattern = re.compile('&lt;div id=\"post_content_.*?&gt;(.*?)&lt;/div&gt;', re.S) items = re.findall(pattern, page) # for item in items: # print item print self.tool.replace(items[1])baseURL = 'http://tieba.baidu.com/p/3138733512'bdtb = BDTB(baseURL, 1)bdtb.getContent(bdtb.getPage(1)) 运行截图如下： 4）替换楼层至于这个问题，我感觉直接提取楼层没什么必要呀，因为只看楼主的话，有些楼层的编号是间隔的，所以我们得到的楼层序号是不连续的，这样我们保存下来也没什么用。 所以可以尝试下面的方法： 1.每打印输出一段楼层，写入一行横线来间隔，或者换行符也好。 2.试着重新编一个楼层，按照顺序，设置一个变量，每打印出一个结果变量加一，打印出这个变量当做楼层。 将getContent方法修改如下： 123456789#获取每一层楼的内容,传入页面内容def getContent(self,page): pattern = re.compile('&lt;div id=\"post_content_.*?&gt;(.*?)&lt;/div&gt;',re.S) items = re.findall(pattern,page) floor = 1 for item in items: print floor,u\"楼------------------------------------------------------------------------------------------------------------------------------------\\n\" print self.tool.replace(item) floor += 1 运行结果截图如下： 4. 写入文件代码： 12file = open(“tb.txt”,”w”)file.writelines(obj) 5. 完善代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134#-*-coding:utf8-*-#created by 10412import urllibimport urllib2import re#处理页面标签类class Tool: #去除img标签,7位长空格 removeImg = re.compile('&lt;img.*?&gt;| &#123;7&#125;|') #删除超链接标签 removeAddr = re.compile('&lt;a.*?&gt;|&lt;/a&gt;') #把换行的标签换为\\n replaceLine = re.compile('&lt;tr&gt;|&lt;div&gt;|&lt;/div&gt;|&lt;/p&gt;') #将表格制表&lt;td&gt;替换为\\t replaceTD= re.compile('&lt;td&gt;') #把段落开头换为\\n加空两格 replacePara = re.compile('&lt;p.*?&gt;') #将换行符或双换行符替换为\\n replaceBR = re.compile('&lt;br&gt;&lt;br&gt;|&lt;br&gt;') #将其余标签剔除 removeExtraTag = re.compile('&lt;.*?&gt;') def replace(self,x): x = re.sub(self.removeImg,\"\",x) x = re.sub(self.removeAddr,\"\",x) x = re.sub(self.replaceLine,\"\\n\",x) x = re.sub(self.replaceTD,\"\\t\",x) x = re.sub(self.replacePara,\"\\n \",x) x = re.sub(self.replaceBR,\"\\n\",x) x = re.sub(self.removeExtraTag,\"\",x) #strip()将前后多余内容删除 return x.strip()#百度贴吧爬虫类class BDTB: #初始化，传入基地址，是否只看楼主的参数 def __init__(self,baseUrl,seeLZ,floorTag): #base链接地址 self.baseURL = baseUrl #是否只看楼主 self.seeLZ = '?see_lz='+str(seeLZ) #HTML标签剔除工具类对象 self.tool = Tool() #全局file变量，文件写入操作对象 self.file = None #楼层标号，初始为1 self.floor = 1 #默认的标题，如果没有成功获取到标题的话则会用这个标题 self.defaultTitle = u\"百度贴吧\" #是否写入楼分隔符的标记 self.floorTag = floorTag #传入页码，获取该页帖子的代码 def getPage(self,pageNum): try: #构建URL url = self.baseURL+ self.seeLZ + '&amp;pn=' + str(pageNum) request = urllib2.Request(url) response = urllib2.urlopen(request) #返回UTF-8格式编码内容 return response.read().decode('utf-8') #无法连接，报错 except urllib2.URLError, e: if hasattr(e,\"reason\"): print u\"连接百度贴吧失败,错误原因\",e.reason return None #获取帖子标题 def getTitle(self,page): #得到标题的正则表达式 pattern = re.compile('&lt;h1 class=\"core_title_txt.*?&gt;(.*?)&lt;/h1&gt;',re.S) result = re.search(pattern,page) if result: #如果存在，则返回标题 return result.group(1).strip() else: return None #获取帖子一共有多少页 def getPageNum(self,page): #获取帖子页数的正则表达式 pattern = re.compile('&lt;li class=\"l_reply_num.*?&lt;/span&gt;.*?&lt;span.*?&gt;(.*?)&lt;/span&gt;',re.S) result = re.search(pattern,page) if result: return result.group(1).strip() else: return None #获取每一层楼的内容,传入页面内容 def getContent(self,page): #匹配所有楼层的内容 pattern = re.compile('&lt;div id=\"post_content_.*?&gt;(.*?)&lt;/div&gt;',re.S) items = re.findall(pattern,page) contents = [] for item in items: #将文本进行去除标签处理，同时在前后加入换行符 content = \"\\n\"+self.tool.replace(item)+\"\\n\" contents.append(content.encode('utf-8')) return contents def setFileTitle(self,title): #如果标题不是为None，即成功获取到标题 if title is not None: self.file = open(title + \".txt\",\"w+\") else: self.file = open(self.defaultTitle + \".txt\",\"w+\") def writeData(self,contents): #向文件写入每一楼的信息 for item in contents: if self.floorTag == '1': #楼之间的分隔符 floorLine = \"\\n\" + str(self.floor) + u\"-----------------------------------------------------------------------------------------\\n\" self.file.write(floorLine) self.file.write(item) self.floor += 1 def start(self): indexPage = self.getPage(1) pageNum = self.getPageNum(indexPage) title = self.getTitle(indexPage) self.setFileTitle(title) if pageNum == None: print \"URL已失效，请重试\" return try: print \"该帖子共有\" + str(pageNum) + \"页\" for i in range(1,int(pageNum)+1): print \"正在写入第\" + str(i) + \"页数据\" page = self.getPage(i) contents = self.getContent(page) self.writeData(contents) #出现写入异常 except IOError,e: print \"写入异常，原因\" + e.message finally: print \"写入任务完成\"print u\"请输入帖子代号\"baseURL = 'http://tieba.baidu.com/p/' + str(raw_input(u'http://tieba.baidu.com/p/'))seeLZ = raw_input(\"是否只获取楼主发言，是输入1，否输入0\\n\")floorTag = raw_input(\"是否写入楼层信息，是输入1，否输入0\\n\")bdtb = BDTB(baseURL,seeLZ,floorTag)bdtb.start() 运行后截图如下： 备注： 运行后注意输入帖子的代号先在网址后空格，再输入帖子代号，输入完再把刚才的空格 删除，只有这样才不会报错。 Traceback (most recent call last):File “E:/python/code/PycharmProject/Python-Projects/baidutieba/BDTB3.py”, line 149,in &lt; module &gt; bdtb.start()File “E:/python/code/PycharmProject/Python-Projects/baidutieba/BDTB3.py”, line 123, in startpageNum = self.getPageNum(indexPage)File “E:/python/code/PycharmProject/Python-Projects/baidutieba/BDTB3.py”, line 86, in getPageNumresult = re.search(pattern,page)File “C:\\Python27\\lib\\re.py”, line 146, in searchreturn _compile(pattern, flags).search(string)TypeError: expected string or buffer","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"Python爬虫实战之爬取糗事百科段子","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Python爬虫实战之爬取糗事百科段子/","text":"完整代码地址：Python爬虫实战之爬取糗事百科段子程序代码详解： Spider1-qiushibaike.py：爬取糗事百科的8小时最新页的段子。包含的信息有作者名称，觉得好笑人数，评论人数，发布的内容。如果发布的内容中含有图片的话，则过滤图片，内容依然显示出来。 Spider2-qiushibaike.py：在Spider1-qiushibaike.py基础上，引入类和方法，进行优化和封装，爬取糗事百科的24小时热门页的段子。进一步优化，每按一次回车更新一条内容，当前页的内容抓取完毕后，自动抓取下一页的内容，按‘q’退出。 Spider3-qiushibaike.py：在Spiders-qiushibaike.py基础上，爬取了百科段子的评论。按C查看当前这个糗事的评论，当切换到查看评论时，换回车显示下一个评论,按Q退出回到查看糗事。糗事段子页数是一页一页加载的，如果你已经看完所有的糗事，就会自动退出！ 本爬虫目标： 抓取糗事百科热门段子 过滤带有图片的段子 实现每按一次回车显示一个段子的发布时间，发布人，段子内容，点赞数，评论人数。 糗事百科是不需要登录的，所以也没必要用到Cookie，另外糗事百科有的段子是附图的，我们把图抓下来图片不便于显示，那么我们就尝试过滤掉有图的段子吧。 好，现在我们尝试抓取一下糗事百科的热门段子吧，每按下一次回车我们显示一个段子。 1.确定URL并抓取页面代码首先我们确定好页面的URL是 http://www.qiushibaike.com/hot/page/1，其中最后一个数字1代表页数，我们可以传入不同的值来获得某一页的段子内容。 2.提取某一页的所有段子好，获取了HTML代码之后，我们开始分析怎样获取某一页的所有段子。 首先我们审查元素看一下，按浏览器的F12，截图如下: 我们可以看到，每一个段子都是 &lt;div class=”article block untagged mb15″ id=”…”&gt;…&lt;/div&gt; 包裹的内容。 现在我们想获取发布人，发布日期，段子内容，点赞人数和评论人数。不过另外注意的是，段子有些是带图片的，如果我们想在控制台显示图片是不现实的，所以我们直接把带有图片的段子给它剔除掉，只保存仅含文本的段子。 所以我们加入如下正则表达式来匹配一下，用到的方法是 re.findall 是找寻所有匹配的内容。方法的用法详情可以看前面说的正则表达式的介绍。 好，我们的正则表达式匹配语句书写如下，在原来的基础上追加如下代码： 123456789#正则表达式匹配 pattern = re.compile(&apos;&lt;div.*?author.*?&gt;.*?&lt;img.*?&gt;.*?&lt;h2&gt;(.*?)&lt;/h2&gt;.*?&lt;div.*?&apos;+ &apos;content&quot;&gt;(.*?)&lt;/div&gt;(.*?)&lt;div.*?class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;.*?class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;&apos;,re.S) items = re.findall(pattern,content) for item in items: haveImg = re.search(&quot;img&quot;,item[2]) if not haveImg: print item[0],item[3],item[4],item[1] #item[0]是作者名称 item[3]好笑人数 item[4]评论人数 item[1]内容 item[2]是内容后面的东西，如果含有图片，过滤掉 现在正则表达式在这里稍作说明 1） .*? 是一个固定的搭配， . 和 * 代表可以匹配任意无限多个字符，加上 ？ 表示使用非贪婪模式进行匹配，也就是我们会尽可能短地做匹配，以后我们还会大量用到 .*? 的搭配。 2）(.*?) 代表一个分组，在这个正则表达式中我们匹配了五个分组，在后面的遍历 item 中，item[0] 就代表第一个 (.*?) 所指代的内容，item[1] 就代表第二个 (.*?) 所指代的内容，以此类推。 3）re.S 标志代表在匹配时为点任意匹配模式，点 . 也可以代表换行符。 这样我们就获取了发布人，发布时间，发布内容，附加图片以及点赞数。 在这里注意一下，我们要获取的内容如果是带有图片，直接输出出来比较繁琐，所以这里我们只获取不带图片的段子就好了。 所以，在这里我们就需要对带图片的段子进行过滤。 我们可以发现，带有图片的段子会带有类似下面的代码，而不带图片的则没有，所以，我们的正则表达式的 item[2] 就是获取了下面的内容，如果不带图片，item[2]获取的内容便是空，所以我们只需要判断 item[2]中是否含有 img 标签就可以了。 整体代码如下： 12345678910111213141516171819202122232425262728293031323334#-*-coding:utf8-*-#created by 10412 2016/8/23#爬取糗事百科的8小时最新页的段子。包含的信息有作者名称，觉得好笑人数，评论人数，发布的内容。#如果发布的内容中含有图片的话，则过滤图片，内容依然显示出来。import urllibimport urllib2import re#自定义输入爬取的页数page = raw_input(&quot;please enter the page number:&quot;)url = &apos;http://www.qiushibaike.com/8hr/page/&apos;+ page +&apos;/?s=4880477&apos;user_agent = &apos;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&apos;headers = &#123; &apos;User-Agent&apos; : user_agent &#125;try: request = urllib2.Request(url,headers = headers) response = urllib2.urlopen(request) content = response.read().decode(&apos;utf-8&apos;) #正则表达式匹配 pattern = re.compile(&apos;&lt;div.*?author.*?&gt;.*?&lt;img.*?&gt;.*?&lt;h2&gt;(.*?)&lt;/h2&gt;.*?&lt;div.*?&apos;+ &apos;content&quot;&gt;(.*?)&lt;/div&gt;(.*?)&lt;div.*?class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;.*?class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;&apos;,re.S) items = re.findall(pattern,content) for item in items: haveImg = re.search(&quot;img&quot;,item[2]) if not haveImg: print item[0],item[3],item[4],item[1] #item[0]是作者名称 item[3]好笑人数 item[4]评论人数 item[1]内容 item[2]是内容后面的东西，如果含有图片，过滤掉except urllib2.URLError, e: if hasattr(e,&quot;code&quot;): print e.code if hasattr(e,&quot;reason&quot;): print e.reason 运行一下看下效果: 恩，带有图片的段子已经被剔除啦。 3.完善交互，设计面向对象模式好啦，现在最核心的部分我们已经完成啦，剩下的就是修一下边边角角的东西，我们想达到的目的是： 按下回车，读取一个段子，显示出段子的发布人，内容，点赞个数及评论数量。 另外我们需要设计面向对象模式，引入类和方法，将代码做一下优化和封装，最后，我们的代码如下所示 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788#-*-coding:utf8-*-#created by 10412# 在Spider1-qiushibaike.py基础上，引入类和方法，进行优化和封装，爬取糗事百科的24小时热门页的段子。# 进一步优化，每按一次回车更新一条内容，当前页的内容抓取完毕后，自动抓取下一页的内容，按‘q’退出。import urllib2import reclass QSBK: def __init__(self): self.pageIndex = 1 self.user_agent = &apos;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&apos; self.headers = &#123;&apos;User-Agent&apos; : self.user_agent&#125; self.stories = [] # 存放程序是否继续运行的变量 self.enable = False # 传入某一页的索引获得页面代码 def getPage(self, pageIndex): try: url = &apos;http://www.qiushibaike.com/hot/page/&apos; + str(pageIndex) request = urllib2.Request(url, headers=self.headers) response = urllib2.urlopen(request) pageCode = response.read().decode(&apos;utf-8&apos;) return pageCode except urllib2.URLError, e: if hasattr(e, &quot;reason&quot;): print u&quot;连接糗事百科失败,错误原因&quot;, e.reason return None # 传入某一页代码，返回本页不带图片的段子列表 def getPageItems(self, pageIndex): pageCode = self.getPage(pageIndex) if not pageCode: print u&quot;出错了&quot; return None pattern = re.compile(&apos;&lt;div class=&quot;author.*?href.*?&lt;img src.*?title=.*?&lt;h2&gt;(.*?)&lt;/h2&gt;.*?&lt;div class=&quot;content&quot;&gt;(.*?)&lt;/div&gt;.*?&lt;i class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;.*?class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;&apos;,re.S) items = re.findall(pattern, pageCode) pageStories = [] for item in items: replaceBR = re.compile(&apos;&lt;br/&gt;&apos;) text = re.sub(replaceBR, &quot;\\n&quot;, item [1] ) pageStories.append([item[0].strip(), text.strip(), item[2].strip(), item[3].strip()]) return pageStories # 加载并提取页面内容，加入到列表中 def loadPage(self): if self.enable == True: if len(self.stories) &lt; 2: # 获取新一页 pageStories = self.getPageItems(self.pageIndex) if pageStories: self.stories.append(pageStories) self.pageIndex += 1 # 调用该方法，回车打印一个段子 def getOneStory(self, pageStories, page): for story in pageStories: input = raw_input() self.loadPage() if input == &quot;Q&quot;: self.enable = False return print u&quot;第%d页\\t发布人:%s\\t赞:%s\\t评论:%s\\n%s&quot; %(page, story[0], story[2], story[2], story [1]) # 开始方法 def start(self): print u&quot;正在读取糗事百科,按回车查看新段子，Q退出&quot; # 使变量为True，程序可以正常运行 self.enable = True # 先加载一页内容 self.loadPage() # 局部变量，控制当前读到了第几页 nowPage = 0 while self.enable: if len(self.stories) &gt; 0: # 从全局list中获取一页的段子 pageStories = self.stories[0] # 当前读到的页数加一 nowPage += 1 # 将全局list中第一个元素删除，因为已经取出 del self.stories[0] # 输出该页的段子 self.getOneStory(pageStories, nowPage)spider = QSBK()spider.start() 好啦，大家来测试一下吧，点一下回车会输出一个段子，包括第几页，发布人，段子内容，点赞数以及评论数量，是不是感觉爽爆了！ 完善更新版爬虫代码在上面爬虫的基础上，还增加爬取了百科段子的评论。按C查看当前这个糗事的评论，当切换到查看评论时，换回车显示下一个评论,按Q退出回到查看糗事。糗事段子页数是一页一页加载的，如果你已经看完所有的糗事，就会自动退出！ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198#-*-coding:utf8-*-#created by 10412#在Spiders-qiushibaike.py基础上，爬取了百科段子的评论。按C查看当前这个糗事的评论，当切换到查看评论时，# 换回车显示下一个评论,按Q退出回到查看糗事。糗事段子页数是一页一页加载的，如果你已经看完所有的糗事，就会自动退出！import urllibimport urllib2import reimport os.pathhtmlCharacterMap = &#123; &apos;&lt;br/&gt;&apos; : &apos;\\n&apos;, &apos;&amp;quot;&apos; : &apos;&quot;&apos;, &apos;&amp;nbsp;&apos; : &apos; &apos;, &apos;&amp;gt;&apos; : &apos;&gt;&apos;, &apos;&amp;lt;&apos; : &apos;&lt;&apos;, &apos;&amp;amp;&apos;: &apos;&amp;&apos;, &apos;&amp;#39&apos;:&quot;&apos;&quot;,&#125;class QSBK(object): &quot;&quot;&quot;糗事百科的爬虫&quot;&quot;&quot; def __init__(self): self.pageIndex = 1 self.pagetotal = 9999 self.user_agent = &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36&apos; self.headers = &#123;&apos;User-Agent&apos; : self.user_agent&#125; self.stories = [] self.comments = [] self.currentStoryId = &apos;&apos; #是否要退出了 self.enable = False #记录当前是否在查看评论 self.viewComment = False def getPageContent(self, pageIndex): try: url = &apos;http://www.qiushibaike.com/8hr/page/%d/&apos; % pageIndex request = urllib2.Request(url, headers=self.headers) print u&apos;开始加载%02d页&apos; % pageIndex response = urllib2.urlopen(request, timeout=5) print u&apos;成功加载%02d页&apos; % pageIndex pageContent = response.read().decode(&apos;utf-8&apos;) return pageContent except urllib2.URLError, e: if hasattr(e, &apos;reason&apos;): print u&quot;连接糗事百科失败，错误原因：&quot;, e.reason return None def getCommentsContent(self, storyId): # 得到段子的评论 try: url = &apos;http://www.qiushibaike.com/article/%s&apos; % storyId request = urllib2.Request(url, headers=self.headers) response = urllib2.urlopen(request, timeout=5) pageContent = response.read().decode(&apos;utf-8&apos;) return pageContent except urllib2.URLError, e: if hasattr(e, &apos;reason&apos;): print u&quot;连接糗事百科失败，错误原因：&quot;, e.reason return None def getPageTotal(self, content): # 得到总页数 if self.pagetotal != 9999: # print u&apos;加载第%d页&apos; % self.pageIndex return pattrenStr = &apos;&lt;span class=&quot;page-numbers&quot;&gt;(?P&lt;pagetotal&gt;.*?)&lt;/span&gt;&apos; pattern = re.compile(pattrenStr, re.S) items = re.findall(pattern, content) if len(items)&gt;0: self.pagetotal = int(items[-1].strip()) print u&apos;总共有%d页&apos; % self.pagetotal def getPageItems(self, pageIndex): pageContent = self.getPageContent(pageIndex) with open(&apos;temp%02d.html&apos; % pageIndex, &apos;w&apos;) as f: f.write(pageContent.encode(&apos;utf-8&apos;)) if not pageContent: print &quot;页面加载失败...&quot; return None self.getPageTotal(pageContent) pattrenStr = r&apos;&lt;h2&gt;(?P&lt;authorname&gt;.*?)&lt;/h2&gt;.*?&apos;\\ r&apos;&lt;div class=&quot;content&quot;&gt;(?P&lt;content&gt;.*?)&lt;/div&gt;&apos;\\ r&apos;(?P&lt;maybehaveimage&gt;.*?)&apos;\\ r&apos;&lt;i class=&quot;number&quot;&gt;(?P&lt;numbervote&gt;.*?)&lt;/i&gt;.*?&apos;\\ r&apos;&lt;span class=&quot;stats-comments&quot;&gt;(?P&lt;comments&gt;.*?)&lt;/div&gt;&apos; pattern = re.compile(pattrenStr, re.S) items = re.findall(pattern, pageContent) return items def getCurrentStoryComments(self, storyId): #切换到查看评论模式 self.viewComment = True content = self.getCommentsContent(storyId) if not content: print &quot;页面加载失败...&quot; return None reStr = r&apos;&lt;div id=&quot;comment-.*?&apos;\\ r&apos;&lt;a href=&quot;/users/.*?/&quot; class=&quot;userlogin&quot; target=&quot;_blank&quot; title=&quot;(?P&lt;username&gt;.*?)&quot;&gt;(?P=username)&lt;/a&gt;.*?&apos;\\ r&apos;&lt;span class=&quot;body&quot;&gt;(?P&lt;comment&gt;.*?)&lt;/span&gt;.*?&apos;\\ r&apos;&lt;div class=&quot;report&quot;&gt;(?P&lt;index&gt;.*?)&lt;/div&gt;&apos; pattern = re.compile(reStr, re.S) items = re.findall(pattern, content) del self.comments[:] for item in items: comentstr = item[0]+&apos;(&apos;+ item[2] + u&apos;楼)&apos; + &apos;\\n&apos; + item[1] + &apos;\\n&apos; for (k,v) in htmlCharacterMap.items(): re.sub(re.compile(k), v, comentstr) self.comments.append(comentstr) if len(self.comments)&gt;0: print &apos;已切换到查看评论，换回车显示下一个评论,按Q退出回到查看糗事&apos; else: print &apos;当前糗事没有评论&apos; self.viewComment = False def getNextPage(self): if self.pageIndex &gt; self.pagetotal: self.enable = False print &quot;你已经看完所有的糗事，现在自动退出！&quot; return items = self.getPageItems(self.pageIndex) self.pageIndex += 1 for item in items: #如果有图片直接跳过，因为图片在终端显示不了 if re.search(&apos;img&apos;, item[2]): continue content = item[1].strip() #转换html的特殊字符 for (k,v) in htmlCharacterMap.items(): content = re.sub(re.compile(k), v, content) authorname = item[0].strip() for (k,v) in htmlCharacterMap.items(): authorname = re.sub(re.compile(k), v, authorname) #找出评论个数，没有为0 pattern = re.compile(r&apos;.*?&lt;a href=&quot;/article/(?P&lt;id&gt;.*?)&quot;.*?&lt;i class=&quot;number&quot;&gt;(?P&lt;number&gt;.*?)&lt;/i&gt;.*?&apos;, re.S) result = re.match(pattern, item[4]) commentnumbers = 0 articleId = &apos;&apos; if result: commentnumbers = result.groupdict().get(&apos;number&apos;, &apos;0&apos;) articleId = result.groupdict().get(&apos;id&apos;, &apos;&apos;) self.stories.append(authorname + &apos;(&apos; + item[3].strip() + u&apos;好笑·&apos; + str(commentnumbers) + u&apos;评论)&apos; + &apos;\\n&apos; + content + &apos;\\n&apos;) self.stories.append(articleId) def getNextComment(self): print self.comments[0] self.comments.pop(0) if len(self.comments)==0: print &apos;你已查看完这个糗事的所有评论,现在自动退出到查看糗事&apos; self.viewComment = False def getOneStory(self): #防止有的页面全是带图片的 while (len(self.stories)==0 and self.enable): self.getNextPage() story = self.stories[0] self.currentStoryId = self.stories[1] print story self.stories.pop(0) self.stories.pop(0) if len(self.stories)==0: self.getNextPage() def start(self): #先删除临时保存的网页 tempfiles = [x for x in os.listdir(&apos;.&apos;) if os.path.isfile(x) and os.path.splitext(x)[1]==&apos;.html&apos; and x.startswith(&apos;temp&apos;)] for file in tempfiles: os.remove(file) print u&quot;正在读取糗事百科，按回车查看下一个糗事，按C查看当前这个糗事的评论，按Q退出或返回&quot; self.enable = True self.getNextPage() while self.enable: input = raw_input() if input.upper() == &quot;Q&quot;: if not self.viewComment: self.enable = False else: self.viewComment = False print &apos;现在退出到查看糗事了&apos; elif input.upper() == &quot;C&quot;: #查看当前看到的糗事的评论 if len(self.currentStoryId)&gt;0: self.getCurrentStoryComments(self.currentStoryId) else: print &apos;这条糗事没有评论&apos; else: if not self.viewComment: self.getOneStory() else: self.getNextComment()if __name__ == &apos;__main__&apos;: spider = QSBK() spider.start()","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"Pyspider框架 —— Python爬虫实战之爬取 V2EX 网站帖子","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Pyspider框架 —— Python爬虫实战之爬取 V2EX 网站帖子/","text":"背景： PySpider：一个国人编写的强大的网络爬虫系统并带有强大的WebUI。采用Python语言编写，分布式架构，支持多种数据库后端，强大的WebUI支持脚本编辑器，任务监视器，项目管理器以及结果查看器。在线示例： http://demo.pyspider.org/ 官方文档： http://docs.pyspider.org/en/latest/ Github : https://github.com/binux/pyspider 本文爬虫代码 Github 地址：https://github.com/zhisheng17/Python-Projects/blob/master/v2ex/V2EX.py 说了这么多，我们还是来看正文吧！ 前提: 你已经安装好了Pyspider 和 MySQL-python（保存数据） 如果你还没安装的话，请看看我的前一篇文章，防止你也走弯路。 Pyspider 框架学习时走过的一些坑 HTTP 599: SSL certificate problem: unable to get local issuer certificate错误 我所遇到的一些错误： 首先，本爬虫目标：使用 Pyspider 框架爬取 V2EX 网站的帖子中的问题和内容，然后将爬取的数据保存在本地。 V2EX 中大部分的帖子查看是不需要登录的，当然也有些帖子是需要登陆后才能够查看的。（因为后来爬取的时候发现一直 error ，查看具体原因后才知道是需要登录的才可以查看那些帖子的）所以我觉得没必要用到 Cookie，当然如果你非得要登录，那也很简单，简单地方法就是添加你登录后的 cookie 了。 我们在 https://www.v2ex.com/ 扫了一遍，发现并没有一个列表能包含所有的帖子，只能退而求其次，通过抓取分类下的所有的标签列表页，来遍历所有的帖子： https://www.v2ex.com/?tab=tech 然后是 https://www.v2ex.com/go/programmer 最后每个帖子的详情地址是 （举例）： https://www.v2ex.com/t/314683#reply1 创建一个项目 在 pyspider 的 dashboard 的右下角，点击 “Create” 按钮 替换 on_start 函数的 self.crawl 的 URL： 123@every(minutes=24 * 60) def on_start(self): self.crawl(&apos;https://www.v2ex.com/&apos;, callback=self.index_page, validate_cert=False) self.crawl 告诉 pyspider 抓取指定页面，然后使用 callback 函数对结果进行解析。 @every) 修饰器，表示 on_start 每天会执行一次，这样就能抓到最新的帖子了。 validate_cert=False 一定要这样，否则会报 HTTP 599: SSL certificate problem: unable to get local issuer certificate错误 首页： 点击绿色的 run 执行，你会看到 follows 上面有一个红色的 1，切换到 follows 面板，点击绿色的播放按钮： 第二张截图一开始是出现这个问题了，解决办法看前面写的文章，后来问题就不再会出现了。 Tab 列表页 : 在 tab 列表页 中，我们需要提取出所有的主题列表页 的 URL。你可能已经发现了，sample handler 已经提取了非常多大的 URL 代码：1234@config(age=10 * 24 * 60 * 60) def index_page(self, response): for each in response.doc(&apos;a[href^=&quot;https://www.v2ex.com/?tab=&quot;]&apos;).items(): self.crawl(each.attr.href, callback=self.tab_page, validate_cert=False) 由于帖子列表页和 tab列表页长的并不一样，在这里新建了一个 callback 为 self.tab_page @config(age=10 24 60 * 60) 在这表示我们认为 10 天内页面有效，不会再次进行更新抓取 Go列表页 : 代码： 1234@config(age=10 * 24 * 60 * 60) def tab_page(self, response): for each in response.doc(&apos;a[href^=&quot;https://www.v2ex.com/go/&quot;]&apos;).items(): self.crawl(each.attr.href, callback=self.board_page, validate_cert=False) 帖子详情页（T）: 你可以看到结果里面出现了一些reply的东西，对于这些我们是可以不需要的，我们可以去掉。 同时我们还需要让他自己实现自动翻页功能。 代码：123456789@config(age=10 * 24 * 60 * 60) def board_page(self, response): for each in response.doc(&apos;a[href^=&quot;https://www.v2ex.com/t/&quot;]&apos;).items(): url = each.attr.href if url.find(&apos;#reply&apos;)&gt;0: url = url[0:url.find(&apos;#&apos;)] self.crawl(url, callback=self.detail_page, validate_cert=False) for each in response.doc(&apos;a.page_normal&apos;).items(): self.crawl(each.attr.href, callback=self.board_page, validate_cert=False) #实现自动翻页功能 去掉后的运行截图： 实现自动翻页后的截图： 此时我们已经可以匹配了所有的帖子的 url 了。 点击每个帖子后面的按钮就可以查看帖子具体详情了。 代码： 12345678910@config(priority=2) def detail_page(self, response): title = response.doc(&apos;h1&apos;).text() content = response.doc(&apos;div.topic_content&apos;).html().replace(&apos;&quot;&apos;, &apos;\\\\&quot;&apos;) self.add_question(title, content) #插入数据库 return &#123; &quot;url&quot;: response.url, &quot;title&quot;: title, &quot;content&quot;: content, &#125; 插入数据库的话，需要我们在之前定义一个add_question函数。 123456789101112131415#连接数据库def __init__(self): self.db = MySQLdb.connect(&apos;localhost&apos;, &apos;root&apos;, &apos;root&apos;, &apos;wenda&apos;, charset=&apos;utf8&apos;) def add_question(self, title, content): try: cursor = self.db.cursor() sql = &apos;insert into question(title, content, user_id, created_date, comment_count) values (&quot;%s&quot;,&quot;%s&quot;,%d, %s, 0)&apos; % (title, content, random.randint(1, 10) , &apos;now()&apos;); #插入数据库的SQL语句 print sql cursor.execute(sql) print cursor.lastrowid self.db.commit() except Exception, e: print e self.db.rollback() 查看爬虫运行结果： 先debug下，再调成running。pyspider框架在windows下的bug 设置跑的速度，建议不要跑的太快，否则很容易被发现是爬虫的，人家就会把你的IP给封掉的 查看运行工作 查看爬取下来的内容 然后再本地数据库GUI软件上查询下就可以看到数据已经保存到本地了。 自己需要用的话就可以导入出来了。 在开头我就告诉大家爬虫的代码了，如果详细的看看那个project，你就会找到我上传的爬取数据了。（仅供学习使用，切勿商用！） 当然你还会看到其他的爬虫代码的了，如果你觉得不错可以给个 Star，或者你也感兴趣的话，你可以fork我的项目，和我一起学习，这个项目长期更新下去。 最后： 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# created by 10412# !/usr/bin/env python# -*- encoding: utf-8 -*-# Created on 2016-10-20 20:43:00# Project: V2EXfrom pyspider.libs.base_handler import *import reimport randomimport MySQLdbclass Handler(BaseHandler): crawl_config = &#123; &#125; def __init__(self): self.db = MySQLdb.connect(&apos;localhost&apos;, &apos;root&apos;, &apos;root&apos;, &apos;wenda&apos;, charset=&apos;utf8&apos;) def add_question(self, title, content): try: cursor = self.db.cursor() sql = &apos;insert into question(title, content, user_id, created_date, comment_count) values (&quot;%s&quot;,&quot;%s&quot;,%d, %s, 0)&apos; % (title, content, random.randint(1, 10) , &apos;now()&apos;); print sql cursor.execute(sql) print cursor.lastrowid self.db.commit() except Exception, e: print e self.db.rollback() @every(minutes=24 * 60) def on_start(self): self.crawl(&apos;https://www.v2ex.com/&apos;, callback=self.index_page, validate_cert=False) @config(age=10 * 24 * 60 * 60) def index_page(self, response): for each in response.doc(&apos;a[href^=&quot;https://www.v2ex.com/?tab=&quot;]&apos;).items(): self.crawl(each.attr.href, callback=self.tab_page, validate_cert=False) @config(age=10 * 24 * 60 * 60) def tab_page(self, response): for each in response.doc(&apos;a[href^=&quot;https://www.v2ex.com/go/&quot;]&apos;).items(): self.crawl(each.attr.href, callback=self.board_page, validate_cert=False) @config(age=10 * 24 * 60 * 60) def board_page(self, response): for each in response.doc(&apos;a[href^=&quot;https://www.v2ex.com/t/&quot;]&apos;).items(): url = each.attr.href if url.find(&apos;#reply&apos;)&gt;0: url = url[0:url.find(&apos;#&apos;)] self.crawl(url, callback=self.detail_page, validate_cert=False) for each in response.doc(&apos;a.page_normal&apos;).items(): self.crawl(each.attr.href, callback=self.board_page, validate_cert=False) @config(priority=2) def detail_page(self, response): title = response.doc(&apos;h1&apos;).text() content = response.doc(&apos;div.topic_content&apos;).html().replace(&apos;&quot;&apos;, &apos;\\\\&quot;&apos;) self.add_question(title, content) #插入数据库 return &#123; &quot;url&quot;: response.url, &quot;title&quot;: title, &quot;content&quot;: content, &#125;","tags":[{"name":"Python","slug":"Python","permalink":"http://yoursite.com/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"},{"name":"Pyspider","slug":"Pyspider","permalink":"http://yoursite.com/tags/Pyspider/"}]},{"title":"MySQL 处理海量数据时的一些优化查询速度方法","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/MySQL-select-good/","text":"在参与实际项目中，当 MySQL 表的数据量达到百万级时，普通的 SQL 查询效率呈直线下降，而且如果 where 中的查询条件较多时，其查询速度无法容忍。想想可知，假如我们查询淘宝的一个订单详情，如果查询时间高达几十秒，这么高的查询延时，任何用户都会抓狂。因此如何提高 SQL 语句查询效率，显得十分重要。 查询速度慢的原因1、没有索引或者没有用到索引（这是查询慢最常见的问题，是程序设计的缺陷） 2、I/O 吞吐量小，形成了瓶颈效应。 3、没有创建计算列导致查询不优化。 4、内存不足 5、网络速度慢 6、查询出的数据量过大（可采用多次查询，其他的方法降低数据量） 7、锁或者死锁（这是查询慢最常见的问题，是程序设计的缺陷） 8、sp_lock,sp_who,活动的用户查看,原因是读写竞争资源。 9、返回了不必要的行和列 10、查询语句不好，没有优化 30 种 SQL 查询语句的优化方法：1、应尽量避免在 where 子句中使用 != 或者 &lt;&gt; 操作符，否则将引擎放弃使用索引而进行全表扫描。 2、应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如： 1select id from t where num is null; 可以在 num 上设置默认值 0 ，确保表中 num 列没有 null 值，然后这样查询： 1select id from t where num = 0; 3、对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。 4、尽量避免在 where 子句中使用 or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如： 1select id from t where num = 10 or num = 20; 可以这样查询： 123select id from t where num = 10union allselect id from t where num = 20; 5、下面的查询也将导致全表扫描：（不能前置百分号） 1select id from t where name like '%abc%'; 若要提高效率，可以考虑全文检索。 6、in 和 not in 也要慎用，否则会导致全表扫描，如： 1select id from t where num in(1, 2, 3); 对于连续的数值，能用 between 就不要用 in 了： 1select id from t where num between 1 and 3; 12345select xx,phone FROM send a JOIN ( select '13891030091' phone union select '13992085916' ………… UNION SELECT '13619100234' ) b on a.Phone=b.phone--替代下面 很多数据隔开的时候in('13891030091','13992085916','13619100234'…………) 7、如果在 where 子句中使用参数，也会导致全表扫描。因为 SQL 只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择到运行时；它必须在编译时进行选择。然而，如果在编译时简历访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描： 1select id from t where num = @num; 可以改为强制查询使用索引： 1select id from t with(index(索引名)) where num = @num; 8、应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如： 1select id from t where num/2 = 100; 应改为： 1select id from t where num = 100 * 2; 9、应尽量避免在 where 子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如： 12select id from t where substring(name, 1, 3) = ’abc’–name; //以abc开头的idselect id from t where datediff(day,createdate,’2005-11-30′) = 0–’2005-11-30′; //生成的id 应改为: 12select id from t where name like ‘abc%’select id from t where createdate &gt;= ’2005-11-30′ and createdate &lt; ’2005-12-1′; 10、不要在 where 子句中的 “=” 左边进行函数，算术运算或者其他表达式运算，否则系统将可能无法正确使用索引。 11、在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。 12、不要些一些没有意义的查询，如需要生成一个空表结构： 1select col1,col2 into #t from t where 1=0; 这类代码不会返回任何结果集，但是会消耗系统资源的，应改成这样： 1create table #t(…) 13、很多时候用 exists 代替 in 是一个好的选择： 1select num from a where num in(select num from b); 用下面的语句替换： 1select num from a where exists(select 1 from b where num=a.num); 14、并不是所有索引对查询都有效，SQL是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL查询可能不会去利用索引，如一表中有字段 sex，male、female几乎各一半，那么即使在sex上建了索引也对查询效率起不了作用。 15、索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有必要。 16、应尽可能的避免更新 clustered 索引数据列，因为 clustered 索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新 clustered 索引数据列，那么需要考虑是否应将该索引建为 clustered 索引。 17、尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连接时会 逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。 18、尽可能的使用 varchar/nvarchar 代替 char/nchar ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。 19、任何地方都不要使用 select * from t ，用具体的字段列表代替 *，不要返回用不到的任何字段。 20、尽量使用表变量来代替临时表。如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。 21、避免频繁创建和删除临时表，以减少系统表资源的消耗。 22、临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件，最好使用导出表。 23、在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先 create table，然后 insert。 24、如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。 25、尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。 26、使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。 27、与临时表一样，游标并不是不可使用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时 间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。 28、在所有的存储过程和触发器的开始处设置 SET NOCOUNT ON ，在结束时设置 SET NOCOUNT OFF 。无需在执行存储过程和触发器的每个语句后向客户端发送 DONE_IN_PROC 消息。 29、尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。 30、尽量避免大事务操作，提高系统并发能力。","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://yoursite.com/tags/MySQL/"}]},{"title":"MyBatis的foreach语句详解","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/MyBatis-foreach/","text":"foreach 的主要用在构建in条件中，它可以在SQL语句中进行迭代一个集合。 foreach 元素的属性主要有 item，index，collection，open，separator，close。 item 表示集合中每一个元素进行迭代时的别名， index 指 定一个名字，用于表示在迭代过程中，每次迭代到的位置， open 表示该语句以什么开始， separator 表示在每次进行迭代之间以什么符号作为分隔 符， close 表示以什么结束。 在使用 foreach 的时候最关键的也是最容易出错的就是 collection 属性，该属性是必须指定的，但是在不同情况 下，该属性的值是不一样的，主要有一下3种情况： 如果传入的是单参数且参数类型是一个List的时候，collection 属性值为 list 如果传入的是单参数且参数类型是一个 array 数组的时候，collection 的属性值为 array 如果传入的参数是多个的时候，我们就需要把它们封装成一个 Map 了，当然单参数也可以封装成map，实际上如果你在传入参数的时候，在 breast 里面也是会把它封装成一个 Map 的，map 的 key 就是参数名，所以这个时候 collection 属性值就是传入的 List 或 array 对象在自己封装的 map 里面的 key 。 下面分别来看看上述三种情况的示例代码： 1.单参数 List 的类型： 123456&lt;select id=\"dynamicForeachTest\" resultType=\"Blog\"&gt; select * from t_blog where id in &lt;foreach collection=\"list\" index=\"index\" item=\"item\" open=\"(\" separator=\",\" close=\")\"&gt; #&#123;item&#125; &lt;/foreach&gt;&lt;/select&gt; 上述 collection 的值为list，对应的 Mapper 是这样的 1public List&lt;Blog&gt; dynamicForeachTest(List&lt;Integer&gt; ids); 测试代码： 12345678910111213@Test public void dynamicForeachTest() &#123; SqlSession session = Util.getSqlSessionFactory().openSession(); BlogMapper blogMapper = session.getMapper(BlogMapper.class); List&lt;Integer&gt; ids = new ArrayList&lt;Integer&gt;(); ids.add(1); ids.add(3); ids.add(6); List&lt;Blog&gt; blogs = blogMapper.dynamicForeachTest(ids); for (Blog blog : blogs) System.out.println(blog); session.close(); &#125; 2.单参数array数组的类型： 123456&lt;select id=\"dynamicForeach2Test\" resultType=\"Blog\"&gt; select * from t_blog where id in &lt;foreach collection=\"array\" index=\"index\" item=\"item\" open=\"(\" separator=\",\" close=\")\"&gt; #&#123;item&#125; &lt;/foreach&gt;&lt;/select&gt; 上述collection为array，对应的Mapper代码： 1public List&lt;Blog&gt; dynamicForeach2Test(int[] ids); 对应的测试代码： 12345678910@Test public void dynamicForeach2Test() &#123; SqlSession session = Util.getSqlSessionFactory().openSession(); BlogMapper blogMapper = session.getMapper(BlogMapper.class); int[] ids = new int[] &#123;1,3,6,9&#125;; List&lt;Blog&gt; blogs = blogMapper.dynamicForeach2Test(ids); for (Blog blog : blogs) System.out.println(blog); session.close(); &#125; 3.自己把参数封装成Map的类型 123456&lt;select id=\"dynamicForeach3Test\" resultType=\"Blog\"&gt; select * from t_blog where title like \"%\"#&#123;title&#125;\"%\" and id in &lt;foreach collection=\"ids\" index=\"index\" item=\"item\" open=\"(\" separator=\",\" close=\")\"&gt; #&#123;item&#125; &lt;/foreach&gt;&lt;/select&gt; 上述collection的值为ids，是传入的参数Map的key，对应的Mapper代码： 1public List&lt;Blog&gt; dynamicForeach3Test(Map&lt;String, Object&gt; params); 对应测试代码： 12345678910111213141516171819@Test public void dynamicForeach3Test() &#123; SqlSession session = Util.getSqlSessionFactory().openSession(); BlogMapper blogMapper = session.getMapper(BlogMapper.class); final List&lt;Integer&gt; ids = new ArrayList&lt;Integer&gt;(); ids.add(1); ids.add(2); ids.add(3); ids.add(6); ids.add(7); ids.add(9); Map&lt;String, Object&gt; params = new HashMap&lt;String, Object&gt;(); params.put(\"ids\", ids); params.put(\"title\", \"中国\"); List&lt;Blog&gt; blogs = blogMapper.dynamicForeach3Test(params); for (Blog blog : blogs) System.out.println(blog); session.close(); &#125;","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://yoursite.com/tags/Mybatis/"},{"name":"foreach","slug":"foreach","permalink":"http://yoursite.com/tags/foreach/"}]},{"title":"《Java 多线程编程核心技术》学习笔记及总结","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Java-Thread/","text":"第一章 —— Java 多线程技能线程技术点： 线程的启动 如何使线程暂停 如何使线程停止 线程的优先级 线程安全相关问题 进程和线程的概念及多线程的优点进程：比如我们电脑运行的 QQ.exe 程序，是操作系统管理的基本运行单元 线程：在进程中独立运行的子任务，比如 QQ.exe 进程中就有很多线程在运行，下载文件线程、发送消息线程、语音线程、视频线程等。 多线程优点：我们电脑可以同时操作不同的软件，边听着歌，敲着代码，查看 pdf 文档，浏览网页等，CPU 在这些任务之间不停的切换，切换非常快，所以我们就觉得他们是在同时运行的。 使用多线程继承 Thread 类JDK 源码注释（Thread.java）如下： 12345678910111213141516171819One is to declare a class to be a subclass(子类) of &lt;code&gt;Thread&lt;/code&gt;. This subclass should override the &lt;code&gt;run&lt;/code&gt; method of class &lt;code&gt;Thread&lt;/code&gt;. An instance of the subclass can then be allocated and started. For example, a thread that computes primeslarger than a stated value could be written as follows://继承 Thread 类class PrimeThread extends Thread &#123; long minPrime; PrimeThread(long minPrime) &#123; this.minPrime = minPrime; &#125; public void run() &#123; // compute primes larger than minPrime 重写 Thread 类的 run 方法 &#125; &#125;The following code would then create a thread and start it running://开启线程 PrimeThread p = new PrimeThread(143); p.start(); 实现 Runnable 接口JDK 源码注释（Thread.java）如下： 12345678910111213141516171819The other way to create a thread is to declare a class that implements the &lt;code&gt;Runnable&lt;/code&gt; interface. That class then implements the &lt;code&gt;run&lt;/code&gt; method. An instance of the class can then be allocated, passed as an argument when creating&lt;code&gt;Thread&lt;/code&gt;, and started. The same example in this other style looks like the following://实现 Runnable 接口 class PrimeRun implements Runnable &#123; long minPrime; PrimeRun(long minPrime) &#123; this.minPrime = minPrime; &#125; public void run() &#123; // compute primes larger than minPrime //重写 run 方法 &#125; &#125;The following code would then create a thread and start it running://开启线程 PrimeRun p = new PrimeRun(143); new Thread(p).start(); currentThread() 方法该方法返回代码段正在被哪个线程调用的信息。 isAlive() 方法判断当前线程是否处于活动状态（已经启动但未终止） sleep() 方法在指定的毫秒数内让当前“正在执行的线程（this.currentThread() 返回的线程）”休眠（暂停执行）。 getId() 方法获取线程的唯一标识 停止线程可以使用 Thread.stop() 方法，但最好不要用，因为这个方法是不安全的，已经弃用作废了。 大多数停止一个线程是使用 Thread.interrupt() 方法 判断线程是否是停止状态 interrupted() 12345//测试当前线程是否已经中断了，这个线程的中断状态会被这个方法清除。//换句话说，如果连续两次调用了这个方法，第二次调用的时候将会返回 false ，public static boolean interrupted() &#123; return currentThread().isInterrupted(true);&#125; isInterrupted() 1234567891011 //测试线程是否已经中断了，线程的状态不会受这个方法的影响 //线程中断被忽略，因为线程处于中断下不处于活动状态的线程由此返回false的方法反映出来 public boolean isInterrupted() &#123; return isInterrupted(false); &#125; /*** Tests if some Thread has been interrupted. The interrupted state* is reset or not based on the value of ClearInterrupted that is* passed.*/private native boolean isInterrupted(boolean ClearInterrupted); 在沉睡中停止1234567891011121314151617181920212223242526public class MyThread2 extends Thread&#123; @Override public void run() &#123; try &#123; System.out.println(\"run start\"); Thread.sleep(20000); System.out.println(\"run end\"); &#125; catch (InterruptedException e) &#123; System.out.println(\"run catch \"+this.isInterrupted()); e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; try &#123; MyThread2 t2 = new MyThread2(); t2.start(); Thread.sleep(200); t2.interrupt(); &#125; catch (InterruptedException e) &#123; System.out.println(\"main catch\"); e.printStackTrace(); &#125; System.out.println(\"main end\"); &#125;&#125; 运行结果： 123456run startmain endrun catch falsejava.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at com.zhisheng.thread.thread1.MyThread2.run(MyThread2.java:12) 从运行结果来看，如果在 sleep 状态下停止某一线程，会进入 catch 语句，并清除停止状态值，使之变成 false。 在停止中沉睡12345678910111213141516171819public class MyThread3 extends Thread&#123; @Override public void run() &#123; try &#123; System.out.println(\"run start\"); Thread.sleep(20000); System.out.println(\"run end\"); &#125; catch (InterruptedException e) &#123; System.out.println(\"run catch \"+this.isInterrupted()); e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; MyThread3 t3 = new MyThread3(); t3.start(); t3.interrupt(); &#125;&#125; 运行结果： 12345run startrun catch falsejava.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at com.zhisheng.thread.thread1.MyThread3.run(MyThread3.java:12) 能停止的线程 —— 暴力停止使用 stop() 方法停止线程 暂停线程可使用 suspend 方法暂停线程，使用 resume() 方法恢复线程的执行。 suspend 和 resume 方法的使用1234567891011121314151617181920212223242526272829public class MyThread4 extends Thread&#123; private int i; public int getI() &#123; return i; &#125; public void setI(int i) &#123; this.i = i; &#125; @Override public void run() &#123; while (true) &#123; i++; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; MyThread4 t4 = new MyThread4(); t4.start(); System.out.println(\"A----- \" + System.currentTimeMillis() + \" ---- \" + t4.getI()); Thread.sleep(2000); System.out.println(\"A----- \" + System.currentTimeMillis() + \" ---- \" + t4.getI()); t4.suspend(); Thread.sleep(2000); t4.resume(); System.out.println(\"B----- \" + System.currentTimeMillis() + \" ---- \" + t4.getI()); Thread.sleep(2000); System.out.println(\"B----- \" + System.currentTimeMillis() + \" ---- \" + t4.getI()); &#125;&#125; 从运行结果来看，线程的确能够暂停和恢复。 但是 suspend 和 resume 方法的缺点就是：不同步，因为线程的暂停导致数据的不同步。 yield 方法123456789101112131415161718/** * A hint to the scheduler that the current thread is willing to yield * its current use of a processor. The scheduler is free to ignore this * hint. * * &lt;p&gt; Yield is a heuristic attempt to improve relative progression * between threads that would otherwise over-utilise a CPU. Its use * should be combined with detailed profiling and benchmarking to * ensure that it actually has the desired effect. * * &lt;p&gt; It is rarely appropriate to use this method. It may be useful * for debugging or testing purposes, where it may help to reproduce * bugs due to race conditions. It may also be useful when designing * concurrency control constructs such as the ones in the * &#123;@link java.util.concurrent.locks&#125; package. */ //暂停当前正在执行的线程对象，并执行其他线程。暂停的时间不确定。 public static native void yield(); 1234567891011121314151617public class MyThread5 extends Thread&#123; @Override public void run() &#123; double start = System.currentTimeMillis(); for (int i = 0; i &lt; 200000; i++) &#123; //yield();//暂停的时间不确定 i++; &#125; double end = System.currentTimeMillis(); System.out.println(\"time is \"+(end - start)); &#125; public static void main(String[] args) &#123; MyThread5 t5 = new MyThread5(); t5.start(); &#125;&#125; 线程的优先级设置优先级的方法：setPriority() 方法 12345678910111213public final void setPriority(int newPriority) &#123; ThreadGroup g; checkAccess(); if (newPriority &gt; MAX_PRIORITY || newPriority &lt; MIN_PRIORITY) &#123; throw new IllegalArgumentException(); &#125; if((g = getThreadGroup()) != null) &#123; if (newPriority &gt; g.getMaxPriority()) &#123; newPriority = g.getMaxPriority(); &#125; setPriority0(priority = newPriority); &#125; &#125; 不一定优先级高的线程就先执行。 守护线程当进程中不存在非守护线程了，则守护线程自动销毁。垃圾回收线程就是典型的守护线程，当进程中没有非守护线程了，则垃圾回收线程也就没有存在的必要了，自动销毁。 12345678910111213141516171819202122/** * Marks this thread as either a &#123;@linkplain #isDaemon daemon&#125; thread * or a user thread. The Java Virtual Machine exits when the only * threads running are all daemon threads. * * &lt;p&gt; This method must be invoked before the thread is started. * * @param on * if &#123;@code true&#125;, marks this thread as a daemon thread * @throws IllegalThreadStateException * if this thread is &#123;@linkplain #isAlive alive&#125; * @throws SecurityException * if &#123;@link #checkAccess&#125; determines that the current * thread cannot modify this thread */ public final void setDaemon(boolean on) &#123; checkAccess(); if (isAlive()) &#123; throw new IllegalThreadStateException(); &#125; daemon = on; &#125; 第二章 —— 对象及变量的并发访问技术点： synchronized 对象监视器为 Object 时的使用 synchronized 对象监视器为 Class 时的使用 非线程安全是如何出现的 关键字 volatile 的主要作用 关键字 volatile 与 synchronized 的区别及使用情况 synchronized 同步方法方法内的变量为线程安全“非线程安全”问题存在于“实例变量”中，如果是方法内部的私有变量，则不存在“非线程安全”问题，所得结果也就是“线程安全”了。 实例变量非线程安全如果多线程共同访问一个对象中的实例变量，则有可能出现“非线程安全”问题。 在两个线程访问同一个对象中的同步方法时一定是线程安全的。 脏读发生脏读的情况是在读取实例变量时，此值已经被其他线程更改过了。 如下例子就可以说明，如果不加 synchronized 关键字在 setValue 和 getValue 方法上，就会出现数据脏读。 123456789101112131415161718192021222324252627282930313233343536373839404142class VarName&#123; private String userName = \"A\"; private String password = \"AA\"; synchronized public void setValue(String userName, String password) &#123; try &#123; this.userName = userName; Thread.sleep(500); this.password = password; System.out.println(\"setValue method Thread name is : \" + Thread.currentThread().getName() + \" userName = \" + userName + \" password = \" + password); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //synchronized public void getValue() &#123; System.out.println(\"getValue method Thread name is : \" + Thread.currentThread().getName() + \" userName = \" + userName + \" password = \" + password); &#125;&#125;class Thread1 extends Thread&#123; private VarName varName; public Thread1(VarName varName) &#123; this.varName = varName; &#125; @Override public void run() &#123; varName.setValue(\"B\", \"BB\"); &#125;&#125;public class Test&#123; public static void main(String[] args) throws InterruptedException &#123; VarName v = new VarName(); Thread1 thread1 = new Thread1(v); thread1.start(); Thread.sleep(200);//打印结果受睡眠时间的影响 v.getValue(); &#125;&#125; synchronized 锁重入关键字 synchronized 拥有锁重入的功能，也就是在使用 synchronized 时，当一个线程得到一个对象锁后，再次请求此对象锁是可以再次得到该对象的锁的。这也证明了在一个 synchronized 方法/块的内部调用本类的其他 synchronized 方法/块时，是永远可以得到锁的。 12345678910111213141516171819202122232425262728293031class Service&#123; synchronized public void service1() &#123; System.out.println(\"service 1\"); service2(); &#125; synchronized public void service2() &#123; System.out.println(\"service 2\"); service3(); &#125; synchronized public void service3() &#123; System.out.println(\"service 3\"); &#125;&#125;class Thread2 extends Thread&#123; @Override public void run() &#123; Service s = new Service(); s.service1(); &#125;&#125;public class Test2&#123; public static void main(String[] args) &#123; Thread2 t2 = new Thread2(); t2.start(); &#125;&#125; 运行结果： 123service 1service 2service 3 同步不具有继承性同步不可以继承。 synchronized 同步语句块synchronized 代码块间的同步性当一个线程访问 object 的一个 synchronized(this) 同步代码块时，其他线程对同一个 object 中所有其他 synchronized(this) 同步代码块的访问将被阻塞，这说明 synchronized 使用的 “对象监视器” 是一个。 将任意对象作为对象监视器多个线程调用同一个对象中的不同名称的 synchronized 同步方法或者 synchronized(this) 同步代码块时，调用的效果就是按顺序执行，也就是同步的，阻塞的。 静态同步 synchronized 方法与 synchronized(class) 代码块关键字 synchronized 还可以应用在 static 静态方法上，如果这样写就是对当前的 *.java 文件对应的 Class 类进行加锁。而 synchronized 关键字加到非 static 静态方法上就是给对象加锁。 多线程的死锁volatile 关键字作用：使变量在多个线程间可见。 通过使用 volatile 关键字，强制的从公共内存中读取变量的值。使用 volatile 关键字增加了实例变量在多个线程之间的可见性，但 volatile 关键字最致命的缺点就是不支持原子性。 关键字 synchronized 和 volatile 比较： 关键字 volatile 是线程同步的轻量实现，所以 volatile 性能肯定要比 synchronized 要好，并且 volatile 只能修饰于变量，而 synchronized 可以修饰方法，以及代码块。 多线程访问 volatile 不会发生阻塞，而 synchronized 会出现阻塞。 volatile 能保证数据的可见性，但不能保证原子性；而 synchronized 可以保证原子性，也可以间接保证可见性，因为它会将私有内存和公有内存中的数据做同步。 关键字 volatile 解决的是变量在多个线程之间的可见性；而 synchronized 关键字解决的是多个线程之间访问资源的同步性。 ​ 第三章 —— 线程间通信技术点： 使用 wait/notify 实现线程间的通信 生产者/消费者模式的实现 方法 join 的使用 ThreadLocal 类的使用 等待/通知机制wait 使线程停止运行，notify 使停止的线程继续运行。 关键字 synchronized 可以将任何一个 Object 对象作为同步对象看待，而 Java 为每个 Object 都实现了 wait() 和 notify() 方法，他们必须用在被 synchronized 同步的 Object 的临界区内。通过调用 wait 方法可以使处于临界区内的线程进入等待状态，同时释放被同步对象的锁。而 notify 操作可以唤醒一个因调用了 wait 方法而处于阻塞状态的线程，使其进入就绪状态。被重新唤醒的线程会试图重新获得临界区的控制权，继续执行临界区内 wait 之后的代码。 wait 方法可以使调用该方法的线程释放共享资源的锁，从运行状态退出，进入等待状态，直到再次被唤醒。 notify() 方法可以随机唤醒等待对列中等待同一共享资源的一个线程，并使该线程退出等待状态，进入可运行状态。 notifyAll() 方法可以随机唤醒等待对列中等待同一共享资源的所有线程，并使这些线程退出等待状态，进入可运行状态。 线程状态示意图： 新创建一个线程对象后，在调用它的 start() 方法，系统会为此线程分配 CPU 资源，使其处于 Runnable（可运行）状态，如果线程抢占到 CPU 资源，此线程就会处于 Running （运行）状态 Runnable 和 Running 状态之间可以相互切换，因为线程有可能运行一段时间后，有其他优先级高的线程抢占了 CPU 资源，此时线程就从 Running 状态变成了 Runnable 状态。 线程进入 Runnable 状态有如下几种情况： 调用 sleep() 方法后经过的时间超过了指定的休眠时间 线程调用的阻塞 IO 已经返回，阻塞方法执行完毕 线程成功的获得了试图同步的监视器 线程正在等待某个通知，其他线程发出了通知 处于挂状态的线程调用了 resume 恢复方法 Blocked 是阻塞的意思，例如线程遇到一个 IO 操作，此时 CPU 处于空闲状态，可能会转而把 CPU 时间片分配给其他线程，这时也可以称为 “暂停”状态。Blocked 状态结束之后，进入 Runnable 状态，等待系统重新分配资源。 出现阻塞状态的有如下几种情况： 线程调用 sleep 方法，主动放弃占用的处理器资源 线程调用了阻塞式 IO 方法，在该方法返回之前，该线程被阻塞 线程试图获得一个同步监视器，但该同步监视器正在被其他线程所持有 线程等待某个通知 程序调用了 suspend 方法将该线程挂起 run 方法运行结束后进入销毁阶段，整个线程执行完毕。 生产者/消费者模式实现一个生产者，一个消费者 存储值对象： 12345678910package com.zhisheng.thread.thread5;/** * Created by 10412 on 2017/6/3. * 存储值对象 */public class ValueObject&#123; public static String value = \"\";&#125; 生产者： 123456789101112131415161718192021222324252627282930package com.zhisheng.thread.thread5;/** * Created by 10412 on 2017/6/3. * 生产者 */public class Product&#123; private String lock; public Product(String lock) &#123; this.lock = lock; &#125; public void setValue() &#123; synchronized (lock) &#123; if (!ValueObject.value.equals(\"\")) &#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; String value = System.currentTimeMillis() + \"_\" + System.nanoTime(); System.out.println(\"生产者 set 的值是：\" + value); ValueObject.value = value; lock.notify(); &#125; &#125;&#125; 消费者： 1234567891011121314151617181920212223242526272829package com.zhisheng.thread.thread5;/** * Created by 10412 on 2017/6/3. * 消费者 */public class Resume&#123; private String lock; public Resume(String lock) &#123; this.lock = lock; &#125; public void getValue() &#123; synchronized (lock) &#123; if (ValueObject.value.equals(\"\")) &#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(\"消费者 get 的值：\" + ValueObject.value); ValueObject.value = \"\"; lock.notify(); &#125; &#125;&#125; 生产者线程： 123456789101112131415161718192021package com.zhisheng.thread.thread5;/** * Created by 10412 on 2017/6/3. * 生产者线程 */public class ProductThread extends Thread&#123; private Product p; public ProductThread(Product p) &#123; this.p = p; &#125; @Override public void run() &#123; while (true) &#123; p.setValue(); &#125; &#125;&#125; 消费者线程： 123456789101112131415161718192021package com.zhisheng.thread.thread5;/** * Created by 10412 on 2017/6/3. * 消费者线程 */public class ResumeThread extends Thread&#123; private Resume r; public ResumeThread(Resume r) &#123; this.r = r; &#125; @Override public void run() &#123; while (true) &#123; r.getValue(); &#125; &#125;&#125; 主函数： 123456789101112131415161718package com.zhisheng.thread.thread5;/** * Created by 10412 on 2017/6/3. * 一个生产者一个消费者测试 */public class Test&#123; public static void main(String[] args) &#123; String str = new String(\"\"); Product p = new Product(str); Resume r = new Resume(str);; ProductThread pt = new ProductThread(p); ResumeThread rt = new ResumeThread(r); pt.start(); rt.start(); &#125;&#125; 题目：创建20个线程，其中10个线程是将数据备份到数据库A，另外10个线程将数据备份到数据库B中去，并且备份数据库A和备份数据库B是交叉进行的。 工具类： 123456789101112131415161718192021222324252627282930313233343536373839404142package com.zhisheng.thread.thread6;/** * Created by 10412 on 2017/6/3. * 创建20个线程，其中10个线程是将数据备份到数据库A，另外10个线程将数据备份到数据库B中去，并且 * 备份数据库A和备份数据库B是交叉进行的 */public class DBTools&#123; volatile private boolean prevIsA = false; //确保A备份先进行 synchronized public void backA() &#123; while (prevIsA == true) &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(\"AAAAA\"); &#125; prevIsA = true; notifyAll(); &#125; synchronized public void backB() &#123; while (prevIsA == false) &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(\"BBBBB\"); &#125; prevIsA = false; notifyAll(); &#125;&#125; 备份A先线程： 123456789101112131415161718package com.zhisheng.thread.thread6;/** * Created by 10412 on 2017/6/3. */public class ThreadA extends Thread&#123; private DBTools dbTools; public ThreadA(DBTools dbTools) &#123; this.dbTools = dbTools; &#125; @Override public void run() &#123; dbTools.backA(); &#125;&#125; 备份B线程： 123456789101112131415161718package com.zhisheng.thread.thread6;/** * Created by 10412 on 2017/6/3. */public class ThreadB extends Thread&#123; private DBTools dbTools; public ThreadB(DBTools dbTools) &#123; this.dbTools = dbTools; &#125; @Override public void run() &#123; dbTools.backB(); &#125;&#125; 测试： 1234567891011121314151617package com.zhisheng.thread.thread6;/** * Created by 10412 on 2017/6/3. */public class Test&#123; public static void main(String[] args) &#123; DBTools dbTools = new DBTools(); for (int i = 0; i &lt; 20; i++) &#123; ThreadB tb = new ThreadB(dbTools); tb.start(); ThreadA ta = new ThreadA(dbTools); ta.start(); &#125; &#125;&#125; Join 方法的使用作用：等待线程对象销毁 join 方法具有使线程排队运行的作用，有些类似同步的运行效果。join 与 synchronized 的区别是：join 在内部使用 wait() 方法进行等待，而 synchronized 关键字使用的是 “对象监视器” 原理做为同步。 在 join 过程中，如果当前线程对象被中断，则当前线程出现异常。 方法 join(long) 中的参数是设定等待的时间。 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 等待该线程终止的时间最长为 millis 毫秒。超时为 0 意味着要一直等下去。 * Waits at most &#123;@code millis&#125; milliseconds for this thread to * die. A timeout of &#123;@code 0&#125; means to wait forever. * * &lt;p&gt; This implementation uses a loop of &#123;@code this.wait&#125; calls * conditioned on &#123;@code this.isAlive&#125;. As a thread terminates the * &#123;@code this.notifyAll&#125; method is invoked. It is recommended that * applications not use &#123;@code wait&#125;, &#123;@code notify&#125;, or * &#123;@code notifyAll&#125; on &#123;@code Thread&#125; instances. * * @param millis * the time to wait in milliseconds * * @throws IllegalArgumentException * if the value of &#123;@code millis&#125; is negative * * @throws InterruptedException * if any thread has interrupted the current thread. The * &lt;i&gt;interrupted status&lt;/i&gt; of the current thread is * cleared when this exception is thrown. */ public final synchronized void join(long millis) throws InterruptedException &#123; long base = System.currentTimeMillis(); long now = 0; if (millis &lt; 0) &#123; throw new IllegalArgumentException(\"timeout value is negative\"); if (millis == 0) &#123; while (isAlive()) &#123; wait(0); &#125; &#125; else &#123; while (isAlive()) &#123; long delay = millis - now; if (delay &lt;= 0) &#123; break; &#125; wait(delay); now = System.currentTimeMillis() - base; &#125; &#125; &#125; 类 ThreadLocal 的使用该类提供了线程局部 (thread-local) 变量。这些变量不同于它们的普通对应物，因为访问某个变量（通过其 get 或set 方法）的每个线程都有自己的局部变量，它独立于变量的初始化副本。ThreadLocal 实例通常是类中的private static 字段，它们希望将状态与某一个线程（例如，用户 ID 或事务 ID）相关联。 get() 方法12345678910111213public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings(\"unchecked\") T result = (T)e.value; return result; &#125; &#125; return setInitialValue(); &#125; 返回此线程局部变量的当前线程副本中的值。如果变量没有用于当前线程的值，则先将其初始化为调用 initialValue() 方法返回的值。 InheritableThreadLocal 类的使用该类扩展了 ThreadLocal，为子线程提供从父线程那里继承的值：在创建子线程时，子线程会接收所有可继承的线程局部变量的初始值，以获得父线程所具有的值。通常，子线程的值与父线程的值是一致的；但是，通过重写这个类中的 childValue 方法，子线程的值可以作为父线程值的一个任意函数。 当必须将变量（如用户 ID 和 事务 ID）中维护的每线程属性（per-thread-attribute）自动传送给创建的所有子线程时，应尽可能地采用可继承的线程局部变量，而不是采用普通的线程局部变量。 第四章 —— Lock 的使用使用 ReentrantLock 类一个可重入的互斥锁 Lock，它具有与使用 synchronized 方法和语句所访问的隐式监视器锁相同的一些基本行为和语义，但功能更强大。 ReentrantLock 将由最近成功获得锁，并且还没有释放该锁的线程所拥有。当锁没有被另一个线程所拥有时，调用 lock 的线程将成功获取该锁并返回。如果当前线程已经拥有该锁，此方法将立即返回。可以使用 isHeldByCurrentThread()和 getHoldCount()方法来检查此情况是否发生。 此类的构造方法接受一个可选的公平 参数。当设置为 true 时，在多个线程的争用下，这些锁倾向于将访问权授予等待时间最长的线程。否则此锁将无法保证任何特定访问顺序。与采用默认设置（使用不公平锁）相比，使用公平锁的程序在许多线程访问时表现为很低的总体吞吐量（即速度很慢，常常极其慢），但是在获得锁和保证锁分配的均衡性时差异较小。不过要注意的是，公平锁不能保证线程调度的公平性。因此，使用公平锁的众多线程中的一员可能获得多倍的成功机会，这种情况发生在其他活动线程没有被处理并且目前并未持有锁时。还要注意的是，未定时的 tryLock方法并没有使用公平设置。因为即使其他线程正在等待，只要该锁是可用的，此方法就可以获得成功。 建议总是 立即实践，使用 lock 块来调用 try，在之前/之后的构造中，最典型的代码如下： 12345678910111213class X &#123; private final ReentrantLock lock = new ReentrantLock(); // ... public void m() &#123; lock.lock(); // block until condition holds try &#123; // ... method body &#125; finally &#123; lock.unlock() &#125; &#125; &#125; ConditionCondition 将 Object 监视器方法（wait、notify 和 notifyAll）分解成截然不同的对象，以便通过将这些对象与任意 Lock 实现组合使用，为每个对象提供多个等待 set（wait-set）。其中，Lock 替代了 synchronized 方法和语句的使用，Condition 替代了 Object 监视器方法的使用。 假定有一个绑定的缓冲区，它支持 put 和 take 方法。如果试图在空的缓冲区上执行 take 操作，则在某一个项变得可用之前，线程将一直阻塞；如果试图在满的缓冲区上执行 put 操作，则在有空间变得可用之前，线程将一直阻塞。我们喜欢在单独的等待 set 中保存 put 线程和 take 线程，这样就可以在缓冲区中的项或空间变得可用时利用最佳规划，一次只通知一个线程。可以使用两个 Condition 实例来做到这一点。 12345678910111213141516171819202122232425262728293031323334353637class BoundedBuffer &#123; final Lock lock = new ReentrantLock(); final Condition notFull = lock.newCondition(); final Condition notEmpty = lock.newCondition(); final Object[] items = new Object[100]; int putptr, takeptr, count; public void put(Object x) throws InterruptedException &#123; lock.lock(); try &#123; while (count == items.length) notFull.await(); items[putptr] = x; if (++putptr == items.length) putptr = 0; ++count; notEmpty.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; public Object take() throws InterruptedException &#123; lock.lock(); try &#123; while (count == 0) notEmpty.await(); Object x = items[takeptr]; if (++takeptr == items.length) takeptr = 0; --count; notFull.signal(); return x; &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; 正确使用 Condition 实现等待/通知MyService.java 123456789101112131415161718192021222324252627282930313233343536package com.zhisheng.thread.Thread9;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * Created by 10412 on 2017/6/4. */public class MyService&#123; private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); public void await() &#123; lock.lock(); try &#123; System.out.println(\"await A\"); condition.await();//使当前执行的线程处于等待状态 waiting System.out.println(\"await B\"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; lock.unlock(); System.out.println(\"释放锁\"); &#125; &#125; public void signal() &#123; lock.lock(); System.out.println(\"signal A\"); condition.signal(); System.out.println(\"signal B\"); lock.unlock(); &#125;&#125; ThreadA.java 123456789101112131415161718package com.zhisheng.thread.Thread9;/** * Created by 10412 on 2017/6/4. */public class ThreadA extends Thread&#123; private MyService service; public ThreadA(MyService service) &#123; this.service = service; &#125; @Override public void run() &#123; service.await(); &#125;&#125; Test.java 123456789101112131415package com.zhisheng.thread.Thread9;/** * Created by 10412 on 2017/6/4. */public class Test&#123; public static void main(String[] args) throws InterruptedException &#123; MyService service = new MyService(); ThreadA ta = new ThreadA(service); ta.start(); Thread.sleep(5000); service.signal(); &#125;&#125; 运行结果： 12345await Asignal Asignal Bawait B释放锁 Object 类中的 wait() 方法相当于 Condition 类中 await() 方法 Object 类中的 wait(long time) 方法相当于 Condition 类中 await(long time, TimeUnit unit) 方法 Object 类中的 notify() 方法相当于 Condition 类中 signal() 方法 Object 类中的 notifyAll() 方法相当于 Condition 类中 signalAll() 方法 题目：实现生产者与消费者 一对一交替打印 MyService.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.zhisheng.thread.thread10;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * Created by 10412 on 2017/6/4. * 实现生产者与消费者 一对一·交替打印 */public class MyService&#123; private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); private boolean flag = false; public void setValue() &#123; lock.lock(); while (flag == true) &#123; try &#123; condition.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(\"SetValue AAAAAA\"); flag = true; condition.signal(); lock.unlock(); &#125; public void getValue() &#123; lock.lock(); while (flag == false) &#123; try &#123; condition.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(\"GetValue BBBB\"); flag = false; condition.signal(); lock.unlock(); &#125;&#125; ThreadA.java 1234567891011121314151617181920package com.zhisheng.thread.thread10;/** * Created by 10412 on 2017/6/4. */public class ThreadA extends Thread&#123; private MyService service; public ThreadA(MyService service) &#123; this.service = service; &#125; @Override public void run() &#123; for (int i = 0; i &lt; Integer.MAX_VALUE; i++) &#123; service.setValue(); &#125; &#125;&#125; ThreadB.java 1234567891011121314151617181920package com.zhisheng.thread.thread10;/** * Created by 10412 on 2017/6/4. */public class ThreadB extends Thread&#123; private MyService service; public ThreadB(MyService service) &#123; this.service = service; &#125; @Override public void run() &#123; for (int i = 0; i &lt; Integer.MAX_VALUE; i++) &#123; service.getValue(); &#125; &#125;&#125; Test.java 123456789101112131415package com.zhisheng.thread.thread10;/** * Created by 10412 on 2017/6/4. */public class Test&#123; public static void main(String[] args) &#123; MyService service = new MyService(); ThreadA ta = new ThreadA(service); ThreadB tb = new ThreadB(service); ta.start(); tb.start(); &#125;&#125; getHoldCount() 查询当前线程保持此锁定的个数，也就是调用 lock() 的方法 getQueueLength() 返回正等待获取此锁定的线程估计数 getWaitQueueLength() 返回等待与此锁定相关的给定条件 Condition 的线程估计数 hasQueuedThread() 查询指定的线程是否正在等待获取此锁定 hasQueuedThreads() 查询是否有线程正在等待获取此锁定 hasWaiters() 查询是否有线程正在等待与此锁定有关的 condition 条件 isFair() 判断是否是公平锁（默认下 ReentrantLock类使用的是非公平锁） isHeldByCurrentThread() 查询当前线程是否保持此锁定 isLocked() 查询此锁定是否由任意线程保持 lockInterruptibly() 如果当前线程未被中断，则获取锁定，如果已经被中断则出现异常 tryLock() 仅在调用时锁定未被另一个线程保持的情况下，才获取该锁定 tryLock(long time, TimeUtil util) 如果锁定在给定的等待时间内没有被另一个线程保持，且当前线程未被中断，则获取该锁定。 使用 ReentrantReadWriteLock 类读写互斥： MyService.java 123456789101112131415161718192021222324252627282930313233package com.zhisheng.thread.Thread11;import java.util.concurrent.locks.ReentrantReadWriteLock;/** * Created by 10412 on 2017/6/4. */public class MyService&#123; private ReentrantReadWriteLock lock = new ReentrantReadWriteLock(); public void read() &#123; lock.readLock().lock(); System.out.println(Thread.currentThread().getName() + \" Read AAA \" + System.currentTimeMillis()); try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; lock.readLock().unlock(); &#125; public void write() &#123; lock.writeLock().lock(); System.out.println(Thread.currentThread().getName() + \" write BBB \" + System.currentTimeMillis()); try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; lock.writeLock().unlock(); &#125;&#125; ThreadA.java 123456789101112131415161718package com.zhisheng.thread.Thread11;/** * Created by 10412 on 2017/6/4. */public class ThreadA extends Thread&#123; private MyService service; public ThreadA(MyService service) &#123; this.service = service; &#125; @Override public void run() &#123; service.read(); &#125;&#125; ThreadB.java 123456789101112131415161718package com.zhisheng.thread.Thread11;/** * Created by 10412 on 2017/6/4. */public class ThreadB extends Thread&#123; private MyService service; public ThreadB(MyService service) &#123; this.service = service; &#125; @Override public void run() &#123; service.write(); &#125;&#125; Test.java 123456789101112131415161718package com.zhisheng.thread.Thread11;/** * Created by 10412 on 2017/6/4. */public class Test&#123; public static void main(String[] args) throws InterruptedException &#123; MyService service = new MyService(); ThreadA ta = new ThreadA(service); ta.setName(\"A\"); ta.start(); Thread.sleep(1000); ThreadB tb = new ThreadB(service); tb.setName(\"B\"); tb.start(); &#125;&#125; 运行结果： 12A Read AAA 1496556770402B write BBB 1496556780402 第六章 —— 单例模式与多线程推荐文章 《深入浅出单实例Singleton设计模式》 立即加载模式 / “饿汉模式”立即加载：使用类的时候已经将对象创建完毕，new 实例化 123456789public class MyObject&#123; private static MyObject object = new MyObject(); private MyObject() &#123; &#125; public static MyObject getInstance() &#123; return object; &#125;&#125; 延迟加载 / “ 懒汉模式 ”就是在调用 get 的时候实例才被创建。在 get() 方法中进行 new 实例化。 12345678910111213public class MyObject&#123; private static MyObject object; private MyObject() &#123; &#125; public static MyObject getInstance() &#123; if (object != null) &#123; &#125; else &#123; object = new MyObject(); &#125; return object; &#125;&#125; 使用 DCL 双重检查锁，解决“懒汉模式”遇到的多线程问题 123456789101112131415161718public class MyObject&#123; private volatile static MyObject object; private MyObject() &#123; &#125; //synchronized public static MyObject getInstance() &#123; if (object != null) &#123; &#125; else &#123; synchronized (MyObject.class) &#123; if (object == null) &#123; object = new MyObject(); &#125; &#125; &#125; return object; &#125;&#125; 使用静态内部类实现单例模式123456789101112public class MyObject&#123; private static class MyObjectHandler &#123; private static MyObject object = new MyObject(); &#125; private MyObject() &#123; &#125; public static MyObject getInstance() &#123; return MyObjectHandler.object; &#125;&#125; 序列化与反序列化的单例模式实现MyObject.java 12345678910111213141516171819202122232425package com.zhisheng.thread.thread15;import java.io.ObjectStreamException;import java.io.Serializable;/** * Created by 10412 on 2017/6/4. */public class MyObject implements Serializable&#123; private static final long serialVersionUID = 888L; private static class MyObjectHandler &#123; private static final MyObject object = new MyObject(); &#125; private MyObject() &#123; &#125; public static MyObject getInstance() &#123; return MyObjectHandler.object; &#125; protected Object readResolve() throws ObjectStreamException &#123; System.out.println(\"调用了readResolve方法！\"); return MyObjectHandler.object; &#125;&#125; SaveAndRead.java 12345678910111213141516171819202122232425262728293031323334353637383940package com.zhisheng.thread.thread15;import java.io.*;/** * Created by 10412 on 2017/6/4. */public class SaveAndRead&#123; public static void main(String[] args) &#123; try &#123; MyObject object = MyObject.getInstance(); FileOutputStream fos = new FileOutputStream(new File(\"fos.txt\")); ObjectOutputStream oos = new ObjectOutputStream(fos); oos.writeObject(object); oos.close(); fos.close(); System.out.println(object.hashCode()); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; FileInputStream fis = new FileInputStream(new File(\"fos.txt\")); ObjectInputStream ois = new ObjectInputStream(fis); MyObject o = (MyObject) ois.readObject(); ois.close(); fis.close(); System.out.println(o.hashCode()); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 这里主要要指出 MyObject.java 中 readResolve 方法 1234protected Object readResolve() throws ObjectStreamException &#123; System.out.println(\"调用了readResolve方法！\"); return MyObjectHandler.object; &#125; 方法 readResolve 允许 class 在反序列化返回对象前替换、解析在流中读出来的对象。实现 readResolve 方法，一个 class 可以直接控制反序化返回的类型和对象引用。 方法 readResolve 会在 ObjectInputStream 已经读取一个对象并在准备返回前调用。ObjectInputStream 会检查对象的 class 是否定义了 readResolve 方法。如果定义了，将由 readResolve 方法指定返回的对象。返回对象的类型一定要是兼容的，否则会抛出 ClassCastException 。 使用 static 代码块实现单例模式1234567891011121314151617package com.zhisheng.thread.thread16;/** * Created by 10412 on 2017/6/4. */public class MyObject&#123; private static MyObject instance = null; private MyObject() &#123; &#125; static &#123; instance = new MyObject(); &#125; public static MyObject getInstance() &#123; return instance; &#125;&#125; ThreadA.java 1234567891011121314package com.zhisheng.thread.thread16;/** * Created by 10412 on 2017/6/4. */public class ThreadA extends Thread&#123; @Override public void run() &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(MyObject.getInstance().hashCode()); &#125; &#125;&#125; Test.java 12345678910111213141516package com.zhisheng.thread.thread16;/** * Created by 10412 on 2017/6/4. */public class Test&#123; public static void main(String[] args) &#123; ThreadA ta1 = new ThreadA(); ThreadA ta2 = new ThreadA(); ThreadA ta3 = new ThreadA(); ta1.start(); ta2.start(); ta3.start(); &#125;&#125; 使用枚举数据类型实现单例模式在使用枚举类时，构造方法会被自动调用，也可以应用这个特性实现单例模式。 123456789101112131415public class MyObject &#123; private enum MyEnumSingleton&#123; INSTANCE; private Resource resource; private MyEnumSingleton()&#123; resource = new Resource(); &#125; public Resource getResource()&#123; return resource; &#125; &#125; public static Resource getResource()&#123; return MyEnumSingleton.INSTANCE.getResource(); &#125;&#125; 测试： 123456789101112131415161718192021import test.MyObject;public class Run &#123; class MyThread extends Thread &#123; @Override public void run() &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(MyObject.getResource().hashCode()); &#125; &#125; &#125; public static void main(String[] args) &#123; Run.MyThread t1 = new Run().new MyThread(); Run.MyThread t2 = new Run().new MyThread(); Run.MyThread t3 = new Run().new MyThread(); t1.start(); t2.start(); t3.start(); &#125;&#125; 这里再推荐一篇 stackoverflow 上的一个问题回答： What is an efficient way to implement a singleton pattern in Java? 总结本篇文章是我读 《Java多线程编程核心技术》 的笔记及自己的一些总结，觉得不错，欢迎点赞和转发。","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"多线程","slug":"多线程","permalink":"http://yoursite.com/tags/多线程/"}]},{"title":"Java NIO 系列教程","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Java NIO 系列教程/","text":"Java NIO（New IO）是从Java 1.4版本开始引入的一个新的IO API，可以替代标准的Java IO API。 Java NIO提供了与标准IO不同的IO工作方式： Channels and Buffers（通道和缓冲区）：标准的IO基于字节流和字符流进行操作的，而NIO是基于通道（Channel）和缓冲区（Buffer）进行操作，数据总是从通道读取到缓冲区中，或者从缓冲区写入到通道中。 Asynchronous IO（异步IO）：Java NIO可以让你异步的使用IO，例如：当线程从通道读取数据到缓冲区时，线程还是可以进行其他事情。当数据被写入到缓冲区时，线程可以继续处理它。从缓冲区写入通道也类似。 Selectors（选择器）：Java NIO引入了选择器的概念，选择器用于监听多个通道的事件（比如：连接打开，数据到达）。因此，单个的线程可以监听多个数据通道。 下面就来详细介绍Java NIO的相关知识。 1、Java NIO 概述Java NIO 由以下几个核心部分组成： Channels Buffers Selectors 虽然 Java NIO 中除此之外还有很多类和组件，但在我看来，Channel，Buffer 和 Selector 构成了核心的 API。其它组件，如 Pipe 和 FileLock，只不过是与三个核心组件共同使用的工具类。因此，在概述中我将集中在这三个组件上。其它组件会在单独的章节中讲到。 Channel 和 Buffer基本上，所有的 IO 在NIO 中都从一个 Channel 开始。Channel 有点象流。 数据可以从 Channel 读到 Buffer 中，也可以从 Buffer 写到 Channel 中。这里有个图示： Channel 和 Buffer 有好几种类型。下面是 JAVA NIO 中的一些主要 Channel 的实现： FileChannel DatagramChannel SocketChannel ServerSocketChannel 正如你所看到的，这些通道涵盖了 UDP 和 TCP 网络 IO，以及文件 IO。 与这些类一起的有一些有趣的接口，但为简单起见，我尽量在概述中不提到它们。本教程其它章节与它们相关的地方我会进行解释。 以下是 Java NIO 里关键的 Buffer 实现： ByteBuffer CharBuffer DoubleBuffer FloatBuffer IntBuffer LongBuffer ShortBuffer 这些 Buffer 覆盖了你能通过 IO 发送的基本数据类型：byte, short, int, long, float, double 和 char。 Java NIO 还有个 MappedByteBuffer，用于表示内存映射文件， 我也不打算在概述中说明。 SelectorSelector 允许单线程处理多个 Channel。如果你的应用打开了多个连接（通道），但每个连接的流量都很低，使用 Selector 就会很方便。例如，在一个聊天服务器中。 这是在一个单线程中使用一个 Selector 处理3个 Channel 的图示： 要使用 Selector，得向 Selector 注册 Channel，然后调用它的 select() 方法。这个方法会一直阻塞到某个注册的通道有事件就绪。一旦这个方法返回，线程就可以处理这些事件，事件的例子有如新连接进来，数据接收等。 2、ChannelJava NIO 的通道类似流，但又有些不同： 既可以从通道中读取数据，又可以写数据到通道。但流的读写通常是单向的。 通道可以异步地读写。 通道中的数据总是要先读到一个 Buffer，或者总是要从一个 Buffer 中写入。 正如上面所说，从通道读取数据到缓冲区，从缓冲区写入数据到通道。如下图所示： Channel 的实现这些是 Java NIO 中最重要的通道的实现： FileChannel DatagramChannel SocketChannel ServerSocketChannel FileChannel 从文件中读写数据。 DatagramChannel 能通过 UDP 读写网络中的数据。 SocketChannel 能通过 TCP 读写网络中的数据。 ServerSocketChannel 可以监听新进来的 TCP 连接，像 Web 服务器那样。对每一个新进来的连接都会创建一个 SocketChannel。 基本的 Channel 示例下面是一个使用 FileChannel 读取数据到 Buffer 中的示例： 12345678910111213141516171819RandomAccessFile aFile = new RandomAccessFile(&quot;data/nio-data.txt&quot;, &quot;rw&quot;);FileChannel inChannel = aFile.getChannel();ByteBuffer buf = ByteBuffer.allocate(48);int bytesRead = inChannel.read(buf);while (bytesRead != -1) &#123;System.out.println(&quot;Read &quot; + bytesRead);buf.flip();while(buf.hasRemaining())&#123;System.out.print((char) buf.get());&#125;buf.clear();bytesRead = inChannel.read(buf);&#125;aFile.close(); 注意 buf.flip() 的调用，首先读取数据到Buffer，然后反转Buffer,接着再从Buffer中读取数据。下一节会深入讲解Buffer的更多细节。 3、BufferJava NIO 中的 Buffer 用于和 NIO 通道进行交互。如你所知，数据是从通道读入缓冲区，从缓冲区写入到通道中的。 缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成 NIO Buffer 对象，并提供了一组方法，用来方便的访问该块内存。 Buffer 的基本用法使用 Buffer 读写数据一般遵循以下四个步骤： 写入数据到 Buffer 调用 flip() 方法 从 Buffer 中读取数据 调用 clear() 方法或者 compact() 方法 当向 buffer 写入数据时，buffer 会记录下写了多少数据。一旦要读取数据，需要先通过 flip() 方法将 Buffer 从写模式切换到读模式。在读模式下，可以读取之前写入到 buffer 的所有数据。 一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入。有两种方式能清空缓冲区：调用 clear() 或 compact() 方法。clear() 方法会清空整个缓冲区。compact() 方法只会清除已经读过的数据，任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。 下面是一个使用Buffer的例子： 12345678910111213141516171819RandomAccessFile aFile = new RandomAccessFile(&quot;data/nio-data.txt&quot;, &quot;rw&quot;);FileChannel inChannel = aFile.getChannel();//create buffer with capacity of 48 bytesByteBuffer buf = ByteBuffer.allocate(48);int bytesRead = inChannel.read(buf); //read into buffer.while (bytesRead != -1) &#123; buf.flip(); //make buffer ready for read while(buf.hasRemaining())&#123; System.out.print((char) buf.get()); // read 1 byte at a time &#125; buf.clear(); //make buffer ready for writing bytesRead = inChannel.read(buf);&#125;aFile.close(); Buffer 的 capacity、 position 和 limit缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成 NIO Buffer 对象，并提供了一组方法，用来方便的访问该块内存。 为了理解 Buffer 的工作原理，需要熟悉它的三个属性： capacity position limit position 和 limit 的含义取决于 Buffer 处在读模式还是写模式。不管 Buffer 处在什么模式，capacity 的含义总是一样的。 这里有一个关于 capacity，position 和 limit 在读写模式中的说明，详细的解释在插图后面。 capacity 作为一个内存块，Buffer 有一个固定的大小值，也叫“capacity”.你只能往里写 capacity 个byte、long，char 等类型。一旦 Buffer 满了，需要将其清空（通过读数据或者清除数据）才能继续写数据往里写数据。 position 当你写数据到 Buffer 中时，position 表示当前的位置。初始的 position 值为 0。当一个 byte、long 等数据写到 Buffer 后， position 会向前移动到下一个可插入数据的 Buffer 单元。position 最大可为 capacity – 1. 当读取数据时，也是从某个特定位置读。当将 Buffer 从写模式切换到读模式，position 会被重置为 0. 当从 Buffer 的 position 处读取数据时，position 向前移动到下一个可读的位置。 limit 在写模式下，Buffer 的 limit 表示你最多能往 Buffer 里写多少数据。 写模式下，limit 等于Buffer 的 capacity。 当切换 Buffer 到读模式时， limit 表示你最多能读到多少数据。因此，当切换 Buffer 到读模式时，limit 会被设置成写模式下的 position 值。换句话说，你能读到之前写入的所有数据（limit被设置成已写数据的数量，这个值在写模式下就是 position） Buffer的类型Java NIO 有以下Buffer类型 ByteBuffer MappedByteBuffer CharBuffer DoubleBuffer FloatBuffer IntBuffer LongBuffer ShortBuffer 如你所见，这些Buffer类型代表了不同的数据类型。换句话说，就是可以通过char，short，int，long，float 或 double类型来操作缓冲区中的字节。 MappedByteBuffer 有些特别，在涉及它的专门章节中再讲。 Buffer 的分配要想获得一个 Buffer 对象首先要进行分配。 每一个 Buffer 类都有一个 allocate 方法。下面是一个分配48字节 capacity 的 ByteBuffer 的例子。 1ByteBuffer buf = ByteBuffer.allocate(48); 这是分配一个可存储1024个字符的 CharBuffer： 1CharBuffer buf = CharBuffer.allocate(1024); 向 Buffer 中写数据写数据到 Buffer 有两种方式： 从 Channel 写到 Buffer。 通过 Buffer 的 put() 方法写到 Buffer 里。 从 Channel 写到 Buffer 的例子： 1int bytesRead = inChannel.read(buf); //read into buffer. 通过put方法写Buffer的例子： 1buf.put(127); put 方法有很多版本，允许你以不同的方式把数据写入到 Buffer 中。例如， 写到一个指定的位置，或者把一个字节数组写入到 Buffer。 更多Buffer实现的细节参考JavaDoc。 flip() 方法 flip() 方法将 Buffer 从写模式切换到读模式。调用 flip() 方法会将 position 设回 0，并将 limit 设置成之前 position 的值。 换句话说，position 现在用于标记读的位置，limit 表示之前写进了多少个 byte、char等 —— 现在能读取多少个byte、char等。 从Buffer中读取数据从Buffer中读取数据有两种方式： 从Buffer读取数据到Channel。 使用get()方法从Buffer中读取数据。 从Buffer读取数据到Channel的例子： 12//read from buffer into channel.int bytesWritten = inChannel.write(buf); 使用get()方法从Buffer中读取数据的例子 1byte aByte = buf.get(); get方法有很多版本，允许你以不同的方式从Buffer中读取数据。例如，从指定position读取，或者从Buffer中读取数据到字节数组。更多Buffer实现的细节参考JavaDoc。 rewind()方法Buffer.rewind()将position设回0，所以你可以重读Buffer中的所有数据。limit保持不变，仍然表示能从Buffer中读取多少个元素（byte、char等）。 clear()与compact()方法一旦读完Buffer中的数据，需要让Buffer准备好再次被写入。可以通过clear()或compact()方法来完成。 如果调用的是clear()方法，position将被设回0，limit被设置成 capacity的值。换句话说，Buffer 被清空了。Buffer中的数据并未清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。 如果Buffer中有一些未读的数据，调用clear()方法，数据将“被遗忘”，意味着不再有任何标记会告诉你哪些数据被读过，哪些还没有。 如果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先先写些数据，那么使用compact()方法。 compact()方法将所有未读的数据拷贝到Buffer起始处。然后将position设到最后一个未读元素正后面。limit属性依然像clear()方法一样，设置成capacity。现在Buffer准备好写数据了，但是不会覆盖未读的数据。 mark()与reset()方法通过调用Buffer.mark()方法，可以标记Buffer中的一个特定position。之后可以通过调用Buffer.reset()方法恢复到这个position。例如： 123buffer.mark();//call buffer.get() a couple of times, e.g. during parsing.buffer.reset(); //set position back to mark. equals()与compareTo()方法可以使用equals()和compareTo()方法两个Buffer。 equals() 当满足下列条件时，表示两个Buffer相等： 有相同的类型（byte、char、int等）。 Buffer中剩余的byte、char等的个数相等。 Buffer中所有剩余的byte、char等都相同。 如你所见，equals只是比较Buffer的一部分，不是每一个在它里面的元素都比较。实际上，它只比较Buffer中的剩余元素。 compareTo()方法 compareTo()方法比较两个Buffer的剩余元素(byte、char等)， 如果满足下列条件，则认为一个Buffer“小于”另一个Buffer： 第一个不相等的元素小于另一个Buffer中对应的元素 。 所有元素都相等，但第一个Buffer比另一个先耗尽(第一个Buffer的元素个数比另一个少)。 （译注：剩余元素是从 position到limit之间的元素） 4、Scatter/GatherJava NIO 开始支持 scatter/gather，scatter/gather 用于描述从 Channel（译者注：Channel 在中文经常翻译为通道）中读取或者写入到 Channel 的操作。 分散（scatter）从 Channel 中读取是指在读操作时将读取的数据写入多个 buffer 中。因此，Channel 将从 Channel 中读取的数据“分散（scatter）”到多个Buffer中。 聚集（gather）写入Channel是指在写操作时将多个buffer的数据写入同一个Channel，因此，Channel 将多个Buffer中的数据“聚集（gather）”后发送到Channel。 scatter / gather经常用于需要将传输的数据分开处理的场合，例如传输一个由消息头和消息体组成的消息，你可能会将消息体和消息头分散到不同的buffer中，这样你可以方便的处理消息头和消息体。 Scattering ReadsScattering Reads是指数据从一个channel读取到多个buffer中。如下图描述： 代码示例如下： 123456ByteBuffer header = ByteBuffer.allocate(128);ByteBuffer body = ByteBuffer.allocate(1024);ByteBuffer[] bufferArray = &#123; header, body &#125;;channel.read(bufferArray); 注意buffer首先被插入到数组，然后再将数组作为channel.read() 的输入参数。read()方法按照buffer在数组中的顺序将从channel中读取的数据写入到buffer，当一个buffer被写满后，channel紧接着向另一个buffer中写。 Scattering Reads在移动下一个buffer前，必须填满当前的buffer，这也意味着它不适用于动态消息(译者注：消息大小不固定)。换句话说，如果存在消息头和消息体，消息头必须完成填充（例如 128byte），Scattering Reads才能正常工作。 Gathering WritesGathering Writes是指数据从多个buffer写入到同一个channel。如下图描述： 代码示例如下： 12345678ByteBuffer header = ByteBuffer.allocate(128);ByteBuffer body = ByteBuffer.allocate(1024);//write data into buffersByteBuffer[] bufferArray = &#123; header, body &#125;;channel.write(bufferArray); buffers数组是write()方法的入参，write()方法会按照buffer在数组中的顺序，将数据写入到channel，注意只有position和limit之间的数据才会被写入。因此，如果一个buffer的容量为128byte，但是仅仅包含58byte的数据，那么这58byte的数据将被写入到channel中。因此与Scattering Reads相反，Gathering Writes能较好的处理动态消息。 5、通道之间的数据传输在Java NIO中，如果两个通道中有一个是FileChannel，那你可以直接将数据从一个channel（译者注：channel中文常译作通道）传输到另外一个channel。 transferFrom()FileChannel的transferFrom()方法可以将数据从源通道传输到FileChannel中（译者注：这个方法在JDK文档中的解释为将字节从给定的可读取字节通道传输到此通道的文件中）。 下面是一个简单的例子： 12345678910RandomAccessFile fromFile = new RandomAccessFile(&quot;fromFile.txt&quot;, &quot;rw&quot;);FileChannel fromChannel = fromFile.getChannel();RandomAccessFile toFile = new RandomAccessFile(&quot;toFile.txt&quot;, &quot;rw&quot;);FileChannel toChannel = toFile.getChannel();long position = 0;long count = fromChannel.size();toChannel.transferFrom(position, count, fromChannel); 方法的输入参数position表示从position处开始向目标文件写入数据，count表示最多传输的字节数。如果源通道的剩余空间小于 count 个字节，则所传输的字节数要小于请求的字节数。 此外要注意，在SoketChannel的实现中，SocketChannel只会传输此刻准备好的数据（可能不足count字节）。因此，SocketChannel可能不会将请求的所有数据(count个字节)全部传输到FileChannel中。 transferTo()transferTo()方法将数据从FileChannel传输到其他的channel中。 下面是一个简单的例子： 12345678910RandomAccessFile fromFile = new RandomAccessFile(&quot;fromFile.txt&quot;, &quot;rw&quot;);FileChannel fromChannel = fromFile.getChannel();RandomAccessFile toFile = new RandomAccessFile(&quot;toFile.txt&quot;, &quot;rw&quot;);FileChannel toChannel = toFile.getChannel();long position = 0;long count = fromChannel.size();fromChannel.transferTo(position, count, toChannel); 是不是发现这个例子和前面那个例子特别相似？除了调用方法的FileChannel对象不一样外，其他的都一样。 上面所说的关于SocketChannel的问题在transferTo()方法中同样存在。SocketChannel会一直传输数据直到目标buffer被填满。 6、SelectorSelector（选择器）是Java NIO中能够检测一到多个NIO通道，并能够知晓通道是否为诸如读写事件做好准备的组件。这样，一个单独的线程可以管理多个channel，从而管理多个网络连接。 为什么使用Selector?仅用单个线程来处理多个Channels的好处是，只需要更少的线程来处理通道。事实上，可以只用一个线程处理所有的通道。对于操作系统来说，线程之间上下文切换的开销很大，而且每个线程都要占用系统的一些资源（如内存）。因此，使用的线程越少越好。 但是，需要记住，现代的操作系统和CPU在多任务方面表现的越来越好，所以多线程的开销随着时间的推移，变得越来越小了。实际上，如果一个CPU有多个内核，不使用多任务可能是在浪费CPU能力。不管怎么说，关于那种设计的讨论应该放在另一篇不同的文章中。在这里，只要知道使用Selector能够处理多个通道就足够了。 下面是单线程使用一个Selector处理3个channel的示例图： Selector的创建通过调用Selector.open()方法创建一个Selector，如下： 1Selector selector = Selector.open(); 向Selector注册通道为了将Channel和Selector配合使用，必须将channel注册到selector上。通过SelectableChannel.register()方法来实现，如下： 123channel.configureBlocking(false);SelectionKey key = channel.register(selector, Selectionkey.OP_READ); 与Selector一起使用时，Channel必须处于非阻塞模式下。这意味着不能将FileChannel与Selector一起使用，因为FileChannel不能切换到非阻塞模式。而套接字通道都可以。 注意register()方法的第二个参数。这是一个“interest集合”，意思是在通过Selector监听Channel时对什么事件感兴趣。可以监听四种不同类型的事件： Connect Accept Read Write 通道触发了一个事件意思是该事件已经就绪。所以，某个channel成功连接到另一个服务器称为“连接就绪”。一个server socket channel准备好接收新进入的连接称为“接收就绪”。一个有数据可读的通道可以说是“读就绪”。等待写数据的通道可以说是“写就绪”。 这四种事件用SelectionKey的四个常量来表示： SelectionKey.OP_CONNECT SelectionKey.OP_ACCEPT SelectionKey.OP_READ SelectionKey.OP_WRITE 如果你对不止一种事件感兴趣，那么可以用“位或”操作符将常量连接起来，如下： 1int interestSet = SelectionKey.OP_READ | SelectionKey.OP_WRITE; 在下面还会继续提到interest集合。 SelectionKey在上一小节中，当向Selector注册Channel时，register()方法会返回一个SelectionKey对象。这个对象包含了一些你感兴趣的属性： interest集合 ready集合 Channel Selector 附加的对象（可选）下面我会描述这些属性。 interest集合就像向Selector注册通道一节中所描述的，interest集合是你所选择的感兴趣的事件集合。可以通过SelectionKey读写interest集合，像这样： 123456int interestSet = selectionKey.interestOps();boolean isInterestedInAccept = (interestSet &amp; SelectionKey.OP_ACCEPT) == SelectionKey.OP_ACCEPT；boolean isInterestedInConnect = interestSet &amp; SelectionKey.OP_CONNECT;boolean isInterestedInRead = interestSet &amp; SelectionKey.OP_READ;boolean isInterestedInWrite = interestSet &amp; SelectionKey.OP_WRITE; 可以看到，用“位与”操作interest 集合和给定的SelectionKey常量，可以确定某个确定的事件是否在interest 集合中。 ready集合ready 集合是通道已经准备就绪的操作的集合。在一次选择(Selection)之后，你会首先访问这个ready set。Selection将在下一小节进行解释。可以这样访问ready集合： 1int readySet = selectionKey.readyOps(); 可以用像检测interest集合那样的方法，来检测channel中什么事件或操作已经就绪。但是，也可以使用以下四个方法，它们都会返回一个布尔类型： 1234selectionKey.isAcceptable();selectionKey.isConnectable();selectionKey.isReadable();selectionKey.isWritable(); Channel + Selector从SelectionKey访问Channel和Selector很简单。如下： 12Channel channel = selectionKey.channel();Selector selector = selectionKey.selector(); 附加的对象可以将一个对象或者更多信息附着到SelectionKey上，这样就能方便的识别某个给定的通道。例如，可以附加 与通道一起使用的Buffer，或是包含聚集数据的某个对象。使用方法如下： 12selectionKey.attach(theObject);Object attachedObj = selectionKey.attachment(); 还可以在用register()方法向Selector注册Channel的时候附加对象。如： 1SelectionKey key = channel.register(selector, SelectionKey.OP_READ, theObject); 通过Selector选择通道一旦向Selector注册了一或多个通道，就可以调用几个重载的select()方法。这些方法返回你所感兴趣的事件（如连接、接受、读或写）已经准备就绪的那些通道。换句话说，如果你对“读就绪”的通道感兴趣，select()方法会返回读事件已经就绪的那些通道。 下面是select()方法： int select() int select(long timeout) int selectNow() select()阻塞到至少有一个通道在你注册的事件上就绪了。 select(long timeout)和select()一样，除了最长会阻塞timeout毫秒(参数)。 selectNow()不会阻塞，不管什么通道就绪都立刻返回（译者注：此方法执行非阻塞的选择操作。如果自从前一次选择操作后，没有通道变成可选择的，则此方法直接返回零。）。 select()方法返回的int值表示有多少通道已经就绪。亦即，自上次调用select()方法后有多少通道变成就绪状态。如果调用select()方法，因为有一个通道变成就绪状态，返回了1，若再次调用select()方法，如果另一个通道就绪了，它会再次返回1。如果对第一个就绪的channel没有做任何操作，现在就有两个就绪的通道，但在每次select()方法调用之间，只有一个通道就绪了。 selectedKeys()一旦调用了select()方法，并且返回值表明有一个或更多个通道就绪了，然后可以通过调用selector的selectedKeys()方法，访问“已选择键集（selected key set）”中的就绪通道。如下所示： 当像Selector注册Channel时，Channel.register()方法会返回一个SelectionKey 对象。这个对象代表了注册到该Selector的通道。可以通过SelectionKey的selectedKeySet()方法访问这些对象。 可以遍历这个已选择的键集合来访问就绪的通道。如下： 123456789101112131415Set selectedKeys = selector.selectedKeys();Iterator keyIterator = selectedKeys.iterator();while(keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if(key.isAcceptable()) &#123; // a connection was accepted by a ServerSocketChannel. &#125; else if (key.isConnectable()) &#123; // a connection was established with a remote server. &#125; else if (key.isReadable()) &#123; // a channel is ready for reading &#125; else if (key.isWritable()) &#123; // a channel is ready for writing &#125; keyIterator.remove();&#125; 这个循环遍历已选择键集中的每个键，并检测各个键所对应的通道的就绪事件。 注意每次迭代末尾的keyIterator.remove()调用。Selector不会自己从已选择键集中移除SelectionKey实例。必须在处理完通道时自己移除。下次该通道变成就绪时，Selector会再次将其放入已选择键集中。 SelectionKey.channel()方法返回的通道需要转型成你要处理的类型，如ServerSocketChannel或SocketChannel等。 wakeUp()某个线程调用select()方法后阻塞了，即使没有通道已经就绪，也有办法让其从select()方法返回。只要让其它线程在第一个线程调用select()方法的那个对象上调用Selector.wakeup()方法即可。阻塞在select()方法上的线程会立马返回。 如果有其它线程调用了wakeup()方法，但当前没有线程阻塞在select()方法上，下个调用select()方法的线程会立即“醒来（wake up）”。 close()用完Selector后调用其close()方法会关闭该Selector，且使注册到该Selector上的所有SelectionKey实例无效。通道本身并不会关闭。 完整的示例这里有一个完整的示例，打开一个Selector，注册一个通道注册到这个Selector上(通道的初始化过程略去),然后持续监控这个Selector的四种事件（接受，连接，读，写）是否就绪。 12345678910111213141516171819202122Selector selector = Selector.open();channel.configureBlocking(false);SelectionKey key = channel.register(selector, SelectionKey.OP_READ);while(true) &#123; int readyChannels = selector.select(); if(readyChannels == 0) continue; Set selectedKeys = selector.selectedKeys(); Iterator keyIterator = selectedKeys.iterator(); while(keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if(key.isAcceptable()) &#123; // a connection was accepted by a ServerSocketChannel. &#125; else if (key.isConnectable()) &#123; // a connection was established with a remote server. &#125; else if (key.isReadable()) &#123; // a channel is ready for reading &#125; else if (key.isWritable()) &#123; // a channel is ready for writing &#125; keyIterator.remove(); &#125;&#125; 7、FileChannelJava NIO中的FileChannel是一个连接到文件的通道。可以通过文件通道读写文件。 FileChannel无法设置为非阻塞模式，它总是运行在阻塞模式下。 打开FileChannel在使用FileChannel之前，必须先打开它。但是，我们无法直接打开一个FileChannel，需要通过使用一个InputStream、OutputStream或RandomAccessFile来获取一个FileChannel实例。下面是通过RandomAccessFile打开FileChannel的示例： 12RandomAccessFile aFile = new RandomAccessFile(&quot;data/nio-data.txt&quot;, &quot;rw&quot;);FileChannel inChannel = aFile.getChannel(); 从FileChannel读取数据调用多个read()方法之一从FileChannel中读取数据。如： 12ByteBuffer buf = ByteBuffer.allocate(48);int bytesRead = inChannel.read(buf); 首先，分配一个Buffer。从FileChannel中读取的数据将被读到Buffer中。 然后，调用FileChannel.read()方法。该方法将数据从FileChannel读取到Buffer中。read()方法返回的int值表示了有多少字节被读到了Buffer中。如果返回-1，表示到了文件末尾。 向FileChannel写数据使用FileChannel.write()方法向FileChannel写数据，该方法的参数是一个Buffer。如： 1234567891011String newData = &quot;New String to write to file...&quot; + System.currentTimeMillis();ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();buf.put(newData.getBytes());buf.flip();while(buf.hasRemaining()) &#123; channel.write(buf);&#125; 注意FileChannel.write()是在while循环中调用的。因为无法保证write()方法一次能向FileChannel写入多少字节，因此需要重复调用write()方法，直到Buffer中已经没有尚未写入通道的字节。 关闭FileChannel用完FileChannel后必须将其关闭。如：1channel.close(); FileChannel的position方法有时可能需要在FileChannel的某个特定位置进行数据的读/写操作。可以通过调用position()方法获取FileChannel的当前位置。 也可以通过调用position(long pos)方法设置FileChannel的当前位置。 这里有两个例子:12long pos = channel.position();channel.position(pos +123); 如果将位置设置在文件结束符之后，然后试图从文件通道中读取数据，读方法将返回-1 —— 文件结束标志。 如果将位置设置在文件结束符之后，然后向通道中写数据，文件将撑大到当前位置并写入数据。这可能导致“文件空洞”，磁盘上物理文件中写入的数据间有空隙。 FileChannel的size方法FileChannel实例的size()方法将返回该实例所关联文件的大小。如:1long fileSize = channel.size(); FileChannel的truncate方法可以使用FileChannel.truncate()方法截取一个文件。截取文件时，文件将中指定长度后面的部分将被删除。如：1channel.truncate(1024); 这个例子截取文件的前1024个字节。 FileChannel的force方法FileChannel.force()方法将通道里尚未写入磁盘的数据强制写到磁盘上。出于性能方面的考虑，操作系统会将数据缓存在内存中，所以无法保证写入到FileChannel里的数据一定会即时写到磁盘上。要保证这一点，需要调用force()方法。 force()方法有一个boolean类型的参数，指明是否同时将文件元数据（权限信息等）写到磁盘上。 下面的例子同时将文件数据和元数据强制写到磁盘上： 查看源代码打印帮助1channel.force(true); 8、SocketChannelJava NIO中的SocketChannel是一个连接到TCP网络套接字的通道。可以通过以下2种方式创建SocketChannel： 打开一个SocketChannel并连接到互联网上的某台服务器。一个新连接到达ServerSocketChannel时，会创建一个SocketChannel。 打开 SocketChannel下面是SocketChannel的打开方式： 12SocketChannel socketChannel = SocketChannel.open();socketChannel.connect(new InetSocketAddress(&quot;http://jenkov.com&quot;, 80)); 关闭 SocketChannel 当用完SocketChannel之后调用SocketChannel.close()关闭SocketChannel： 1socketChannel.close(); 从 SocketChannel 读取数据要从SocketChannel中读取数据，调用一个read()的方法之一。以下是例子： 12ByteBuffer buf = ByteBuffer.allocate(48);int bytesRead = socketChannel.read(buf); 首先，分配一个Buffer。从SocketChannel读取到的数据将会放到这个Buffer中。 然后，调用SocketChannel.read()。该方法将数据从SocketChannel 读到Buffer中。read()方法返回的int值表示读了多少字节进Buffer里。如果返回的是-1，表示已经读到了流的末尾（连接关闭了）。 写入 SocketChannel写数据到SocketChannel用的是SocketChannel.write()方法，该方法以一个Buffer作为参数。示例如下： 1234567891011String newData = &quot;New String to write to file...&quot; + System.currentTimeMillis();ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();buf.put(newData.getBytes());buf.flip();while(buf.hasRemaining()) &#123; channel.write(buf);&#125; 注意SocketChannel.write()方法的调用是在一个while循环中的。Write()方法无法保证能写多少字节到SocketChannel。所以，我们重复调用write()直到Buffer没有要写的字节为止。 非阻塞模式可以设置 SocketChannel 为非阻塞模式（non-blocking mode）.设置之后，就可以在异步模式下调用connect(), read() 和write()了。 connect()如果SocketChannel在非阻塞模式下，此时调用connect()，该方法可能在连接建立之前就返回了。为了确定连接是否建立，可以调用finishConnect()的方法。像这样： 123456socketChannel.configureBlocking(false);socketChannel.connect(new InetSocketAddress(&quot;http://jenkov.com&quot;, 80));while(! socketChannel.finishConnect() )&#123; //wait, or do something else...&#125; write()非阻塞模式下，write()方法在尚未写出任何内容时可能就返回了。所以需要在循环中调用write()。前面已经有例子了，这里就不赘述了。 read()非阻塞模式下,read()方法在尚未读取到任何数据时可能就返回了。所以需要关注它的int返回值，它会告诉你读取了多少字节。 非阻塞模式与选择器非阻塞模式与选择器搭配会工作的更好，通过将一或多个SocketChannel注册到Selector，可以询问选择器哪个通道已经准备好了读取，写入等。Selector与SocketChannel的搭配使用会在后面详讲。 9、ServerSocketChannelJava NIO中的 ServerSocketChannel 是一个可以监听新进来的TCP连接的通道, 就像标准IO中的ServerSocket一样。ServerSocketChannel类在 java.nio.channels包中。 这里有个例子： 12345678910ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();serverSocketChannel.socket().bind(new InetSocketAddress(9999));while(true)&#123; SocketChannel socketChannel = serverSocketChannel.accept(); //do something with socketChannel...&#125; 打开 ServerSocketChannel通过调用 ServerSocketChannel.open() 方法来打开ServerSocketChannel.如： 1ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); 关闭 ServerSocketChannel通过调用ServerSocketChannel.close() 方法来关闭ServerSocketChannel. 如： 1serverSocketChannel.close(); 监听新进来的连接通过 ServerSocketChannel.accept() 方法监听新进来的连接。当 accept()方法返回的时候,它返回一个包含新进来的连接的 SocketChannel。因此, accept()方法会一直阻塞到有新连接到达。 通常不会仅仅只监听一个连接,在while循环中调用 accept()方法. 如下面的例子： 123456while(true)&#123; SocketChannel socketChannel = serverSocketChannel.accept(); //do something with socketChannel...&#125; 当然,也可以在while循环中使用除了true以外的其它退出准则。 非阻塞模式ServerSocketChannel可以设置成非阻塞模式。在非阻塞模式下，accept() 方法会立刻返回，如果还没有新进来的连接,返回的将是null。 因此，需要检查返回的SocketChannel是否是null.如： 12345678910111213ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();serverSocketChannel.socket().bind(new InetSocketAddress(9999));serverSocketChannel.configureBlocking(false);while(true)&#123; SocketChannel socketChannel = serverSocketChannel.accept(); if(socketChannel != null)&#123; //do something with socketChannel... &#125;&#125; 10、Java NIO DatagramChannelJava NIO中的DatagramChannel是一个能收发UDP包的通道。因为UDP是无连接的网络协议，所以不能像其它通道那样读取和写入。它发送和接收的是数据包。 打开 DatagramChannel下面是 DatagramChannel 的打开方式： 12DatagramChannel channel = DatagramChannel.open();channel.socket().bind(new InetSocketAddress(9999)); 这个例子打开的 DatagramChannel可以在UDP端口9999上接收数据包。 接收数据通过receive()方法从DatagramChannel接收数据，如： 123ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();channel.receive(buf); receive()方法会将接收到的数据包内容复制到指定的Buffer. 如果Buffer容不下收到的数据，多出的数据将被丢弃。 发送数据通过send()方法从DatagramChannel发送数据，如: 12345678String newData = &quot;New String to write to file...&quot; + System.currentTimeMillis();ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();buf.put(newData.getBytes());buf.flip();int bytesSent = channel.send(buf, new InetSocketAddress(&quot;jenkov.com&quot;, 80)); 这个例子发送一串字符到”jenkov.com”服务器的UDP端口80。 因为服务端并没有监控这个端口，所以什么也不会发生。也不会通知你发出的数据包是否已收到，因为UDP在数据传送方面没有任何保证。 连接到特定的地址可以将DatagramChannel“连接”到网络中的特定地址的。由于UDP是无连接的，连接到特定地址并不会像TCP通道那样创建一个真正的连接。而是锁住DatagramChannel ，让其只能从特定地址收发数据。 这里有个例子: 1channel.connect(new InetSocketAddress(&quot;jenkov.com&quot;, 80)); 当连接后，也可以使用read()和write()方法，就像在用传统的通道一样。只是在数据传送方面没有任何保证。这里有几个例子： 12int bytesRead = channel.read(buf);int bytesWritten = channel.write(but); 11、PipeJava NIO 管道是2个线程之间的单向数据连接。Pipe有一个source通道和一个sink通道。数据会被写到sink通道，从source通道读取。 这里是Pipe原理的图示： 创建管道通过Pipe.open()方法打开管道。例如： 1Pipe pipe = Pipe.open(); 向管道写数据要向管道写数据，需要访问sink通道。像这样： 1Pipe.SinkChannel sinkChannel = pipe.sink(); 通过调用SinkChannel的write()方法，将数据写入SinkChannel,像这样： 12345678910String newData = &quot;New String to write to file...&quot; + System.currentTimeMillis();ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();buf.put(newData.getBytes());buf.flip();while(buf.hasRemaining()) &#123; sinkChannel.write(buf);&#125; 从管道读取数据从读取管道的数据，需要访问source通道，像这样： 1Pipe.SourceChannel sourceChannel = pipe.source(); 调用source通道的read()方法来读取数据，像这样： 123ByteBuffer buf = ByteBuffer.allocate(48);int bytesRead = sourceChannel.read(buf); read()方法返回的int值会告诉我们多少字节被读进了缓冲区。 12、Java NIO与IO的对比当学习了Java NIO和IO的API后，一个问题马上涌入脑海： 我应该何时使用IO，何时使用NIO呢？在本文中，我会尽量清晰地解析Java NIO和IO的差异、它们的使用场景，以及它们如何影响您的代码设计。 Java NIO和IO的主要区别下表总结了Java NIO和IO之间的主要差别，我会更详细地描述表中每部分的差异。 IO NIO Stream oriented Buffer oriented Blocking IO Non blocking IO 无 Selectors 面向流与面向缓冲Java NIO和IO之间第一个最大的区别是，IO是面向流的，NIO是面向缓冲区的。 Java IO面向流意味着每次从流中读一个或多个字节，直至读取所有字节，它们没有被缓存在任何地方。此外，它不能前后移动流中的数据。如果需要前后移动从流中读取的数据，需要先将它缓存到一个缓冲区。 Java NIO的缓冲导向方法略有不同。数据读取到一个它稍后处理的缓冲区，需要时可在缓冲区中前后移动。这就增加了处理过程中的灵活性。但是，还需要检查是否该缓冲区中包含所有您需要处理的数据。而且，需确保当更多的数据读入缓冲区时，不要覆盖缓冲区里尚未处理的数据。 阻塞与非阻塞IOJava IO的各种流是阻塞的。这意味着，当一个线程调用read() 或 write()时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了。 Java NIO的非阻塞模式，使一个线程从某通道发送请求读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取。而不是保持线程阻塞，所以直至数据变的可以读取之前，该线程可以继续做其他的事情。 非阻塞写也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。 线程通常将非阻塞IO的空闲时间用于在其它通道上执行IO操作，所以一个单独的线程现在可以管理多个输入和输出通道（channel）。 选择器（Selectors）Java NIO的选择器允许一个单独的线程来监视多个输入通道，你可以注册多个通道使用一个选择器，然后使用一个单独的线程来“选择”通道：这些通道里已经有可以处理的输入，或者选择已准备写入的通道。这种选择机制，使得一个单独的线程很容易来管理多个通道。 NIO和IO如何影响应用程序的设计无论您选择IO或NIO工具箱，可能会影响您应用程序设计的以下几个方面： 对NIO或IO类的API调用。 数据处理。 用来处理数据的线程数。 API调用当然，使用NIO的API调用时看起来与使用IO时有所不同，但这并不意外，因为并不是仅从一个InputStream逐字节读取，而是数据必须先读入缓冲区再处理。 数据处理使用纯粹的NIO设计相较IO设计，数据处理也受到影响。 在IO设计中，我们从InputStream或 Reader逐字节读取数据。假设你正在处理一基于行的文本数据流，例如： 1234Name: AnnaAge: 25Email: anna@mailserver.comPhone: 1234567890 该文本行的流可以这样处理： 1234567InputStream input = … ; // get the InputStream from the client socketBufferedReader reader = new BufferedReader(new InputStreamReader(input));String nameLine = reader.readLine();String ageLine = reader.readLine();String emailLine = reader.readLine();String phoneLine = reader.readLine(); 请注意处理状态由程序执行多久决定。换句话说，一旦reader.readLine()方法返回，你就知道肯定文本行就已读完， readline()阻塞直到整行读完，这就是原因。你也知道此行包含名称；同样，第二个readline()调用返回的时候，你知道这行包含年龄等。 正如你可以看到，该处理程序仅在有新数据读入时运行，并知道每步的数据是什么。一旦正在运行的线程已处理过读入的某些数据，该线程不会再回退数据（大多如此）。下图也说明了这条原则： 上图：从一个阻塞的流中读数据 而一个NIO的实现会有所不同，下面是一个简单的例子： 123ByteBuffer buffer = ByteBuffer.allocate(48);int bytesRead = inChannel.read(buffer); 注意第二行，从通道读取字节到ByteBuffer。当这个方法调用返回时，你不知道你所需的所有数据是否在缓冲区内。你所知道的是，该缓冲区包含一些字节，这使得处理有点困难。假设第一次 read(buffer)调用后，读入缓冲区的数据只有半行，例如，“Name:An”，你能处理数据吗？显然不能，需要等待，直到整行数据读入缓存，在此之前，对数据的任何处理毫无意义。 所以，你怎么知道是否该缓冲区包含足够的数据可以处理呢？好了，你不知道。发现的方法只能查看缓冲区中的数据。其结果是，在你知道所有数据都在缓冲区里之前，你必须检查几次缓冲区的数据。这不仅效率低下，而且可以使程序设计方案杂乱不堪。例如： 12345ByteBuffer buffer = ByteBuffer.allocate(48);int bytesRead = inChannel.read(buffer);while(! bufferFull(bytesRead) ) &#123;bytesRead = inChannel.read(buffer);&#125; bufferFull()方法必须跟踪有多少数据读入缓冲区，并返回真或假，这取决于缓冲区是否已满。换句话说，如果缓冲区准备好被处理，那么表示缓冲区满了。 bufferFull()方法扫描缓冲区，但必须保持在bufferFull（）方法被调用之前状态相同。如果没有，下一个读入缓冲区的数据可能无法读到正确的位置。这是不可能的，但却是需要注意的又一问题。 如果缓冲区已满，它可以被处理。如果它不满，并且在你的实际案例中有意义，你或许能处理其中的部分数据。但是许多情况下并非如此。下图展示了“缓冲区数据循环就绪”： 上图：从一个通道里读数据，直到所有的数据都读到缓冲区里 总结NIO可让您只使用一个（或几个）单线程管理多个通道（网络连接或文件），但付出的代价是解析数据可能会比从一个阻塞流中读取数据更复杂。 如果需要管理同时打开的成千上万个连接，这些连接每次只是发送少量的数据，例如聊天服务器，实现NIO的服务器可能是一个优势。同样，如果你需要维持许多打开的连接到其他计算机上，如P2P网络中，使用一个单独的线程来管理你所有出站连接，可能是一个优势。一个线程多个连接的设计方案如下图所示： 上图：单线程管理多个连接 如果你有少量的连接使用非常高的带宽，一次发送大量的数据，也许典型的IO服务器实现可能非常契合。下图说明了一个典型的IO服务器设计： 上图：一个典型的IO服务器设计：一个连接通过一个线程处理 注明：文章转载自 NIO|并发编程网，二次转载请务必注明原出处！","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"NIO","slug":"NIO","permalink":"http://yoursite.com/tags/NIO/"}]},{"title":"Java连接Oracle数据库的三种连接方式","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Java连接Oracle数据库的三种连接方式/","text":"背景：这两天在学习Oracle数据库，这里就总结下自己上课所学的知识，同时记录下来，方便整理当天所学下的知识，也同时方便日后自己查询。 SQL语句的话，这里我就不多讲了，感觉和其他的数据库（MySQL、SQL Server）都是类似，区别不大。 今天在这里就写下 Java 连接 Oracle 数据库的三种连接方式。 工具： Oracle Database 10g Express Edition cmd命令窗口 IDEA 2016.1.3 ojdbc6_g.jar（数据库驱动包） jdk 1.8 创建数据库表：首先在本地写好创建的数据库表的创建代码后，然后粘贴在cmd命令窗口下，即可创建成功。（前提是进入安装好了oracle，进入了用户，然后在当前用户下创建这个表） 部门表：tb1_dept （含有id name city三个属性） 12345create table tb1_dept( id number(5) primary key, name varchar2(10) not null, city varchar2(10) not null); 插入数据：然后同样写好插入数据的sql语句，这里我就写三条数据。 123insert into tb1_dept(id, name, city) values(1,&apos;java&apos;, &apos;南昌&apos;);insert into tb1_dept(id, name, city) values(2,&apos;c&apos;, &apos;上海&apos;);insert into tb1_dept(id, name, city) values(3,&apos;java&apos;, &apos;南昌&apos;); 好，数据库表已经创建好了，接下来我们需要准备的是数据库驱动包。 这里我用的是 ojdbc6_g.jar 驱动包。 接下来先了解一些基础知识： JDBC的六大步骤：这里我们就按照jdbc的这六大步骤执行下去： 注册驱动 获取连接 获取执行sql语句对象 执行sql语句 处理结果集 关闭资源 URL：统一资源定位器 oracle URL： jdbc:oracle:thin:@localhost:1521:XE jdbc:oracle:thin:@127.0.0.1:1521:XE MySQL URL：jdbc:mysql://localhost:3306/数据库名称 thin：小型驱动，驱动方式 @localhost 本机ip地址 127.0.0.1 XE：数据库的名字 ipconfig：ip地址查询 URI：统一资源标识符 URN：用特定命名空间的名字标识资源 如果你不知道 URL、 URI、URN三者的区别的话，那么你可以参考下面我推荐的一篇文章。 你知道URL、URI和URN三者之间的区别吗？ 三种连接方式代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586package cn.zhisheng.test.jdbc;import oracle.jdbc.driver.OracleDriver;import java.sql.*;import java.util.Properties;/** * Created by 10412 on 2016/12/27. * JDBC的六大步骤 * JAVA连接Oracle的三种方式 */public class JdbcTest&#123; public static void main(String[] args) &#123; Connection connect = null; Statement statement = null; ResultSet resultSet = null; try &#123; //第一步：注册驱动 //第一种方式：类加载(常用) //Class.forName(\"oracle.jdbc.OracleDriver\"); //第二种方式：利用Driver对象 Driver driver = new OracleDriver(); DriverManager.deregisterDriver(driver); //第三种方式:利用系统参数 需在idea中配置program arguments为下面的参数 //-Djdbc.drivers = oracle.jdbc.OracleDriver //第二步：获取连接 //第一种方式：利用DriverManager（常用） //connect = DriverManager.getConnection(\"jdbc:oracle:thin:@localhost:1521:XE\", \"你的oracle数据库用户名\", \"用户名密码\"); //第二种方式：直接使用Driver Properties pro = new Properties(); pro.put(\"user\", \"你的oracle数据库用户名\"); pro.put(\"password\", \"用户名密码\"); connect = driver.connect(\"jdbc:oracle:thin:@localhost:1521:XE\", pro); //测试connect正确与否 System.out.println(connect); //第三步：获取执行sql语句对象 //第一种方式:statement //statement = connect.createStatement(); //第二种方式：PreStatement PreparedStatement preState = connect.prepareStatement(\"select * from tb1_dept where id = ?\"); //第四步：执行sql语句 //第一种方式： //resultSet = statement.executeQuery(\"select * from tb1_dept\"); //第二种方式： preState.setInt(1, 2);//1是指sql语句中第一个？, 2是指第一个？的values值 //resultSet = preState.executeQuery(); //执行查询语句 //查询任何语句，如果有结果集，返回true，没有的话返回false,注意如果是插入一条数据的话，虽然是没有结果集，返回false，但是却能成功的插入一条数据 boolean execute = preState.execute(); System.out.println(execute); //第五步：处理结果集 while (resultSet.next()) &#123; int id = resultSet.getInt(\"id\"); String name = resultSet.getString(\"name\"); String city = resultSet.getString(\"city\"); System.out.println(id+\" \"+name+\" \"+city); //打印输出结果集 &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; //第六步：关闭资源 try &#123; if (resultSet!=null) resultSet.close(); if (statement!=null) statement.close(); if (connect!=null) connect.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 注解：1、 第一步：注册驱动 中的第三种方法 利用系统参数 需在idea中配置program arguments为下面的参数 这里我说一下怎么在IDEA中的配置方式吧 运行截图： OK ! 下篇文章将写 JDBC 的封装。","tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/数据库/"},{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"Oracle","slug":"Oracle","permalink":"http://yoursite.com/tags/Oracle/"}]},{"title":"【字符串】判断两字符串是否互为旋转词？","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/【字符串】判断两字符串是否互为旋转词？/","text":"相关阅读：字符串逆序问题的解决方法题目： 如果对于一个字符串A，将A的前面任意一部分挪到后边去形成的字符串称为A的旋转词。 比如A=”12345”,A的旋转词有”12345”,”23451”,”34512”,”45123”和”51234”。 对于两个字符串A和B，请判断A和B是否互为旋转词。 给定两个字符串A和B及他们的长度lena，lenb，请返回一个bool值，代表他们是否互为旋转词。 测试样例： “cdab”,4,”abcd”,4 返回：true 通过代码： 1234567891011121314import java.util.*;public class Rotation&#123; public static boolean chkRotation(String A, int lena, String B, int lenb) &#123; // write code here if (lena != lenb)&#123; return false; &#125;else &#123; String str = A + A; return str.contains(B); &#125; &#125;&#125; 也可以使用 indexOf。 其区别是： contains 是找指定字符串是否包含一个字符串，返回值的 boolean 类型，即只有 true 和 false indexOf 有多个重载，但无论哪个，都是做一定的匹配，然后把匹配的第一个字符的位置返回，返回的是 int 类型，如果没找到，那么返回 -1 稍微再深究一下的我看了下 contains 的源码，结果发现他调用的是 indexOf 方法。 源码如下： 1234567891011/** * Returns true if and only if this string contains the specified * sequence of char values. * * @param s the sequence to search for * @return true if this string contains &#123;@code s&#125;, false otherwise * @since 1.5 */ public boolean contains(CharSequence s) &#123; return indexOf(s.toString()) &gt; -1; &#125; 意思就是如上面的区别所说的，他只有两个返回值 true 和 false。 于是我们继续看一下 indexOf 方法的源码： 1234567891011121314151617/** * Returns the index within this string of the first occurrence of the * specified substring. * * &lt;p&gt;The returned index is the smallest value &lt;i&gt;k&lt;/i&gt; for which: * &lt;blockquote&gt;&lt;pre&gt; * this.startsWith(str, &lt;i&gt;k&lt;/i&gt;) * &lt;/pre&gt;&lt;/blockquote&gt; * If no such value of &lt;i&gt;k&lt;/i&gt; exists, then &#123;@code -1&#125; is returned. * * @param str the substring to search for. * @return the index of the first occurrence of the specified substring, * or &#123;@code -1&#125; if there is no such occurrence. public int indexOf(String str) &#123; return indexOf(str, 0); &#125; 继续可以发现他又调用了 indexOf 的两个参数方法，只不过索引是 0 。 然后我继续看带有两个参数的 indexOf 方法源码如下： 123456789101112131415161718192021/** * Returns the index within this string of the first occurrence of the * specified substring, starting at the specified index. * * &lt;p&gt;The returned index is the smallest value &lt;i&gt;k&lt;/i&gt; for which: * &lt;blockquote&gt;&lt;pre&gt; * &lt;i&gt;k&lt;/i&gt; &amp;gt;= fromIndex &#123;@code &amp;&amp;&#125; this.startsWith(str, &lt;i&gt;k&lt;/i&gt;) * &lt;/pre&gt;&lt;/blockquote&gt; * If no such value of &lt;i&gt;k&lt;/i&gt; exists, then &#123;@code -1&#125; is returned. * * @param str the substring to search for. * @param fromIndex the index from which to start the search. * @return the index of the first occurrence of the specified substring, * starting at the specified index, * or &#123;@code -1&#125; if there is no such occurrence. */ public int indexOf(String str, int fromIndex) &#123; return indexOf(value, 0, value.length, str.value, 0, str.value.length, fromIndex); &#125; 哈哈，发现他又调用了 indexOf 的方法，这次终于我们可以看到最后的 查找算法 如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Code shared by String and StringBuffer to do searches. The * source is the character array being searched, and the target * is the string being searched for. * * @param source the characters being searched. * @param sourceOffset offset of the source string. * @param sourceCount count of the source string. * @param target the characters being searched for. * @param targetOffset offset of the target string. * @param targetCount count of the target string. * @param fromIndex the index to begin searching from. */static int indexOf(char[] source, int sourceOffset, int sourceCount, char[] target, int targetOffset, int targetCount, int fromIndex) &#123; if (fromIndex &gt;= sourceCount) &#123; return (targetCount == 0 ? sourceCount : -1); &#125; if (fromIndex &lt; 0) &#123; fromIndex = 0; &#125; if (targetCount == 0) &#123; return fromIndex; &#125; char first = target[targetOffset]; int max = sourceOffset + (sourceCount - targetCount); for (int i = sourceOffset + fromIndex; i &lt;= max; i++) &#123; /* Look for first character. */ if (source[i] != first) &#123; while (++i &lt;= max &amp;&amp; source[i] != first); &#125; /* Found first character, now look at the rest of v2 */ if (i &lt;= max) &#123; int j = i + 1; int end = j + targetCount - 1; for (int k = targetOffset + 1; j &lt; end &amp;&amp; source[j] == target[k]; j++, k++); if (j == end) &#123; /* Found whole string. */ return i - sourceOffset; &#125; &#125; &#125; return -1;&#125; 总结：遇到这种问题多查看源码，想深入就得从底层做起！","tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/数据结构/"},{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/算法/"},{"name":"字符串","slug":"字符串","permalink":"http://yoursite.com/tags/字符串/"},{"name":"旋转词","slug":"旋转词","permalink":"http://yoursite.com/tags/旋转词/"}]},{"title":"字符串","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/【字符串】字符串逆序/","text":"题目一：如果一个字符串 str ，把字符串 str 前面的任意部分挪到后面去形成的字符串叫做 str 的旋转词。比如 str = “ 1234 ” ， 那么 str 的旋转词有 “ 1234 ” ， “ 2341 ” ， “ 3412 ” ， “ 4123 ” 。给定两个字符串 a 和 b ，请判断 a 和 b 是否互为旋转词？举例： a = “ cdab “ , b = “ abcd “ 。返回 true。 a = “ 1ab2 “ , b = “ ab12 “ 。返回 false。 a = “ 2ab1 “ , b= “ ab12 “ 。 返回 true。 思路：最优解时间复杂度为 O(N) 先判断字符串 a 和 b 是否长度相等。 如果长度相等，生成 a + a 的大字符串。 然后判断大字符串中是否包含 b 字符串。（使用 kmp 算法判断）如果大字符串中包含字符串 b ，那么字符串 a 和 b 就互为旋转词。 举例： a = “ 1234 “ a + a = “ 12341234 “ 很明显发现，如果字符串 a 的长度为 N，在 a + a 的大字符串中，任意一个长度为 N 的子串都是 a 的旋转词。 题目二：给定一个字符串 a， 请在单词间做逆序调整。 举例： “ pig loves dog “ 逆序成 “ dog loves pig “ 。 “ I’m a student. “ 逆序成 “ student. a I’m “ 思路： 实现将字符串局部所有字符逆序的函数 f 利用 f 将字符串所有字符逆序 找到逆序后的字符串中每一个单词的区域，利用 f 将每一个单词的区域逆序 题目三：给定一个字符串 a 和一个整数 i。N为字符串的长度，i 为 a 中的位置，将 a [ 0 … i ] 移到右侧，a [ i + 1 … N - 1 ]移到左侧。 举例： a = “ ABCDE “ ，i = 2 。将 str 调整为 “ DEABC “ 。 要求：时间复杂度为 O(N)，额外空间复杂度为 O(1)。 思路： 先将 a[ 0 … i ] 部分的字符逆序 再将 a[ i + 1 … N - 1 ] 部分的字符逆序 最后将整体的字符 a 逆序 题目四：给定一个字符串类型的数组 strs，请找到一种拼接顺序，使得将所有的字符串拼接起来组成的大字符串是所有可能性中字典顺序最小的，并返回这个字符串。 举例： strs = [ “ abc “ , “ de “ ]，可以拼接成 “ abcde “，也可以拼接成 “ deabc “，但是前者的字典顺序更小，所以返回 “ abcde “ 。 strs = [ “ b “, “ ba “ ], 可以拼接成 “ bba “, 也可以拼接成 “ bab “,但是后者的字典顺序更小，所以返回 “ bab “。 思路：最优解的时间复杂度O(N*logN)，其实质是一种排序的实现。 方案二中是比较两个字符串彼此拼接后的字典顺序，所以能成功。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/数据结构/"},{"name":"算法","slug":"算法","permalink":"http://yoursite.com/tags/算法/"},{"name":"字符串","slug":"字符串","permalink":"http://yoursite.com/tags/字符串/"}]},{"title":"Java读取文件","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/java读取文件/","text":"以字节为单位读取文件 以字符为单位读取文件 以行为单位读取文件 随机读取文件内容 ReadFromFile.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263package cn.zhisheng.io;import java.io.*;/** * java读取文件 * Created by 10412 on 2016/12/29. */public class ReadFromFile&#123; /** * 以字节为单位读取文件，常用于读二进制文件，如图片、声音、影像等文件 * @param fileName 文件名 */ public static void readFileByBytes(String fileName) &#123; File file = new File(fileName); InputStream in = null; try &#123; System.out.println(\"以字节为单位读取文件内容，一次读取一个字节\"); //一次读一个字节 in = new FileInputStream(file); int tempbyte; while ((tempbyte = in.read()) != -1) &#123; System.out.println(tempbyte); &#125; in.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); return; &#125; try &#123; System.out.println(\"以字节为单位读取文件内容，一次读取多个字节\"); //一次读取多个字节 byte[] tempbytes = new byte[100]; int byteread = 0; in = new FileInputStream(fileName); ReadFromFile.showAvailableBytes(in); // 读入多个字节到字节数组中，byteread为一次读入的字节数 while ((byteread = in.read(tempbytes)) != -1) &#123; System.out.write(tempbytes, 0, byteread); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; if (in != null) &#123; try &#123; in.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /** * 以字符为单位读取文件，常用于读文本，数字等类型的文件 * @param fileName 文件名 */ public static void readFileByChars(String fileName) &#123; File file = new File(fileName); Reader reader = null; try &#123; System.out.println(\"以字符为单位读取文件内容，一次读一个字符：\"); // 一次读一个字符 reader = new InputStreamReader(new FileInputStream(file)); int tempchar; while ((tempchar = reader.read()) != -1) &#123; // 对于windows下，\\r\\n这两个字符在一起时，表示一个换行。 // 但如果这两个字符分开显示时，会换两次行。 // 因此，屏蔽掉\\r，或者屏蔽\\n。否则，将会多出很多空行。 if (((char)tempchar) != '\\r') &#123; System.out.print((char) tempchar); &#125; &#125; reader.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; System.out.println(\"以字符为单位读取文件内容，一次读多个字符：\"); //一次读多个字符 char[] tempchars = new char[30]; int charread = 0; reader = new InputStreamReader(new FileInputStream(fileName)); // 读入多个字符到字符数组中，charread为一次读取字符数 while ((charread = reader.read(tempchars)) != -1) &#123; // 同样屏蔽掉\\r不显示 if ((charread == tempchars.length) &amp;&amp; (tempchars[tempchars.length - 1]) != '\\r') &#123; System.out.print(tempchars); &#125; else &#123; for (int i = 0; i &lt; charread; i++ ) &#123; if (tempchars[i] == '\\r') &#123; continue; &#125; else &#123; System.out.print(tempchars[i]); &#125; &#125; &#125; &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; if (reader != null) &#123; try &#123; reader.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /** * 以行为单位读取文件，常用于读面向行的格式化文件 * @param fileName 文件名 */ public static void readFileByLines(String fileName) &#123; File file = new File(fileName); BufferedReader reader =null; try &#123; System.out.println(\"以行为单位读取文件内容，一次读一整行：\"); reader = new BufferedReader(new FileReader(file)); String tempString = null; int line = 1; // 一次读入一行，直到读入null为文件结束 while ((tempString = reader.readLine()) != null) &#123; // 显示行号 System.out.println(\"line \"+line+\": \"+tempString); line++; &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; if (reader != null) &#123; try &#123; reader.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /** * 随机读取文件内容 * @param fileName 文件名 */ public static void readFileBRandomAccess(String fileName) &#123; RandomAccessFile randomFile = null; try &#123; System.out.println(\"随机读取一段文件内容：\"); // 打开一个随机访问文件流，按只读方式 randomFile = new RandomAccessFile(fileName, \"r\"); // 文件长度，字节数 long fileLength = randomFile.length(); // 读文件的起始位置 int beginIndex = (fileLength &gt; 4) ? 4 : 0; // 将读文件的开始位置移到beginIndex位置 randomFile.seek(beginIndex); byte[] bytes = new byte[10]; int byteread = 0; // 一次读10个字节，如果文件内容不足10个字节，则读剩下的字节。 // 将一次读取的字节数赋给byteread while ((byteread = randomFile.read(bytes)) != -1) &#123; System.out.write(bytes, 0, byteread); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; if (randomFile != null) try &#123; randomFile.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * 显示输入流中剩余的字节数 * @param in */ public static void showAvailableBytes(InputStream in) &#123; try &#123; System.out.println(\"当前字节流输入流中剩余的字节数为:\"+in.available()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; String fileName = \"C:\\\\Users\\\\10412\\\\Desktop\\\\1.txt\"; //文本文件 //String fileName = \"C:\\\\Users\\\\10412\\\\Desktop\\\\sp20161227_204413.png\"; //图片文件 //readFileByBytes(fileName); //readFileByChars(fileName); //readFileByLines(fileName); readFileBRandomAccess(fileName); &#125;&#125; 文件追加内容AppendToFile.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package cn.zhisheng.io;import java.io.FileNotFoundException;import java.io.FileWriter;import java.io.IOException;import java.io.RandomAccessFile;/** * 追加内容到文件尾部 * Created by 10412 on 2016/12/29. */public class AppendToFile&#123; /** * 第一种方法追加文件：使用RandomAccessFile * @param fileName 文件名 * @param content 追加内容 */ public static void appendMethod1(String fileName, String content) &#123; try &#123; // 打开一个随机访问文件流，按读写方式 RandomAccessFile randomFile = new RandomAccessFile(fileName, \"rw\"); // 文件长度，字节数 long fileLength = randomFile.length(); //将写文件指针移到文件尾 randomFile.seek(fileLength); randomFile.writeBytes(content); randomFile.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 第二种方法追加文件：使用FileWriter * @param fileName 文件名 * @param content 追加内容 */ public static void appendMethod2(String fileName, String content) &#123; try &#123; //打开一个写文件器，构造函数中的第二个参数true表示以追加形式写文件 FileWriter writer = new FileWriter(fileName, true); writer.write(content); writer.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; String fileName = \"C:\\\\Users\\\\10412\\\\Desktop\\\\1.txt\"; //文本文件 String content = \"new append!\"; //按方法1追加文件// AppendToFile.appendMethod1(fileName, content);// AppendToFile.appendMethod1(fileName, \"\\new append. 第一种方法\\n\"); //按照方法2追加文件 AppendToFile.appendMethod2(fileName, content); AppendToFile.appendMethod2(fileName, \"\\nnew append. 第二种方法\\n\"); //显示文件内容 ReadFromFile.readFileByLines(fileName); &#125;&#125;","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"文件","slug":"文件","permalink":"http://yoursite.com/tags/文件/"}]},{"title":"HashMap、Hashtable、HashSet 和 ConcurrentHashMap 的比较","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/HashMap-Hashtable/","text":"HashMap 和 Hashtable 的比较是 Java 面试中的常见问题，用来考验程序员是否能够正确使用集合类以及是否可以随机应变使用多种思路解决问题。HashMap 的工作原理、ArrayList 与 Vector 的比较以及这个问题是有关 Java 集合框架的最经典的问题。Hashtable 是个过时的集合类，存在于 Java API 中很久了。在 Java 4 中被重写了，实现了 Map 接口，所以自此以后也成了 Java 集合框架中的一部分。Hashtable 和 HashMap 在 Java 面试中相当容易被问到，甚至成为了集合框架面试题中最常被考的问题，所以在参加任何 Java 面试之前，都不要忘了准备这一题。这篇文章中，我们不仅将会看到 HashMap 和 Hashtable 的区别，还将看到它们之间的相似之处。 HashMap 和 Hashtable 的区别HashMap 和 Hashtable 都实现了 Map 接口，但决定用哪一个之前先要弄清楚它们之间的分别。主要的区别有：线程安全性，同步 (synchronization)，以及速度。 HashMap 几乎可以等价于 Hashtable，除了 HashMap 是非 synchronized 的，并可以接受 null(HashMap 可以接受为 null 的键值 (key) 和值 (value)，而 Hashtable 则不行)。 HashMap 是非 synchronized，而 Hashtable 是 synchronized，这意味着 Hashtable 是线程安全的，多个线程可以共享一个 Hashtable；而如果没有正确的同步的话，多个线程是不能共享 HashMap 的。Java 5 提供了 ConcurrentHashMap，它是 HashTable 的替代，比 HashTable 的扩展性更好。 另一个区别是 HashMap 的迭代器 (Iterator) 是 fail-fast 迭代器，而 Hashtable 的 enumerator 迭代器不是 fail-fast 的。所以当有其它线程改变了 HashMap 的结构（增加或者移除元素），将会抛出ConcurrentModificationException，但迭代器本身的 remove() 方法移除元素则不会抛出ConcurrentModificationException 异常。但这并不是一个一定发生的行为，要看 JVM。这条同样也是Enumeration 和 Iterato r的区别。 由于 Hashtable 是线程安全的也是 synchronized，所以在单线程环境下它比 HashMap 要慢。如果你不需要同步，只需要单一线程，那么使用 HashMap 性能要好过 Hashtable。 HashMap 不能保证随着时间的推移 Map 中的元素次序是不变的。 要注意的一些重要术语：1) sychronized 意味着在一次仅有一个线程能够更改 Hashtable。就是说任何线程要更新 Hashtable 时要首先获得同步锁，其它线程要等到同步锁被释放之后才能再次获得同步锁更新 Hashtable。 2) Fail-safe 和 iterator 迭代器相关。如果某个集合对象创建了 Iterator 或者 ListIterator，然后其它的线程试图“结构上”更改集合对象，将会抛出 ConcurrentModificationException 异常。但其它线程可以通过 set() 方法更改集合对象是允许的，因为这并没有从“结构上”更改集合。但是假如已经从结构上进行了更改，再调用 set() 方法，将会抛出 IllegalArgumentException 异常。 3) 结构上的更改指的是删除或者插入一个元素，这样会影响到 map 的结构。 我们能否让 HashMap 同步？HashMap 可以通过下面的语句进行同步：Map m = Collections.synchronizeMap(hashMap); 结论Hashtable 和 HashMap 有几个主要的不同：线程安全以及速度。仅在你需要完全的线程安全的时候使用Hashtable，而如果你使用 Java 5 或以上的话，请使用 ConcurrentHashMap 吧。 转载自：HashMap和Hashtable的区别 关于 HashMap 线程不安全这一点，《Java并发编程的艺术》一书中是这样说的： HashMap 在并发执行 put 操作时会引起死循环，导致 CPU 利用率接近 100%。因为多线程会导致 HashMap 的 Node 链表形成环形数据结构，一旦形成环形数据结构，Node 的 next 节点永远不为空，就会在获取 Node 时产生死循环。 原因： 疫苗：JAVA HASHMAP的死循环 —— 酷壳 HashMap在java并发中如何发生死循环 How does a HashMap work in JAVA 下面的是自己有道云笔记中记录的： HashMap ， HashTable 和 HashSet 区别 关于 HashMap 的一些说法： a) HashMap 实际上是一个“链表散列”的数据结构，即数组和链表的结合体。HashMap 的底层结构是一个数组，数组中的每一项是一条链表。 b) HashMap 的实例有俩个参数影响其性能： “初始容量” 和 装填因子。 c) HashMap 实现不同步，线程不安全。 HashTable 线程安全 d) HashMap 中的 key-value 都是存储在 Entry 中的。 e) HashMap 可以存 null 键和 null 值，不保证元素的顺序恒久不变，它的底层使用的是数组和链表，通过hashCode() 方法和 equals 方法保证键的唯一性 f) 解决冲突主要有三种方法：定址法，拉链法，再散列法。HashMap 是采用拉链法解决哈希冲突的。 注： 链表法是将相同 hash 值的对象组成一个链表放在 hash 值对应的槽位； 用开放定址法解决冲突的做法是：当冲突发生时，使用某种探查(亦称探测)技术在散列表中形成一个探查(测)序列。 沿此序列逐个单元地查找，直到找到给定 的关键字，或者碰到一个开放的地址(即该地址单元为空)为止（若要插入，在探查到开放的地址，则可将待插入的新结点存人该地址单元）。 拉链法解决冲突的做法是： 将所有关键字为同义词的结点链接在同一个单链表中 。若选定的散列表长度为m，则可将散列表定义为一个由m个头指针组成的指针数 组T[0..m-1]。凡是散列地址为i的结点，均插入到以T[i]为头指针的单链表中。T中各分量的初值均应为空指针。在拉链法中，装填因子α可以大于1，但一般均取α≤1。拉链法适合未规定元素的大小。 Hashtable 和 HashMap 的区别： a) 继承不同。 public class Hashtable extends Dictionary implements Map public class HashMap extends AbstractMap implements Map b) Hashtable 中的方法是同步的，而 HashMap 中的方法在缺省情况下是非同步的。在多线程并发的环境下，可以直接使用 Hashtable，但是要使用 HashMap 的话就要自己增加同步处理了。 c) Hashtable 中， key 和 value 都不允许出现 null 值。 在 HashMap 中， null 可以作为键，这样的键只有一个；可以有一个或多个键所对应的值为 null 。当 get() 方法返回 null 值时，即可以表示 HashMap 中没有该键，也可以表示该键所对应的值为 null 。因此，在 HashMap 中不能由 get() 方法来判断 HashMap 中是否存在某个键， 而应该用 containsKey() 方法来判断。 d) 两个遍历方式的内部实现上不同。Hashtable、HashMap 都使用了Iterator。而由于历史原因，Hashtable还使用了 Enumeration 的方式 。 e) 哈希值的使用不同，HashTable 直接使用对象的 hashCode。而 HashMap 重新计算 hash 值。 f) Hashtable 和 HashMap 它们两个内部实现方式的数组的初始大小和扩容的方式。HashTable 中 hash 数组默认大小是11，增加的方式是 old*2+1。HashMap 中 hash 数组的默认大小是 16，而且一定是2的指数。 注： HashSet 子类依靠 hashCode() 和 equal() 方法来区分重复元素。 HashSet 内部使用 Map 保存数据，即将 HashSet 的数据作为 Map 的 key 值保存，这也是 HashSet 中元素不能重复的原因。而 Map 中保存 key 值的,会去判断当前 Map 中是否含有该 Key 对象，内部是先通过 key 的hashCode, 确定有相同的 hashCode 之后，再通过 equals 方法判断是否相同。 《HashMap 的工作原理》 HashMap的工作原理是近年来常见的Java面试题。几乎每个Java程序员都知道HashMap，都知道哪里要用HashMap，知道 Hashtable和HashMap之间的区别，那么为何这道面试题如此特殊呢？是因为这道题考察的深度很深。这题经常出现在高级或中高级面试中。投资银行更喜欢问这个问题，甚至会要求你实现HashMap来考察你的编程能力。ConcurrentHashMap和其它同步集合的引入让这道题变得更加复杂。让我们开始探索的旅程吧！ 先来些简单的问题“你用过HashMap吗？” “什么是HashMap？你为什么用到它？” 几乎每个人都会回答“是的”，然后回答HashMap的一些特性，譬如HashMap可以接受null键值和值，而Hashtable则不能；HashMap是非synchronized;HashMap很快；以及HashMap储存的是键值对等等。这显示出你已经用过HashMap，而且对它相当的熟悉。但是面试官来个急转直下，从此刻开始问出一些刁钻的问题，关于HashMap的更多基础的细节。面试官可能会问出下面的问题： “你知道HashMap的工作原理吗？” “你知道HashMap的get()方法的工作原理吗？” 你也许会回答“我没有详查标准的Java API，你可以看看Java源代码或者Open JDK。”“我可以用Google找到答案。” 但一些面试者可能可以给出答案，“HashMap是基于hashing的原理，我们使用put(key, value)存储对象到HashMap中，使用get(key)从HashMap中获取对象。当我们给put()方法传递键和值时，我们先对键调用hashCode()方法，返回的hashCode用于找到bucket位置来储存Entry对象。”这里关键点在于指出，HashMap是在bucket中储存键对象和值对象，作为Map.Entry。这一点有助于理解获取对象的逻辑。如果你没有意识到这一点，或者错误的认为仅仅只在bucket中存储值的话，你将不会回答如何从HashMap中获取对象的逻辑。这个答案相当的正确，也显示出面试者确实知道hashing以及HashMap的工作原理。但是这仅仅是故事的开始，当面试官加入一些Java程序员每天要碰到的实际场景的时候，错误的答案频现。下个问题可能是关于HashMap中的碰撞探测(collision detection)以及碰撞的解决方法： “当两个对象的hashcode相同会发生什么？” 从这里开始，真正的困惑开始了，一些面试者会回答因为hashcode相同，所以两个对象是相等的，HashMap将会抛出异常，或者不会存储它们。然后面试官可能会提醒他们有equals()和hashCode()两个方法，并告诉他们两个对象就算hashcode相同，但是它们可能并不相等。一些面试者可能就此放弃，而另外一些还能继续挺进，他们回答“因为hashcode相同，所以它们的bucket位置相同，‘碰撞’会发生。因为HashMap使用链表存储对象，这个Entry(包含有键值对的Map.Entry对象)会存储在链表中。”这个答案非常的合理，虽然有很多种处理碰撞的方法，这种方法是最简单的，也正是HashMap的处理方法。但故事还没有完结，面试官会继续问： “如果两个键的hashcode相同，你如何获取值对象？” 面试者会回答：当我们调用get()方法，HashMap会使用键对象的hashcode找到bucket位置，然后获取值对象。面试官提醒他如果有两个值对象储存在同一个bucket，他给出答案:将会遍历链表直到找到值对象。面试官会问因为你并没有值对象去比较，你是如何确定确定找到值对象的？除非面试者直到HashMap在链表中存储的是键值对，否则他们不可能回答出这一题。 其中一些记得这个重要知识点的面试者会说，找到bucket位置之后，会调用keys.equals()方法去找到链表中正确的节点，最终找到要找的值对象。完美的答案！ 许多情况下，面试者会在这个环节中出错，因为他们混淆了hashCode()和equals()方法。因为在此之前hashCode()屡屡出现，而equals()方法仅仅在获取值对象的时候才出现。一些优秀的开发者会指出使用不可变的、声明作final的对象，并且采用合适的equals()和hashCode()方法的话，将会减少碰撞的发生，提高效率。不可变性使得能够缓存不同键的hashcode，这将提高整个获取对象的速度，使用String，Interger这样的wrapper类作为键是非常好的选择。 如果你认为到这里已经完结了，那么听到下面这个问题的时候，你会大吃一惊。 “如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？” 除非你真正知道HashMap的工作原理，否则你将回答不出这道题。默认的负载因子大小为0.75，也就是说，当一个map填满了75%的bucket时候，和其它集合类(如ArrayList等)一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中。这个过程叫作rehashing，因为它调用hash方法找到新的bucket位置。 如果你能够回答这道问题，下面的问题来了： “你了解重新调整HashMap大小存在什么问题吗？” 你可能回答不上来，这时面试官会提醒你当多线程的情况下，可能产生条件竞争(race condition)。 当重新调整HashMap大小的时候，确实存在条件竞争，因为如果两个线程都发现HashMap需要重新调整大小了，它们会同时试着调整大小。在调整大小的过程中，存储在链表中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将元素放在链表的尾部，而是放在头部，这是为了避免尾部遍历(tail traversing)。如果条件竞争发生了，那么就死循环了。这个时候，你可以质问面试官，为什么这么奇怪，要在多线程的环境下使用HashMap呢？：） 热心的读者贡献了更多的关于HashMap的问题： 为什么String, Interger这样的wrapper类适合作为键？ String, Interger这样的wrapper类作为HashMap的键是再适合不过了，而且String最为常用。因为String是不可变的，也是final的，而且已经重写了equals()和hashCode()方法了。其他的wrapper类也有这个特点。不可变性是必要的，因为为了要计算hashCode()，就要防止键值改变，如果键值在放入时和获取时返回不同的hashcode的话，那么就不能从HashMap中找到你想要的对象。不可变性还有其他的优点如线程安全。如果你可以仅仅通过将某个field声明成final就能保证hashCode是不变的，那么请这么做吧。因为获取对象的时候要用到equals()和hashCode()方法，那么键对象正确的重写这两个方法是非常重要的。如果两个不相等的对象返回不同的hashcode的话，那么碰撞的几率就会小些，这样就能提高HashMap的性能。 我们可以使用自定义的对象作为键吗？ 这是前一个问题的延伸。当然你可能使用任何对象作为键，只要它遵守了equals()和hashCode()方法的定义规则，并且当对象插入到Map中之后将不会再改变了。如果这个自定义对象时不可变的，那么它已经满足了作为键的条件，因为当它创建之后就已经不能改变了。 我们可以使用CocurrentHashMap来代替Hashtable吗？ 这是另外一个很热门的面试题，因为ConcurrentHashMap越来越多人用了。我们知道Hashtable是synchronized的，但是ConcurrentHashMap同步性能更好，因为它仅仅根据同步级别对map的一部分进行上锁。ConcurrentHashMap当然可以代替HashTable，但是HashTable提供更强的线程安全性。看看 这篇博客 查看Hashtable和ConcurrentHashMap的区别。 我个人很喜欢这个问题，因为这个问题的深度和广度，也不直接的涉及到不同的概念。让我们再来看看这些问题设计哪些知识点： hashing的概念 HashMap中解决碰撞的方法 equals()和hashCode()的应用，以及它们在HashMap中的重要性 不可变对象的好处 HashMap多线程的条件竞争 重新调整HashMap的大小 总结HashMap的工作原理HashMap基于hashing原理，我们通过put()和get()方法储存和获取对象。当我们将键值对传递给put()方法时，它调用键对象的hashCode()方法来计算hashcode，让后找到bucket位置来储存值对象。当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用链表来解决碰撞问题，当发生碰撞了，对象将会储存在链表的下一个节点中。 HashMap在每个链表节点中储存键值对对象。 当两个不同的键对象的hashcode相同时会发生什么？ 它们会储存在同一个bucket位置的链表中。键对象的equals()方法用来找到键值对。 因为HashMap的好处非常多，我曾经在电子商务的应用中使用HashMap作为缓存。因为金融领域非常多的运用Java，也出于性能的考虑，我们会经常用到HashMap和ConcurrentHashMap。你可以查看更多的关于HashMap的文章: HashMap和Hashtable的区别 HashMap和HashSet的区别 转载自：HashMap的工作原理 其他的 HashMap 学习资料： jdk7中HashMap知识点整理 HashMap源码分析（四）put-jdk8-红黑树的引入 JDK7与JDK8中HashMap的实现 JDK1.8HashMap原理和源码分析(java面试收藏) 谈谈ConcurrentHashMap1.7和1.8的不同实现 jdk1.8的HashMap和ConcurrentHashMap ConcurrentHashMap源码分析（JDK8版本） 最后谢谢阅读，如果可以的话欢迎大家转发和点赞。如需转载注明原地址就行。 群 528776268 欢迎各位大牛进群一起讨论。","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"JVM性能调优监控工具jps、jstack、jmap、jhat、jstat等使用详解","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/JVM性能调优监控工具jps、jstack、jmap、jhat、jstat等使用详解/","text":"javap 和 javac javac -verbose 类名.java java -verbose 类名 javap -c 类名 javap -verbose 类名 javap -help用法: javap 其中, 可能的选项包括: -help –help -? 输出此用法消息 -version 版本信息 -v -verbose 输出附加信息 -l 输出行号和本地变量表 -public 仅显示公共类和成员 -protected 显示受保护的/公共类和成员 -package 显示程序包/受保护的/公共类 和成员 (默认) -p -private 显示所有类和成员 -c 对代码进行反汇编 -s 输出内部类型签名 -sysinfo 显示正在处理的类的 系统信息 (路径, 大小, 日期, MD5 散列) -constants 显示最终常量 -classpath 指定查找用户类文件的位置 -cp 指定查找用户类文件的位置 -bootclasspath 覆盖引导类文件的位置 javac -help用法: javac 其中, 可能的选项包括: -g 生成所有调试信息 -g:none 不生成任何调试信息 -g:{lines,vars,source} 只生成某些调试信息 -nowarn 不生成任何警告 -verbose 输出有关编译器正在执行的操作的消息 -deprecation 输出使用已过时的 API 的源位置 -classpath &lt;路径&gt; 指定查找用户类文件和注释处理程序的位置 -cp &lt;路径&gt; 指定查找用户类文件和注释处理程序的位置 -sourcepath &lt;路径&gt; 指定查找输入源文件的位置 -bootclasspath &lt;路径&gt; 覆盖引导类文件的位置 -extdirs &lt;目录&gt; 覆盖所安装扩展的位置 -endorseddirs &lt;目录&gt; 覆盖签名的标准路径的位置 -proc:{none,only} 控制是否执行注释处理和/或编译。 -processor [,,…] 要运行的注释处理程序的名称; 绕过默认的搜索进程 -processorpath &lt;路径&gt; 指定查找注释处理程序的位置 -parameters 生成元数据以用于方法参数的反射 -d &lt;目录&gt; 指定放置生成的类文件的位置 -s &lt;目录&gt; 指定放置生成的源文件的位置 -h &lt;目录&gt; 指定放置生成的本机标头文件的位置 -implicit:{none,class} 指定是否为隐式引用文件生成类文件 -encoding &lt;编码&gt; 指定源文件使用的字符编码 -source &lt;发行版&gt; 提供与指定发行版的源兼容性 -target &lt;发行版&gt; 生成特定 VM 版本的类文件 -profile &lt;配置文件&gt; 请确保使用的 API 在指定的配置文件中可用 -version 版本信息 -help 输出标准选项的提要 -A关键字[=值] 传递给注释处理程序的选项 -X 输出非标准选项的提要 -J&lt;标记&gt; 直接将 &lt;标记&gt; 传递给运行时系统 -Werror 出现警告时终止编译 @&lt;文件名&gt; 从文件读取选项和文件名 jps用来查看基于HotSpot的JVM里面中，所有具有访问权限的Java进程的具体状态, 包括进程ID，进程启动的路径及启动参数等等，与unix上的ps类似，只不过jps是用来显示java进程，可以把jps理解为ps的一个子集。 使用jps时，如果没有指定hostid，它只会显示本地环境中所有的Java进程；如果指定了hostid，它就会显示指定hostid上面的java进程，不过这需要远程服务上开启了jstatd服务。 jps -helpusage: jps [-help] jps [-q] [-mlvV] [&lt;hostid&gt;] Definitions: &lt;hostid&gt;: &lt;hostname&gt;[:&lt;port&gt;] -q：忽略输出的类名、Jar名以及传递给main方法的参数，只输出pid。 -m：输出传递给main方法的参数，如果是内嵌的JVM则输出为null。 -l：输出完全的包名，应用主类名，jar的完全路径名 -v：输出传给jvm的参数 -V：输出通过标记的文件传递给JVM的参数（.hotspotrc文件，或者是通过参数-XX:Flags=指定的文件）。 -J 用于传递jvm选项到由javac调用的java加载器中，例如，“-J-Xms48m”将把启动内存设置为48M，使用-J选项可以非常方便的向基于Java的开发的底层虚拟机应用程序传递参数。 jstackjstack用于打印出给定的java进程ID或core file或远程调试服务的Java堆栈信息，如果是在64位机器上，需要指定选项”-J-d64”，Windows的jstack使用方式只支持以下的这种方式： jstack [-l] pid 如果java程序崩溃生成core文件，jstack工具可以用来获得core文件的java stack和native stack的信息，从而可以轻松地知道java程序是如何崩溃和在程序何处发生问题。另外，jstack工具还可以附属到正在运行的java程序中，看到当时运行的java程序的java stack和native stack的信息, 如果现在运行的java程序呈现hung的状态，jstack是非常有用的。 jstack -helpUsage: jstack [-l] &lt;pid&gt; (to connect to running process) jstack -F [-m] [-l] &lt;pid&gt; (to connect to a hung process) jstack [-m] [-l] &lt;executable&gt; &lt;core&gt; (to connect to a core file) jstack [-m] [-l] [server_id@]&lt;remote server IP or hostname&gt; (to connect to a remote debug server) Options: -F to force a thread dump. Use when jstack &lt;pid&gt; does not respond (process is hung)(当’jstack [-l] pid’没有相应的时候强制打印栈信息) -m to print both java and native frames (mixed mode)(打印java和native c/c++框架的所有栈信息.) -l long listing. Prints additional information about locks (长列表. 打印关于锁的附加信息,例如属于java.util.concurrent的ownable synchronizers列表.) -h or -help to print this help message (打印帮助信息) jstatJstat 用于监控基于HotSpot的JVM，对其堆的使用情况进行实时的命令行的统计，使用jstat我们可以对指定的JVM做如下监控： 类的加载及卸载情况 查看新生代、老生代及持久代的容量及使用情况 查看新生代、老生代及持久代的垃圾收集情况，包括垃圾回收的次数及垃圾回收所占用的时间 查看新生代中Eden区及Survior区中容量及分配情况等 jstat -help Usage: jstat -help|-options jstat -&lt;option&gt; [-t] [-h&lt;lines&gt;] &lt;vmid&gt; [&lt;interval&gt; [&lt;count&gt;]] Definitions:&gt; An option reported by the -options option Virtual Machine Identifier. A vmid takes the following form: [@[:]] Where is the local vm identifier for the target Java virtual machine, typically a process id; is the name of the host running the target Java virtual machine; and is the port number for the rmiregistry on the target host. See the jvmstat documentation for a more complete description of the Virtual Machine Identifier. Number of samples between header lines. Sampling interval. The following forms are allowed: [“ms”|”s”] Where is an integer and the suffix specifies the units as milliseconds(“ms”) or seconds(“s”). The default units are “ms”. Number of samples to take before terminating. -J Pass directly to the runtime system. 参考文章1、jstat命令详解 2、jstat命令(Java Virtual Machine Statistics Monitoring Tool) 3、http://docs.oracle.com/javase/1.5.0/docs/tooldocs/share/jstat.html#class_option jmap打印出某个java进程（使用pid）内存内的，所有‘对象’的情况（如：产生那些对象，及其数量）。 可以输出所有内存中对象的工具，甚至可以将VM 中的heap，以二进制输出成文本。使用方法 jmap -histo pid 如果连用SHELL jmap -histo pid&gt;a.log 可以将其保存到文本中去，在一段时间后，使用文本对比工具，可以对比出GC回收了哪些对象。 jmap -dump:format=b,file=outfile 3024 可以将3024进程的内存heap输出出来到outfile文件里，再配合MAT（内存分析工具(Memory Analysis Tool），使用参见：http://blog.csdn.net/fenglibing/archive/2011/04/02/6298326.aspx）或与jhat (Java Heap Analysis Tool)一起使用，能够以图像的形式直观的展示当前内存是否有问题。 64位机上使用需要使用如下方式： jmap -J-d64 -heap pid jmap -helpUsage: jmap [option] &lt;pid&gt; (to connect to running process) jmap [option] &lt;executable &lt;core&gt; (to connect to a core file) jmap [option] [server_id@]&lt;remote server IP or hostname&gt; (to connect to remote debug server) where is one of: &lt;none&gt; to print same info as Solaris pmap -heap to print java heap summary -histo[:live] to print histogram of java object heap; if the &quot;live&quot; suboption is specified, only count live objects -clstats to print class loader statistics -finalizerinfo to print information on objects awaiting finalization -dump:&lt;dump-options&gt; to dump java heap in hprof binary format dump-options: live dump only live objects; if not specified, all objects in the heap are dumped. format=b binary format file=&lt;file&gt; dump heap to &lt;file&gt; Example: jmap -dump:live,format=b,file=heap.bin &lt;pid&gt; -F force. Use with -dump:&lt;dump-options&gt; &lt;pid&gt; or -histo to force a heap dump or histogram when &lt;pid&gt; does not respond. The &quot;live&quot; suboption is not supported in this mode. -h | -help to print this help message -J&lt;flag&gt; to pass &lt;flag&gt; directly to the runtime system 参数说明 1)、options： executable Java executable from which the core dump was produced.(可能是产生core dump的java可执行程序) core 将被打印信息的core dump文件 remote-hostname-or-IP 远程debug服务的主机名或ip server-id 唯一id,假如一台主机上多个远程debug服务 2）、基本参数： -dump:[live,]format=b,file= 使用hprof二进制形式,输出jvm的heap内容到文件=. live子选项是可选的，假如指定live选项,那么只输出活的对象到文件. -finalizerinfo 打印正等候回收的对象的信息. -heap 打印heap的概要信息，GC使用的算法，heap的配置及wise heap的使用情况. -histo[:live] 打印每个class的实例数目,内存占用,类全名信息. VM的内部类名字开头会加上前缀”*”. 如果live子参数加上后,只统计活的对象数量. -permstat 打印classload和jvm heap长久层的信息. 包含每个classloader的名字,活泼性,地址,父classloader和加载的class数量. 另外,内部String的数量和占用内存数也会打印出来. -F 强迫.在pid没有相应的时候使用-dump或者-histo参数. 在这个模式下,live子参数无效. -h | -help 打印辅助信息 -J 传递参数给jmap启动的jvm. pid 需要被打印配相信息的java进程id,创业与打工的区别 - 博文预览,可以用jps查问. jinfojinfo 可以输出并修改运行时的java 进程的opts。 用处比较简单，用于输出JAVA系统参数及命令行参数。 用法是 jinfo -opt pid 如：查看2788的MaxPerm大小可以用 jinfo -flag MaxPermSize 2788。 jinfo -help Usage: jinfo [option] &lt;pid&gt; (to connect to running process) jinfo [option] &lt;executable &lt;core&gt; (to connect to a core file) jinfo [option] [server_id@]&lt;remote server IP or hostname&gt; (to connect to remote debug server) where is one of: -flag &lt;name&gt; to print the value of the named VM flag -flag [+|-]&lt;name&gt; to enable or disable the named VM flag -flag &lt;name&gt;=&lt;value&gt; to set the named VM flag to the given value -flags to print VM flags -sysprops to print Java system properties &lt;no option&gt; to print both of the above -h | -help to print this help message jconsole一个java GUI监视工具，可以以图表化的形式显示各种数据。并可通过远程连接监视远程的服务器VM。用java写的GUI程序，用来监控VM，并可监控远程的VM，非常易用，而且功能非常强。命令行里打 jconsole，选则进程就可以了。 需要注意的就是在运行jconsole之前，必须要先设置环境变量DISPLAY，否则会报错误，Linux下设置环境变量如下： export DISPLAY=:0.0 可以这里选择查看本地进程的状况，还是远程进程的状况 通过这张图可以看到内存、线程、类及CPU使用的一些情况。 jvisualvm参考文章： 程序员必备利器—Java程序性能分析工具Java VisualVM（Visual GC） jhat用于对JAVA heap进行离线分析的工具，他可以对不同虚拟机中导出的heap信息文件进行分析，如Linux上导出的文件可以拿到WINDOWS上进行分析，可以查找诸如内存方面的问题，使用方式可以查看这篇文章： jhat命令 不过jhat和MAT比较起来，就没有MAT那么直观了，MAT是以图形界面的方式展现结果，MAT的使用方式可以参看文章： MAT(Memory Analyzer Tool)工具入门介绍 Usage:jhat [-stack ] [-refs ] [-port ] [-baseline ] [-debug ] [-version] [-h|-help] -J&lt;flag&gt; Pass &lt;flag&gt; directly to the runtime system. For example, -J-mx512m to use a maximum heap size of 512MB -stack false: Turn off tracking object allocation call stack. -refs false: Turn off tracking of references to objects -port &lt;port&gt;: Set the port for the HTTP server. Defaults to 7000 -exclude &lt;file&gt;: Specify a file that lists data members that should be excluded from the reachableFrom query. -baseline &lt;file&gt;: Specify a baseline object dump. Objects in both heap dumps with the same ID and same class will be marked as not being &quot;new&quot;. -debug &lt;int&gt;: Set debug level. 0: No debug output 1: Debug hprof file parsing 2: Debug hprof file parsing, no server -version Report version number -h|-help Print this help and exit &lt;file&gt; The file to read jdb用来对core文件和正在运行的Java进程进行实时地调试，里面包含了丰富的命令帮助您进行调试，它的功能和Sun studio里面所带的dbx非常相似，但 jdb是专门用来针对Java应用程序的。 jstatdjstatd是一个基于RMI（Remove Method Invocation）的服务程序，它用于监控基于HotSpot的JVM中资源的创建及销毁，并且提供了一个远程接口允许远程的监控工具连接到本地的JVM执行命令。 jstatd是基于RMI的，所以在运行jstatd的服务器上必须存在RMI注册中心，如果没有通过选项”-p port”指定要连接的端口，jstatd会尝试连接RMI注册中心的默认端口。 用法： jstatd [-nr] [-p port] [-n rminame] -nr 如果RMI注册中心没有找到，不会创建一个内部的RMI注册中心。 -p port RMI注册中心的端口号，默认为1099。 -n rminame 默认为JStatRemoteHost；如果同一台主机上同时运行了多个jstatd服务，rminame可以用于唯一确定一个jstatd服务；这里需要注意一下，如果开启了这个选项，那么监控客户端远程连接时，必须同时指定hostid及vmid，才可以唯一确定要连接的服务，这个可以参看jps章节中列出远程服务器上Java进程的示例。 -J 用于传递jvm选项到由javac调用的java加载器中，例如，“-J-Xms48m”将把启动内存设置为48M，使用-J选项可以非常方便的向基于Java的开发的底层虚拟机应用程序传递参数。 参考文章 JDK内置工具使用","tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"},{"name":"性能调优工具","slug":"性能调优工具","permalink":"http://yoursite.com/tags/性能调优工具/"}]},{"title":"奇怪的Java题：为什么128 == 128返回为False，而127 == 127会返回为True?","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/奇怪的Java题：为什么128 == 128返回为False，而127 == 127会返回为True-/","text":"这是我们今天要讨论的话题，因为我觉得它非常的有趣。 如果你运行如下代码： 12345678910class A&#123; public static void main(String[] args) &#123; Integer a = 128, b = 128; System.out.println(a == b); Integer c = 127, d = 127; System.out.println(c == d); &#125;&#125; 你会得到如下结果： 12falsetrue 我们知道，如果两个引用指向同一个对象，那么==就成立；反之，如果两个引用指向的不是同一个对象，那么==就不成立，即便两个引用的内容是一样的。因此，结果就会出现false。 这是非常有趣的地方。如果你查看Integer.java类，你会找到IntegerCache.java这个内部私有类，它为-128到127之间的所有整数对象提供缓存。 这个东西为那些数值比较小的整数提供内部缓存，当进行如此声明时： 1Integer c = 127 它的内部就是这样的： 1Integer var3 = Integer.valueOf(127); 其实我通过将A.class文件反编译后，代码如下图： 如果我们观察valueOf()类函数，我们可以看到： 12345public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); &#125; 通过看源码能够知道，整数类型在-128～127之间时，会使用缓存，造成的效果就是，如果已经创建了一个相同的整数，使用valueOf创建第二次时，不会使用new关键字，而用已经缓存的对象。所以使用valueOf方法创建两次对象，若对应的数值相同，且数值在-128～127之间时，两个对象都指向同一个地址。 因此。。。 1Integer c = 127, d = 127; 两者指向同样的对象。 这就是为什么下面这段代码的结果为true了： 1System.out.println(c == d); 现在你可能会问，为什么会为-128到127之间的所有整数设置缓存？ 这是因为在这个范围内的小数值整数在日常生活中的使用频率要比其它的大得多，多次使用相同的底层对象这一特性可以通过该设置进行有效的内存优化。你可以使用reflection API任意使用这个功能。 运行下面的这段代码，你就会明白它的神奇所在了。 12345678910111213public static void main(String[] args) throws NoSuchFieldException, IllegalAccessException &#123; Class cache = Integer.class.getDeclaredClasses()[0]; Field myCache = cache.getDeclaredField(&quot;cache&quot;); myCache.setAccessible(true); Integer[] newCache = (Integer[]) myCache.get(cache); newCache[132] = newCache[133]; int a = 2; int b = a + a; System.out.printf(&quot;%d + %d = %d&quot;, a, a, b); // &#125; 打印结果竟然是： 12 + 2 = 5 我们再次看一下反汇编代码： 是不是又和上面的是同一个问题呢？ 但是结果为什么是 2 + 2 = 5 呢？ 我们继续去看一下 Integer 源码，去深入了解 Integer 缓存机制，下面截个图： 根据源码可以发现最后修改 Integer 缓存上限时候的方法有点小瑕疵。我们看看Api给我们怎么建议的一段话：1the size of the cache may be controlled by the &#123;@code -XX:AutoBoxCacheMax=&lt;size&gt;&#125; option. 原来我们只需要：运行时设置 -XX:AutoBoxCacheMax=133 就OK。 参考文章： 奇怪的Java题：为什么1000 == 1000返回为False，而100 == 100会返回为True?","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"利用Github Page 搭建个人博客网站","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/利用Github Page 搭建个人博客网站/","text":"前言最近这几天，没事干，想找点事折腾下，于是自己便想到了自己一直想干的一件事：搭建一个属于自己的博客网站。目前搭建个人 blog 网站最好的是用 wordpress ，但是那个折腾起来好像还挺麻烦的，再加上还需要自己修改些前端代码和用 PHP 做（虽然我学了几天拍黄片，但是早已忘了），然后就是用 Github Page 吧，自己也一直在这个最大的交友网站装 X 。想想就用这个吧（后来好像觉得这个还挺省事的） 再说说拥有个人博客网站的好处吧： 装 X（如果网站够炫） 很好的用来总结自己所学的知识 面试加分（在简历上放上自己的个人网站链接，面试官就可以更好的了解你，知道你所学知识的深度和广度） 不再受其他博客平台的规则所束缚 如果你现在还没有自己个人博客网站的话，那么我觉得你看完本篇博客后，强烈的建议你去折腾折腾下，搞个自己的，让自己也能够体验装 X 的感觉。 要想用搭建一个个人博客网站，首先你得有一个域名，这样别人才可以通过域名访问，其次你还要一个空间来存放你的页面。 域名 域名的话，你可以在万网、阿里云、腾讯云等注册，我的域名 www.54tianzhisheng.cn 就是在腾讯云注册的，记得是腾讯云一元钱（一个域名+主机）搞的，这是腾讯云对学生才有这优惠。 .cn 的域名需要备案，备案的审核速度我觉得还是挺快的，还需要上传证件。当然你也可以买其他的那些不需要备案的域名，省得麻烦事。 空间 空间有免费的空间，也有收费的空间。免费的当然就不够稳定了，收费的就很贵了，终究是很不爽，有没有什么地方是既免费又稳定的空间呢？有，Github 。它允许上传个人网站项目并自定义你的域名，而且又有稳定的服务，实在是不能够在好了。 下面就一起跟着我来一步一步的利用 Github 搭建个人博客网站吧！ 1. 拥有一个域名这个步骤我就不详述了。 举例： 打开腾讯云官网 搜索你想要的域名，下单买一个 2. 拥有一个 Github账号互联网崇尚自由与分享。Github 是一个全世界程序员聚集的地方，大家相互分享自己写的代码，提升别人，也提升自己。大家都在为着开源社区努力着。因为我从开源项目中学到很多知识，所以我也非常愿意分享我的所见所学所得，我的 Github 主页：https://github.com/zhisheng17 （欢迎 follow 和对我的项目给个 star 或者 fork 我的项目一起来和我完善项目） 如果还没有 Github 账号的话你就先去注册一个吧，有的话，直接登录就行，后面的操作都要用到 Github 的。 3. Github 上新建个人网站项目登录 GitHub 之后，在页面右上角点击 + 加号按钮，点击 New repository。 由于我们是新建一个个人网站项目，所有仓库的名称需要安装 GitHub 个人网站项目的规定来写。 规则就是： YOUR-GITHUB-USERNAME.github.io 比如我的 GitHub 用户名是 zhisheng17，那我就要填写 zhisheng17.github.io。然后选择公开模式，接着点击创建仓库按钮。 创建成功之后，进入了项目主页面。点击设置按钮。 进入之后，滚动页面到下方。点击页面自动生成器按钮。 点击右下方继续去布局按钮。 选择一个模板，点击发布页面按钮。 这个时候，你就可以通过YOUR-GITHUB-USERNAME.github.io来访问此页面了。 4. 上传个人网页到 Github自动生成页面，肯定不符合我们的要求，我们希望能够自己设计自己的个人网站。我们可以自己编写一个网页文件，命名为 index.html。然后上传到 GitHub个人网站项目上。这里为了节约时间，可以先下载我的个人网站项目代码，然后修改为你的网页上传到 GitHub。 下面介绍详细步骤。 进入此项目https://github.com/zhisheng17/zhisheng17.github.io，然后下载源码。解压之后，拿到里面的index.html文件。 然后进入自己的个人网站项目主页 YOUR-GITHUB-USERNAME/YOUR-GITHUB-USERNAME.github.io。点击上传文件按钮，进入上传文件页面，将 index.html 文件拖入蓝色大圈圈区域，点击提交按钮即可提交成功。此时打开网址 YOUR-GITHUB-USERNAME.github.io 就可以看到主页已经改变为我们自己的网页了。 通过 zhisheng17.github.io 查看效果： 5. 域名CNAME到个人网站项目网页上传成功了，我们不想一直通过YOUR-GITHUB-USERNAME.github.io来访问我们的个人网站，而是希望通过自己的域名来访问。 下面讲述详细步骤。 点击我们的个人网站项目设置选项卡，滚动到下面，就会发现一个自定义域名卡片。输入我们买的域名，然后点击保存。 接着我们还要将我们的域名解析到这个个人网站项目上。因为我的域名是在腾讯云上面买的，所以我打开腾讯云域名管理页面，进行相关的设置。 接着，点击添加一条域名解析记录，主机填写www，代表你是一级域名来访问，指向填写YOUR-GITHUB-USERNAME.github.io，然后点击保存按钮。应该要等会，域名的解析时间可能不一样，我的腾讯云就是很慢的 6. 访问你的域名所有这些步骤做完之后，在浏览器里输入自己的域名，回车键一按，就会返回我们刚刚上传到 GitHub 的index.html 页面了。 这里只是入门了 GitHub 搭建个人网站的功能，GitHub 官方推荐 Jekyll 博客系统来发布自己的页面。以后有数据更新，都可以通过 Jekyll 来重新编译整个网站。（期待后续我的使用 Jekyll 博客系统发布自己博客的文章吧） 7. 注意事项尽管GitHub个人网站项目是免费的，但是却有一些限制。总体来说，完全够用，甚至太多了。 单个仓库大小不超过1GB，上传单个文件大小不能超过100MB，如果通过浏览器上传不能超过25MB 个人网站项目也不例外，最大空间1GB 个人网站项目每个月访问请求数不能超过10万次，总流量不能超过100GB 个人网站项目一小时创建数量不能超过10个 当然了，这些政策可能随时改变，可以通过此网页查看最新政策。 https://help.github.com/articles/what-is-github-pages/#recommended-limits 新增由于问题太多了，所以新写了篇文章：Github page + Hexo + yilia 搭建博客可能会遇到的所有疑问","tags":[{"name":"Github Page","slug":"Github-Page","permalink":"http://yoursite.com/tags/Github-Page/"},{"name":"博客网站","slug":"博客网站","permalink":"http://yoursite.com/tags/博客网站/"}]},{"title":"Hexo + yilia 主题实现文章目录","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Hexo-yilia-toc/","text":"前提为了方便查看每篇文章的目录结构，可以定位到想看的地方，特地找了下如何实现这个功能。 添加 CSS 样式打开 themes\\yilia\\source 下的 main.234bc0.css 文件，直接在后面添加如下代码：123456789/* 新添加的 */#container .show-toc-btn,#container .toc-article&#123;display:block&#125;.toc-article&#123;z-index:100;background:#fff;border:1px solid #ccc;max-width:250px;min-width:150px;max-height:500px;overflow-y:auto;-webkit-box-shadow:5px 5px 2px #ccc;box-shadow:5px 5px 2px #ccc;font-size:12px;padding:10px;position:fixed;right:35px;top:129px&#125;.toc-article .toc-close&#123;font-weight:700;font-size:20px;cursor:pointer;float:right;color:#ccc&#125;.toc-article .toc-close:hover&#123;color:#000&#125;.toc-article .toc&#123;font-size:12px;padding:0;line-height:20px&#125;.toc-article .toc .toc-number&#123;color:#333&#125;.toc-article .toc .toc-text:hover&#123;text-decoration:underline;color:#2a6496&#125;.toc-article li&#123;list-style-type:none&#125;.toc-article .toc-level-1&#123;margin:4px 0&#125;.toc-article .toc-child&#123;&#125;@-moz-keyframes cd-bounce-1&#123;0%&#123;opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;60%&#123;opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)&#125;100%&#123;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;&#125;@-webkit-keyframes cd-bounce-1&#123;0%&#123;opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;60%&#123;opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)&#125;100%&#123;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;&#125;@-o-keyframes cd-bounce-1&#123;0%&#123;opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;60%&#123;opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)&#125;100%&#123;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;&#125;@keyframes cd-bounce-1&#123;0%&#123;opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;60%&#123;opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)&#125;100%&#123;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;&#125;.show-toc-btn&#123;display:none;z-index:10;width:30px;min-height:14px;overflow:hidden;padding:4px 6px 8px 5px;border:1px solid #ddd;border-right:none;position:fixed;right:40px;text-align:center;background-color:#f9f9f9&#125;.show-toc-btn .btn-bg&#123;margin-top:2px;display:block;width:16px;height:14px;background:url(http://7xtawy.com1.z0.glb.clouddn.com/show.png) no-repeat;-webkit-background-size:100%;-moz-background-size:100%;background-size:100%&#125;.show-toc-btn .btn-text&#123;color:#999;font-size:12px&#125;.show-toc-btn:hover&#123;cursor:pointer&#125;.show-toc-btn:hover .btn-bg&#123;background-position:0 -16px&#125;.show-toc-btn:hover .btn-text&#123;font-size:12px;color:#ea8010&#125;.toc-article li ol, .toc-article li ul &#123; margin-left: 30px;&#125;.toc-article ol, .toc-article ul &#123; margin: 10px 0;&#125; 修改 article.ejs 文件打开 themes\\yilia\\layout\\_partial 文件夹下的 article.ejs 文件, 在 &lt;/header&gt; &lt;% } %&gt; 下面加入如下内容（注意位置） 123456789101112131415161718192021222324252627&lt;!-- 目录内容 --&gt;&lt;% if (!index &amp;&amp; post.toc)&#123; %&gt; &lt;p class=&quot;show-toc-btn&quot; id=&quot;show-toc-btn&quot; onclick=&quot;showToc();&quot; style=&quot;display:none&quot;&gt; &lt;span class=&quot;btn-bg&quot;&gt;&lt;/span&gt; &lt;span class=&quot;btn-text&quot;&gt;文章导航&lt;/span&gt; &lt;/p&gt; &lt;div id=&quot;toc-article&quot; class=&quot;toc-article&quot;&gt; &lt;span id=&quot;toc-close&quot; class=&quot;toc-close&quot; title=&quot;隐藏导航&quot; onclick=&quot;showBtn();&quot;&gt;×&lt;/span&gt; &lt;strong class=&quot;toc-title&quot;&gt;文章目录&lt;/strong&gt; &lt;%- toc(post.content) %&gt; &lt;/div&gt; &lt;script type=&quot;text/javascript&quot;&gt; function showToc()&#123; var toc_article = document.getElementById(&quot;toc-article&quot;); var show_toc_btn = document.getElementById(&quot;show-toc-btn&quot;); toc_article.setAttribute(&quot;style&quot;,&quot;display:block&quot;); show_toc_btn.setAttribute(&quot;style&quot;,&quot;display:none&quot;); &#125;; function showBtn()&#123; var toc_article = document.getElementById(&quot;toc-article&quot;); var show_toc_btn = document.getElementById(&quot;show-toc-btn&quot;); toc_article.setAttribute(&quot;style&quot;,&quot;display:none&quot;); show_toc_btn.setAttribute(&quot;style&quot;,&quot;display:block&quot;); &#125;; &lt;/script&gt; &lt;% &#125; %&gt;&lt;!-- 目录内容结束 --&gt; 然后若想要文章显示目录，在每篇文章开头加入：toc: true 即可。 参考文章：Hexo+yilia主题实现文章目录和添加视频 新增由于问题太多了，所以新写了篇文章：Github page + Hexo + yilia 搭建博客可能会遇到的所有疑问","tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"}]},{"title":"深入分析 Java Web 中的中文编码问题","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/深入分析 Java Web 中的中文编码问题/","text":"背景： 编码问题一直困扰着程序开发人员，尤其是在 Java 中更加明显，因为 Java 是跨平台的语言，在不同平台的编码之间的切换较多。接下来将介绍 Java 编码问题出现的根本原因；在 Java 中经常遇到的几种编码格式的区别；在 Java 中经常需要编码的场景；出现中文问题的原因分析；在开发 Java Web 中可能存在编码的几个地方；一个 HTTP 请求怎么控制编码格式；如何避免出现中文编码问题等。 1、几种常见的编码格式1.1 为什么要编码 在计算机中存储信息的最小单元是 1 个字节，即 8 个 bit， 所以能表示的字符范围是 0 ~ 255 个。 要表示的符号太多，无法用 1 个字节来完全表示。 1.2 如何翻译计算机中提供多种翻译方式，常见的有 ASCII、ISO-8859-1、GB2312、GBK、UTF-8、UTF-16等。这些都规定了转化的规则，按照这个规则就可以让计算机正确的表示我们的字符。下面介绍这几种编码格式： ASCII 码 总共有 128 个，用 1 个字节的低 7 位表示， 0 ~ 31 是控制字符如换行、回车、删除等，32 ~ 126 是打印字符，可以通过键盘输入并且能够显示出来。 ISO-8859-1 128 个字符显然是不够用的，所以 ISO 组织在 ASCII 的基础上扩展，他们是 ISO-8859-1 至 ISO-8859-15，前者涵盖大多数字符，应用最广。ISO-8859-1 仍是单字节编码，它总归能表示 256 个字符。 GB2312 它是双字节编码，总的编码范围是 A1 ~ F7，其中 A1 ~ A9 是符号区，总共包含 682 个符号；B0 ~ F7 是汉字区，包含 6763 个汉字。 GBk GBK 为《汉字内码扩展规范》，为 GB2312 的扩展，它的编码范围是 8140 ~ FEFE（去掉XX7F），总共有 23940 个码位，能表示 21003 个汉字，和 GB2312的编码兼容，不会有乱码。 UTF-16 它具体定义了 Unicode 字符在计算机中的存取方法。UTF-16 用两个字节来表示 Unicode 的转化格式，它采用定长的表示方法，即不论什么字符用两个字节表示。两个字节是 16 个 bit，所以叫 UTF-16。它表示字符非常方便，没两个字节表示一个字符，这就大大简化了字符串操作。 UTF-8 虽说 UTF-16 统一采用两个字节表示一个字符很简单方便，但是很大一部分字符用一个字节就可以表示，如果用两个字节表示，存储空间放大了一倍，在网络带宽有限的情况下会增加网络传输的流量。UTF-8 采用了一种变长技术，每个编码区域有不同的字码长度不同类型的字符可以由 1 ~ 6 个字节组成。 UTF-8 有以下编码规则： 如果是 1 个字节，最高位（第 8 位）为 0，则表示这是一个 ASCII 字符（00 ~ 7F） 如果是 1 个字节，以 11 开头，则连续的 1 的个数暗示这个字符的字节数 如果是 1 个字节，以 10 开头，表示它不是首字节，则需要向前查找才能得到当前字符的首字节 ​ 2、在 Java 中需要编码的场景2.1 在 I/O 操作中存在的编码 如上图：Reader 类是在 Java 的 I/O 中读取符的父类，而 InputStream 类是读字节的父类， InputStreamReader 类就是关联字节到字符的桥梁，它负责在 I/O 过程中处理读取字节到字符的转换，而对具体字节到字符的解码实现，它又委托 StreamDecoder 去做，在 StreamDecoder 解码过程中必须由用户指定 Charset 编码格式。值得注意的是，如果你没有指定 Charset，则将使用本地环境中默认的字符集，如在中文环境中将使用 GBK 编码。 如下面一段代码，实现了文件的读写功能： 12345678910111213141516171819202122232425String file = \"c:/stream.txt\";String charset = \"UTF-8\";// 写字符换转成字节流FileOutputStream outputStream = new FileOutputStream(file);OutputStreamWriter writer = new OutputStreamWriter(outputStream, charset);try &#123; writer.write(\"这是要保存的中文字符\");&#125; finally &#123; writer.close();&#125;// 读取字节转换成字符FileInputStream inputStream = new FileInputStream(file);InputStreamReader reader = new InputStreamReader(inputStream, charset);StringBuffer buffer = new StringBuffer();char[] buf = new char[64];int count = 0;try &#123; while ((count = reader.read(buf)) != -1) &#123; buffer.append(buffer, 0, count); &#125;&#125; finally &#123; reader.close();&#125; 在我们的应用程序中涉及 I/O 操作时，只要注意指定统一的编解码 Charset 字符集，一般不会出现乱码问题。 2.2 在内存操作中的编码在内存中进行从字符到字节的数据类型转换。 1、String 类提供字符串转换到字节的方法，也支持将字节转换成字符串的构造函数。 123String s = \"字符串\"；byte[] b = s.getBytes(\"UTF-8\");String n = new String(b, \"UTF-8\"); 2、Charset 提供 encode 与 decode，分别对应 char[] 到 byte[] 的编码 和 byte[] 到 char[] 的解码。 123Charset charset = Charset.forName(\"UTF-8\");ByteBuffer byteBuffer = charset.encode(string);CharBuffer charBuffer = charset.decode(byteBuffer); … 3、在 Java 中如何编解码Java 编码类图 首先根据指定的 charsetName 通过 Charset.forName(charsetName) 设置 Charset 类，然后根据 Charset 创建 CharsetEncoder 对象，再调用 CharsetEncoder.encode 对字符串进行编码，不同的编码类型都会对应到一个类中，实际的编码过程是在这些类中完成的。下面是 String. getBytes(charsetName) 编码过程的时序图 Java 编码时序图 从上图可以看出根据 charsetName 找到 Charset 类，然后根据这个字符集编码生成 CharsetEncoder，这个类是所有字符编码的父类，针对不同的字符编码集在其子类中定义了如何实现编码，有了 CharsetEncoder 对象后就可以调用 encode 方法去实现编码了。这个是 String.getBytes 编码方法，其它的如 StreamEncoder 中也是类似的方式。 经常会出现中文变成“？”很可能就是错误的使用了 ISO-8859-1 这个编码导致的。中文字符经过 ISO-8859-1 编码会丢失信息，通常我们称之为“黑洞”，它会把不认识的字符吸收掉。由于现在大部分基础的 Java 框架或系统默认的字符集编码都是 ISO-8859-1，所以很容易出现乱码问题，后面将会分析不同的乱码形式是怎么出现的。 几种编码格式的比较对中文字符后面四种编码格式都能处理，GB2312 与 GBK 编码规则类似，但是 GBK 范围更大，它能处理所有汉字字符，所以 GB2312 与 GBK 比较应该选择 GBK。UTF-16 与 UTF-8 都是处理 Unicode 编码，它们的编码规则不太相同，相对来说 UTF-16 编码效率最高，字符到字节相互转换更简单，进行字符串操作也更好。它适合在本地磁盘和内存之间使用，可以进行字符和字节之间快速切换，如 Java 的内存编码就是采用 UTF-16 编码。但是它不适合在网络之间传输，因为网络传输容易损坏字节流，一旦字节流损坏将很难恢复，想比较而言 UTF-8 更适合网络传输，对 ASCII 字符采用单字节存储，另外单个字符损坏也不会影响后面其它字符，在编码效率上介于 GBK 和 UTF-16 之间，所以 UTF-8 在编码效率上和编码安全性上做了平衡，是理想的中文编码方式。 4、在 Java Web 中涉及的编解码对于使用中文来说，有 I/O 的地方就会涉及到编码，前面已经提到了 I/O 操作会引起编码，而大部分 I/O 引起的乱码都是网络 I/O，因为现在几乎所有的应用程序都涉及到网络操作，而数据经过网络传输都是以字节为单位的，所以所有的数据都必须能够被序列化为字节。在 Java 中数据被序列化必须继承 Serializable 接口。 一段文本它的实际大小应该怎么计算，我曾经碰到过一个问题：就是要想办法压缩 Cookie 大小，减少网络传输量，当时有选择不同的压缩算法，发现压缩后字符数是减少了，但是并没有减少字节数。所谓的压缩只是将多个单字节字符通过编码转变成一个多字节字符。减少的是 String.length()，而并没有减少最终的字节数。例如将“ab”两个字符通过某种编码转变成一个奇怪的字符，虽然字符数从两个变成一个，但是如果采用 UTF-8 编码这个奇怪的字符最后经过编码可能又会变成三个或更多的字节。同样的道理比如整型数字 1234567 如果当成字符来存储，采用 UTF-8 来编码占用 7 个 byte，采用 UTF-16 编码将会占用 14 个 byte，但是把它当成 int 型数字来存储只需要 4 个 byte 来存储。所以看一段文本的大小，看字符本身的长度是没有意义的，即使是一样的字符采用不同的编码最终存储的大小也会不同，所以从字符到字节一定要看编码类型。 我们能够看到的汉字都是以字符形式出现的，例如在 Java 中“淘宝”两个字符，它在计算机中的数值 10 进制是 28120 和 23453，16 进制是 6bd8 和 5d9d，也就是这两个字符是由这两个数字唯一表示的。Java 中一个 char 是 16 个 bit 相当于两个字节，所以两个汉字用 char 表示在内存中占用相当于四个字节的空间。 这两个问题搞清楚后，我们看一下 Java Web 中那些地方可能会存在编码转换？ 用户从浏览器端发起一个 HTTP 请求，需要存在编码的地方是 URL、Cookie、Parameter。服务器端接受到 HTTP 请求后要解析 HTTP 协议，其中 URI、Cookie 和 POST 表单参数需要解码，服务器端可能还需要读取数据库中的数据，本地或网络中其它地方的文本文件，这些数据都可能存在编码问题，当 Servlet 处理完所有请求的数据后，需要将这些数据再编码通过 Socket 发送到用户请求的浏览器里，再经过浏览器解码成为文本。这些过程如下图所示： 一次 HTTP 请求的编码示例 4.1 URL 的编解码用户提交一个 URL，这个 URL 中可能存在中文，因此需要编码，如何对这个 URL 进行编码？根据什么规则来编码？有如何来解码？如下图一个 URL： 上图中以 Tomcat 作为 Servlet Engine 为例，它们分别对应到下面这些配置文件中：Port 对应在 Tomcat 的 中配置，而 Context Path 在 中配置，Servlet Path 在 Web 应用的 web.xml 中的 1234&lt;servlet-mapping&gt; &lt;servlet-name&gt;junshanExample&lt;/servlet-name&gt; &lt;url-pattern&gt;/servlets/servlet/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 中配置，PathInfo 是我们请求的具体的 Servlet，QueryString 是要传递的参数，注意这里是在浏览器里直接输入 URL 所以是通过 Get 方法请求的，如果是 POST 方法请求的话，QueryString 将通过表单方式提交到服务器端。 上图中 PathInfo 和 QueryString 出现了中文，当我们在浏览器中直接输入这个 URL 时，在浏览器端和服务端会如何编码和解析这个 URL 呢？为了验证浏览器是怎么编码 URL 的我选择的是360极速浏览器并通过 Postman 插件观察我们请求的 URL 的实际的内容，以下是 URL： HTTP://localhost:8080/examples/servlets/servlet/君山?author=君山 君山的编码结果是：e5 90 9b e5 b1 b1，和《深入分析 Java Web 技术内幕》中的结果不一样，这是因为我使用的浏览器和插件和原作者是有区别的，那么这些浏览器之间的默认编码是不一样的，原文中的结果是： 君山的编码结果分别是：e5 90 9b e5 b1 b1，be fd c9 bd，查阅上一届的编码可知，PathInfo 是 UTF-8 编码而 QueryString 是经过 GBK 编码，至于为什么会有“%”？查阅 URL 的编码规范 RFC3986 可知浏览器编码 URL 是将非 ASCII 字符按照某种编码格式编码成 16 进制数字然后将每个 16 进制表示的字节前加上“%”，所以最终的 URL 就成了上图的格式了。 从上面测试结果可知浏览器对 PathInfo 和 QueryString 的编码是不一样的，不同浏览器对 PathInfo 也可能不一样，这就对服务器的解码造成很大的困难，下面我们以 Tomcat 为例看一下，Tomcat 接受到这个 URL 是如何解码的。 解析请求的 URL 是在 org.apache.coyote.HTTP11.InternalInputBuffer 的 parseRequestLine 方法中，这个方法把传过来的 URL 的 byte[] 设置到 org.apache.coyote.Request 的相应的属性中。这里的 URL 仍然是 byte 格式，转成 char 是在 org.apache.catalina.connector.CoyoteAdapter 的 convertURI 方法中完成的： 12345678910111213141516171819202122232425262728293031323334protected void convertURI(MessageBytes uri, Request request) throws Exception &#123; ByteChunk bc = uri.getByteChunk(); int length = bc.getLength(); CharChunk cc = uri.getCharChunk(); cc.allocate(length, -1); String enc = connector.getURIEncoding(); if (enc != null) &#123; B2CConverter conv = request.getURIConverter(); try &#123; if (conv == null) &#123; conv = new B2CConverter(enc); request.setURIConverter(conv); &#125; &#125; catch (IOException e) &#123;...&#125; if (conv != null) &#123; try &#123; conv.convert(bc, cc, cc.getBuffer().length - cc.getEnd()); uri.setChars(cc.getBuffer(), cc.getStart(), cc.getLength()); return; &#125; catch (IOException e) &#123;...&#125; &#125; &#125; // Default encoding: fast conversion byte[] bbuf = bc.getBuffer(); char[] cbuf = cc.getBuffer(); int start = bc.getStart(); for (int i = 0; i &lt; length; i++) &#123; cbuf[i] = (char) (bbuf[i + start] &amp; 0xff); &#125; uri.setChars(cbuf, 0, length); &#125; 从上面的代码中可以知道对 URL 的 URI 部分进行解码的字符集是在 connector 的 中定义的，如果没有定义，那么将以默认编码 ISO-8859-1 解析。所以如果有中文 URL 时最好把 URIEncoding 设置成 UTF-8 编码。 QueryString 又如何解析？ GET 方式 HTTP 请求的 QueryString 与 POST 方式 HTTP 请求的表单参数都是作为 Parameters 保存，都是通过 request.getParameter 获取参数值。对它们的解码是在 request.getParameter 方法第一次被调用时进行的。request.getParameter 方法被调用时将会调用 org.apache.catalina.connector.Request 的 parseParameters 方法。这个方法将会对 GET 和 POST 方式传递的参数进行解码，但是它们的解码字符集有可能不一样。POST 表单的解码将在后面介绍，QueryString 的解码字符集是在哪定义的呢？它本身是通过 HTTP 的 Header 传到服务端的，并且也在 URL 中，是否和 URI 的解码字符集一样呢？从前面浏览器对 PathInfo 和 QueryString 的编码采取不同的编码格式不同可以猜测到解码字符集肯定也不会是一致的。的确是这样 QueryString 的解码字符集要么是 Header 中 ContentType 中定义的 Charset 要么就是默认的 ISO-8859-1，要使用 ContentType 中定义的编码就要设置 connector 的 中的 useBodyEncodingForURI 设置为 true。这个配置项的名字有点让人产生混淆，它并不是对整个 URI 都采用 BodyEncoding 进行解码而仅仅是对 QueryString 使用 BodyEncoding 解码，这一点还要特别注意。 从上面的 URL 编码和解码过程来看，比较复杂，而且编码和解码并不是我们在应用程序中能完全控制的，所以在我们的应用程序中应该尽量避免在 URL 中使用非 ASCII 字符，不然很可能会碰到乱码问题，当然在我们的服务器端最好设置 中的 URIEncoding 和 useBodyEncodingForURI 两个参数。 4.2 HTTP Header 的编解码当客户端发起一个 HTTP 请求除了上面的 URL 外还可能会在 Header 中传递其它参数如 Cookie、redirectPath 等，这些用户设置的值很可能也会存在编码问题，Tomcat 对它们又是怎么解码的呢？ 对 Header 中的项进行解码也是在调用 request.getHeader 是进行的，如果请求的 Header 项没有解码则调用 MessageBytes 的 toString 方法，这个方法将从 byte 到 char 的转化使用的默认编码也是 ISO-8859-1，而我们也不能设置 Header 的其它解码格式，所以如果你设置 Header 中有非 ASCII 字符解码肯定会有乱码。 我们在添加 Header 时也是同样的道理，不要在 Header 中传递非 ASCII 字符，如果一定要传递的话，我们可以先将这些字符用 org.apache.catalina.util.URLEncoder 编码然后再添加到 Header 中，这样在浏览器到服务器的传递过程中就不会丢失信息了，如果我们要访问这些项时再按照相应的字符集解码就好了。 4.3 POST 表单的编解码在前面提到了 POST 表单提交的参数的解码是在第一次调用 request.getParameter 发生的，POST 表单参数传递方式与 QueryString 不同，它是通过 HTTP 的 BODY 传递到服务端的。当我们在页面上点击 submit 按钮时浏览器首先将根据 ContentType 的 Charset 编码格式对表单填的参数进行编码然后提交到服务器端，在服务器端同样也是用 ContentType 中字符集进行解码。所以通过 POST 表单提交的参数一般不会出现问题，而且这个字符集编码是我们自己设置的，可以通过 request.setCharacterEncoding(charset) 来设置。 另外针对 multipart/form-data 类型的参数，也就是上传的文件编码同样也是使用 ContentType 定义的字符集编码，值得注意的地方是上传文件是用字节流的方式传输到服务器的本地临时目录，这个过程并没有涉及到字符编码，而真正编码是在将文件内容添加到 parameters 中，如果用这个编码不能编码时将会用默认编码 ISO-8859-1 来编码。 4.4 HTTP BODY 的编解码当用户请求的资源已经成功获取后，这些内容将通过 Response 返回给客户端浏览器，这个过程先要经过编码再到浏览器进行解码。这个过程的编解码字符集可以通过 response.setCharacterEncoding 来设置，它将会覆盖 request.getCharacterEncoding 的值，并且通过 Header 的 Content-Type 返回客户端，浏览器接受到返回的 socket 流时将通过 Content-Type 的 charset 来解码，如果返回的 HTTP Header 中 Content-Type 没有设置 charset，那么浏览器将根据 Html 的 中的 charset 来解码。如果也没有定义的话，那么浏览器将使用默认的编码来解码。 4.5 其它需要编码的地方除了 URL 和参数编码问题外，在服务端还有很多地方可能存在编码，如可能需要读取 xml、velocity 模版引擎、JSP 或者从数据库读取数据等。xml 文件可以通过设置头来制定编码格式 1&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; Velocity 模版设置编码格式： 1services.VelocityService.input.encoding=UTF-8 JSP 设置编码格式： 1&lt;%@page contentType=&quot;text/html; charset=UTF-8&quot;%&gt; 访问数据库都是通过客户端 JDBC 驱动来完成，用 JDBC 来存取数据要和数据的内置编码保持一致，可以通过设置 JDBC URL 来制定如 MySQL：url=”jdbc:mysql://localhost:3306/DB?useUnicode=true&amp;characterEncoding=GBK”。 5、常见问题分析下面看一下，当我们碰到一些乱码时，应该怎么处理这些问题？出现乱码问题唯一的原因都是在 char 到 byte 或 byte 到 char 转换中编码和解码的字符集不一致导致的，由于往往一次操作涉及到多次编解码，所以出现乱码时很难查找到底是哪个环节出现了问题，下面就几种常见的现象进行分析。 5.1 中文变成了看不懂的字符例如，字符串“淘！我喜欢！”变成了“Ì Ô £ ¡Î Ò Ï²»¶ £ ¡”编码过程如下图所示： 字符串在解码时所用的字符集与编码字符集不一致导致汉字变成了看不懂的乱码，而且是一个汉字字符变成两个乱码字符。 5.2 一个汉字变成一个问号例如，字符串“淘！我喜欢！”变成了“？？？？？？”编码过程如下图所示: 将中文和中文符号经过不支持中文的 ISO-8859-1 编码后，所有字符变成了“？”，这是因为用 ISO-8859-1 进行编解码时遇到不在码值范围内的字符时统一用 3f 表示，这也就是通常所说的“黑洞”，所有 ISO-8859-1 不认识的字符都变成了“？”。 5.3 一个汉字变成两个问号例如，字符串“淘！我喜欢！”变成了“？？？？？？？？？？？？”编码过程如下图所示: 这种情况比较复杂，中文经过多次编码，但是其中有一次编码或者解码不对仍然会出现中文字符变成“？”现象，出现这种情况要仔细查看中间的编码环节，找出出现编码错误的地方。 5.4 一种不正常的正确编码还有一种情况是在我们通过 request.getParameter 获取参数值时，当我们直接调用 String value = request.getParameter(name); 会出现乱码，但是如果用下面的方式 String value = String(request.getParameter(name).getBytes(&quot; ISO-8859-1&quot;), &quot;GBK&quot;); 解析时取得的 value 会是正确的汉字字符，这种情况是怎么造成的呢？ 看下如所示： 这种情况是这样的，ISO-8859-1 字符集的编码范围是 0000-00FF，正好和一个字节的编码范围相对应。这种特性保证了使用 ISO-8859-1 进行编码和解码可以保持编码数值“不变”。虽然中文字符在经过网络传输时，被错误地“拆”成了两个欧洲字符，但由于输出时也是用 ISO-8859-1，结果被“拆”开的中文字的两半又被合并在一起，从而又刚好组成了一个正确的汉字。虽然最终能取得正确的汉字，但是还是不建议用这种不正常的方式取得参数值，因为这中间增加了一次额外的编码与解码，这种情况出现乱码时因为 Tomcat 的配置文件中 useBodyEncodingForURI 配置项没有设置为”true”，从而造成第一次解析式用 ISO-8859-1 来解析才造成乱码的。 6、总结本文首先总结了几种常见编码格式的区别，然后介绍了支持中文的几种编码格式，并比较了它们的使用场景。接着介绍了 Java 那些地方会涉及到编码问题，已经 Java 中如何对编码的支持。并以网络 I/O 为例重点介绍了 HTTP 请求中的存在编码的地方，以及 Tomcat 对 HTTP 协议的解析，最后分析了我们平常遇到的乱码问题出现的原因。 综上所述，要解决中文问题，首先要搞清楚哪些地方会引起字符到字节的编码以及字节到字符的解码，最常见的地方就是读取会存储数据到磁盘，或者数据要经过网络传输。然后针对这些地方搞清楚操作这些数据的框架的或系统是如何控制编码的，正确设置编码格式，避免使用软件默认的或者是操作系统平台默认的编码格式。 注明：文章大部分参考书籍《深入 Java Web 技术内幕》第三章，自己有删减，二次转载请也务必注明此出处。","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"编码","slug":"编码","permalink":"http://yoursite.com/tags/编码/"}]},{"title":"程序访问文件的几种方式","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/程序访问文件的几种方式/","text":"IO程序访问文件的几种方式 读取和写入文件 I/O 操作都调用操作系统提供的接口。因为磁盘设备是由操作系统管理的，应用程序要访问物理设备只能通过系统调用的方式来工作。读和写分别对应 read() 和 write() 两个系统调用。而只要是系统调用就可能存在内核空间地址和用户空间地址切换的问题，这也是为什么操作系统为了保护系统本身的运行安全而将内核程序运行使用的内存空间和用户程序运行的内存空间进行隔离造成的。虽然这样可以保证内核程序运行的安全性，但是也存在数据可能需要从内核空间向用户空间复制的问题。 如果遇到非常耗时的操作，如磁盘 I/O， 数据从磁盘复制到内核空间，然后又从内核空间复制到用户空间，将会非常缓慢。这时操作系统为了加速 I/O 访问，在内核空间使用缓存机制，也就是将从磁盘读取的文件按照一定的组织方式进行缓存，如果用户程序访问的是同一段磁盘地址的空间数据，那么操作系统将从内核缓存中直接取出返回给用户程序，这样就可以减小 I/O 的响应时间。 1. 标准访问文件的方式标准访问文件的方式就是当应用程序调用 read() 接口时，操作系统检查在内核的高速缓存中有没有需要的数据，如果已经缓存了，那么就直接从缓存中返回，如果没有，则从磁盘中读取，然后缓存在操作系统的缓存中。 写入的方式是，用户的应用程序调用 write() 接口将数据从用户地址空间复制到内核地址空间的缓存中。这时对用户程序来说写操作就已经完成了，至于什么时候再写到磁盘中由操作系统决定，除非显示地调用 sync 同步命令。 标准访问文件的方式如下图所示： 2. 直接 I/O 的方式直接 I/O 方式就是应用程序直接访问磁盘数据，而不经过操作系统内核数据缓冲区，这样做的目的就是减少一次从内核缓冲区到用户程序缓存的数据复制。此种方式通常是在对数据的缓存管理由应用程序实现的数据库管理系统中。如在数据库管理系统中，系统明确的知道应该缓存哪些数据，应该失效哪些数据，还可以对一些热点的数据进行预加载，提前将热点数据加载到内存，可以加速数据的访问效率。在这些情况下，如果是由操作系统进行缓存，则很难做到，因为操作系统并不知道哪些是热点数据，哪些数据是访问一次后再也不会访问了，操作系统就是简单的缓存最近一次从磁盘读取的数据。 但是直接 I/O 也有负面的影响，如果访问的数据不再应用程序缓存中，则每次数据的加载都需要从磁盘读取，样加载的话速度非常的慢，通常是直接 I/O 与 异步 I/O 结合使用，会得到较好的性能。 直接 I/O 的方式如下图所示： 3. 同步访问文件的方式同步访问文件的方式就是数据的读取和写入都是同步操作的，它与标准访问文件的方式不同的是，只有当数据被成功写到磁盘时才返回给应用程序成功的标志。 这种访问文件的方式性能比较差，只有在一些数据安全性要求比较高的场景中才会使用，而且通常这种方式的硬件都是定制的。 同步访问文件的方式如下图所示： 4. 异步访问文件的方式异步访问文件的方式就是当访问数据的线程发出请求之后，线程会接着去处理其他事情，而不是阻塞等待，当请求的数据返回后继续处理下面的操作。这种方式可以明显的提高应用程序的效率，但是不会改变访问文件的效率。 异步访问文件的方式如下图所示： 5. 内存映射的方式内存映射的方式是指操作系统将内存中的某一块区域与磁盘中的文件关联起来，当要访问内存中的一段数据时，转换为访问文件的某一段数据。这种方式的目的同样是减少数据从内核空间缓存到用户空间缓存的数据复制操作，因为这两个空间的数据是共享的。 内存映射的方式如下图所示： 注：以上参考书籍《深入分析Java Web 技术内幕修订版》许令波，更多精彩知识还请看原书。","tags":[{"name":"IO","slug":"IO","permalink":"http://yoursite.com/tags/IO/"}]},{"title":"深度探究Java 中 finally 语句块","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/深度探究Java 中 finally 语句块/","text":"乍看这个题目，是不是有人会问，这个谁不知道啊，大凡熟悉 Java 编程的人都知道 finally 语句块的作用和用法。有什么可深度辨析的呢？事实并非如此，我发现即使写了很多年 Java 程序的人，也不一定能够透彻的理解 finally 语句块。本篇将以生动形象的案例来带您由浅入深的来分析一下这个小小的 finally，希望这篇文章能够让您真正的理解 finally 语句块的本质，至少阅读完本篇文章后，没有觉得浪费了时间。 可不能小看这个简单的 finally，看似简单的问题背后，却隐藏了无数的玄机。接下来我就带您一步一步的揭开这个 finally 的神秘面纱。 问题分析首先来问大家一个问题：finally 语句块一定会执行吗？很多人都认为 finally 语句块是肯定要执行的，其中也包括一些很有经验的 Java 程序员。可惜并不像大多人所认为的那样，对于这个问题，答案当然是否定的，我们先来看下面这个例子。 清单 1.12345678910111213141516171819public class Test &#123;public static void main(String[] args) &#123;System.out.println(\"return value of test(): \" + test()); &#125;public static int test() &#123;int i = 1;// if(i == 1)// return 0;System.out.println(\"the previous statement of try block\");i = i / 0;try &#123; System.out.println(\"try block\"); return i; &#125;finally &#123; System.out.println(\"finally block\"); &#125; &#125;&#125; 清单 1 的执行结果如下： 1234the previous statement of try block Exception in thread &quot;main&quot; java.lang.ArithmeticException: / by zero at com.bj.charlie.Test.test(Test.java:15) at com.bj.charlie.Test.main(Test.java:6) 另外，如果去掉上例中被注释的两条语句前的注释符，执行结果则是： 1return value of test(): 0 在以上两种情况下，finally 语句块都没有执行，说明什么问题呢？只有与 finally 相对应的 try 语句块得到执行的情况下，finally 语句块才会执行。以上两种情况，都是在 try 语句块之前返回（return）或者抛出异常，所以 try 对应的 finally 语句块没有执行。 那好，即使与 finally 相对应的 try 语句块得到执行的情况下，finally 语句块一定会执行吗？不好意思，这次可能又让大家失望了，答案仍然是否定的。请看下面这个例子（清单 2）。 清单 2.12345678910111213141516public class Test &#123;public static void main(String[] args) &#123;System.out.println(\"return value of test(): \" + test()); &#125;public static int test() &#123;int i = 1;try &#123;System.out.println(\"try block\");System.exit(0);return i;&#125;finally &#123;System.out.println(\"finally block\"); &#125; &#125;&#125; 清单 2 的执行结果如下： 1try block finally 语句块还是没有执行，为什么呢？因为我们在 try 语句块中执行了 System.exit (0) 语句，终止了 Java 虚拟机的运行。那有人说了，在一般的 Java 应用中基本上是不会调用这个 System.exit(0) 方法的。OK ！没有问题，我们不调用 System.exit(0) 这个方法，那么 finally 语句块就一定会执行吗？ 再一次让大家失望了，答案还是否定的。当一个线程在执行 try 语句块或者 catch 语句块时被打断（interrupted）或者被终止（killed），与其相对应的 finally 语句块可能不会执行。还有更极端的情况，就是在线程运行 try 语句块或者 catch 语句块时，突然死机或者断电，finally 语句块肯定不会执行了。可能有人认为死机、断电这些理由有些强词夺理，没有关系，我们只是为了说明这个问题。 finally 语句剖析说了这么多，还是让我们拿出些有说服力的证据吧！还有什么证据比官方的文档更具说服力呢？让我们来看看官方网站上的《The Java Tutorials》中是怎样来描述 finally 语句块的吧！以下内容原封不动的摘自于《 The Java Tutorials 》文档。 The finally BlockThe finally block always executes when the try block exits. This ensures that the finally block is executed even if an unexpected exception occurs. But finally is useful for more than just exception handling — it allows the programmer to avoid having cleanup code accidentally bypassed by a return,continue, or break. Putting cleanup code in a finally block is always a good practice, even when no exceptions are anticipated.Note: If the JVM exits while the try or catch code is being executed, then the finally block may not execute. Likewise, if the thread executing the try or catch code is interrupted or killed, the finally block may not execute even though the application as a whole continues. 请仔细阅读并认真体会一下以上两段英文，当你真正的理解了这两段英文的确切含义，你就可以非常自信的来回答“finally 语句块是否一定会执行？”这样的问题。看来，大多时候，并不是 Java 语言本身有多么高深，而是我们忽略了对基础知识的深入理解。 接下来，我们看一下 finally 语句块是怎样执行的。在排除了以上 finally 语句块不执行的情况后，finally 语句块就得保证要执行，既然 finally 语句块一定要执行，那么它和 try 语句块与 catch 语句块的执行顺序又是怎样的呢？还有，如果 try 语句块中有 return 语句，那么 finally 语句块是在 return 之前执行，还是在 return 之后执行呢？带着这样一些问题，我们还是以具体的案例来讲解。 关于 try、catch、finally 的执行顺序问题，我们还是来看看权威的论述吧！以下内容摘自 Java 语言规范第四版（《The Java™ Programming Language, Fourth Edition》）中对于 try，catch，和 finally 的描述。 12.4. Try, catch, and finallyYou catch exceptions by enclosing code in Try blocks. The basic syntax for a Try block is:try {statements} catch (exception_type1 identifier1) {statements} catch (exception_type2 identifier2) {statements…} finally {statements} where either at least one catch clause, or the finally clause, must be present. The body of the try statement is executed until either an exception is thrown or the body finishes successfully. If an exception is thrown, each catch clause is examined in turn, from first to last, to see whether the type of the exception object is assignable to the type declared in the catch. When an assignable catch clause is found, its block is executed with its identifier set to reference the exception object. No other catch clause will be executed. Any number of catch clauses, including zero, can be associated with a particular TRy as long as each clause catches a different type of exception. If no appropriate catch is found, the exception percolates out of the try statement into any outer try that might have a catch clause to handle it. If a finally clause is present with a try, its code is executed after all other processing in the try is complete. This happens no matter how completion was achieved, whether normally, through an exception, or through a control flow statement such as return or break. 上面这段文字的大体意思是说，不管 try 语句块正常结束还是异常结束，finally 语句块是保证要执行的。如果 try 语句块正常结束，那么在 try 语句块中的语句都执行完之后，再执行 finally 语句块。如果 try 中有控制转移语句（return、break、continue）呢？那 finally 语句块是在控制转移语句之前执行，还是之后执行呢？似乎从上面的描述中我们还看不出任何端倪，不要着急，后面的讲解中我们会分析这个问题。如果 try 语句块异常结束，应该先去相应的 catch 块做异常处理，然后执行 finally 语句块。同样的问题，如果 catch 语句块中包含控制转移语句呢？ finally 语句块是在这些控制转移语句之前，还是之后执行呢？我们也会在后续讨论中提到。 其实，关于 try，catch，finally 的执行流程远非这么简单，有兴趣的读者可以参考 Java 语言规范第三版（《The Java™ Language Specification, Third Edition》）中对于 Execution of try-catch-finally 的描述，非常复杂的一个流程。限于篇幅的原因，本文不做摘录，请感兴趣的读者自行阅读。 finally 语句示例说明下面，我们先来看一个简单的例子（清单 3）。 清单 3.12345678910public class Test &#123;public static void main(String[] args) &#123;try &#123;System.out.println(\"try block\");return ;&#125; finally &#123;System.out.println(\"finally block\"); &#125; &#125;&#125; 清单 3 的执行结果为：12try blockfinally block 清单 3 说明 finally 语句块在 try 语句块中的 return 语句之前执行。我们再来看另一个例子（清单 4）。 清单 4.123456789101112131415161718public class Test &#123;public static void main(String[] args) &#123;System.out.println(\"reture value of test() : \" + test()); &#125;public static int test()&#123;int i = 1;try &#123; System.out.println(\"try block\"); i = 1 / 0; return 1;&#125;catch (Exception e)&#123;System.out.println(\"exception block\");return 2;&#125;finally &#123;System.out.println(\"finally block\"); &#125; &#125;&#125; 清单 4 的执行结果为：1234try blockexception blockfinally blockreture value of test() : 2 清单 4 说明了 finally 语句块在 catch 语句块中的 return 语句之前执行。 从上面的清单 3 和清单 4，我们可以看出，其实 finally 语句块是在 try 或者 catch 中的 return 语句之前执行的。更加一般的说法是，finally 语句块应该是在控制转移语句之前执行，控制转移语句除了 return 外，还有 break 和 continue。另外，throw 语句也属于控制转移语句。虽然 return、throw、break 和 continue 都是控制转移语句，但是它们之间是有区别的。其中 return 和 throw 把程序控制权转交给它们的调用者（invoker），而 break 和 continue 的控制权是在当前方法内转移。请大家先记住它们的区别，在后续的分析中我们还会谈到。 还是得来点有说服力的证据，下面这段摘自 Java 语言规范第四版（《The Java™ Programming Language, Fourth Edition》），请读者自己体会一下其含义。 Afinallyclause can also be used to clean up forbreak,continue, andreturn, which is one reason you will sometimes see atryclause with nocatchclauses. When any control transfer statement is executed, all relevantfinallyclauses are executed. There is no way to leave atryblock without executing itsfinallyclause. 好了，看到这里，是不是有人认为自己已经掌握了 finally 的用法了？先别忙着下结论，我们再来看两个例子 – 清单 5 和清单 6。 清单 5.123456789101112public class Test &#123; public static void main(String[] args) &#123; System.out.println(\"return value of getValue(): \" + getValue()); &#125; public static int getValue() &#123; try &#123; return 0; &#125; finally &#123; return 1; &#125; &#125; &#125; 清单 5 的执行结果：1return value of getValue(): 1 清单 6.12345678910111213public class Test &#123;public static void main(String[] args) &#123; System.out.println(\"return value of getValue(): \" + getValue()); &#125;public static int getValue() &#123; int i = 1; try &#123; return i; &#125; finally &#123; i++; &#125; &#125;&#125; 清单 6 的执行结果：1return value of getValue(): 1 利用我们上面分析得出的结论：finally 语句块是在 try 或者 catch 中的 return 语句之前执行的。 由此，可以轻松的理解清单 5 的执行结果是 1。因为 finally 中的 return 1；语句要在 try 中的 return 0；语句之前执行，那么 finally 中的 return 1；语句执行后，把程序的控制权转交给了它的调用者 main（）函数，并且返回值为 1。那为什么清单 6 的返回值不是 2，而是 1 呢？按照清单 5 的分析逻辑，finally 中的 i++；语句应该在 try 中的 return i；之前执行啊？ i 的初始值为 1，那么执行 i++；之后为 2，再执行 return i；那不就应该是 2 吗？怎么变成 1 了呢？ 关于 Java 虚拟机是如何编译 finally 语句块的问题，有兴趣的读者可以参考《 The JavaTM Virtual Machine Specification, Second Edition 》中 7.13 节 Compiling finally。那里详细介绍了 Java 虚拟机是如何编译 finally 语句块。实际上，Java 虚拟机会把 finally 语句块作为 subroutine（对于这个 subroutine 不知该如何翻译为好，干脆就不翻译了，免得产生歧义和误解。）直接插入到 try 语句块或者 catch 语句块的控制转移语句之前。但是，还有另外一个不可忽视的因素，那就是在执行 subroutine（也就是 finally 语句块）之前，try 或者 catch 语句块会保留其返回值到本地变量表（Local Variable Table）中。待 subroutine 执行完毕之后，再恢复保留的返回值到操作数栈中，然后通过 return 或者 throw 语句将其返回给该方法的调用者（invoker）。请注意，前文中我们曾经提到过 return、throw 和 break、continue 的区别，对于这条规则（保留返回值），只适用于 return 和 throw 语句，不适用于 break 和 continue 语句，因为它们根本就没有返回值。 是不是不太好理解，那我们就用具体的例子来做形象的说明吧！ 为了能够解释清单 6 的执行结果，我们来分析一下清单 6 的字节码（byte-code）： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647Compiled from &quot;Test.java&quot; public class Test extends java.lang.Object&#123; public Test(); Code: 0: aload_0 1:invokespecial#1; //Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return LineNumberTable: line 1: 0 public static void main(java.lang.String[]); Code: 0: getstatic #2; //Field java/lang/System.out:Ljava/io/PrintStream; 3: new #3; //class java/lang/StringBuilder 6: dup 7: invokespecial #4; //Method java/lang/StringBuilder.&quot;&lt;init&gt;&quot;:()V 10: ldc #5; //String return value of getValue(): 12: invokevirtual #6; //Method java/lang/StringBuilder.append:( Ljava/lang/String;)Ljava/lang/StringBuilder; 15: invokestatic #7; //Method getValue:()I 18: invokevirtual #8; //Method java/lang/StringBuilder.append:(I)Ljava/lang/StringBuilder; 21: invokevirtual #9; //Method java/lang/StringBuilder.toString:()Ljava/lang/String; 24: invokevirtual #10; //Method java/io/PrintStream.println:(Ljava/lang/String;)V 27: return public static int getValue(); Code: 0: iconst_1 1: istore_0 2: iload_0 3: istore_1 4: iinc 0, 1 7: iload_1 8: ireturn 9: astore_2 10: iinc 0, 1 13: aload_2 14: athrow Exception table: from to target type 2 4 9 any 9 10 9 any &#125; 对于 Test（）构造方法与 main（）方法，在这里，我们不做过多解释。让我们来分析一下 getValue（）方法的执行。在这之前，先让我把 getValue（）中用到的虚拟机指令解释一下，以便读者能够正确的理解该函数的执行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445461. iconst_Description: Push the int constant (-1, 0, 1, 2, 3, 4 or 5) onto the operand stack.Forms: iconst_m1 = 2 (0x2) iconst_0 = 3 (0x3) iconst_1 = 4 (0x4)iconst_2 = 5 (0x5) iconst_3 = 6 (0x6) iconst_4 = 7 (0x7) iconst_5 = 8 (0x8)2. istore_Description: Store int into local variable. The must be an index into thelocal variable array of the current frame.Forms: istore_0 = 59 (0x3b) istore_1 = 60 (0x3c) istore_2 = 61 (0x3d)istore_3 = 62 (0x3e)3. iload_Description: Load int from local variable. The must be an index into thelocal variable array of the current frame.Forms: iload_0 = 26 (0x1a) iload_1 = 27 (0x1b) iload_2 = 28 (0x1c) iload_3 = 29 (0x1d)4. iinc index, constDescription: Increment local variable by constant. The index is an unsigned byte thatmust be an index into the local variable array of the current frame. The const is animmediate signed byte. The local variable at index must contain an int. The valueconst is first sign-extended to an int, and then the local variable at index isincremented by that amount.Forms: iinc = 132 (0x84)Format:iincindexconst5. ireturnDescription: Return int from method.Forms: ireturn = 172 (0xac)6. astore_Description: Store reference into local variable. The must be an index into thelocal variable array of the current frame.Forms: astore_0 = 75 (0x4b) astore_1 = 76 (0x4c) astore_2 =77 (0x4d) astore_3 =78 (0x4e)7. aload_Description: Load reference from local variable. The must be an index into thelocal variable array of the current frame.Forms: aload_0 = 42 (0x2a) aload_1 = 43 (0x2b) aload_2 = 44 (0x2c) aload_3 = 45 (0x2d)8. athrowDescription: Throw exception or error.Forms: athrow = 191 (0xbf) 有了以上的 Java 虚拟机指令，我们来分析一下其执行顺序：分为正常执行（没有 exception）和异常执行（有 exception）两种情况。我们先来看一下正常执行的情况，如图 1 所示： ###图 1. getValue（）函数正常执行的情况 由上图，我们可以清晰的看出，在 finally 语句块（iinc 0, 1）执行之前，getValue（）方法保存了其返回值（1）到本地表量表中 1 的位置，完成这个任务的指令是 istore_1；然后执行 finally 语句块（iinc 0, 1），finally 语句块把位于 0 这个位置的本地变量表中的值加 1，变成 2；待 finally 语句块执行完毕之后，把本地表量表中 1 的位置上值恢复到操作数栈（iload _1），最后执行 ireturn 指令把当前操作数栈中的值（1）返回给其调用者（main）。这就是为什么清单 6 的执行结果是 1，而不是 2 的原因。 再让我们来看看异常执行的情况。是不是有人会问，你的清单 6 中都没有 catch 语句，哪来的异常处理呢？我觉得这是一个好问题，其实，即使没有 catch 语句，Java 编译器编译出的字节码中还是有默认的异常处理的，别忘了，除了需要捕获的异常，还可能有不需捕获的异常（如：RunTimeException 和 Error）。 从 getValue（）方法的字节码中，我们可以看到它的异常处理表（exception table）， 如下：123Exception table:from to target type2 4 9 any 它的意思是说：如果从 2 到 4 这段指令出现异常，则由从 9 开始的指令来处理。 图 2. getValue（）函数异常执行的情况 先说明一点，上图中的 exception 其实应该是 exception 对象的引用，为了方便说明，我直接把它写成 exception 了。 由上图（图 2）可知，当从 2 到 4 这段指令出现异常时，将会产生一个 exception 对象，并且把它压入当前操作数栈的栈顶。接下来是 astore_2 这条指令，它负责把 exception 对象保存到本地变量表中 2 的位置，然后执行 finally 语句块，待 finally 语句块执行完毕后，再由 aload _2 这条指令把预先存储的 exception 对象恢复到操作数栈中，最后由 athrow 指令将其返回给该方法的调用者（main）。 通过以上的分析，大家应该已经清楚 try-catch-finally 语句块的执行流程了吧！为了更具说服力，我们还是来引经据典吧！下面这段仍然摘自 Java 语言规范第四版 《The Java™ Programming Language, Fourth Edition》，请读者自己体会吧！ a finally clause is always entered with a reason. That reason may be that the try code finished normally, that it executed a control flow statement such as return, or that an exception was thrown in code executed in the Try block. The reason is remembered when the finally clause exits by falling out the bottom. However, if the finally block creates its own reason to leave by executing a control flow statement (such as break or return) or by throwing an exception, that reason supersedes the original one, and the original reason is forgotten. For example, consider the following code:try {// … do something …return 1;} finally {return 2;}When the Try block executes its return, the finally block is entered with the “reason” of returning the value 1. However, inside the finally block the value 2 is returned, so the initial intention is forgotten. In fact, if any of the other code in the try block had thrown an exception, the result would still be to return 2. If the finally block did not return a value but simply fell out the bottom, the “return the value 1 ″ reason would be remembered and carried out. 好了，有了以上的知识，让我们再来看以下 3 个例子。 清单 7.123456789101112131415public class Test &#123; public static void main(String[] args) &#123; System.out.println(\"return value of getValue(): \" + getValue()); &#125; @SuppressWarnings(\"finally\") public static int getValue() &#123; int i = 1; try &#123; i = 4; &#125; finally &#123; i++; return i; &#125; &#125; &#125; 清单 7 的执行结果：1return value of getValue(): 5 清单 8.1234567891011121314public class Test &#123;public static void main(String[] args) &#123; System.out.println(\"return value of getValue(): \" + getValue()); &#125;public static int getValue() &#123; int i = 1; try &#123; i = 4; &#125; finally &#123; i++; &#125; return i; &#125;&#125; 清单 8 的执行结果：1return value of getValue(): 5 清单 7 和清单 8 应该还比较简单吧！利用我们上面讲解的知识，很容易分析出其结果。让我们再来看一个稍微复杂一点的例子 – 清单 9。我建议大家最好先不要看执行结果，运用学过的知识来分析一下，看是否能推断出正确的结果。 清单 9.1234567891011121314151617public class Test &#123;public static void main(String[] args) &#123;System.out.println(test()); &#125;public static String test() &#123;try &#123;System.out.println(\"try block\");return test1();&#125; finally &#123;System.out.println(\"finally block\"); &#125; &#125;public static String test1() &#123;System.out.println(\"return statement\");return \"after return\"; &#125;&#125; 清单 9 的结果：1234try blockreturn statementfinally blockafter return 你分析对了吗？其实这个案例也不算很难，return test1();这条语句等同于 : 12String tmp = test1();return tmp; 这样，就应该清楚为什么是上面所示的执行结果了吧！ 如果还是不怎么清楚可以在 IDE 下试用下 Debug，然后查看具体的运行步骤。 好了，就写到这吧！希望大家看完这篇文章能够有所收获！ 总结没想到吧！一个小小的、看似简单的 finally 语句块背后居然隐藏了这么多玄机。看来，我们平时还是应该认真的阅读 Java 相关的基础文档，比如：Java 语言规范、Java 虚拟机规范等，很多棘手的问题都可以从中得到答案。只有真正的吃透了基础知识，才能达到运用自如的境界！ 参考资料参考 The Java Tutorials，查看对 finally 语句块的描述。 参考 《The Java Programming Language, Fourth Edition》，查看 Java 语言规范第四版中对 finally 语句块的解释。 参考 《The Java Language Specification, Third Edition》，查看 Java 语言规范第三版中对 finally 语句块执行流程的具体描述。 参考 《The Java Virtual Machine Specification, Second Edition》，查看 Java 虚拟机是如何来编译 finally 语句块的知识。 查看文章 《Java 的异常处理机制(try … catch … finally)》，了解更多关于 Java 中 finally 语句块的分析。 查看文章 《Java 中 finally 的辨析》，了解更多关于 Java 中 finally 语句块的分析。 查看文章《关于 Java 中 finally 语句块的深度辨析》","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"finally","slug":"finally","permalink":"http://yoursite.com/tags/finally/"}]},{"title":"解决jdk1.8中发送邮件失败（handshake_failure）问题","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/解决jdk1.8中发送邮件失败（handshake_failure）问题/","text":"暑假在家做一个类似知乎的问答型网站（代码可见：Github/wenda 喜欢的可以给个star或者自己fork然后修改，目前功能还未很完善），其中有一个站内邮件通知系统（这里简单的讲一个例子：如果用户登录的时候出现异常，那么就会通过邮件发送通知用户）。然而却碰到一个问题。问题错误信息如下： 发送邮件失败Mail server connection failed; nested exception is javax.mail.MessagingException: Could not connect to SMTP host: smtp.qq.com, port: 465;nested exception is: javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure. Failed messages: javax.mail. MessagingException: Could not connect to SMTP host: smtp.qq.com, port: 465;nested exception is: javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure 自己在将错误信息代码google了一下，找了很久发现很多解决方案，包括stackoverflow上的一些解决方案，但还是没用。然后呢用百度试了下，结果在第一条是开源中国的一篇博客:javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure。 点进去是这样的：（如下图） 结果就是：这个问题是jdk导致的，jdk1.8里面有一个jce的包，安全性机制导致的访问https会报错，官网上有替代的jar包，如果替换掉就可以了。问题的解决方法还可以就是在整个项目中把你的jdk换成是1.7去，同样也可以解决这个我问题。 这两个jar包的下载地址：http://www.oracle.com/technetwork/java/javase/downloads/jce-7-download-432124.html 然后下载之后，把这个压缩文件解压，得到两个jar包去覆盖jdk安装目录下的jre\\lib\\security\\下相同的jar包就能解决java8的邮件发送问题。 接着用QQ邮箱我亲测有用，但是要注意一点就是：开启SMTP服务后要记得将你的16位授权码作为你的qq邮箱登录密码。 MailSender.java中mailSender.setPassword(“16位授权码”); mailSender.setHost(“smtp.qq.com”);mailSender.setPort(465); 下面把完整代码发布出来： 1. LoginExceptionHandler.java 12345678910111213141516171819202122232425262728293031323334package com.nowcoder.async.handler;import com.nowcoder.async.EventHandler;import com.nowcoder.async.EventModel;import com.nowcoder.async.EventType;import com.nowcoder.util.MailSender;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import java.util.Arrays;import java.util.HashMap;import java.util.List;import java.util.Map;/** * Created by 10412 on 2016/8/10. */@Componentpublic class LoginExceptionHandler implements EventHandler&#123; @Autowired MailSender mailSender; @Override public void doHandle(EventModel model) &#123; // xxxx判断发现这个用户登陆异常 Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); map.put(\"username\", model.getExt(\"username\")); mailSender.sendWithHTMLTemplate(model.getExt(\"email\"), \"登陆IP异常\", \"mails/login_exception.html\", map); &#125; @Override public List&lt;EventType&gt; getSupportEventTypes() &#123; return Arrays.asList(EventType.LOGIN); &#125;&#125; 2. LoginController.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114package com.nowcoder.controller;import com.nowcoder.async.EventModel;import com.nowcoder.async.EventProducer;import com.nowcoder.async.EventType;import com.nowcoder.service.UserService;import org.apache.commons.lang.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.CookieValue;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.RequestParam;import javax.servlet.http.Cookie;import javax.servlet.http.HttpServletResponse;import java.util.Map;/** * Created by 10412 on 2016/7/2. */@Controllerpublic class LoginController &#123; private static final Logger logger = LoggerFactory.getLogger(LoginController.class); @Autowired UserService userService; @Autowired EventProducer eventProducer; @RequestMapping(path = &#123;&quot;/reg/&quot;&#125;, method = &#123;RequestMethod.POST&#125;) public String reg(Model model, @RequestParam(&quot;username&quot;) String username, @RequestParam(&quot;password&quot;) String password, @RequestParam(&quot;next&quot;) String next, @RequestParam(value=&quot;rememberme&quot;, defaultValue = &quot;false&quot;) boolean rememberme, HttpServletResponse response) &#123; try &#123; Map&lt;String, Object&gt; map = userService.register(username, password); if (map.containsKey(&quot;ticket&quot;)) &#123; Cookie cookie = new Cookie(&quot;ticket&quot;, map.get(&quot;ticket&quot;).toString()); cookie.setPath(&quot;/&quot;); if (rememberme) &#123; cookie.setMaxAge(3600*24*5); &#125; response.addCookie(cookie); if (StringUtils.isNotBlank(next)) &#123; return &quot;redirect:&quot; + next; &#125; return &quot;redirect:/&quot;; &#125; else &#123; model.addAttribute(&quot;msg&quot;, map.get(&quot;msg&quot;)); return &quot;login&quot;; &#125; &#125; catch (Exception e) &#123; logger.error(&quot;注册异常&quot; + e.getMessage()); model.addAttribute(&quot;msg&quot;, &quot;服务器错误&quot;); return &quot;login&quot;; &#125; &#125; @RequestMapping(path = &#123;&quot;/reglogin&quot;&#125;, method = &#123;RequestMethod.GET&#125;) public String regloginPage(Model model, @RequestParam(value = &quot;next&quot;, required = false) String next) &#123; model.addAttribute(&quot;next&quot;, next); return &quot;login&quot;; &#125; @RequestMapping(path = &#123;&quot;/login/&quot;&#125;, method = &#123;RequestMethod.POST&#125;) public String login(Model model, @RequestParam(&quot;username&quot;) String username, @RequestParam(&quot;password&quot;) String password, @RequestParam(value=&quot;next&quot;, required = false) String next, @RequestParam(value=&quot;rememberme&quot;, defaultValue = &quot;false&quot;) boolean rememberme, HttpServletResponse response) &#123; try &#123; Map&lt;String, Object&gt; map = userService.login(username, password); if (map.containsKey(&quot;ticket&quot;)) &#123; Cookie cookie = new Cookie(&quot;ticket&quot;, map.get(&quot;ticket&quot;).toString()); cookie.setPath(&quot;/&quot;); if (rememberme) &#123; cookie.setMaxAge(3600*24*5); &#125; response.addCookie(cookie); eventProducer.fireEvent(new EventModel(EventType.LOGIN) .setExt(&quot;username&quot;, username).setExt(&quot;email&quot;, &quot;****@qq.com&quot;) .setActorId((int)map.get(&quot;userId&quot;))); if (StringUtils.isNotBlank(next)) &#123; return &quot;redirect:&quot; + next; &#125; return &quot;redirect:/&quot;; &#125; else &#123; model.addAttribute(&quot;msg&quot;, map.get(&quot;msg&quot;)); return &quot;login&quot;; &#125; &#125; catch (Exception e) &#123; logger.error(&quot;登陆异常&quot; + e.getMessage()); return &quot;login&quot;; &#125; &#125; @RequestMapping(path = &#123;&quot;/logout&quot;&#125;, method = &#123;RequestMethod.GET, RequestMethod.POST&#125;) public String logout(@CookieValue(&quot;ticket&quot;) String ticket) &#123; userService.logout(ticket); return &quot;redirect:/&quot;; &#125;&#125; 3. EventHandler.java1234567891011121314package com.nowcoder.async;import java.util.List;/** * Created by 10412 on 2016/8/10. */public interface EventHandler&#123; void doHandle(EventModel model); List&lt;EventType&gt; getSupportEventTypes();&#125; 4. MailSender.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package com.nowcoder.util;import org.apache.velocity.app.VelocityEngine;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.InitializingBean;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.mail.javamail.JavaMailSenderImpl;import org.springframework.mail.javamail.MimeMessageHelper;import org.springframework.stereotype.Service;import org.springframework.ui.velocity.VelocityEngineUtils;import javax.mail.internet.InternetAddress;import javax.mail.internet.MimeMessage;import javax.mail.internet.MimeUtility;import java.util.Map;import java.util.Properties;/** * Created by 10412 on 2016/8/10. // ***@qq.com wnppafhsbrcgbfbh（16位授权码） */@Servicepublic class MailSender implements InitializingBean &#123; private static final Logger logger = LoggerFactory.getLogger(MailSender.class); private JavaMailSenderImpl mailSender; @Autowired private VelocityEngine velocityEngine; public boolean sendWithHTMLTemplate(String to, String subject, String template, Map&lt;String, Object&gt; model) &#123; try &#123; String nick = MimeUtility.encodeText(\"***\"); InternetAddress from = new InternetAddress(nick + \"&lt;****@qq.com&gt;\"); MimeMessage mimeMessage = mailSender.createMimeMessage(); MimeMessageHelper mimeMessageHelper = new MimeMessageHelper(mimeMessage); String result = VelocityEngineUtils .mergeTemplateIntoString(velocityEngine, template, \"UTF-8\", model); mimeMessageHelper.setTo(to); mimeMessageHelper.setFrom(from); mimeMessageHelper.setSubject(subject); mimeMessageHelper.setText(result, true); mailSender.send(mimeMessage); return true; &#125; catch (Exception e) &#123; logger.error(\"发送邮件失败\" + e.getMessage()); return false; &#125; &#125; @Override public void afterPropertiesSet() throws Exception &#123; mailSender = new JavaMailSenderImpl(); mailSender.setUsername(\"***@qq.com\"); mailSender.setPassword(\"wnppafhsbrcgbfbh\"); //qq邮箱开启smtp服务后使用16位授权码在第三方登录// mailSender.setHost(\"smtp.exmail.qq.com\"); mailSender.setHost(\"smtp.qq.com\"); mailSender.setPort(465);// mailSender.setHost(\"smtp.163.com\"); //163邮箱// mailSender.setPort(25); mailSender.setProtocol(\"smtps\"); mailSender.setDefaultEncoding(\"utf8\"); Properties javaMailProperties = new Properties(); javaMailProperties.put(\"mail.smtp.ssl.enable\", true); //javaMailProperties.put(\"mail.smtp.auth\", true); //javaMailProperties.put(\"mail.smtp.starttls.enable\", true); mailSender.setJavaMailProperties(javaMailProperties); &#125;&#125; 5. login_exception.html 发送消息模板（可自定义） 1你好$username，你的登陆有问题! 一切都好了，运行。 登录。 发送邮件过来了。 总结来说：这个错误就是jdk1.8中的一个jce的包，安全性机制导致访问https会报错。","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"邮件发送","slug":"邮件发送","permalink":"http://yoursite.com/tags/邮件发送/"}]},{"title":"《疯狂 Java 突破程序员基本功的 16 课》读书笔记","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Java-16-lession/","text":"第 1 课 —— 数组与内存控制数组初始化数组初始化之后，该数组的长度是不可变的（可通过数组的 length 属性访问数组的长度）。Java 中的数组必须经过初始化（为数组对象的元素分配内存空间，并为每个数组元素指定初始值）才可使用。 数组初始化的形式： 静态初始化：初始化时由程序员显示的指定每个数组的初始值，系统决定数组长度。 动态初始化：初始化时程序员只指定数组的长度，系统为数组元素分配初始值。 使用数组数组元素就是变量：例如 int[] 数组元素相当于 int 类型的变量 当通过索引来使用数组元素时（访问数组元素的值、为数组元素赋值），将该数组元素当成普通变量使用即可。 第 2 课 —— 对象与内存的控制Java 内存管理分为：内存分配和内存回收。 内存分配：创建 Java 对象时 JVM 为该对象在堆内存中所分配的内存空间。 内存回收：当 Java 对象失去引用，变成垃圾，JVM 的垃圾回收机制自动清理该对象，并回收内存 实例变量 和 类变量局部变量特点：作用时间短，存储在方法的栈内存中 种类： 形参：方法签名中定义的局部变量，由方法调用者负责为其赋值，随方法结束而消亡 方法内的局部变量：方法内定义的局部变量，必须在方法内对其进行显示初始化，从初始化后开始生效，随方法结束而消亡 代码块内的局部变量：在代码块中定义的局部变量，必须在代码块中进行显示初始化，从初始化后开始生效，随代码块结束而消亡 成员变量类体内定义的变量，如果该成员变量没有使用 static 修饰，那该成员变量又被称为非静态变量或实例变量，如果使用 static 修饰，则该成员变量又可被称为静态变量或类变量。 实例变量和类变量的属性使用 static 修饰的成员变量是类变量，属于该类本身，没有使用 static 修饰的成员变量是实例变量，属于该类的实例，在同一个类中，每一个类只对应一个 Class 对象，但每个类可以创建多个对象。 由于同一个 JVM 内的每个类只对应一个 CLass 对象，因此同一个 JVM 内的一个类的类变量只需要一块内存空间；但对于实例变量而言，该类每创建一次实例，就需要为该实例变量分配一块内存空间。也就是说，程序中创建了几个实例，实例变量就需要几块内存空间。 这里我想到一道面试题目： 123456789101112public class A&#123; &#123; System.out.println(\"我是代码块\"); &#125; static&#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); &#125;&#125; 结果： 123我是静态代码块我是代码块我是代码块 静态代码块只执行一次，而代码块每创建一个实例，就会打印一次。 实例变量的初始化时机程序可在3个地方对实例变量执行初始化： 定义实例变量时指定初始值 非静态初始化块中对实例变量指定初始值 构造器中对实例变量指定初始值 上面第一种和第二种方式比第三种方式更早执行，但第一、二种方式的执行顺序与他们在源程序中的排列顺序相同。 同样在上面那个代码上加上一个变量 weight 的成员变量，我们来验证下上面的初始化顺序： 1、定义实例变量指定初始值 在 非静态初始化块对实例变量指定初始值 之后: 123456789101112131415public class A&#123; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; double weight = 2.0; static&#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 结果是： 1234我是静态代码块我是代码块我是代码块2.0 2、定义实例变量指定初始值 在 非静态初始化块对实例变量指定初始值 之前: 123456789101112131415public class A&#123; double weight = 2.0; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; static&#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 结果为： 1234我是静态代码块我是代码块我是代码块2.1 大家有没有觉得很奇怪？ 我来好好说清楚下： 定义实例变量时指定的初始值、初始代码块中为实例变量指定初始值的语句的地位是平等的，当经过编译器处理后，他们都将会被提取到构造器中。也就是说，这条语句 double weight = 2.0; 实际上会被分成如下 2 次执行： double weight; : 创建 Java 对象时系统根据该语句为该对象分配内存。 weight = 2.1; : 这条语句将会被提取到 Java 类的构造器中执行。 只说原理，大家肯定不怎么信，那么还有拿出源码来，这样才有信服能力的吗？是不？ 这里我直接使用软件将代码的字节码文件反编译过来，看看里面是怎样的组成？ 第一个代码的反编译源码如下： 1234567891011121314151617181920public class A&#123; double weight; public A() &#123; this.weight = 2.1D; System.out.println(\"我是代码块\"); this.weight = 2.0D; &#125; static &#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 第二个代码反编译源码如下： 1234567891011121314151617181920public class A&#123; double weight; public A() &#123; this.weight = 2.0D; this.weight = 2.1D; System.out.println(\"我是代码块\"); &#125; static &#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 这下子满意了吧！ 通过反编译的源码可以看到该类定义的 weight 实例变量时不再有初始值，为 weight 指定初始值的代码也被提到了构造器中去了，但是我们也可以发现之前规则也是满足的。 他们的赋值语句都被合并到构造器中，在合并过程中，定义的变量语句转换得到的赋值语句，初始代码块中的语句都转换得到的赋值语句，总是位于构造器的所有语句之前，合并后，两种赋值语句的顺序也保持了它们在 Java 源代码中的顺序。 大致过程应该了解了吧？如果还不怎么清楚的，建议还是自己将怎个过程在自己的电脑上操作一遍，毕竟光看不练假把式。 类变量的初始化时机JVM 对每一个 Java 类只初始化一次，因此 Java 程序每运行一次，系统只为类变量分配一次内存空间，执行一次初始化。程序可在两个地方对类变量执行初始化： 定义类变量时指定初始值 静态初始化代码块中对类变量指定初始值 这两种方式的执行顺序与它们在源代码中的排列顺序相同。 还是用上面那个示例，我们在其基础上加个被 static 修饰的变量 height： 1、定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之后： 123456789101112131415161718public class A&#123; double weight = 2.0; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; static&#123; height = 10.1; System.out.println(\"我是静态代码块\"); &#125; static double height = 10.0; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 运行结果： 12345我是静态代码块我是代码块我是代码块2.110.0 2、定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之前： 123456789101112131415161718public class A&#123; static double height = 10.0; double weight = 2.0; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; static&#123; height = 10.1; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 运行结果： 12345我是静态代码块我是代码块我是代码块2.110.1 其运行结果正如我们预料，但是我们还是看看反编译后的代码吧！ 第一种情况下反编译的代码： 1234567891011121314151617181920212223public class A&#123; double weight; public A() &#123; this.weight = 2.0D; this.weight = 2.1D; System.out.println(\"我是代码块\"); &#125; static &#123; System.out.println(\"我是静态代码块\"); &#125; static double height = 10.0D; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 第二种情况下反编译的代码： 123456789101112131415161718192021222324public class A&#123; static double height = 10.0D; double weight; public A() &#123; this.weight = 2.0D; this.weight = 2.1D; System.out.println(\"我是代码块\"); &#125; static &#123; height = 10.1D; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 通过反编译源码，可以看到第一种情况下(定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之后): 我们在 静态初始化代码块中对类变量指定初始值 已经不存在了，只有一个类变量指定的初始值 static double height = 10.0D; , 而在第二种情况下（定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之前）和之前的源代码顺序是一样的，没啥区别。 上面的代码中充分的展示了类变量的两种初始化方式 ：每次运行该程序时，系统会为 A 类执行初始化，先为所有类变量分配内存空间，再按照源代码中的排列顺序执行静态初始代码块中所指定的初始值和定义类变量时所指定的初始值。 父类构造器当创建任何 Java 对象时，程序总会先依次调用每个父类非静态初始化代码块、父类构造器（总是从 Object 开始）执行初始化，最后才调用本类的非静态初始化代码块、构造器执行初始化。 隐式调用和显示调用当调用某个类的构造器来创建 Java 对象时，系统总会先调用父类的非静态初始化代码块进行初始化。这个调用是隐式执行的，而且父类的静态初始化代码块总是会被执行。接着会调用父类的一个或多个构造器执行初始化，这个调用既可以是通过 super 进行显示调用，也可以是隐式调用。 当所有父类的非静态初始代码块、构造器依次调用完成后，系统调用本类的非静态代码块、构造器执行初始化，最后返回本类的实例。至于调用父类的哪个构造器执行初始化，分以下几种情况： 子类构造器执行体的第一行代码使用 super 显式调用父类构造器，系统将根据 super 调用里传入的实参列表来确定调用父类的哪个构造器； 子类构造器执行体的第一行代码使用 this 显式调用本类中的重载构造器，系统将根据 this 调用里传入的实参列表来确定奔雷的另一个构造器（执行本类中另一个构造器时即进入第一种情况）； 子类构造器中既没有 super 调用，也没有 this 调用，系统将会在执行子类构造器之前，隐式调用父类无参构造器。 注：super 和 this 必须在构造器的第一行，且不能同时存在。 推荐一篇博客：Java初始化顺序 文章从无继承和继承两种情况下分析了 Java 初始化的顺序。 Java初始化顺序如图： 访问子类对象的实例变量调用被子类重写的方法父子实例的内存控制继承成员变量和继承方法的区别方法的行为总是表现出它们实际类型的行为；实例变量的值总是表现出声明这些变量所用类型的行为。 内存中的子类实例父、子类的类变量final 修饰符final 可以修饰变量、方法、类。 修饰变量，变量被赋初始值之后，不能够对他在进行修改 修饰方法，不能够被重写 修饰类，不能够被继承 final 修饰的实例变量只能在如下位置指定初始值： 定义 final 实例变量时指定初始值 在非静态代码块中为 final 实例变量指定初始值 在构造器中为 final 实例变量指定初始值 final 修饰的类变量只能在如下位置指定初始值： 定义 final 类变量时指定初始值 在静态代码块中为 final 类变量指定初始值 第 3 课 —— 常见 Java 集合的实现细节Java 集合框架类图： Set 和 MapSet 代表一种集合元素无序、集合元素不可重复的集合，Map 则代表一种由多个 key-value 对组合的集合，Map 集合类似于传统的关联数组。 Set 和 Map 的关系1、Map 集合中的 key 不能重复且没有顺序。将这些 key 组合起来就是一个 Set 集合。所以有一个 Set&lt;k&gt; keySet() 方法来返回所有 key 组成的 Set 集合。 2、Set 也可以转换成 Map。（在 Set 中将 每一对 key 和 value 存放在一起） HashMap 和 HashSetHashSet：系统采用 Hash 算法决定集合元素的存储位置。（基于 HashMap 实现的） HashMap：系统将 value 当成 key 的附属，系统根据 Hash 算法决定 key 的存储位置。 HashSet 的绝大部分方法都是通过调用 HashMap 的方法实现的，因此 HashSet 和 HashMap 两个集合在实现本质上是相同的。 TreeMap 和 TreeSetTreeSet 底层使用 TreeMap 来包含 Set 集合中的所有元素。 TreeMap 采用的是一种“红黑树”的排序二叉树来保存 Map 中每个 Entry —— 每个 Entry 都被当成 “红黑树” 的一个节点对待。 Map 和 ListMap 的 values() 方法不管是 HashMap 还是 TreeMap ，它们的 values() 方法都可以返回其所有 value 组成的 Collection 集合，其实是一个不存储元素的 Collection 集合，当程序遍历 Collection 集合时，实际上就是遍历 Map 对象的 value。 HashMap 和 TreeMap 的 values() 方法并未把 Map 中的 values 重新组合成一个包含元素的集合对象，这样就可以降低系统内存开销。 Map 和 List 的关系底层实现很相似；用法上很相似。 Map 接口提供 get(K key) 方法允许 Map 对象根据 key 来取得 value； List 接口提供了 get(int index) 方法允许 List 对象根据元素索引来取得 value； ArrayList 和 LinkedListList 集合的实现类，主要有 ArrayList 、Vector 和 LinkedList。 ArrayList 是一个可改变大小的数组.当更多的元素加入到 ArrayList 中时, 其大小将会动态地增长. 内部的元素可以直接通过 get 与 set 方法进行访问, 因为 ArrayList 本质上就是一个数组. LinkedList 是一个双链表, 在添加和删除元素时具有比 ArrayList 更好的性能. 但在 get 与 set 方面弱于ArrayList. 当然, 这些对比都是指数据量很大或者操作很频繁的情况下的对比, 如果数据和运算量很小,那么对比将失去意义. Vector 和 ArrayList 类似, 但属于强同步类。如果你的程序本身是线程安全的(thread-safe,没有在多个线程之间共享同一个集合/对象),那么使用 ArrayList 是更好的选择。 Vector 和 ArrayList 在更多元素添加进来时会请求更大的空间。Vector 每次请求其大小的双倍空间，而 ArrayList每次对 size 增长 50%. 而 LinkedList 还实现了 Queue 接口, 该接口比 List 提供了更多的方法,包括 offer(), peek(), poll()等. 注意: 默认情况下 ArrayList 的初始容量非常小, 所以如果可以预估数据量的话, 分配一个较大的初始值属于最佳实践, 这样可以减少调整大小的开销。 ArrayList与LinkedList性能对比 时间复杂度对比如下: LinkedList 更适用于: 没有大规模的随机读取 大量的增加/删除操作 Iterator 迭代器是一个迭代器接口，专门用于迭代各种 Collection 集合，包括 Set 集合和 List 集合。 第 4 课 —— Java 的内存回收Java 引用的种类对象在内存中的状态JVM 垃圾回收机制，是否回收一个对象的标准在于：是否还有引用变量引用该对象？只要有引用变量引用该对象，垃圾回收机制就不会回收它。 Java 语言对对象的引用有： 强引用 软引用 弱引用 虚引用 强引用程序创建一个对象，并把这个对象赋给一个引用变量，这个引用变量就是强引用。当一个对象被一个或者一个以上的强引用变量所引用时，它处于可达状态，它是不会被系统的垃圾回收机制回收。 软引用软引用需要通过 SoftReference 类来实现，当一个对象只具有软引用时，它有可能会被垃圾回收机制回收。对于只有软引用的对象而言，当系统内存空间足够时，它不会被系统回收，程序也可使用该对象；当系统内存空间不足时，系统将会回收它。 弱引用弱引用和软引用有点相似，区别在于弱引用所引用对象的生存期更短。 虚引用虚引用主要用于跟踪对象被垃圾回收的状态，虚引用不能单独使用，虚引用必须和引用队列联合使用。 Java 的内存泄漏ArrayList.java 中的 remove 方法 1234567891011public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue; &#125; 其中 elementData[--size] = null; // clear to let GC do its work 语句是清除数组元素的引用，避免内存的泄漏，如果没有这句的话，那么就是只有两个作用： 修饰 Stack 的属性，也就是将值减 1； 返回索引为 size -1 的值。 垃圾回收机制 跟踪并监控每个 Java 对象，当某个对象处于不可达状态时，回收该对象所占用的内存。 清理内存分配，回收过程中产生的内存碎片。 垃圾回收的基本算法对于一个垃圾回收器的设计算法来说，大概有如下几个设计： 串行回收 和 并行回收 串行回收：不管系统有多少个 CPU，始终使用一个 CPU 来执行垃圾回收操作 并行回收：把整个回收工作拆分成多部分，每个部分由一个 CPU 负责，从而让多个 CPU 并行回收 并发执行 和 应用程序停止 压缩 和 不压缩 和 复制 复制：将堆内分成两个相同的空间，从根开始访问每一个关联的可达对象，将空间A的可达对象全部复制到空间B，然后一次性回收整个空间A。 标记清除：也就是 不压缩 的回收方式。垃圾回收器先从根开始访问所有可达对象，将它们标记为可达状态，然后再遍历一次整个内存区域，把所有没有标记为可达的对象进行回收处理。 标记压缩：这是压缩方式，这种方式充分利用上述两种算法的优点，垃圾回收器先从根开始访问所有可达对象，将他们标记为可达状态，接下来垃圾回收器会将这些活动对象搬迁在一起，这个过程叫做内存压缩，然后垃圾回收机制再次回收那些不可达对象所占用的内存空间，这样就避免了回收产生的内存碎片。 堆内存的分代回收1、Young 代 2、Old 代 3、Permanent 代 内存管理小技巧 尽量使用直接量 使用 StringBuilder 和 StringBuffer 进行字符串拼接 尽早释放无用对象的引用 尽量少用静态变量 避免在经常调用的方法、循环中创建 Java 对象 缓存经常使用的对象 尽量不要使用 finalize 方法 考虑使用 SoftReference 第 5 课 —— 表达式中的陷阱关于字符串的陷阱JVM 对字符串的处理String java = new String(&quot;Java&quot;) 这句创建了两个字符串对象，一个是 “Java” 这个直接量对应的字符串对象，另外一个是 new String() 构造器返回的字符串对象。 Java 程序中创建对象的方法： 通过 new 调用构造器创建 Java 对象 通过 Class 对象的 newInstance() 方法调用构造器创建 Java 对象 通过 Java 的反序列化机制从 IO 流中恢复 Java 对象 通过 Java 对象提供的 clone() 方法复制一个新的 Java 对象 对于字符串以及 Byte、Short、Int、Long、Character、Float、Double 和 Boolean 这些基本类型的包装类 直接量的方式来创建 Java 对象 Integer in = 5； 通过简单的算法表达式，连接运算来创建 Java 对象 String str = “a” + “b”; （如果这个字符串表达式的值在编译时确定下来，那么 JVM 会在编译时计算该字符串变量的值，并让它指向字符串池中对应的字符串。如果这些算法表达式都是字符串直接量、整数直接量，没有变量和方法参与，那么就可以在编译期就可以确定字符串的值；如果使用了变量、调用了方法，那么只有等到运行时才能确定字符串表达式的值；如果字符串连接运算所有的变量都可执行 “宏替换”（使用 final 修饰的变量），那在编译时期也能确定字符串连接表达式的值） 对于 Java 程序的字符直接量，JVM 会使用一个字符串池来保护它们；当第一次使用某个字符串直接量时，JVM 会将它放入字符串池进行缓存。在一般的情况下，字符串池中的字符串对象不会被垃圾回收器回收，当程序再次需要使用该字符串时，无需重新创建一个新的字符串，而是直接让引用变量指向字符串池中已有的字符串。 不可变的字符串String 类是一个不可变类，当一个 String 对象创建完成后，该 String 类里包含的字符序列就被固定下来，以后永远不能修改。 如果程序需要一个字符序列会发生改变的字符串，那么建议使用 StringBuilder （效率比 StringBuffer 高） 字符串比较如果要比较两个字符串是否相同，用 == 进行判断就行，但如果要判断两个字符串所包含的字符序列是否相同，则应该用 String 重写过的 equals() 方法进行比较。 123456789101112131415161718192021222324252627public boolean equals(Object anObject) &#123; //如果两个字符串相同 if (this == anObject) &#123; return true; &#125; //如果anObject是String类型 if (anObject instanceof String) &#123; String anotherString = (String)anObject; //n代表字符串的长度 int n = value.length; //如果两个字符串长度相等 if (n == anotherString.value.length) &#123; //获取当前字符串、anotherString底层封装的字符数组 char v1[] = value; char v2[] = anotherString.value; int i = 0; //逐一比较v1 和 v2数组中的每个字符 while (n-- != 0) &#123; if (v1[i] != v2[i]) return false; i++; &#125; return true; &#125; &#125; return false; &#125; 还可以使用 String 提供的 compareTo() 方法返回两个字符串的大小 123456789101112131415161718public int compareTo(String anotherString) &#123; int len1 = value.length; int len2 = anotherString.value.length; int lim = Math.min(len1, len2); char v1[] = value; char v2[] = anotherString.value; int k = 0; while (k &lt; lim) &#123; char c1 = v1[k]; char c2 = v2[k]; if (c1 != c2) &#123; return c1 - c2; &#125; k++; &#125; return len1 - len2; &#125; 表达式类型的陷阱表达式类型的自动提升 所有 byte、short、char类型将被提升到 int 类型参与运算 整个算术表达式的数据类型自动提升到与表达式中最高等级操作数同样的类型，操作数的等级排列如下：char -&gt; int -&gt; long -&gt;float -&gt; double byte -&gt; short -&gt; int -&gt; long -&gt;float -&gt; double 复合赋值运算符的陷阱Java 语言允许所有的双目运算符和 = 一起结合组成复合赋值运算符，如 +=、-=、*=、/=、%= 、&amp;= 等，复合赋值运算符包含了一个隐式的类型转换。 123//下面这两条语句不等价a = a + 5; //a += 5; //实际上等价于 a = (a的类型) (a + 5); 复合赋值运算符会自动的将它计算的结果值强制转换为其左侧变量的类型。 输入法导致的陷阱注释的字符必须合法转义字符的陷阱 慎用字符的 Unicode 转义形式 中止行注释的转义字符 泛型可能引起的错误原始类型变量的赋值 当程序把一个原始类型的变量赋给一个带有泛型信息的变量时，总是可以通过编译（只是会提示警告信息） 当程序试图访问带泛型声明的集合的集合元素时，编译器总是把集合元素当成泛型类型处理（它并不关心集合里集合元素的实际类型） 当程序试图访问带泛型声明的集合的集合元素时，JVM会遍历每个集合元素自动执行强制转型，如果集合元素的实际类型与集合所带的泛型信息不匹配，运行时将引发 ClassCastException 原始类型带来的擦除当把一个具有泛型信息的对象赋给另一个没有泛型信息的变量时，所有在尖括号之间的类型信息都会丢弃。 创建泛型数组的陷阱Java 中不允许创建泛型数组 正则表达式的陷阱有些符号本身就是正则表达式，我们需要对符号做转义运算。 多线程的陷阱不要调用 run 方法开启线程是用 start() 方法，而不是 run() 方法。 静态的同步方法对于同步代码块而言，程序必须显式为它指定同步监视器；对于同步非静态方法而言，该方法的同步监视器是 this —— 即调用该方法的 Java 对象；对于静态的同步方法而言，该方法的同步监视器不是 this，而是该类本身。 第 6 课 —— 流程控制的陷阱switch 语句陷阱break 语句不要忘记写 switch 的表达式类型： byte short int char enum String （Jdk 1.7 以后有 String） 标签引起的陷阱Java 中的标签通常是和循环中的 break 和 continue 结合使用，让 break 直接终止标签所标识的循环，让 continue 语句忽略标签所标识的循环的剩下语句。 。。 第 7 课 —— 面向对象的陷阱instanceof 运算符的陷阱instanceof 它用于判断前面的对象是否是后面的类或其子类、实现类的实例。如果是返回 true，否则返回 false。 instanceof 运算符前面操作数的编译时类型必须是： 要么与后面的类相同 要么是后面类的父类 要么是后面类型的子类 构造器陷阱构造器是 Java 中每个类都会提供的一个“特殊方法”。构造器负责对 Java 对象执行初始化操作，不管是定义实例变量时指定的初始值，还是在非静态初始化代码块中所做的操作，实际上都会被提取到构造器中执行。 构造器不能声明返回值类型，也不能使用void声明构造器没有返回值。 构造器创建对象吗构造器并不会创建 Java 对象，构造器只是负责执行初始化，在构造器执行之前，Java 对象所需要的内存空间，是由 new 关键字申请出来的。绝大部分时候，程序使用 new 关键字为一个 Java 对象申请空间之后，都需要使用构造器为这个对象执行初始化，但在某些时候，程序创建 Java 对象无需调用构造器，如下： 使用反序列化的方式恢复 Java 对象 使用 clone 方法复制 Java 对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778package com.zhisheng.test;import java.io.*;/** * Created by 10412 on 2017/5/31. */class Wolf implements Serializable&#123; private String name; public Wolf(String name) &#123; System.out.println(\"调用了有参构造方法\"); this.name = name; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Wolf wolf = (Wolf) o; return name != null ? name.equals(wolf.name) : wolf.name == null; &#125; @Override public int hashCode() &#123; return name != null ? name.hashCode() : 0; &#125;&#125;public class SerializableTest&#123; public static void main(String[] args) &#123; Wolf w = new Wolf(\"灰太狼\"); System.out.println(\"对象创建完成\"); Wolf w2 = null; ObjectInputStream ois = null; ObjectOutputStream oos = null; try &#123; //创建输出对象流 oos = new ObjectOutputStream(new FileOutputStream(\"a.bin\")); //创建输入对象流 ois = new ObjectInputStream(new FileInputStream(\"a.bin\")); //序列输出java 对象 oos.writeObject(w); oos.flush(); //反序列化恢复java对象 w2 = (Wolf) ois.readObject(); System.out.println(w); System.out.println(w2); //两个对象的实例变量值完全相等，输出true System.out.println(w.equals(w2)); //两个对象不同，输出false System.out.println(w == w2); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125;finally &#123; if (ois!=null) try &#123; ois.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; if (oos!=null) try &#123; oos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 程序运行结果： 123456调用了有参构造方法对象创建完成com.zhisheng.test.Wolf@1b15382com.zhisheng.test.Wolf@1b15382truefalse 正如结果所示：创建 Wolf 对象时，程序调用了相应的构造器来对该对象执行初始化；当程序通过反序列化机制恢复 Java 对象时，系统无需在调用构造器来进行初始化。通过反序列化恢复出来的 Wolf 对象和原来的 Wolf 对象具有完全相同的实例变量值，但系统会产生两个对象。 无限递归构造器12345678910111213public class ConstrutionTest&#123; ConstrutionTest ct; &#123; ct = new ConstrutionTest(); &#125; public ConstrutionTest() &#123; System.out.println(\"无参构造器\"); &#125; public static void main(String[] args) &#123; ConstrutionTest ct = new ConstrutionTest(); &#125;&#125; 运行结果抛出异常 java.lang.StackOverflowError 因为不管定义实例变量时指定的初始值，还是在非静态初始化代码块中执行的初始化操作，最终都将提取到构造器中执行，因为代码中递归调用了类的构造器，最终导致出现 java.lang.StackOverflowError 异常。 到底调用哪个重载方法1、第一阶段 JVM 将会选取所有可获得并匹配调用的方法或者构造器 2、第二个阶段决定到底要调用哪个方法，此时 JVM 会在第一阶段所选取的方法或者构造器中再次选取最精确匹配的那一个。 1234567891011121314151617public class OverrideTest&#123; public void info(Object obj, int a) &#123; System.out.println(\"obj 参数\" + obj); System.out.println(\"整型参数 \" + a); &#125; public void info(Object[] obj, double a) &#123; System.out.println(\"obj 参数\" + obj); System.out.println(\"整型参数 \" + a); &#125; public static void main(String[] args) &#123; OverrideTest o = new OverrideTest(); o.info(null, 5); &#125;&#125; 报错如下： 12Error:(20, 10) java: 对info的引用不明确 com.zhisheng.test.OverrideTest 中的方法 info(java.lang.Object,int) 和 com.zhisheng.test.OverrideTest 中的方法 info(java.lang.Object[],double) 都匹配 在这种复杂的条件下，JVM 无法判断哪个方法更匹配实际调用，将会导致程序编译错误。 方法重写的陷阱无法重写父类 private 方法。如果子类有一个与父类 private 方法具有相同方法名、相同形参列表、相同返回值类型的方法，依然不是重写，只是子类定义了一个与父类相同的方法。 static 关键字static 可以修饰类中定义的成员：field、方法、内部类、初始化代码块、内部枚举类 静态方法属于类被 static 修饰的成员（field、方法、内部类、初始化块、内部枚举类）属于类本身，而不是单个的 Java 对象。静态方法也是属于类。 第 8 课 —— 异常捕捉的陷阱正确关闭资源的方式 使用 finally 块来保证回收，保证关闭操作总是会被执行 关闭每个资源之前首先保证引用该资源的引用变量不为 null 为每个物理资源单独使用 try .. catch 块关闭资源，保证关闭资源时引发的异常不会影响其他资源的关闭。 finally 块陷阱finally 执行顺序，看我以前写的一篇文章《深度探究Java 中 finally 语句块》。 catch 块用法在 try 块后使用 catch 块来捕获多个异常时，程序应该小心多个 catch 块之间的顺序：捕获父类异常的 catch 块都应该排在捕获子类异常的 catch 块之后（先处理小异常，再处理大异常），否则出现编译错误。 继承得到的异常子类重写父类方法时，不能声明抛出比父类方法类型更多、范围更大的异常。 二叉树性质： 二叉树第 i 层上的节点数目至多为 2 ^(i - 1) (i &gt;= 1) 深度为 k 的二叉树至多有 2 ^ k - 1 个节点 在任何一颗二叉树中，如果其叶子结点的数量为 n0，度为 2 的子节点数量为 n2，则 n0 = n2 + 1 具有 n 个节点的完全二叉树的深度为 log n + 1 (log 的底为 2) 对于一棵有 n 个节点的完全二叉树的节点按层自左向右编号，则对任一编号为 i 的节点有如下性质： 当 i == 1 时，节点 i 是二叉树的根；若 i &gt; 1 时，则节点的父节点是 i/2 当 2i &lt;= n，则节点 i 有左孩子，左孩子的编号是 2i，否则，节点无左孩子，并且是叶子结点 若 2i + 1 &lt;= n ，则节点 i 有右孩子，右孩子的编号是 2i + 1；否则，节点无右孩子。 对于一颗 n 个节点的完全二叉树的节点按层自左向右编号，1 ~ n/2 范围的节点都是有孩子节点的非叶子结点，其余的节点全部都是叶子结点。编号为 n/2 的节点有可能只有左节点，也可能既有左节点，又有右节点。 选择排序直接选择排序需要经过 n - 1 趟比较 第一趟比较：程序将记录定位在第一个数据上，拿第一个数据依次和它后面的每个数据进行比较，如果第一个数据大于后面某个数据，交换它们。。依此类推，经过第一趟比较，这组数据中最小的数据被选出来，它被排在第一位。 第二趟比较：程序将记录定位在第二个数据上，拿第二个数据依次和它后面每个数据进行比较，如果第二个数据大于后面某个数据，交换它们。。依次类推，经过第二趟比较，这组数据中第二小的数据被选出，它排在第二位 。。 按此规则一共进行 n-1 趟比较，这组数据中第 n - 1小（第二大）的数据被选出，被排在第 n -1 位（倒数第一位）；剩下的就是最大的数据，它排在最后。 直接选择排序的优点就是算法简单，容易实现，缺点就是每趟只能确定一个元素，n个数组需要进行 n-1 趟比较。 堆排序 建堆 拿堆的根节点和最后一个节点交换 交换排序冒泡排序第一趟：依次比较0和1，1和2，2和3 … n-2 和 n - 1 索引的元素，如果发现第一个数据大于后一个数据，交换它们，经过第一趟，最大的元素排到了最后。 第二趟：依次比较0和1，1和2，2和3 … n-3 和 n - 2 索引的元素，如果发现第一个数据大于后一个数据，交换它们，经过第二趟，第二大的元素排到了倒数第二位 。。 第 n -1 趟：依次比较0和1元素，如果发现第一个数据大于后一个数据，交换它们，经过第 n - 1 趟，第二小的元素排到了第二位。 快速排序从待排的数据序列中任取一个数据作为分界值，所有比它小的数据元素一律放在左边，所有比他大的元素一律放在右边，这样一趟下来，该序列就分成了两个子序列，接下来对两个子序列进行递归，直到每个子序列只剩一个，排序完成。 插入排序直接插入排序依次将待排序的数据元素按其关键字值的大小插入前面的有序序列。 折半插入排序当第 i - 1 趟需要将第 i 个元素插入前面的 0 ~ i -1 个元素序列中时： 计算 0 ~ i - 1 索引的中间点，也就是用 i 索引处的元素和 （0 + i - 1）/2 索引处的元素进行比较，如果 i 索引处的元素大，就直接在 （（0 + i - 1）/2 ） ~ （i - 1）后半个范围内进行搜索，反之在前半个范围搜索。 重复上面步骤 确定第 i 个元素的插入位置，就将该位置的后面所有元素整体后移一位，然后将第 i 个元素放入该位置。","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"从对象深入分析 Java 中实例变量和类变量的区别","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/java-var/","text":"实例变量 和 类变量局部变量特点：作用时间短，存储在方法的栈内存中 种类： 形参：方法签名中定义的局部变量，由方法调用者负责为其赋值，随方法结束而消亡 方法内的局部变量：方法内定义的局部变量，必须在方法内对其进行显示初始化，从初始化后开始生效，随方法结束而消亡 代码块内的局部变量：在代码块中定义的局部变量，必须在代码块中进行显示初始化，从初始化后开始生效，随代码块结束而消亡 成员变量类体内定义的变量，如果该成员变量没有使用 static 修饰，那该成员变量又被称为非静态变量或实例变量，如果使用 static 修饰，则该成员变量又可被称为静态变量或类变量。 实例变量和类变量的属性使用 static 修饰的成员变量是类变量，属于该类本身，没有使用 static 修饰的成员变量是实例变量，属于该类的实例，在同一个类中，每一个类只对应一个 Class 对象，但每个类可以创建多个对象。 由于同一个 JVM 内的每个类只对应一个 CLass 对象，因此同一个 JVM 内的一个类的类变量只需要一块内存空间；但对于实例变量而言，该类每创建一次实例，就需要为该实例变量分配一块内存空间。也就是说，程序中创建了几个实例，实例变量就需要几块内存空间。 这里我想到一道面试题目： 123456789101112public class A&#123; &#123; System.out.println(\"我是代码块\"); &#125; static&#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); &#125;&#125; 结果： 123我是静态代码块我是代码块我是代码块 静态代码块只执行一次，而代码块每创建一个实例，就会打印一次。 实例变量的初始化时机程序可在3个地方对实例变量执行初始化： 定义实例变量时指定初始值 非静态初始化块中对实例变量指定初始值 构造器中对实例变量指定初始值 上面第一种和第二种方式比第三种方式更早执行，但第一、二种方式的执行顺序与他们在源程序中的排列顺序相同。 同样在上面那个代码上加上一个变量 weight 的成员变量，我们来验证下上面的初始化顺序： 1、定义实例变量指定初始值 在 非静态初始化块对实例变量指定初始值 之后: 123456789101112131415public class A&#123; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; double weight = 2.0; static&#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 结果是： 1234我是静态代码块我是代码块我是代码块2.0 2、定义实例变量指定初始值 在 非静态初始化块对实例变量指定初始值 之前: 123456789101112131415public class A&#123; double weight = 2.0; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; static&#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 结果为： 1234我是静态代码块我是代码块我是代码块2.1 大家有没有觉得很奇怪？ 我来好好说清楚下： 定义实例变量时指定的初始值、初始代码块中为实例变量指定初始值的语句的地位是平等的，当经过编译器处理后，他们都将会被提取到构造器中。也就是说，这条语句 double weight = 2.0; 实际上会被分成如下 2 次执行： double weight; : 创建 Java 对象时系统根据该语句为该对象分配内存。 weight = 2.1; : 这条语句将会被提取到 Java 类的构造器中执行。 只说原理，大家肯定不怎么信，那么还有拿出源码来，这样才有信服能力的吗？是不？ 这里我直接使用软件将代码的字节码文件反编译过来，看看里面是怎样的组成？ 第一个代码的反编译源码如下： 1234567891011121314151617181920public class A&#123; double weight; public A() &#123; this.weight = 2.1D; System.out.println(\"我是代码块\"); this.weight = 2.0D; &#125; static &#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 第二个代码反编译源码如下： 1234567891011121314151617181920public class A&#123; double weight; public A() &#123; this.weight = 2.0D; this.weight = 2.1D; System.out.println(\"我是代码块\"); &#125; static &#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 这下子满意了吧！ 通过反编译的源码可以看到该类定义的 weight 实例变量时不再有初始值，为 weight 指定初始值的代码也被提到了构造器中去了，但是我们也可以发现之前规则也是满足的。 他们的赋值语句都被合并到构造器中，在合并过程中，定义的变量语句转换得到的赋值语句，初始代码块中的语句都转换得到的赋值语句，总是位于构造器的所有语句之前，合并后，两种赋值语句的顺序也保持了它们在 Java 源代码中的顺序。 大致过程应该了解了吧？如果还不怎么清楚的，建议还是自己将怎个过程在自己的电脑上操作一遍，毕竟光看不练假把式。 类变量的初始化时机JVM 对每一个 Java 类只初始化一次，因此 Java 程序每运行一次，系统只为类变量分配一次内存空间，执行一次初始化。程序可在两个地方对类变量执行初始化： 定义类变量时指定初始值 静态初始化代码块中对类变量指定初始值 这两种方式的执行顺序与它们在源代码中的排列顺序相同。 还是用上面那个示例，我们在其基础上加个被 static 修饰的变量 height： 1、定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之后： 123456789101112131415161718public class A&#123; double weight = 2.0; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; static&#123; height = 10.1; System.out.println(\"我是静态代码块\"); &#125; static double height = 10.0; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 运行结果： 12345我是静态代码块我是代码块我是代码块2.110.0 2、定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之前： 123456789101112131415161718public class A&#123; static double height = 10.0; double weight = 2.0; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; static&#123; height = 10.1; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 运行结果： 12345我是静态代码块我是代码块我是代码块2.110.1 其运行结果正如我们预料，但是我们还是看看反编译后的代码吧！ 第一种情况下反编译的代码： 1234567891011121314151617181920212223public class A&#123; double weight; public A() &#123; this.weight = 2.0D; this.weight = 2.1D; System.out.println(\"我是代码块\"); &#125; static &#123; System.out.println(\"我是静态代码块\"); &#125; static double height = 10.0D; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 第二种情况下反编译的代码： 123456789101112131415161718192021222324public class A&#123; static double height = 10.0D; double weight; public A() &#123; this.weight = 2.0D; this.weight = 2.1D; System.out.println(\"我是代码块\"); &#125; static &#123; height = 10.1D; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 通过反编译源码，可以看到第一种情况下(定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之后): 我们在 静态初始化代码块中对类变量指定初始值 已经不存在了，只有一个类变量指定的初始值 static double height = 10.0D; , 而在第二种情况下（定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之前）和之前的源代码顺序是一样的，没啥区别。 上面的代码中充分的展示了类变量的两种初始化方式 ：每次运行该程序时，系统会为 A 类执行初始化，先为所有类变量分配内存空间，再按照源代码中的排列顺序执行静态初始代码块中所指定的初始值和定义类变量时所指定的初始值。","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"记录下自己第一次坐飞机的感受","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/feiji/","text":"前提因为学校的原因，需要在大三的时候出去实训几个月，然后选择的是在昆山，公司规定要在 2017 年 4 月 19 号之前赶到，所以我提前订的机票是 17 号，之所以会订机票，其实很大一部分原因是由于自己之前没有坐过飞机，所以想体验一下坐飞机的感脚（嘿嘿），再加上这是机票的淡季，所以坐飞机的价钱比做高铁还便宜。整个行程：学校 ——&gt; 南昌机场 ——&gt; 上海虹桥机场 ——&gt; 公司 学校出发10 点左右去食堂吃了个早餐，然后打包好自己的行李，由于第一次坐飞机，所以打算提前时间去机场，然后在机场熟悉熟悉环境，不然的话错过飞机，那就 gg 了。大概 11：30 了，就拖着行李出门，在学校门口叫了辆 滴滴车 去机场（因为公交车要坐好久并且去机场的路好烂），叫的滴滴车他走的是高速，很快。 南昌机场不到半小时就到了机场，首先干的事就是取票。 然后将行李箱免费托运（小于20公斤可免费托运，超过的话那就准备好 Money 吧），一个同行的同学就超重了。干完了这些事就没啥事了，然后就坐在凳子上看起了最近很火的电视剧《人民的名义》，的确很好看的，没看过的话，可以去看看。 大概 13：40 时，准备进站了，这里要说的就比较有趣了。我就具体的说下： 一开始排队（人不多），轮到我，把机票和身份证给她，然后对着摄像头拍了个照就进去检查随身携带的物品了。 这里之前我就听说了：说是飞机上不能携带液体，不知道我书包带的洗发液能不能过关？同学和我之前还在一起开玩笑说：1、如果不能带，我就当场一口闷了它。（这也行，下图随意找了个表情包） 2、我免费请大家洗个头 电脑和雨伞而外检查 书包和随身携带物品都拿出来过安检 人扫描（检查的挺细致的，比火车严多了） 结果检查人员告知我：把 洗发液 和 两个螺丝刀 拿出来。（我当场就震惊了，我那么小的螺丝刀都被检查到了，这几乎把我的书包给透视了一个遍） 然后呢，自己只好将违禁物品都拿出来，再次检查就可以通过了。 走进去的时候拍的一张照片, （电梯好长，不过后来在上海机场下车才发现那个也还好了） 进去后，又有一个小的进站室，又在那里等待了会。 又开始检票，检完票后坐公交车去飞机下面（因为飞机场太大，我坐的那架飞机不是靠近站台的） 上飞机后从窗口拍的一张照片 飞机上看见飞机的空姐和空少了。。（空姐很漂亮，嘻嘻，没拍照片） 飞机起飞的时候，机舱里有语音播放叫我们系上安全带，并且关闭手机，所以后面就没照片了。 起飞感觉走了好远，然后突然一加速，飞机就慢慢的向上飞起，整个人重心都是往后倒的。 不一会，飞机就飞的好高了，从窗口望下去，下面的感觉好美，一路上的风景感觉都挺漂亮的，有时还可以看到白云。 大概一个小时后，空姐推着个车，提供给我们点零食（面包、饼干、榨菜）和饮料（橙汁、苹果醋、矿泉水、咖啡，可自选一样，可再加一杯），服务还是挺贴心的，我在这里给中国东方航空点个赞。 飞机中途有时会颠簸，机舱里语音说是遇到空气气流的影响。（大部分时间还是很好的） 说说飞机下降的时候吧，同样，感觉声音很大，耳朵有点受不了。不知道空姐、空少们怎么长期受得了这这声音，长期下去，那估计耳朵都要出毛病的吧？ 上海虹桥机场机场给人的感觉很大，走了那个电梯都走了很远。 就只有一张照片。 然后再去取我的行李箱，有个专门来自南昌的取行李处，行李箱一个的放在传送带上，自己拿自己的行李出去。这里有个疑问，如果行李被别人提走的话，那该咋办，我看出口都没设置检查行李是否和本人的匹配？ 公司公司提前有大巴来接我们，坐上大巴，然后在车上躺了下，就睡着了，直到到达公司。 住宿地址和公司有点远，所以买了辆 死飞自行车，方便出行，自己周末也可以出去玩玩。 总结第一次坐飞机，感觉还是挺好的。流程也不复杂，所以不麻烦。 不过自己要注意一些东西： 不要携带飞机上违禁物品（液体、超过多大的充电宝、刀枪等） 一定要提前动身，以防发生突发事件 注意保管好自己的身份证 最后这是我在我博客上写的第一篇随笔文章，不知道咋写，就随便写。 因为星期六（昨天）出去玩了，所以今天早上起来写下来这篇，有些事还是得赶紧做掉去，不然一直拖，拖着拖着就没有想做的欲望了。 公司周围环境挺好的，水乡之地，有个阳澄湖，盛产大闸蟹，周围的大闸蟹庄，那叫一个字：多。有空的时候我尽量多去逛逛，拍点照片回来。 今后也会多写点随笔，记录生活中一些有趣且值得纪念的点滴。 感谢阅读！！！","tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]},{"title":"通过项目逐步深入了解Mybatis（二）","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/通过项目逐步深入了解Mybatis(二)/","text":"转载请务必注明出处，原创不易！ 相关文章：通过项目逐步深入了解Mybatis&lt;一&gt;本项目全部代码地址：Github-Mybatis Mybatis 解决 jdbc 编程的问题1、 数据库链接创建、释放频繁造成系统资源浪费从而影响系统性能，如果使用数据库链接池可解决此问题。 解决：在SqlMapConfig.xml中配置数据链接池，使用连接池管理数据库链接。 2、 Sql语句写在代码中造成代码不易维护，实际应用sql变化的可能较大，sql变动需要改变java代码。 解决：将Sql语句配置在XXXXmapper.xml文件中与java代码分离。 3、 向sql语句传参数麻烦，因为sql语句的where条件不一定，可能多也可能少，占位符需要和参数一一对应。 解决：Mybatis自动将java对象映射至sql语句，通过statement中的parameterType定义输入参数的类型。 4、 对结果集解析麻烦，sql变化导致解析代码变化，且解析前需要遍历，如果能将数据库记录封装成pojo对象解析比较方便。 解决：Mybatis自动将sql执行结果映射至java对象，通过statement中的resultType定义输出结果的类型。 Mybatis 与 Hibernate 不同Mybatis和hibernate不同，它不完全是一个ORM框架，因为MyBatis需要程序员自己编写Sql语句，不过mybatis可以通过XML或注解方式灵活配置要运行的sql语句，并将java对象和sql语句映射生成最终执行的sql，最后将sql执行的结果再映射生成java对象。 Mybatis学习门槛低，简单易学，程序员直接编写原生态sql，可严格控制sql执行性能，灵活度高，非常适合对关系数据模型要求不高的软件开发，例如互联网软件、企业运营类软件等，因为这类软件需求变化频繁，一但需求变化要求成果输出迅速。但是灵活的前提是mybatis无法做到数据库无关性，如果需要实现支持多种数据库的软件则需要自定义多套sql映射文件，工作量大。 Hibernate对象/关系映射能力强，数据库无关性好，对于关系模型要求高的软件（例如需求固定的定制化软件）如果用hibernate开发可以节省很多代码，提高效率。但是Hibernate的学习门槛高，要精通门槛更高，而且怎么设计O/R映射，在性能和对象模型之间如何权衡，以及怎样用好Hibernate需要具有很强的经验和能力才行。 总之，按照用户的需求在有限的资源环境下只要能做出维护性、扩展性良好的软件架构都是好架构，所以框架只有适合才是最好。 Mybatis 开发 dao两种方法 原始 dao 开发方法（程序需要编写 dao 接口和 dao 实现类）（掌握） Mybatis 的 mapper 接口（相当于 dao 接口）代理开发方法（掌握） 需求将下边的功能实现Dao： 根据用户id查询一个用户信息 根据用户名称模糊查询用户信息列表 添加用户信息 Mybatis 配置文件 SqlMapConfig.xml Sqlsession 的使用范围SqlSession 中封装了对数据库的操作，如：查询、插入、更新、删除等。 通过 SqlSessionFactory 创建 SqlSession，而 SqlSessionFactory 是通过 SqlSessionFactoryBuilder 进行创建。 1、SqlSessionFactoryBuilderSqlSessionFactoryBuilder 用于创建 SqlSessionFacoty，SqlSessionFacoty 一旦创建完成就不需要SqlSessionFactoryBuilder 了，因为 SqlSession 是通过 SqlSessionFactory 生产，所以可以将SqlSessionFactoryBuilder 当成一个工具类使用，最佳使用范围是方法范围即方法体内局部变量。 2、SqlSessionFactorySqlSessionFactory 是一个接口，接口中定义了 openSession 的不同重载方法，SqlSessionFactory 的最佳使用范围是整个应用运行期间，一旦创建后可以重复使用，通常以单例模式管理 SqlSessionFactory。 3、SqlSessionSqlSession 是一个面向用户的接口， sqlSession 中定义了数据库操作，默认使用 DefaultSqlSession 实现类。 执行过程如下： 1）、 加载数据源等配置信息 Environment environment = configuration.getEnvironment(); 2）、 创建数据库链接 3）、 创建事务对象 4）、 创建Executor，SqlSession 所有操作都是通过 Executor 完成，mybatis 源码如下： 12345678910if (ExecutorType.BATCH == executorType) &#123; executor = newBatchExecutor(this, transaction); &#125; elseif (ExecutorType.REUSE == executorType) &#123; executor = new ReuseExecutor(this, transaction); &#125; else &#123; executor = new SimpleExecutor(this, transaction); &#125;if (cacheEnabled) &#123; executor = new CachingExecutor(executor, autoCommit); &#125; 5）、 SqlSession的实现类即 DefaultSqlSession，此对象中对操作数据库实质上用的是 Executor 结论：每个线程都应该有它自己的SqlSession实例。SqlSession的实例不能共享使用，它也是线程不安全的。因此最佳的范围是请求或方法范围(定义局部变量使用)。绝对不能将SqlSession实例的引用放在一个类的静态字段或实例字段中。 打开一个SqlSession；使用完毕就要关闭它。通常把这个关闭操作放到 finally 块中以确保每次都能执行关闭。如下： 123456SqlSession session = sqlSessionFactory.openSession(); try &#123; // do work &#125; finally &#123; session.close();&#125; 原始 Dao 开发方法思路：需要程序员编写 Dao 接口和 Dao 实现类； 需要在 Dao 实现类中注入 SqlsessionFactory ，在方法体内通过 SqlsessionFactory 创建 Sqlsession。 Dao接口1234567891011public interface UserDao //dao接口，用户管理&#123; //根据id查询用户信息 public User findUserById(int id) throws Exception; //添加用户信息 public void addUser(User user) throws Exception; //删除用户信息 public void deleteUser(int id) throws Exception;&#125; Dao 实现类1234567891011121314151617181920212223242526272829303132333435363738394041public class UserDaoImpl implements UserDao //dao接口实现类&#123; //需要在 Dao 实现类中注入 SqlsessionFactory //这里通过构造方法注入 private SqlSessionFactory sqlSessionFactory; public UserDaoImpl(SqlSessionFactory sqlSessionFactory) &#123; this.sqlSessionFactory = sqlSessionFactory; &#125; @Override public User findUserById(int id) throws Exception &#123; //在方法体内通过 SqlsessionFactory 创建 Sqlsession SqlSession sqlSession = sqlSessionFactory.openSession(); User user = sqlSession.selectOne(\"test.findUserById\", id); sqlSession.close(); return user; &#125; @Override public void insertUser(User user) throws Exception &#123; //在方法体内通过 SqlsessionFactory 创建 Sqlsession SqlSession sqlSession = sqlSessionFactory.openSession(); //执行插入的操作 sqlSession.insert(\"test.insetrUser\", user); //提交事务 sqlSession.commit(); //释放资源 sqlSession.close(); &#125; @Override public void deleteUser(int id) throws Exception &#123; //在方法体内通过 SqlsessionFactory 创建 Sqlsession SqlSession sqlSession = sqlSessionFactory.openSession(); sqlSession.delete(\"test.deleteUserById\", id); //提交事务 sqlSession.commit(); sqlSession.close(); &#125;&#125; 测试12345678910111213141516171819202122232425public class UserDaoImplTest&#123; private SqlSessionFactory sqlSessionFactory; //此方法是在 testFindUserById 方法之前执行的 @Before public void setup() throws Exception &#123; //创建sqlSessionFactory //Mybatis 配置文件 String resource = \"SqlMapConfig.xml\"; //得到配置文件流 InputStream inputStream = Resources.getResourceAsStream(resource); //创建会话工厂,传入Mybatis的配置文件信息 sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); &#125; @Test public void testFindUserById() throws Exception &#123; //创建UserDao的对象 UserDao userDao = new UserDaoImpl(sqlSessionFactory); //调用UserDao方法 User user = userDao.findUserById(1); System.out.println(user); &#125;&#125; 通过id查询用户信息测试结果如下：（其他的可以自己在写测试代码，原理类似） 问题原始Dao开发中存在以下问题： Dao方法体存在重复代码：通过 SqlSessionFactory 创建 SqlSession，调用 SqlSession 的数据库操作方法 调用 sqlSession 的数据库操作方法需要指定 statement 的i d，这里存在硬编码，不得于开发维护。 调用 sqlSession 的数据库操作方法时传入的变量，由于 sqlsession 方法使用泛型，即使变量类型传入错误，在编译阶段也不报错，不利于程序员开发。 Mybatis 的 mapper 接口思路程序员需要编写 mapper.xml 映射文件 只需要程序员编写Mapper接口（相当于Dao接口），需遵循一些开发规范，mybatis 可以自动生成 mapper 接口类代理对象。 开发规范： 在 mapper.xml 中 namespace 等于 mapper 接口地址 1&lt;mapper namespace=\"cn.zhisheng.mybatis.mapper.UserMapper\"&gt;&lt;/mapper&gt; 在 xxxmapper.java 接口中的方法名要与 xxxMapper.xml 中 statement 的 id 一致。 在 xxxmapper.java 接口中的输入参数类型要与 xxxMapper.xml 中 statement 的 parameterType 指定的参数类型一致。 在 xxxmapper.java 接口中的返回值类型要与 xxxMapper.xml 中 statement 的 resultType 指定的类型一致。 UserMapper.java 12//根据id查询用户信息 public User findUserById(int id) throws Exception; UserMapper.xml 123&lt;select id=\"findUserById\" parameterType=\"int\" resultType=\"cn.zhisheng.mybatis.po.User\"&gt; select * from user where id = #&#123;1&#125;&lt;/select&gt; 总结：以上的开发规范主要是对下边的代码进行统一的生成： 1234User user = sqlSession.selectOne(\"test.findUserById\", id);sqlSession.insert(\"test.insetrUser\", user);sqlSession.delete(\"test.deleteUserById\", id);List&lt;User&gt; list = sqlSession.selectList(\"test.findUserByName\", username); 测试测试之前记得在 SqlMapConfig.xml 文件中添加加载映射文件 UserMapper.xml： 1&lt;mapper resource=\"mapper/UserMapper.xml\"/&gt; 测试代码： 1234567891011121314151617181920212223242526public class UserMapperTest&#123; private SqlSessionFactory sqlSessionFactory; //此方法是在 testFindUserById 方法之前执行的 @Before public void setup() throws Exception &#123; //创建sqlSessionFactory //Mybatis 配置文件 String resource = \"SqlMapConfig.xml\"; //得到配置文件流 InputStream inputStream = Resources.getResourceAsStream(resource); //创建会话工厂,传入Mybatis的配置文件信息 sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); &#125; @Test public void testFindUserById() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建usermapper对象,mybatis自动生成代理对象 UserMapper userMapper = sqlSession.getMapper(UserMapper.class); //调用UserMapper的方法 User user = userMapper.findUserById(1); System.out.println(user); &#125;&#125; 通过id查询用户信息测试结果如下：（其他的请自己根据上下文写测试代码，或者去看我 Github-Mybatis学习笔记 上看我这个项目的全部代码） 通过姓名查询用户信息： 代理对象内部调用 selectOne 或者 selectList 如果 mapper 方法返回单个 pojo 对象（非集合对象），代理对象内部通过 selectOne 查询数据库 如果 mapper 方法返回集合对象，代理对象内部通过 selectList 查询数据库 mapper接口方法参数只能有一个是否影响系统开发 mapper 接口方法参数只能有一个，系统是否不利于维护？ 系统框架中，dao层的代码是被业务层公用的。 即使 mapper 接口只有一个参数，可以使用包装类型的 pojo 满足不同的业务方法的需求。 注意：持久层方法的参数可以包装类型、map…. ，service方法中不建议使用包装类型。（不利于业务层的可扩展性） SqlMapConfig.xml 文件Mybatis 的全局配置变量，配置内容和顺序如下： properties（属性） settings（全局配置参数） typeAliases（类型别名） typeHandlers（类型处理器） objectFactory（对象工厂） plugins（插件） environments（环境集合属性对象） ​ environment（环境子属性对象） ​ transactionManager（事务管理） ​ dataSource（数据源） mappers（映射器） properties 属性需求：将数据库连接参数单独配置在 db.properties 中，只需要在 SqlMapConfig.xml 中加载该配置文件 db.properties 的属性值。在 SqlMapConfig.xml 中就不需要直接对数据库的连接参数进行硬编码了。方便以后对参数进行统一的管理，其他的xml文件可以引用该 db.properties 。 db.properties 1234jdbc.driver=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://localhost:3306/mybatis_test?characterEncoding=utf-8jdbc.username=rootjdbc.password=root 那么 SqlMapConfig.xml 中的配置变成如下： 12345678910111213141516&lt;!--加载配置文件--&gt; &lt;properties resource=\"db.properties\"&gt;&lt;/properties&gt; &lt;!-- 和spring整合后 environments配置将废除--&gt; &lt;environments default=\"development\"&gt; &lt;environment id=\"development\"&gt; &lt;!-- 使用jdbc事务管理,事务由 Mybatis 控制--&gt; &lt;transactionManager type=\"JDBC\" /&gt; &lt;!-- 数据库连接池--&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"$&#123;jdbc.driver&#125;\" /&gt; &lt;property name=\"url\" value=\"$&#123;jdbc.url&#125;\" /&gt; &lt;property name=\"username\" value=\"$&#123;jdbc.username&#125;\" /&gt; &lt;property name=\"password\" value=\"$&#123;jdbc.password&#125;\" /&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; 配置完成后我们测试一下是否能够和刚才一样的能够成功呢？那么我就先在db.properties中把数据库密码故意改错，看是否是正确的？不出意外的话是会报错的。 注意： MyBatis 将按照下面的顺序来加载属性： 在 properties 元素体内定义的属性首先被读取。 然后会读取 properties 元素中 resource 或 url 加载的属性，它会覆盖已读取的同名属性。 最后读取 parameterType 传递的属性，它会覆盖已读取的同名属性。 因此，通过parameterType传递的属性具有最高优先级，resource或 url 加载的属性次之，最低优先级的是 properties 元素体内定义的属性。 建议： 不要在 properties 元素体内添加任何属性值，只将属性值定义在 db.properties 文件之中。 在 db.properties 文件之中定义的属性名要有一定的特殊性。如 xxx.xxx.xxx settings（全局配置参数）Mybatis 框架在运行时可以调整一些运行参数 比如：开启二级缓存、开启延迟加载。。。 typeAliases（类型别名）需求： 在mapper.xml中，定义很多的statement，statement需要parameterType指定输入参数的类型、需要resultType指定输出结果的映射类型。 如果在指定类型时输入类型全路径，不方便进行开发，可以针对parameterType或resultType指定的类型定义一些别名，在mapper.xml中通过别名定义，方便开发。 Mybatis支持的别名： 别名 映射的类型 _byte byte _long long _short short _int int _integer int _double double _float float _boolean boolean string String byte Byte long Long short Short int Integer integer Integer double Double float Float boolean Boolean date Date decimal BigDecimal bigdecimal BigDecimal 自定义别名： 在 SqlMapConfig.xml 中配置：(设置别名) 1234567&lt;typeAliases&gt; &lt;!-- 单个别名定义 --&gt; &lt;typeAlias alias=\"user\" type=\"cn.zhisheng.mybatis.po.User\"/&gt; &lt;!-- 批量别名定义，扫描整个包下的类，别名为类名（首字母大写或小写都可以） --&gt; &lt;package name=\"cn.zhisheng.mybatis.po\"/&gt; &lt;package name=\"其它包\"/&gt;&lt;/typeAliases&gt; 在 UserMapper.xml 中引用别名：( resultType 为 user ) 123&lt;select id=\"findUserById\" parameterType=\"int\" resultType=\"user\"&gt; select * from user where id = #&#123;id&#125;&lt;/select&gt; 测试结果： typeHandlers（类型处理器）mybatis中通过typeHandlers完成jdbc类型和java类型的转换。 通常情况下，mybatis提供的类型处理器满足日常需要，不需要自定义. mybatis支持类型处理器： 类型处理器 Java类型 JDBC类型 BooleanTypeHandler Boolean，boolean 任何兼容的布尔值 ByteTypeHandler Byte，byte 任何兼容的数字或字节类型 ShortTypeHandler Short，short 任何兼容的数字或短整型 IntegerTypeHandler Integer，int 任何兼容的数字和整型 LongTypeHandler Long，long 任何兼容的数字或长整型 FloatTypeHandler Float，float 任何兼容的数字或单精度浮点型 DoubleTypeHandler Double，double 任何兼容的数字或双精度浮点型 BigDecimalTypeHandler BigDecimal 任何兼容的数字或十进制小数类型 StringTypeHandler String CHAR和VARCHAR类型 ClobTypeHandler String CLOB和LONGVARCHAR类型 NStringTypeHandler String NVARCHAR和NCHAR类型 NClobTypeHandler String NCLOB类型 ByteArrayTypeHandler byte[] 任何兼容的字节流类型 BlobTypeHandler byte[] BLOB和LONGVARBINARY类型 DateTypeHandler Date（java.util） TIMESTAMP类型 DateOnlyTypeHandler Date（java.util） DATE类型 TimeOnlyTypeHandler Date（java.util） TIME类型 SqlTimestampTypeHandler Timestamp（java.sql） TIMESTAMP类型 SqlDateTypeHandler Date（java.sql） DATE类型 SqlTimeTypeHandler Time（java.sql） TIME类型 ObjectTypeHandler 任意 其他或未指定类型 EnumTypeHandler Enumeration类型 VARCHAR-任何兼容的字符串类型，作为代码存储（而不是索引）。 mappers（映射器） 使用相对于类路径的资源，如： 使用完全限定路径如： 使用 mapper 接口类路径 如： 注意：此种方法要求 mapper 接口名称和 mapper 映射文件名称相同，且放在同一个目录中。 注册指定包下的所有mapper接口如：注意：此种方法要求 mapper 接口名称和 mapper 映射文件名称相同，且放在同一个目录中。 Mapper.xml 映射文件Mapper.xml映射文件中定义了操作数据库的sql，每个sql是一个statement，映射文件是mybatis的核心。 输入映射通过 parameterType 指定输入参数的类型，类型可以是简单类型、hashmap、pojo的包装类型。 传递 pojo 包装对象 （重点） 开发中通过pojo传递查询条件 ，查询条件是综合的查询条件，不仅包括用户查询条件还包括其它的查询条件（比如将用户购买商品信息也作为查询条件），这时可以使用包装对象传递输入参数。 定义包装对象 定义包装对象将查询条件(pojo)以类组合的方式包装起来。 UserQueryVo.java 123456789101112131415public class UserQueryVo //用户包装类型&#123; //在这里包装所需要的查询条件 //用户查询条件 private UserCustom userCustom; public UserCustom getUserCustom() &#123; return userCustom; &#125; public void setUserCustom(UserCustom userCustom) &#123; this.userCustom = userCustom; &#125; //还可以包装其他的查询条件，比如订单、商品&#125; UserCustomer.java 1234public class UserCustom extends User //用户的扩展类&#123; //可以扩展用户的信息&#125; UserMapper.xml 文件 1234567&lt;!--用户信息综合查询 #&#123;userCustom.sex&#125; :取出pojo包装对象中的性别值 #&#123;userCustom.username&#125; :取出pojo包装对象中的用户名称 --&gt; &lt;select id=\"findUserList\" parameterType=\"cn.zhisheng.mybatis.po.UserQueryVo\" resultType=\"cn.zhisheng.mybatis.po.UserCustom\"&gt; select * from user where user.sex = #&#123;userCustom.sex&#125; and user.username like '%$&#123;userCustom.username&#125;%' &lt;/select&gt; UserMapper.java 12//用户信息综合查询public List&lt;UserCustom&gt; findUserList(UserQueryVo userQueryVo) throws Exception; 测试代码 1234567891011121314151617//测试用户信息综合查询 @Test public void testFindUserList() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建usermapper对象,mybatis自动生成代理对象 UserMapper userMapper = sqlSession.getMapper(UserMapper.class); //创建包装对象，设置查询条件 UserQueryVo userQueryVo = new UserQueryVo(); UserCustom userCustom = new UserCustom(); userCustom.setSex(\"男\"); userCustom.setUsername(\"张小明\"); userQueryVo.setUserCustom(userCustom); //调用UserMapper的方法 List&lt;UserCustom&gt; list = userMapper.findUserList(userQueryVo); System.out.println(list); &#125; 测试结果 输出映射 resultType 使用 resultType 进行输出映射，只有查询出来的列名和 pojo 中的属性名一致，该列才可以映射成功。 如果查询出来的列名和 pojo 中的属性名全部不一致，没有创建 pojo 对象。 只要查询出来的列名和 pojo 中的属性有一个一致，就会创建 pojo 对象。 输出简单类型需求：用户信息综合查询列表总数，通过查询总数和上边用户综合查询列表才可以实现分页 实现： 1234567&lt;!--用户信息综合查询总数 parameterType:指定输入的类型和findUserList一样 resultType:输出结果类型为 int --&gt; &lt;select id=\"findUserCount\" parameterType=\"cn.zhisheng.mybatis.po.UserQueryVo\" resultType=\"int\"&gt; select count(*) from user where user.sex = #&#123;userCustom.sex&#125; and user.username like '%$&#123;userCustom.username&#125;%' &lt;/select&gt; 12//用户信息综合查询总数 public int findUserCount(UserQueryVo userQueryVo) throws Exception; 12345678910111213141516//测试用户信息综合查询总数 @Test public void testFindUserCount() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建usermapper对象,mybatis自动生成代理对象 UserMapper userMapper = sqlSession.getMapper(UserMapper.class); //创建包装对象，设置查询条件 UserQueryVo userQueryVo = new UserQueryVo(); UserCustom userCustom = new UserCustom(); userCustom.setSex(\"男\"); userCustom.setUsername(\"张小明\"); userQueryVo.setUserCustom(userCustom); //调用UserMapper的方法 System.out.println(userMapper.findUserCount(userQueryVo)); &#125; 注意：查询出来的结果集只有一行且一列，可以使用简单类型进行输出映射。 输出pojo对象和pojo列表 不管是输出的pojo单个对象还是一个列表（list中包括pojo），在mapper.xml中resultType指定的类型是一样的。 在mapper.java指定的方法返回值类型不一样： 1、输出单个pojo对象，方法返回值是单个对象类型 12//根据id查询用户信息 public User findUserById(int id) throws Exception; 2、输出pojo对象list，方法返回值是List 12//根据用户名查询用户信息 public List&lt;User&gt; findUserByUsername(String userName) throws Exception; resultType总结： 输出pojo对象和输出pojo列表在sql中定义的resultType是一样的。 返回单个pojo对象要保证sql查询出来的结果集为单条，内部使用session.selectOne方法调用，mapper接口使用pojo对象作为方法返回值。 返回pojo列表表示查询出来的结果集可能为多条，内部使用session.selectList方法，mapper接口使用List对象作为方法返回值。 resultMap resultType 可以指定 pojo 将查询结果映射为 pojo，但需要 pojo 的属性名和 sql 查询的列名一致方可映射成功。 如果sql查询字段名和pojo的属性名不一致，可以通过resultMap将字段名和属性名作一个对应关系 ，resultMap实质上还需要将查询结果映射到pojo对象中。 resultMap可以实现将查询结果映射为复杂类型的pojo，比如在查询结果映射对象中包括pojo和list实现一对一查询和一对多查询。 使用方法： 1、定义 resultMap 2、使用 resultMap 作为 statement 的输出映射类型 将下面的 sql 使用 User 完成映射 1select id id_, username username_ from user where id = #&#123;value&#125; User 类中属性名和上边查询的列名不一致。 所以需要： 1、定义 resultMap 1234567891011121314151617181920&lt;!--定义 resultMap 将select id id_, username username_ from user where id = #&#123;value&#125; 和User类中的属性做一个映射关系 type: resultMap最终映射的java对象类型 id:对resultMap的唯一标识 --&gt; &lt;resultMap id=\"userResultMap\" type=\"user\"&gt; &lt;!--id表示查询结果中的唯一标识 column：查询出来的列名 property：type指定pojo的属性名 最终resultMap对column和property做一个映射关系（对应关系） --&gt; &lt;id column=\"id_\" property=\"id\"/&gt; &lt;!--result: 对普通结果映射定义 column：查询出来的列名 property：type指定pojo的属性名 最终resultMap对column和property做一个映射关系（对应关系） --&gt; &lt;result column=\"username_\" property=\"username\"/&gt; &lt;/resultMap&gt; 2、使用 resultMap 作为 statement 的输出映射类型 12345&lt;!--使用 resultMap 作为输出映射类型 resultMap=\"userResultMap\":其中的userResultMap就是我们刚才定义的 resultMap 的id值,如果这个resultMap在其他的mapper文件中，前边须加上namespace --&gt; &lt;select id=\"findUserByIdResultMap\" parameterType=\"int\" resultMap=\"userResultMap\"&gt; select id id_, username username_ from user where id = #&#123;value&#125; &lt;/select&gt; 3、UserMapper.java 12//根据id查询用户信息，使用 resultMap 输出public User findUserByIdResultMap(int id) throws Exception; 4、测试 1234567891011//测试根据id查询用户信息，使用 resultMap 输出 @Test public void testFindUserByIdResultMap() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建usermapper对象,mybatis自动生成代理对象 UserMapper userMapper = sqlSession.getMapper(UserMapper.class); //调用UserMapper的方法 User user = userMapper.findUserByIdResultMap(1); System.out.println(user); &#125; 5、测试结果 动态 SQL通过mybatis提供的各种标签方法实现动态拼接sql。 需求： 用户信息综合查询列表和用户信息查询列表总数这两个 statement的定义使用动态sql。 对查询条件进行判断，如果输入的参数不为空才进行查询条件拼接。 UserMapper.xml (findUserList的配置如下，那么findUserCount的也是一样的，这里就不全部写出来了) 1234567891011121314&lt;select id=\"findUserList\" parameterType=\"cn.zhisheng.mybatis.po.UserQueryVo\" resultType=\"cn.zhisheng.mybatis.po.UserCustom\"&gt; select * from user &lt;!--where可以自动的去掉条件中的第一个and--&gt; &lt;where&gt; &lt;if test=\"userCustom != null\"&gt; &lt;if test=\"userCustom.sex != null and userCustom.sex != ''\"&gt; and user.sex = #&#123;userCustom.sex&#125; &lt;/if&gt; &lt;if test=\"userCustom.username != null\"&gt; and user.username like '%$&#123;userCustom.username&#125;%' &lt;/if&gt; &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; 测试代码：因为设置了动态的sql，如果不设置某个值，那么条件就不会拼接在sql上 所以我们就注释掉设置username的语句 1//userCustom.setUsername(\"张小明\"); 测试结果： Sql 片段通过上面的其实看到在 where sql语句中有很多重复代码，我们可以将其抽取出来，组成一个sql片段，其他的statement就可以引用这个sql片段，利于系统的开发。 这里我们就拿上边sql 中的where定义一个sq片段如下： 123456789101112131415&lt;!--sql片段 id:唯一标识 经验：是基于单表来定义sql片段，这样的话sql片段的可重用性才高 一般不包含where --&gt; &lt;sql id=\"query_user_where\"&gt; &lt;if test=\"userCustom != null\"&gt; &lt;if test=\"userCustom.sex != null and userCustom.sex != ''\"&gt; and user.sex = #&#123;userCustom.sex&#125; &lt;/if&gt; &lt;if test=\"userCustom.username != null\"&gt; and user.username like '%$&#123;userCustom.username&#125;%' &lt;/if&gt; &lt;/if&gt; &lt;/sql&gt; 那么我们该怎样引用这个sql片段呢？如下： 12345select * from user &lt;where&gt; &lt;!--refid: 指定sql片段的id，如果是写在其他的mapper文件中，则需要在前面加上namespace--&gt; &lt;include refid=\"query_user_where\"/&gt; &lt;/where&gt; 测试的话还是那样了，就不继续说了，前面已经说了很多了。 foreach向sql传递数组或List，mybatis使用foreach解析 需求： 在用户查询列表和查询总数的statement中增加多个id输入查询。 sql语句如下： 123SELECT * FROM USER WHERE id=1 OR id=10 ORid=16或者SELECT * FROM USER WHERE id IN(1,10,16) 在输入参数类型中添加 List ids 传入多个 id 12345public class UserQueryVo //用户包装类型&#123; //传入多个id private List&lt;Integer&gt; ids;&#125; 修改 UserMapper.xml文件 WHERE id=1 OR id=10 OR id=16 在查询条件中，查询条件定义成一个sql片段，需要修改sql片段。 12345678910111213141516171819202122&lt;if test=\"ids!=null\"&gt; &lt;!-- 使用 foreach遍历传入ids collection：指定输入 对象中集合属性 item：每个遍历生成对象中 open：开始遍历时拼接的串 close：结束遍历时拼接的串 separator：遍历的两个对象中需要拼接的串 --&gt; &lt;!-- 使用实现下边的sql拼接： AND (id=1 OR id=10 OR id=16) --&gt; &lt;foreach collection=\"ids\" item=\"user_id\" open=\"AND (\" close=\")\" separator=\"or\"&gt; &lt;!-- 每个遍历需要拼接的串 --&gt; id=#&#123;user_id&#125; &lt;/foreach&gt; &lt;!-- 实现 “ and id IN(1,10,16)”拼接 --&gt; &lt;!-- &lt;foreach collection=\"ids\" item=\"user_id\" open=\"and id IN(\" close=\")\" separator=\",\"&gt; 每个遍历需要拼接的串 #&#123;user_id&#125; &lt;/foreach&gt; --&gt; &lt;/if&gt; 测试代码： 1234567 //传入多个idList&lt;Integer&gt; ids = new ArrayList&lt;&gt;();ids.add(1);ids.add(10);ids.add(16);//将ids传入statement中userQueryVo.setIds(ids); 期待后续的文章吧！","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://yoursite.com/tags/Mybatis/"},{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://yoursite.com/tags/SpringMVC/"}]},{"title":"关于String s = new String(\"xyz\"); 创建几个对象的问题","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/String-new/","text":"你知道在 java 中除了 8 种基本类型外，其他的都是类对象以及其引用。所以 &quot;xyz &quot;在 java 中它是一个String 对象.对于 string 类对象来说他的对象值是不能修改的，也就是具有不变性。 看：1234String s= &quot;hello &quot;; s= &quot;Java &quot;; String s1= &quot;hello &quot;; String s2=new String( &quot;hello &quot;); 结果如下图： 啊，s 所引用的 string 对象不是被修改了吗？之前所说的不变性，去那里了啊？你别着急，让我告诉你说发生了什么事情：在 jvm 的工作过程中，会创建一片的内存空间专门存入 string 对象。我们把这片内存空间叫做 string 池。 String s = “hello “; 当 jvm 看到 “hello”，在 string 池创建 string 对象存储它，并将他的引用返回给s。s = “java “，当 jvm 看到 “java “，在 string 池创建新的 string 对象存储它，再把新建的 string 对象的引用返回给s。而原先的 “hello”仍然在 string 池内。没有消失，他是不能被修改的。 所以我们仅仅是改变了s的引用，而没有改变他所引用的对象，因为 string 对象的值是不能被修改的。 String s1 = “hello” ; jvm 首先在 string 池内里面看找不找得到字符串 “hello”, 如果找得到,返回他的引用给 s1，否则的话就会创建新的 string 对象，放到 string 池里。这里由于 s = “hello”了,对象已经被引用，所以依据规则 s 和 s1 都是引用同一个对象。所以 s == s1 将返回 true。( == 对于非基本类型，是比较两引用是否引用内存中的同一个对象，这里先不区分 == 和 equals 的区别 ) String s2 = new String( “hello”); jvm 首先在 string 池内里面看找不找得到字符串 “hello”, 如果找得到, 不做任何事情，否则的话就会创建新的 string 对象，放到 string 池里面。由于遇到了 new，还会在内存上（不是 string 池里面）创建 string 对象存储 “hello”，并将内存上的（不是 string 池内的）string 对象返回给 s2。所以 s == s2 将返回 false，不是引用同一个对象。 好现在我们看题目：String s = new String( “xyz “);首先在 string 池内找，找到？不创建 string 对象，否则创建一个对象，遇到 new 运算符号了，在内存上创建 string 对象，并将其返回给 s，又一个对象 所以总共是 1个 或者 2个对象 。 而为什么在网上都说 String s=new String(“hello”); 创建了2个对象？那可能因为它就写这么一句代码，误让人默认的认为执行代码之前并不实例任何一个 String 对象过（也许 很多人不会这么想，），跟着别人或者不经思考的就说2个，斟是说存放在栈内存中专门存放 String 对象引用的 s 变量是一个对象！","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"String","slug":"String","permalink":"http://yoursite.com/tags/String/"}]},{"title":"通过项目逐步深入了解Mybatis（一）","date":"2017-06-11T16:00:00.000Z","path":"2017/06/12/通过项目逐步深入了解Mybatis(一)/","text":"Mybatis 和 SpringMVC 通过订单商品案例驱动 官方中文地址：http://www.mybatis.org/mybatis-3/zh/ 官方托管地址：https://github.com/mybatis/mybatis-3 本项目全部代码地址：https://github.com/zhisheng17/mybatis 如果觉得不错的话，欢迎给个 star ， 如果你想完善这个项目的话，你也可以 fork 后修改然后推送给我。 基础知识：对原生态 jdbc 程序（单独使用 jdbc 开发）问题总结1、环境​ java 环境 ：jdk1.8.0_77 ​ 开发工具 ： IDEA 2016.1 ​ 数据库 ： MySQL 5.7 2、创建数据库​ mybatis_test.sql ​ Tables ：items、orderdetail、orders、user 3、JDBC 程序​ 使用 JDBC 查询 MySQL 数据库中用户表的记录 ​ 代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980package cn.zhisheng.mybatis.jdbc;/** * Created by 10412 on 2016/11/27. */import java.sql.*;/** *通过单独的jdbc程序来总结问题 */public class JdbcTest&#123; public static void main(String[] args) &#123; //数据库连接 Connection connection = null; //预编译的Statement，使用预编译的Statement可以提高数据库性能 PreparedStatement preparedStatement = null; //结果集 ResultSet resultSet = null; try &#123; //加载数据库驱动 Class.forName(\"com.mysql.jdbc.Driver\"); //通过驱动管理类获取数据库链接 connection = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/mybatis_test?characterEncoding=utf-8\", \"root\", \"root\"); //定义sql语句 ?表示占位符（在这里表示username） String sql = \"select * from user where username = ?\"; //获取预处理statement preparedStatement = connection.prepareStatement(sql); //设置参数，第一个参数为sql语句中参数的序号（从1开始），第二个参数为设置的参数值 preparedStatement.setString(1, \"王五\"); //向数据库发出sql执行查询，查询出结果集 resultSet = preparedStatement.executeQuery(); //遍历查询结果集 while(resultSet.next()) &#123; System.out.println(resultSet.getString(\"id\")+\" \"+resultSet.getString(\"username\")); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally&#123; //释放资源 if(resultSet!=null) &#123; try &#123; resultSet.close(); &#125; catch (SQLException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; if(preparedStatement!=null) &#123; try &#123; preparedStatement.close(); &#125; catch (SQLException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; if(connection!=null) &#123; try &#123; connection.close(); &#125; catch (SQLException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 4、问题总结 数据库连接，使用时就创建，不使用立即释放，对数据库频繁连接开启和关闭，造成数据库资源的浪费，影响数据库性能。 解决方法：使用数据库连接池管理数据库连接。 将 sql 语句硬编码到 java 代码中，如果 sql 语句需要修改，那么就需要重新编译 java 代码，不利于系统的维护。 设想：将 sql 语句配置在 xml 配置文件中，即使 sql 语句发生变化，也不需要重新编译 java 代码。 向 preparedStatement 中设置参数，对占位符号位置和设置参数值，硬编码在 java 代码中，同样也不利于系统的维护。 设想：将 sql 语句、占位符、参数值配置在 xml 配置文件中。 从 resultSet 中遍历结果集数据时，存在硬编码，将获取表的字段进行硬编码，不利于系统维护。 设想：将查询的结果集自动映射成 java 对象。 Mybatis框架原理（掌握）1、Mybatis 是什么？​ Mybatis 是一个持久层的架构，是 appach 下的顶级项目。 ​ Mybatis 原先是托管在 googlecode 下，再后来是托管在 Github 上。 ​ Mybatis 让程序员将主要的精力放在 sql 上，通过 Mybatis 提供的映射方式，自由灵活生成（半自动，大部分需要程序员编写 sql ）满足需要 sql 语句。 ​ Mybatis 可以将向 preparedStatement 中的输入参数自动进行输入映射，将查询结果集灵活的映射成 java 对象。（输出映射） 2、Mybatis 框架 注解： SqlMapConfig.xml （Mybatis的全局配置文件，名称不定）配置了数据源、事务等 Mybatis 运行环境 Mapper.xml 映射文件（配置 sql 语句） SqlSessionFactory （会话工厂）根据配置文件配置工厂、创建 SqlSession SqlSession （会话）面向用户的接口、操作数据库（发出 sql 增删改查） Executor （执行器）是一个接口（基本执行器、缓存执行器）、SqlSession 内部通过执行器操作数据库 Mapped Statement （底层封装对象）对操作数据库存储封装，包括 sql 语句、输入参数、输出结果类型 ​ Mybatis入门程序1、需求实现以下功能： 根据用户id查询一个用户信息 根据用户名称模糊查询用户信息列表 添加用户 更新用户 删除用户 2、环境java 环境 ：jdk1.8.0_77 开发工具 ： IDEA 2016.1 数据库 ： MySQL 5.7 Mybatis 运行环境（ jar 包） MySQL 驱动包 其他依赖包 3、 log4j.properties在classpath下创建log4j.properties如下： 1234567# Global logging configuration#在开发环境日志级别要设置为DEBUG、生产环境要设置为INFO或者ERRORlog4j.rootLogger=DEBUG, stdout# Console output...log4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%5p [%t] - %m%n Mybatis默认使用log4j作为输出日志信息。 4、工程结构 5、SqlMapConfig.xml配置 Mybatis 的运行环境、数据源、事务等 1234567891011121314151617181920&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;!-- 和spring整合后 environments配置将废除--&gt; &lt;environments default=\"development\"&gt; &lt;environment id=\"development\"&gt; &lt;!-- 使用jdbc事务管理,事务由 Mybatis 控制--&gt; &lt;transactionManager type=\"JDBC\" /&gt; &lt;!-- 数据库连接池,由Mybatis管理，数据库名是mybatis_test，Mysql用户名root，密码root --&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"com.mysql.jdbc.Driver\" /&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/mybatis_test?characterEncoding=utf-8\" /&gt; &lt;property name=\"username\" value=\"root\" /&gt; &lt;property name=\"password\" value=\"root\" /&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt;&lt;/configuration&gt; 6、创建 po 类Po 类作为 mybatis 进行 sql 映射使用，po 类通常与数据库表对应，User.java 如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package cn.zhisheng.mybatis.po;import java.util.Date;/** * Created by 10412 on 2016/11/28. */public class User&#123; private int id; private String username; // 用户姓名 private String sex; // 性别 private Date birthday; // 生日 private String address; // 地址 //getter and setter public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public Date getBirthday() &#123; return birthday; &#125; public void setBirthday(Date birthday) &#123; this.birthday = birthday; &#125; public String getSex() &#123; return sex; &#125; public void setSex(String sex) &#123; this.sex = sex; &#125; public String getAddress() &#123; return address; &#125; public void setAddress(String address) &#123; this.address = address; &#125;&#125; 7、根据用户 id（主键）查询用户信息 映射文件 User.xml（原在 Ibatis 中命名）在 Mybatis 中命名规则为 xxxmapper.xml 在映射文件中配置 sql 语句 User.xml 123456&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapperPUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\"\"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"test\"&gt;&lt;/mapper&gt; namespace ：命名空间，对 sql 进行分类化管理，用于隔离 sql 语句，后面会讲另一层非常重要的作用。 ​ 在 User.xml 中加入 123456789101112&lt;!--通过select执行数据库查询 id:标识映射文件中的sql 将sql语句封装到mappedStatement对象中，所以id称为Statement的id #&#123;&#125;：表示占位符 #&#123;id&#125;：其中的id表示接收输入的参数，参数名称就是id，如果输入参数是简单类型，那么#&#123;&#125;中的参数名可以任意，可以是value或者其他名称 parameterType：表示指定输入参数的类型 resultType：表示指定sql输出结果的所映射的java对象类型 --&gt;&lt;!-- 根据id获取用户信息 --&gt; &lt;select id=\"findUserById\" parameterType=\"int\" resultType=\"cn.zhisheng.mybatis.po.User\"&gt; select * from user where id = #&#123;id&#125; &lt;/select&gt; User.xml 映射文件已经完全写好了，那接下来就需要在 SqlMapConfig.xml中加载映射文件 User.xml 1234&lt;!--加载映射文件--&gt; &lt;mappers&gt; &lt;mapper resource=\"sqlmap/User.xml\"/&gt; &lt;/mappers&gt; ​ 编写程序 `MybatisFirst.java` ​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445package cn.zhisheng.mybatis.first;import cn.zhisheng.mybatis.po.User;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.Test;import java.io.IOException;import java.io.InputStream; /*** Created by 10412 on 2016/11/28.*/public class MybatisFirst&#123; //根据id查询用户信息，得到用户的一条记录 @Test public void findUserByIdTest() throws IOException &#123; //Mybatis 配置文件 String resource = \"SqlMapConfig.xml\"; //得到配置文件流 InputStream inputStream = Resources.getResourceAsStream(resource); //创建会话工厂,传入Mybatis的配置文件信息 SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); //通过工厂得到SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //通过SqlSession操作数据库 //第一个参数：映射文件中Statement的id，等于 = namespace + \".\" + Statement的id //第二个参数：指定和映射文件中所匹配的parameterType类型的参数 //sqlSession.selectOne 结果与映射文件中所匹配的resultType类型的对象 User user = sqlSession.selectOne(\"test.findUserById\", 1); System.out.println(user); //释放资源 sqlSession.close(); &#125;&#125; 然后运行一下这个测试，发现结果如下就代表可以了： 8、根据用户名称模糊查询用户信息列表 映射文件 依旧使用 User.xml 文件，只不过要在原来的文件中加入 123456789&lt;!-- 自定义条件查询用户列表 resultType：指定就是单条记录所映射的java对象类型 $&#123;&#125;:表示拼接sql串，将接收到的参数内容不加修饰的拼接在sql中 使用$&#123;&#125;拼接sql，会引起sql注入 $&#123;value&#125;：接收输入参数的内容，如果传入类型是简单类型，$&#123;&#125;中只能够使用value--&gt; &lt;select id=\"findUserByUsername\" parameterType=\"java.lang.String\" resultType=\"cn.zhisheng.mybatis.po.User\"&gt; select * from user where username like '%$&#123;value&#125;%' &lt;/select&gt; 编写程序 依旧直接在刚才那个 MybatisFirst.java 中加入测试代码： 12345678910111213141516171819202122232425262728//根据用户名称模糊查询用户信息列表 @Test public void findUserByUsernameTest() throws IOException &#123; //Mybatis 配置文件 String resource = \"SqlMapConfig.xml\"; //得到配置文件流 InputStream inputStream = Resources.getResourceAsStream(resource); //创建会话工厂,传入Mybatis的配置文件信息 SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); //通过工厂得到SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //通过SqlSession操作数据库 //第一个参数：映射文件中Statement的id，等于 = namespace + \".\" + Statement的id //第二个参数：指定和映射文件中所匹配的parameterType类型的参数 //selectList 查询结果可能多条 //list中的user和映射文件中resultType所指定的类型一致 List&lt;User&gt; list = sqlSession.selectList(\"test.findUserByUsername\", \"小明\"); System.out.println(list); //释放资源 sqlSession.close(); &#125; 同样测试一下findUserByUsernameTest ，如果运行结果如下就代表没问题： 提示：通过这个代码可以发现，其中有一部分代码是冗余的，我们可以将其封装成一个函数。 1234567public void createSqlSessionFactory() throws IOException &#123; // 配置文件 String resource = \"SqlMapConfig.xml\"; InputStream inputStream = Resources.getResourceAsStream(resource); // 使用SqlSessionFactoryBuilder从xml配置文件中创建SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); &#125; 注意：1、#{ } 和 ${ } 的区别 #{ }表示一个占位符号，通过#{ }可以实现 preparedStatement 向占位符中设置值，自动进行java 类型和 jdbc 类型转换，#{ } 可以有效防止sql注入。#{ } 可以接收简单类型值或 pojo 属性值（通过 OGNL 读取对象中的值，属性.属性.属性..方式获取对象属性值）。 如果 parameterType 传输单个简单类型值，#{ }括号中可以是 value 或其它名称。 ${ } 表示拼接 sql 串，通过${ }可以将 parameterType 传入的内容拼接在 sql 中且不进行 jdbc 类型转换， ${ }可以接收简单类型值或 pojo 属性值（（通过 OGNL 读取对象中的值，属性.属性.属性..方式获取对象属性值）），如果 parameterType 传输单个简单类型值，${}括号中只能是 value。 2、parameterType 和 resultType 区别 parameterType：指定输入参数类型，mybatis 通过 ognl 从输入对象中获取参数值拼接在 sql 中。 resultType：指定输出结果类型，mybatis 将 sql 查询结果的一行记录数据映射为 resultType 指定类型的对象。 3、selectOne 和 selectList 区别 selectOne 查询一条记录来进行映射，如果使用selectOne查询多条记录则抛出异常： org.apache.ibatis.exceptions.TooManyResultsException: Expected one result (or null) to bereturned by selectOne(), but found: 3 at selectList 可以查询一条或多条记录来进行映射。 9、添加用户 映射文件 在 User.xml 中加入： 12345678&lt;!-- 添加用户 --&gt; &lt;insert id=\"insetrUser\" parameterType=\"cn.zhisheng.mybatis.po.User\" &gt; &lt;selectKey keyProperty=\"id\" order=\"AFTER\" resultType=\"java.lang.Integer\"&gt; select LAST_INSERT_ID() &lt;/selectKey&gt; insert into user(username, birthday, sex, address) values(#&#123;username&#125;, #&#123;birthday&#125;, #&#123;sex&#125;, #&#123;address&#125;) &lt;/insert&gt; 注意: selectKey将主键返回，需要再返回 添加selectKey实现将主键返回 keyProperty:返回的主键存储在pojo中的哪个属性 order：selectKey的执行顺序，是相对与insert语句来说，由于mysql的自增原理执行完insert语句之后才将主键生成，所以这里selectKey的执行顺序为after resultType:返回的主键是什么类型 LAST_INSERT_ID():是mysql的函数，返回auto_increment自增列新记录id值。 然后在 MybatisFirst.java 中写一个测试函数，代码如下 123456789101112131415161718192021@Test public void insetrUser() throws IOException, ParseException &#123; //Mybatis 配置文件 String resource = \"SqlMapConfig.xml\"; //得到配置文件流 InputStream inputStream = Resources.getResourceAsStream(resource); //创建会话工厂,传入Mybatis的配置文件信息 SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); //通过工厂得到SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); User user = new User(); SimpleDateFormat sdf = new SimpleDateFormat (\"yyyy-MM-dd\"); user.setUsername(\"田志声\"); user.setSex(\"男\"); user.setBirthday(sdf.parse(\"2016-11-29\")); user.setAddress(\"江西南昌\"); sqlSession.insert(\"test.insetrUser\", user); sqlSession.commit(); //释放资源 sqlSession.close(); &#125; 然后 run 一下，如果出现的结果如下，那么就是成功了。 同时数据库也能查询到刚插入的用户信息： 10、自增主键返回 与 非自增主键返回 MySQL 自增主键：执行 insert 提交之前自动生成一个自增主键，通过 MySQL 函数获取到刚插入记录的自增主键： LAST_INSERT_ID() ，是在 insert 函数之后调用。 非自增主键返回：使用 MySQL 的 uuid() 函数生成主键，需要修改表中 id 字段类型为 String ，长度设置为 35 位，执行思路：先通过 uuid() 查询到主键，将主键输入到 sql 语句中；执行 uuid() 语句顺序相对于 insert 语句之前执行。 刚才那个插入用户的地方，其实也可以通过 uuid() 来生成主键，如果是这样的话，那么我们就需要在 User.xml 中加入如下代码： 123456789&lt;!--使用 MySQL 的 uuid()生成主键 执行过程： 首先通过uuid()得到主键，将主键设置到user对象的id属性中 其次执行insert时，从user对象中取出id属性值 --&gt;&lt;selectKey keyProperty=\"id\" order=\"BEFORE\" resultType=\"java.lang.String\"&gt; select uuid()&lt;/selectKey&gt;insert into user(id, username, birthday, sex, address) values(#&#123;id&#125;, #&#123;username&#125;, #&#123;birthday&#125;, #&#123;sex&#125;, #&#123;address&#125;) Oracle 使用序列生成主键 首先自定义一个序列且用于生成主键，selectKey使用如下： 12345678&lt;insert id=\"insertUser\" parameterType=\"cn.itcast.mybatis.po.User\"&gt; &lt;selectKey resultType=\"java.lang.Integer\" order=\"BEFORE\" keyProperty=\"id\"&gt; SELECT 自定义序列.NEXTVAL FROM DUAL &lt;/selectKey&gt;insert into user(id,username,birthday,sex,address) values(#&#123;id&#125;,#&#123;username&#125;,#&#123;birthday&#125;,#&#123;sex&#125;,#&#123;address&#125;)&lt;/insert&gt; ​ 11、删除用户前面说了这么多了，这里就简单来说明下： 在 User.xml 文件中加入如下代码： 1234&lt;!--删除用户--&gt; &lt;delete id=\"deleteUserById\" parameterType=\"int\"&gt; delete from user where user.id = #&#123;id&#125; &lt;/delete&gt; 在 MybatisFirst.java 文件中加入如下代码： 1234567891011121314151617181920212223242526272829//删除用户 @Test public void deleteUserByIdTest() throws IOException &#123; //Mybatis 配置文件 String resource = \"SqlMapConfig.xml\"; //得到配置文件流 InputStream inputStream = Resources.getResourceAsStream(resource); //创建会话工厂,传入Mybatis的配置文件信息 SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); //通过工厂得到SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //通过SqlSession操作数据库 //第一个参数：映射文件中Statement的id，等于 = namespace + \".\" + Statement的id //第二个参数：指定和映射文件中所匹配的parameterType类型的参数 sqlSession.delete(\"test.deleteUserById\", 26); //提交事务 sqlSession.commit(); //释放资源 sqlSession.close(); &#125; 测试结果如下： 之前的数据库 user 表查询结果： 执行完测试代码后，结果如下： 12、更新用户信息在 User.xml 中加入如下代码： 123456789&lt;!--根据id更新用户 需要输入用户的id 传入用户要更新的信息 parameterType指定user对象，包括id和更新信息，id必须存在 #&#123;id&#125;：从输入对象中获取id属性值--&gt;&lt;update id=\"updateUserById\" parameterType=\"cn.zhisheng.mybatis.po.User\"&gt; update user set username = #&#123;username&#125;, birthday = #&#123;birthday&#125;, sex = #&#123;sex&#125;, address = #&#123;address&#125; where user.id = #&#123;id&#125; &lt;/update&gt; 然后在 MybatisFirst.java 中加入 12345678910111213141516171819202122232425262728293031323334353637//根据id更新用户信息 @Test public void updateUserByIdTest() throws IOException, ParseException &#123; //Mybatis 配置文件 String resource = \"SqlMapConfig.xml\"; //得到配置文件流 InputStream inputStream = Resources.getResourceAsStream(resource); //创建会话工厂,传入Mybatis的配置文件信息 SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); //通过工厂得到SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //为了设置生日的日期输入 SimpleDateFormat sdf = new SimpleDateFormat (\"yyyy-MM-dd\"); User user = new User(); //根据id更新用户信息 user.setId(24); user.setUsername(\"张四风\"); user.setBirthday(sdf.parse(\"2015-01-12\")); user.setSex(\"女\"); user.setAddress(\"上海黄埔\"); //通过SqlSession操作数据库 //第一个参数：映射文件中Statement的id，等于 = namespace + \".\" + Statement的id //第二个参数：指定和映射文件中所匹配的parameterType类型的参数 sqlSession.update(\"test.updateUserById\", user); //提交事务 sqlSession.commit(); //释放资源 sqlSession.close(); &#125; 测试结果如下： 查看数据库，id 为 24 的用户信息是否更新了： 啊，是不是很爽，所有的需求都完成了。 没错，这只是 Mybatis 的一个简单的入门程序，简单的实现了对数据库的增删改查功能，通过这个我们大概可以了解这个编程方式了。 期待接下来的 Mybatis高级知识文章吧！ 更多文章请见 微信公众号：猿blog","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://yoursite.com/tags/Mybatis/"},{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://yoursite.com/tags/SpringMVC/"}]},{"title":" 六月 —— 愿你做最美好的自己！","date":"2017-06-01T16:00:00.000Z","path":"2017/06/02/poetry2/","text":"美妙的六月已经到来，送你八封信，愿时光轻缓，愿微风正好，愿你做最美好的自己。 第一封 —— 关于压力 鸡蛋，从外打破是食物，从内打破是生命。人生亦是，从外打破是压力，从内打破是成长。如果你等待别人打破你，那么你注定成为别人的食物；如果能让自己从内打破，那么你会发现自己的成长相当于一种重生。 第二封 —— 关于读书 读书是一种充实人生的艺术，没有书的人生就像空心的竹子一样，空洞无物。犹太人让孩子们亲吻涂有蜂蜜的书本，是为了让他们记住：书本是甜的，要让甜蜜充满人生就要读书。读书是一本人生最难得的存折，一点一滴地积累，最后你会发现：自己是世界上最富有的人。第三封 —— 关于人际关系 你可以要求自己守信，但不能要求别人守信；你可以要求自己对人好，但不能期待别人对你好。你怎样对人，并不代表人家就会怎么对你。如果看不透这一点，你只会徒添不必要的烦恼。 第四封 —— 关于孤独 每个人都要经历一段孤独的日子，每段路都有一段独孤的时光。父母不可能一直帮着你，朋友也不可能一直围着你转。孤独不是孤僻，更不是寂寞。经历过孤独的人，内心更坚强，不管处于什么样的环境都能让自己安静，更好地调整状态，面对环境。 第五封 —— 关于修养 看别人不顺眼，是自己修养不够。人愤怒的那一瞬间，智商是零，过一分钟后恢复正常。人的优雅关键在于控制自己的惰绪，用嘴伤害人，是最愚蠢的一种行为。 第六封 —— 关于现实 现实有太多的不如意，就算生活给你的是垃圾，你同样能把垃圾踩在脚底下登上世界之巅。你要把自己逼出最大的潜能，没有人会为你的未来买单，你要么努力向上爬，要么烂在社会最底层的泥里，这就是生活。 第七封 —— 关于自己 一个人经过不同程度的鍛炼，就获得不同程度的修养。好比香料，捣得愈碎，磨得愈细，香得愈浓烈。我们曾如此渴望命运的波澜，到最后才发现：人生最曼妙的风景，竟是内心的淡定与从容。我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系。 第八封 —— 关于幸福 常有人说，我现在不幸福，等我结了婚或买了房……就幸福了。事实是，幸福的人在哪儿都幸福，不幸福的人在哪儿都不幸福。所以要先培养自己的幸福力，不论发生什么，别人都动不了你的自在开心。这才是真正强大的气场和自信。 幸福的人生，需要三种姿态：对过去，要淡；对现在，要惜；对未来，要信。愿你拥有幸福的能力，做最美好的自己。","tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]},{"title":"最近很火的鸡汤，分享给大家","date":"2017-05-11T16:00:00.000Z","path":"2017/05/12/poetry/","text":"&lt;最近很火的一首小诗，分享给大家&gt; 纽约时间比加州时间早三个小时， New York is 3 hours ahead of California, 但加州时间并没有变慢。 but it does not make California slow. 有人22岁就毕业了， Someone graduated at the age of 22, 但等了五年才找到好的工作！ but waited 5 years before securing a good job! 有人25岁就当上CEO， Someone became a CEO at 25, 却在50岁去世。 and died at 50. 也有人迟到50岁才当上CEO， While another became a CEO at 50, 然后活到90岁。 and lived to 90 years. 有人依然单身， Someone is still single, 同时也有人已婚。 while someone else got married. 奥巴马55岁就退休， Obama retires at 55, 川普70岁才开始当总统。 but Trump starts at 70. 世上每个人本来就有自己的发展时区。 Absolutely everyone in this world works based on their Time Zone. 身边有些人看似走在你前面， People around you might seem to go ahead of you, 也有人看似走在你后面。 some might seem to be behind you. 但其实每个人在自己的时区有自己的步程。 But everyone is running their own RACE, in their own TIME. 不用嫉妒或嘲笑他们。 Don’t envy them or mock them. 他们都在自己的时区里，你也是！ They are in their TIME ZONE, and you are in yours! 生命就是等待正确的行动时机。 Life is about waiting for the right moment to act. 所以，放轻松。 So, RELAX. 你没有落后。 You’re not LATE. 你没有领先。 You’re not EARLY. 在命运为你安排的属于自己的时区里，一切都准时。 You are very much ON TIME, and in your TIME ZONE Destiny set up for you.","tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"}]},{"title":"Github pages + Hexo 博客 yilia 主题使用畅言评论系统","date":"2017-04-12T16:00:00.000Z","path":"2017/04/13/Hexo-yilia-changyan/","text":"前言Hexo的Yilia主题由于原来使用的是多说的留言板，近期多说公告要停止提供服务了，所以我就把多说换成搜狐的畅言了，下面写一个简单的小教程。 注册畅言进入畅言官网 , 点击右上角 “免费注册”，并填写注册信息。（注意域名需要备案信息） 登录并进入畅言后台注册完后，登录进入畅言官网，获取你的畅言 app id 和 app key。 使用畅言系统下面说下修改评论为畅言的方法，其实方法和多说是差不多的。 1、修改 themes\\yilia\\layout\\_partial\\article.ejs 模板，把如下代码 1234567&lt;% if (!index &amp;&amp; post.comments &amp;&amp; config.disqus_shortname)&#123; %&gt; &lt;section id=\"comments\"&gt; &lt;div id=\"disqus_thread\"&gt; 这里还有很多代码 &lt;/div&gt; &lt;/section&gt; &lt;% &#125; %&gt; 修改为： 1234567891011121314151617181920 &lt;% if (!index &amp;&amp; post.comments)&#123; %&gt; &lt;section id=&quot;comments&quot;&gt;&lt;!--高速版，加载速度快，使用前需测试页面的兼容性--&gt;&lt;div id=&quot;SOHUCS&quot; sid=&quot;&lt;%= page.title %&gt;&quot;&gt;&lt;/div&gt;&lt;script&gt; (function()&#123; var appid = &apos;你的APP ID&apos;, conf = &apos;你的APP KEY&apos;; var doc = document, s = doc.createElement(&apos;script&apos;), h = doc.getElementsByTagName(&apos;head&apos;)[0] || doc.head || doc.documentElement; s.type = &apos;text/javascript&apos;; s.charset = &apos;utf-8&apos;; s.src = &apos;http://assets.changyan.sohu.com/upload/changyan.js?conf=&apos;+ conf +&apos;&amp;appid=&apos; + appid; h.insertBefore(s,h.firstChild); window.SCS_NO_IFRAME = true; &#125;)()&lt;/script&gt; &lt;/section&gt; &lt;% &#125; %&gt; 上面的APP ID和APP KEY是在畅言设置中得到。 这里需要注意一点的是：sid=&quot;&lt;%= page.title %&gt;&quot;&gt; 这样的话畅言就可以直接根据对应的文章来识别，使得文章有对应的评论，不会都乱在一起。 2、在每篇文章开头的 front-matter 中添加一句comments: true，然后回到博客根目录执行命令 hexo d -g ，重新生成博客并部署博客，然后刷新，任选一篇文章进入下拉，会发现评论功能可以使用了。 修改 BUG但是，这是你会发现一个 Bug，表情按钮点击不了，原因是被左侧的 div 层覆盖了，回到我们刚才改过的代码，找到 &lt;div id=&quot;SOHUCS&quot; 开头的一串代码。并做如下更改 1&lt;div id=\"SOHUCS\" sid=\"&lt;%=title %&gt;\" style=\"padding: 0px 30px 0px 46px;\"&gt;&lt;/div&gt; 加上上面这一段样式代码，即可修复。 参考文章： 1、Hexo博客yilia主题更换畅言评论系统 2、在Hexo中使用畅言评论系统 新增由于问题太多了，所以新写了篇文章：Github page + Hexo + yilia 搭建博客可能会遇到的所有疑问","tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"},{"name":"yilia","slug":"yilia","permalink":"http://yoursite.com/tags/yilia/"}]}]